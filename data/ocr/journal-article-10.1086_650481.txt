<body xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:mml="http://www.w3.org/1998/Math/MathML"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
      <sec id="sc1">
         <title>Introduction</title>
         <p>This paper is divided into two sections. In the first section, I use the semiotic theory of philosopher Charles Sanders Peirce (1978 [1931]) as a framework for evaluating the emergence of symbolism in the hominin archaeological record. In the second section, I attempt to identify the important selective factors giving rise to symbolism in hominins and the role that enhanced working memory may have played in this evolution. Overall, the following conclusions are drawn. (1) It is unclear whether iconic or indexical artifacts are primary in the archaeological record. (2) Both iconic and indexical artifacts are present sometime between 500,000 and 100,000 years before present (ybp) in the form of pigment use, imposed form on tools, composite tools, and beads. (3) True symbols (in the Peircian sense) emerge very late in the form of decorative items, abstract figurines, and cave art. (4) Adult symbol learning may be prohibitively challenging, thus requiring that symbol acquisition be carried out early in development when certain cognitive limitations actually make its acquisition more feasible. (5) The emergence of iconic and indexical artifacts roughly corresponds with an enlargement of the hominin brain, suggesting that the increased immaturity of hominin offspring facilitated this cognitive advance. (6) An enhancement of working‐memory capacity was probably not essential in establishing symbolic function in hominins, but it may have been critical in permitting <italic>Homo sapiens</italic> to acquire highly complex symbol systems (such as modern language). (7) Social factors such as a secure and stable domestic environment and increasingly demanding social rituals were key selection factors in the enhancement of working‐memory capacity.</p>
      </sec>
      <sec id="sc2">
         <title>Peircian Semiotics</title>
         <p>One of the most thorough examinations of semiotics (the study of signs and symbols) comes from philosopher C. S. Peirce (1978 [1931]). Peirce defined three levels of reference: iconic, indexical, and symbolic. Iconic signs are ones that bear a perceptual or physical resemblance to the things they signify, such as a round pebble being used to represent a soccer ball. Indexical signs are ones that indicate (as the name implies) the presence of what they signify based on a temporal or spatial association. For example, a weather vane indicates the wind (when the wind blows, the vane moves), tears indicate sadness, smoke indicates fire, and so on.</p>
         <p>While both iconic and indexical signs can be thought of as “symbolic” in the sense that one thing is standing for another, Peirce reserves the term “symbol” for only those occasions where the relationship between the signifier and the signified is purely arbitrary. For example, the dollar sign ($) is truly symbolic because its relationship to money is based solely on convention. The dollar sign does not resemble real currency (and therefore is not connected iconically to it), and it occurs only rarely in temporal or spatial proximity to currency (and therefore is not connected indexically to it). While these levels of reference are distinguishable, they are not necessarily mutually exclusive. A sign can sometimes overlap levels of reference. A smiley face looks like a smiling face but can also indicate happiness or the intention of conveying a joke (as is the case with electronic messages).</p>
         <sec id="sc2.1">
            <title>Cognitive Requirements of Referential Thinking</title>
            <p>Peirce (1978 [1931]) observed that the evolution of written language appeared to progress from icons to more purely symbolic representations. This led him to argue that interpreting signs progresses hierarchically. To interpret indexes, one must understand icons. To interpret symbols, one must understand indexes. In this way, higher levels of reference (symbols, indexes) are built up from lower levels (indexes, icons). For example, understanding that smoke serves as an index of (indicator of) fire requires that one recognize (iconically) how a current experience of smoke is related to past experiences of it, very likely based on visual and olfactory resemblance cues. Furthermore, one must then associate this iconically based set of common experiences with the consistent presence of another stimulus: fire. Thus, iconic relationships cue a set of past experiences that then lead to an indexical association (“Gee, every time I see and smell X, Y is there, too”).</p>
            <p>In a similar fashion, symbolic relationships are built on indexical ones. The temporal/spatial relationships that form the basis of indexes provide the scaffolding or foundation on which purely conventional associations can be constructed. Thus, in children’s word learning, concrete nouns (“dog,” “ball,” “mommy”) typically compose the largest single category of their initial vocabulary (Nelson 1973). Very quickly, though, it is not so much what words refer to (point to or indicate) in the real world that drives vocabulary learning but how they relate grammatically and semantically to other words (Taylor and Gelman 1988, 1989). Or, put another way, while word learning may get started indexically (based on a word‐object association), it does not really take off until it switches to a symbol‐symbol‐based reference system.</p>
            <p>All of this raises the possibility that the human mind, with its full‐blown capacity for (Peircian) symbolism, was preceded by hominin minds capable of less sophisticated forms of referential thinking (iconic and indexical). There is reason to suspect that this might be the case. Studies of ape language learning and language development in humans show that in both cases there is progression from an initial indexical phase to one of symbolic understanding (see discussion in Deacon 1997:85–92, 135–142). Thus, we might expect that the emergence of fully symbolic artifacts in the archaeological record would be preceded by iconic and indexical ones, providing evidence for the existence of these presymbolic hominin minds.</p>
         </sec>
         <sec id="sc2.2">
            <title>Crossing the Symbolic Threshold</title>
            <p>The purely arbitrary nature of symbolic associations makes them especially taxing to acquire. Neuroscience studies show that a broad network of neocortical structures is necessary for the acquisition of the purely arbitrary associations required when learning symbols (Asaad, Rainer, and Miller 1998; Chee et al. 2000; Eacott and Gaffan 1992). These structures include the inferior temporal cortex, the dorsolateral prefrontal cortex, and the uncinate fascicles connecting the two. Furthermore, unless one is a human child, acquiring and using symbols is laborious and unnatural. Ape language studies have typically required hundreds, sometimes thousands, of repetitive trials in order to train subjects to use languagelike symbols (Greenfield and Savage‐Rumbaugh 1990; Savage‐Rumbaugh, Rumbaugh, and Boysen 1978). In one well‐known study, two chimpanzee subjects—Austin and Sherman—had to be explicitly trained in what their linguistic symbols (lexigrams) <italic>did not</italic> refer to in order to make their language learning feasible.</p>
            <p>There are two major challenges that must be overcome in acquiring symbols. First, unlike icons or indexes, symbol learning has little to no contextual support. An icon, by virtue of its appearance, cues its referent. An index also cues its referent by virtue of repeated temporal or spatial co‐occurrence. Symbols, however, neither look like nor are temporally or spatially associated with their referent. Indeed, with some symbols, such as the word “angel” or the good‐luck horseshoe above my door, no tangible referent is even possible. Thus, unlike icons or indexes, symbols provide little or no cuing support for their referents.</p>
            <p>Second, what a symbol refers to is only partially based on the (already arbitrary and uncued) symbol‐referent connection. Often, an even more important determinant for a symbol’s referent is the complex relationship the symbol has to other symbols in the context of its use. For example, in the context of “the boy who cried wolf,” the word “wolf” refers to a dangerous member of the canine family. In the context of hearing my wife’s assessment of her sister’s latest boyfriend, “He’s a wolf in sheep’s clothing,” “wolf” refers to a slick operator, a pretending opportunist looking for an easy victim. With symbols, it is often how the symbol relates to other symbols that determines its referent. This property does not apply just to linguistic symbols. In the context of a low‐slung tool belt, a cheap cigar, and a worn‐out New York Yankees cap, the dollar sign most likely means “cash only.” In the context of classical music and caviar, it most likely means “American Express Platinum.”</p>
            <p>There is, however, an important caveat to the apparent difficulties of symbol learning. It could be that these difficulties make it especially hard for <italic>adults</italic> to acquire symbols, so much so that any potential adaptive advantage would be lost by the amount of effort and energy required to achieve the skill. In contrast to those of adults, children’s immature brains may actually have an advantage in acquiring symbols by virtue of their inability to keep track of the many possible object‐to‐symbol indexical relationships. This capacity limitation produces a strong preference for processing more global, higher‐order, symbol‐to‐symbol relations at the expense gaining a complete grasp of the symbol’s external referent. In this way, very young children may naturally fall into a learning strategy advantageous for acquiring symbol systems such as language (Elman 1993, 1999; Newport 1990).</p>
            <p>For example, many very young children understand where and how to use the word “please” before they fully grasp its meaning. They recognize that it fits in with other (linguistic) symbols (such as those that request things) under certain conditions (such as when communicating with adults). Focusing exclusively on how it works within the larger system not only is practical (it gets you things) but also may be necessary, given the cognitive limitations of young children’s minds. This “advantageous limitation” may help to explain why human language learning is characterized by a critical period in early childhood and why Kanzi—the most celebrated ape language learner—acquired his linguistic skills so effortlessly relative to other (generally older) ape subjects.</p>
         </sec>
         <sec id="sc2.3">
            <title>Iconic Artifacts</title>
            <p>Given the hierarchical nature of referential cognition, we might expect that evidence of iconic artifacts is primary in the archaeological record, followed by indexes and symbols. Indeed, some researchers have argued for just such a sequence (see, e.g., Bednarik 2003 and comment by Bowyer [2003]). There is some indication that this might be true. Evidence of red ochre and other mineral pigments dates as far back as 300,000 ybp in the hominin archaeological record (Barham 2002; Clark and Brown 2001). Barham (2002), for example, has unearthed more than 300 pieces of variously colored mineral pigments dated anywhere from 270,000 to 170,000 ybp from the Twin Rivers site in Zambia. The pigments appear to have been intentionally transported to the site from remote locations and show evidence of deliberate modification. Although red ochre and other mineral pigments can have utilitarian uses (Bahn and Vertut 1988; Keeley 1980), evidence from many of these finds suggests ritual rather than practical use (Knight, Power, and Watts 1995; Watts 2002). Barham (2002:187) notes that the wide variety of different pigments found at the Twin Rivers site argues against purely practical use.</p>
            <p>If mineral pigments had ritual meaning, what was it? Our hominin ancestors’ earliest use of pigments, according to Kuhn and Stiner (2007<italic>b</italic>:46), was probably to enhance or alter physical appearance—making the wearer more dramatic, impressive, or intimidating—as is not uncommon in contemporary traditional societies. Using archaeological and anthropological data, Knight, Power, and Watts (1995) have proposed a somewhat different function for early pigments, especially red ochre. In their view, red ochre would have signified menstrual blood and the fertility associated with it. Common to both these views is the notion that the pigment’s meaning would have been closely tied to perceptual appearance—either the appearance of the pigment itself (as blood) or the appearance of what it enhances (facial expressions, bodily contours, etc.). All of this suggests that where ritual use of mineral pigments appears likely from the archaeological record, its reference value was iconic in nature. In other words, it stood for things by virtue of perceptual cuing.</p>
            <p>Pigment use may not be the only example of early iconicity in the archaeological record. Two Late‐Lower/Early‐Middle Paleolithic artifacts—the Berekhat Ram and Tan‐Tan figurines—are noteworthy in that they appear to have been intentionally worked by hominins to enhance their natural anthropomorphic quality. The Berekhat Ram figurine is from the Golan Heights and dates to around 233,000 ybp, while the Tan‐Tan artifact was found by the Draa River near the Algerian town of Tan‐Tan and is dated to around 400,000 ybp (Bednarik 2003; d’Errico and Nowell 2000). The singularity of these finds warrants caution in their interpretation; however, if there is any meaning to them at all, it would likely be iconic.</p>
         </sec>
         <sec id="sc2.4">
            <title>Indexical Artifacts</title>
            <p>The primacy of iconicity in the archaeological record is challenged somewhat by stone tools. The earliest tools emerge well before any iconic artifacts and, according to Byers (1994), may represent natural indexes of their uses. In other words, a sharp edge indicates cutting, an arrowhead indicates hunting, and so on. Their indexical quality stands independent of whether or not one can firmly establish the toolmaker’s intentions in creating the tool. Thus, over time, one edge may end up being useful only for scraping while another may end up being useful only for slicing—these uses being primarily determined by the stone itself and its pattern of wear, not by what the toolmaker had in mind when he or she originally knapped it. So even though the toolmaker may never have set out to create “scrapers” and “slicers,” the “scraper” edge indicates scraping, and the “slicer” edge indicates slicing.</p>
            <p>This logic, however, is not universally accepted. One problem is that it is partially based on the Gibsonian theoretical notion of <italic>affordances</italic>, in which an object’s uses are thought to be directly perceivable to the observer. However, the extent to which the Gibsonian approach can be successfully applied to hominin tool use is unclear (see, e.g., Davidson and Noble 1989). Moreover, it is doubtful that early hominins possessed a conceptual understanding of simple Oldowan edges as “tools” (as opposed to merely bodily extensions for achieving certain ends). In all likelihood, the conceptual category “tool” did not emerge until the advent of the Achealean handaxe (see Coolidge and Wynn 2009:112).</p>
            <p>Less controversial is the idea that an imposed form on a tool or the style of a tool can be used as evidence of the tool’s indexical nature. Chase (1991), following Sackett (1982, 1986), argues that where we can identify style in artifacts—that is, a consistent material patterning that cannot be attributed to the constraints of raw materials or artifact function—then that style serves as an index of the people who made the artifacts. Borrowing Chase’s (1991:198) example, if one group traditionally creates stools with three legs and another with four, then the number of legs on the remains of stools serves as a reliable indicator of a particular group.</p>
            <p>However, identifying imposed form or style can be complicated. For example, some have argued that late Acheulean handaxes (500,000 ybp) show a degree of symmetry and attention to form indicative of the toolmaker having followed a mental image of the desired finished product (Mithen 2006:188; Wynn 2002). Additionally, Schick and Toth (1993:282–283) contend that by around 300,000 ybp, regionally variant handaxe styles had emerged. Others, however, contend that the final form of a handaxe is more parsimoniously explained as the result of resource availability and multiple resharpenings over the course of the tool’s history (Jones 1979; McPherron 2000).</p>
            <p>While raw material and functional constraints may explain some handaxes, contemporary studies of handaxe manufacture suggest that the presence of imposed form or style is not entirely illusory. A well‐shaped handaxe is not an easy tool to make. Stephen Edwards, an experienced stone knapper, claims that many months of concerted effort would be required for one to reach a skill level comparable to late Acheulean handaxe makers (Edwards 2001:606). Additionally, Oakley (1981) describes two handaxes from England in which the stones were knapped so as to center fossilized shells preserved in them. The care and skill required to do this indicate that the toolmaker was well aware of the visual effect he or she was creating. All of this supports the hypothesis that at least in some cases, the imposition of form or style on a handaxe was not accidental. Instead, the handaxe was being appreciated as a social signal of the maker’s skill and/or cultural identity. Kohn and Mithen (1999) have argued that handaxes may have served an important function in mate selection as an indicator of planning ability, motor skill, resourcefulness, aesthetic appreciation, and overall good genes.</p>
            <p>Around this same time (300,000 ybp), the first evidence of composite tools also emerges (Ambrose 2001; McBrearty and Tryon 2006). As both Wadley (2010, in this issue; Wadley, Hodgskiss, and Grant 2009) and Ambrose (2001, 2010, in this issue) have pointed out, composite tools make special demands on the cognitive system. Composite tools are those in which multiple components (such as a point hafted to a handle or shaft using some binding material or adhesive) are assembled using a multistage process sometimes extending over hours or days. The assembly process requires planning a sequence of motor actions and monitoring the progress of each stage (Haidle 2010, in this issue). A distinct advantage would have gone to those toolmakers who during the course of the assembly process could read signs indicating the future viability of the tool. The viscosity of the adhesive, the feel of the point’s fit to the shaft, and the sturdiness of the binding material may all have provided key indicators of the progress of the ongoing tool‐construction process. If properly understood, these signs may have allowed for critical adjustments in the process leading to a better tool. Note how all these signs would have been in reference to a distant event—the future state of the tool.</p>
            <p>Finally, beads and body adornments very likely qualify as another form of indexical artifact. In traditional (and even modern) societies, the way people ornament themselves is often an indicator of their social affiliations, such as age cohort, ethnic group, and marital and/or economic status (Kuhn and Stiner 2007<italic>a</italic>). Recent finds push the emergence of beads to well into the African Middle Stone Age. Intentionally perforated shell beads dated to over 75,000 ybp have been unearthed from Blombos Cave in South Africa (Henshilwood et al. 2004), and even older shell beads (100,000–135,000 ybp) were found among artifacts uncovered from the Skhul site in Israel and Oued Djebbana in Algeria (Vanhaeren et al. 2006). Among contemporary !Kung hunter‐gatherers, shell beads like these are often used as gifts, called <italic>hxaro</italic>, that reinforce reciprocal relations among different bands (Wiessner 1982). The fact that the Skhul and Oued Djebbana sites are remote from the seashore origin of these shells supports the notion that they were transported there, possible by trade and/or gift‐giving networks.</p>
            <p>What made the shells valuable, however, was what they represented when worn. Recently, Kuhn and Stiner (2007<italic>a</italic>) have argued that beads represent an important transition in social marking. While pigments applied to the body might be used to signal information about a person’s social status or kin affiliation, Kuhn and Stiner point out that pigment signaling is greatly limited in terms of durability, transferability, standardization, and a host of other factors. Beads, on the other hand, overcome these limitations, thus providing a more effective means of communicating social information to “intimate strangers”—those familiar enough with the wearer to know the meaning of the signal (“She’s the chief’s daughter”) but not familiar enough to know the wearer personally. In the context of hxaro (!Kung gift‐giving alliances), for instance, an abundance of beads signals a rich network of friends and allies beholden to the wearer—a cautionary note to potential rivals and something very difficult to convey using pigments.</p>
            <p>If we take imposed form and/or style on handaxes, composite tools, and beads as the securest evidence of indexes, then collectively this evidence overlaps with the emergence of iconicity in the form of pigment use (a time frame running roughly from 500,000 to 100,000 ybp). The early portion of this time frame also corresponds to an increase in brain size marking the emergence of archaic <italic>Homo sapiens</italic> (Ruff, Trinkaus, and Holliday 1997). A larger hominin brain would have meant even more altricial offspring. Given that immature primate brains appear to have some advantages in associative learning, this increased altriciality may have actually facilitated the emergence of these new forms of referential thinking and laid the foundation for the capacity to acquire symbols.</p>
         </sec>
      </sec>
      <sec id="sc3">
         <title>The Emergence of Symbolism</title>
         <p>The elaborate burials, spectacular cave art, and highly imaginative imagery and artifacts from the Upper Paleolithic stand as singular achievements of modern <italic>Homo sapiens</italic>. While archaics such as Neanderthals made use of beads and pigments (d’Errico et al. 2003), they produced none of the decorative abstract remains that seem to demand a symbolic interpretation. Certainly, Upper Paleolithic art contains iconic and indexical elements; however, some of it also appears to be purely culturally defined (i.e., symbolic). Such items as the sorcerer image of Les Trois‐Frères cave; the lion‐headed man from Holenstein‐Stadel; the circles, spirals, and other geometric forms found in Australian rock art; and the numerous mythic and therianthropic images of European cave art appear to possess elements that may be completely understandable only from within the cultural context of their creation.</p>
         <p>Evidence cited earlier suggests that the move from index to symbol is not easy. Recently, Wynn, Coolidge, and Bright (2009) have provided an analysis of the cognitive requirements behind the creation of the Holenstein‐Stadel artifact. They argue that the artisan needed to hold two disparate concepts (person and lion) in active attention while building an imagined superordinate category capable of uniting them (e.g., spiritual agent). It is precisely this sort of cognitive process—in which arbitrary connections must be made between sign and referent—that is central to symbol construction. The fact that abstract artifacts such as the Holenstein‐Stadel appear relatively late in hominin evolution seems to reinforce the notion that symbol building is cognitively demanding.</p>
         <p>The ability to hold disparate concepts in active attention and operate on them by building innovative connections places great demands on working memory. Indeed, Wynn and Coolidge (2007) argue that an enhancement of working‐memory capacity was essential to the evolution of uniquely human cognition, including its powerful symbolic function. However, this enhancement is probably not a prerequisite for acquiring symbols per se—after all, apes have successfully learned rudimentary linguistic systems. Instead, an enhancement of working memory was probably more crucial in terms of the complexity of the symbolic system that the brain can support. The bonobo Kanzi learned to use a lexigram language system with relative ease (very likely aided by his early exposure). However, in stark contrast to human children, his language abilities have remained static at about the level of a three‐year‐old child, and short‐term memory capacity has proven to be a major limiting factor (see Savage‐Rumbaugh, Shanker, and Taylor 1998:69–73).</p>
         <p>Enhanced working‐memory capacity may have been critical in allowing <italic>H. sapiens</italic> to move from having limited symbolic potential to being symbol dependent—from protolanguage to syntactically and recursively complex language, from beads to abstract art. This transition may, in fact, be a requisite one for symbolism to provide any substantive fitness advantage. Until the brain is capable of supporting a level of symbolic complexity such that social life itself can be symbolically organized, then symbolism may be of only marginal consequence. What, then, were the selective factors behind this watershed transition?</p>
         <sec id="sc3.1">
            <title>Social Complexity and Cognitive Evolution</title>
            <p>That social pressure might be critical in human brain and cognitive evolution is becoming an increasingly prominent theme (Alexander 1989; Dunbar 2007; Geary 2005; Powell, Shennen, and Thomas 2009). Those of our ancestors who were best able to track social relationships, build strategic alliances, and make vital judgments about intentions and trustworthiness very likely gained a fitness advantage. Furthermore, a long tradition of psychological research has established the inferentially and computationally demanding nature of social cognition (Gilbert, Pelham, and Krull 1988; Heider 1958; Trope 1986). These demands are especially acute in executive functions such as attention, working memory, and cognitive control. One recent study has provided both correlational and experimental data supporting the notion that social engagement itself activates working memory and improves cognitive functioning (Ybarra et al. 2008). Thus, as the <italic>H. sapiens</italic> social world expanded and complexified, executive functions including working memory came under increasing pressure.</p>
            <p>Archaic hominins, such as Neanderthals, were expert toolmakers and highly proficient hunters whose technical skills and behavioral competence differed little from their <italic>H. sapiens</italic> counterparts. The social worlds of archaics and <italic>H. sapiens</italic>, however, were quite distinct. Fossil evidence indicates that <italic>H. sapiens</italic> social groups were larger and more complex than those of Neanderthals. Relative to Neanderthals, European Cro‐Magnon campsites are larger, more frequent, more intensely used and occupied, and (typically) more spatially structured (Bar‐Yosef 2002; Dickson 1990:84–92, 180–189; Hoffecker 2002:129, 136; Stringer and Gamble 1993:154–158). Many of these sites show evidence of seasonal aggregation, larger population density, and other signs of social complexity and stratification (Mellars 1996; Vanhaeren and d’Errico 2005).</p>
            <p>
               <italic>Homo sapiens</italic> groups were not just larger than those of Neanderthals; they were compositionally quite different as well. Increased longevity among <italic>H. sapiens</italic> meant that their groups were composed of considerably more older adults relative to young adults, adolescents, and children. Using dental samples, Caspari and Lee (2004) found evidence that the ratio of older to younger adults (“older” defined as two times the average age of first reproduction) increased significantly in Upper Paleolithic modern humans compared with Australopiths, early <italic>Homo</italic>, and Neanderthals. In fact, it was only among modern humans that older adults actually outnumbered their younger counterparts. An increased supply of adults may have been important in allowing modern humans to adopt more clearly defined sex roles, with males hunting and females gathering (Kuhn and Stiner 2006). Sex role specialization appears not to have been characteristic of Neanderthals.</p>
            <p>
               <italic>Homo sapiens</italic> were also interacting with other groups more frequently, resulting in raw material and (very likely) informational exchanges (Adler et al. 2006; Feblot‐Augustins 1999; Gamble 1999). The emergence of shell beads sometime between 100,000 and 70,000 ybp suggests that social marking was becoming increasingly important, as would be expected with a rise in intergroup interactions. Furthermore, often these shell beads are found quite distant from their origin, suggesting the existence of expanded trading networks (Vanhaeren et al. 2006). The existence of expanded trade networks is further bolstered by the presence of tools made from “exotic” nonlocal raw materials in the Still Bay and Howiesons Poort tool industries, dated to around 70,000–60,000 ybp (Ambrose 2002, 2010; Henshilwood 2007). Evidence of similarly extensive trade networks is lacking among archaic hominins. Among its myriad impacts, the increasingly sophisticated social world of <italic>H. sapiens</italic> would have had two ramifications relevant to the expansion of working‐memory capacity: (1) it would have created a more stable and secure rearing environment for children and (2) it would have put unprecedented stress on social rituals.</p>
         </sec>
         <sec id="sc3.2">
            <title>Social Complexity and Ontogeny: Allostatic Load and Joint Engagement</title>
            <p>Greater social organization provides a more secure and stable environment within which to raise children. The consistent presence of a sizable older generation among ancestral human groups meant more eyes to supervise and protect children and more hands to procure resources. Often these resources were being procured with increasingly effective technologies that reduced the physical burden on the user (such as lethal projectile hunting weapons). Male specialization in the most dangerous and strenuous activities meant that females could remain closer to camp, expending more energy on child rearing and protection. Additionally, because males and females were specializing in procuring different food resources, scarcity in any one commodity (such as reduced numbers of large game) would not necessarily endanger survival. These factors, in conjunction with intergroup resource exchange networks, resulted in a social, economic, and domestic world of <italic>H. sapiens</italic> that was quite different from that of their archaic contemporaries. Life for <italic>H. sapiens</italic> was not easy, but it was far less precarious than that of any other hominin.</p>
            <p>Evidence for this can be found in the fact that Neanderthals and their children endured higher levels of stress and deprivation compared with those experienced by <italic>H. sapiens</italic>. Across most of Europe, Neanderthals were almost exclusively big‐game predators (Bocherens et al. 2001, 2005; Marean 2007). Lacking projectile hunting implements, Neanderthals regularly confronted large, dangerous beasts, such as mammoths and rhinos, with spears designed to be thrust in at close range (Churchill 1993; Shea 1997). Unsurprisingly, this tactic produced extensive head, neck, and upper‐body trauma (Berger and Trinkaus 1995). The lack of sex role specialization meant that women and youngsters were very likely participating in big‐game hunts and sharing in the burdens.</p>
            <p>This relative homogeneity in resource procurement also meant that Neanderthal nutritional needs were more subject to stress when big game became scarce. Analyses of tooth samples show that Neanderthal children endured greater nutritional stress than Cro‐Magnon children (Soffer 1994). Moreover, nutritional stress in general seems to have afflicted Neanderthals to a greater degree than Cro‐Magnons (Stiner 1991; Stringer and Gamble 1993:166). While evidence of cannibalism (presumably owing to nutritional stress) is present from Neanderthal sites (Defleur et al. 1993, 1999), similar evidence from Cro‐Magnon sites is lacking (see Klein and Edgar 2002:198). Neanderthal morality rates were also extremely high—fewer than 10% of Neanderthals lived to over age 35. Among extant hunter‐gatherer and tribal agriculturalists, about 50% are over this age (Trinkaus and Thompson 1987). Among the !Kung San of southern Africa, while life expectancy at birth is about only 30 years, nearly 80% of the adult population lives to over age 60 (Blurton Jones 2002:314).</p>
            <p>A more secure and stable domestic environment has positive effects on cognitive development. A recent analysis has shown that higher levels of allostatic load during childhood have damaging effects on numerous measures of intellectual performance, including working‐memory capacity (Evans and Schamberg 2009). Allostatic load is a composite measure of stress endured during childhood that includes measures taken from blood pressure readings, overnight cortisol and catecholamines levels, and body mass index. Evans and Schamberg (2009) contend that allostatic load represents “an index of chronic stress” representing “the degree of cumulative wear and tear on the body during [the child’s] early lifetime” (6546–6547). The duration and degree of allostatic load incurred during childhood are significant predictors of young adult working‐memory capacity. This study confirmed and extended earlier neurocognitive studies by Farah (Farah et al. 2006; Noble, McCandliss, and Farah 2007) showing that an impoverished upbringing detrimentally affected brain systems associated with working memory, cognitive control, and executive functions in kindergarten and first‐grade children.</p>
            <p>If allostatic stress during ontogeny is detrimental to the development of working memory, then to the extent that the <italic>H. sapiens</italic> social/developmental environment reduced this stress, selection pressure against greater working‐memory capacity would have been eased. But a more stable and secure domestic world would also have created positive selection pressure for greater working memory by virtue of greater opportunities for mother‐infant joint engagement. Joint engagement refers to instances in which mother and infant share attentional focus together on a third object such as a toy.</p>
            <p>The greater number of adults present in <italic>H. sapiens</italic> groups meant that more help was available to the mother, allowing her to devote more time tending to a young infant as opposed to actively gathering or processing resources. By contrast, the high level of skeletal robusticity found in Neanderthal females suggests that mothers (and their children) were highly active, working hard for a living (Lieberman, Devlin, and Pearson 2001; Lieberman and Pearson 2001). Thus, the physical demands on <italic>H. sapiens</italic> mothers were, in all likelihood, not as great as those on Neanderthal mothers, potentially leaving <italic>H. sapiens</italic> mothers more time and energy for child care. To the extent that <italic>H. sapiens</italic> mothers had increased opportunities for joint engagement with their infants, the cognitive development of those infants would have been enhanced.</p>
            <p>Infants demonstrate a number of cognitive skills in the context of joint engagement that are either absent or less sophisticated outside of this context (Bigelow, MacLean, and Proctor 2004; Carpenter, Nagell, and Tomasello 1998; Moll, Carpenter, and Tomasello 2007; Ratner, Foley, and Gimpert 2002; Tomasello and Haberl 2003). These skills include word and vocabulary acquisition, organizational abilities, more sophisticated forms of play, and understanding the mental states of others. One possible reason for these cognitive enhancements is that joint engagement heightens cortical activity in infants’ brains (Striano et al. 2006).</p>
         </sec>
         <sec id="sc3.3">
            <title>Building Greater Social Complexity: Social Rituals</title>
            <p>Evidence reviewed earlier indicates that <italic>H. sapiens</italic> social groups were larger and more complex and included more intergroup interactions than did social groups of Neanderthals. But by what mechanism is such a social world constructed? Throughout the animal world, when careful communication is required, ritualized behavior is present. In the current context, “ritual” refers to a stereotyped and generally invariantly sequenced pattern of behavior.</p>
            <p>As highly social animals, it is not surprising that primates have an extensive range of social rituals designed to build trust, promote group harmony, and reinforce social relations (Goodall 1986; van Roosmalen and Klein 1988:515). For example, when bonobo, chimpanzee, and spider monkey foraging groups reunite, they engage in a number of rituals of welcoming and social reaffirmation, such as mutual embracing, kissing, group pant‐hooting, and grooming. Two male baboons wishing to form a social alliance will engage in mutual scrotum grasping (Smuts and Watanabe 1990). Among chimpanzees, reconciliation between combatants is signaled by bowing and begging gestures (on the part of the loser), followed by kissing and embracing (by the winner; de Waal 1990).</p>
            <p>The wealth of social rituals present among our primate cousins indicates that our hominin ancestors were preadapted for using ritualized behavior as a means of social bonding and could call on a rich repertoire of them in their everyday social lives. Thus, faced with the challenge of constructing and managing larger, more complex social groups and communicating carefully and effectively to suspicious out‐group members, our ancestors would have naturally turned to a mechanism with a deep history of facilitating social bonding: ritual.</p>
            <p>Evidence from traditional societies shows that social rituals are often physically and mentally demanding, requiring great behavioral discipline and cognitive control. They frequently require one to inhibit reflexive, prepotent responses in order to show commitment to group norms or a willingness to adhere to rules of reciprocity (for a more in‐depth discussion, see Rossano 2009). For example, a truce among the Amazonian Yąnomamö can be achieved only after warriors show they can resists the taunts, threats, insults, and brandished weapons of their rivals (Chagnon 1968). Rites of initiation common among traditional peoples often require the initiate to endure isolation, deprivation, physical pain, and psychological stress (Catlin 1867; Glucklich 2001; Power 1998:122–125; Whitehouse 1996). Possibly the most dramatic of these initiations was the famous Mandan Indian Sun Dance ceremony, in which new warriors were suspended from the top beam of a large ceremonial enclosure with ropes attached to skewers embedded in their chests (Catlin 1867). They might remain there for hours or days as dancing and chanting went on below them.</p>
            <p>The critical point about these social rituals is that they would have placed great demands on the ability to inhibit prepotent responses while maintaining focus on the need to complete the ritual in order to achieve a highly valued social goal (e.g., acceptance in the group, an alliance between groups, a peace treaty, etc.). Cognitive neuroscience research shows that this ability taxes two parts of the brain integral to working‐memory capacity: the dorsolateral prefrontal cortex and the anterior cingulate cortex (Beauregard, Levesque, and Bourgouin 2001; Cunningham et al. 2004; Hester, Murphy, and Garavan 2004; Kelly et al. 2006). Furthermore, increased working‐memory capacity allows for greater cognitive resources to be dedicated to conscious inhibitory processes (Kelly et al. 2006). This suggests that in our ancestral past, those with greater working‐memory capacity would have had an advantage over others in their capacity to complete successfully demanding social rituals.</p>
         </sec>
      </sec>
      <sec id="sc4">
         <title>Summary</title>
         <p>It is unclear whether the archaeological record supports a progression from icons to indexes to symbols as predicted by Peircian semiotics. The least contentious interpretation is that iconic artifacts (in the form of pigment use) and indexical ones (handaxes with imposed form, composite tools, beads) are roughly contemporaneous (within a time frame around 500,000–100,000 ybp).</p>
         <p>Clearly, the move to symbolic thinking arrives later. Its foundations were probably laid in the increased altriciality associated with the expansion in brain size occurring around 500,000 ybp. Given that apes can acquire rudimentary symbol systems, an enhancement of working‐memory capacity was probably not essential in giving hominins symbolic potential. Instead, greater working‐memory capacity permitted the acquisition of more complex (and more powerful) symbol systems, such as that associated with modern language. The key selective forces behind enhanced working memory were likely associated with increased social complexity: more demanding social rituals, reductions in allostatic load during childhood, and increased opportunities for mother‐infant joint engagement.</p>
      </sec>
   </body>