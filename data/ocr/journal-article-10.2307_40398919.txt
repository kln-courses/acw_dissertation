<plain_text><page sequence="1">Attributions of Trust in Decision Support Technologies: A Study of Recommendation Agents for E-Commerce WEIQUAN WANG AND IZAK BENB ASAT Weiquan Wang is an Assistant Professor in the Department of Information Systems at City University of Hong Kong. He received his Ph.D. in Management Information Systems from the University of British Columbia. His research interests include online recommendation agents, trust in technological artifacts, user acceptance of information technology, and human-computer interaction. Izak Benbasat is a Fellow of the Royal Society of Canada and a CANADA Research Chair in Information Technology Management at the Sauder School of Business, Uni- versity of British Columbia, Canada. He received his Ph.D. in Management Information Systems from the University of Minnesota. He currently serves on the editorial boards of Journal of the AIS and Journal of Management Information Systems. He was edi- tor-in-chief of Information Systems Research, editor of the Information Systems and Decision Support Systems Department of Management Science, and a senior editor of MIS Quarterly. The general theme of his research is improving the interactions among information technology (IT), management, and IT users. Abstract: As organizations increasingly utilize Web-based technologies to support customers better, trust in decision support technologies has emerged as an important issue in online environments. In this study, we identify six reasons users trust (or do not trust) a technology in the early stages of its use by extending the theories of trust formation in interpersonal and organizational contexts to that of decision support technologies. We study the particular context of decision support technologies for e-commerce: online recommendation agents (RAs), which facilitate users' decision making by providing advice on what to buy based on user-specified needs and pref- erences. A laboratory experiment is conducted using a multimethod approach to collect data. Both quantitative data about participants' trust in RAs and written protocols that explain the reasons for their levels of trust are collected. A content analysis of the written protocols identifies both positive and negative trust attributions that are then mapped to six trust reasons. A structural equation modeling analysis is employed to test the causal strengths of the trust reasons in explaining participants' trust in RAs. The results reveal that in the early stages of trust formation, four positive reasons (i.e., knowledge-based, interactive, calculative, and dispositional) are associated with higher trust in RAs and two negative reasons (i.e., calculative and interactive) are associated with lower trust in RAs. The results also demonstrate some distinctive features of trust formation with respect to decision support technologies. We discuss the research and practical implications of the findings and describe opportunities for future research. Journal of Management Information Systems / Spring 2008, Vol. 24, No. 4, pp. 249-273. © 2008 M.E. Sharpe, Inc. 0742-1222 / 2008 $9.50 + 0.00. DOI 10.2753/MIS0742-1222240410</page><page sequence="2">250 WANG AND BENBASAT Key words and phrases: decision support technology, reasons to trust, recommenda- tion agents, trust attribution, trust in technology. Organizations increasingly provide Web-based decision support technologies to customers to improve their online experiences [21, 66]. The key to the ultimate suc- cess of such support technologies is the users' trust in them [12, 16, 27, 46, 66]. It has been widely recognized that trust is an important factor in business-to-consumer (B2C) e-commerce, and although considerable research on trust in both offline and online environments has been undertaken (e.g., [39, 41]), the reasons users trust deci- sion support technologies for e-commerce remain an unexplored topic. Thus, the main goal of this study is to reveal users' attributions of trust in online decision support technologies by extending the theory of interpersonal and organizational trust, as well as by taking into consideration the characteristics of human-computer interactions. This is an important research topic in that it will inform scholars and designers of how individuals interact with technology to assess its trustworthiness and provide guidelines for the design of trustworthy technologies. The formation of trust is a complex phenomenon. Conjunctive reasons are involved when forming certain beliefs (e.g., trust) [23, 31]. Prior research suggests that trust can be attributed to a variety of causes, such as dispositional, calculative, institutional, and knowledge-based reasons (e.g., [14, 43]). In this study, we identify the reasons the user trusts (or does not trust) an online decision support technology. The theory of social responses to computers developed by Reeves and Nass [51] and the empirical work they and their collaborators have conducted suggest that people treat computers as //* they are social actors, thus they generally respond to them based on the rules that apply to social relationships. Therefore, we contend that many of the trust findings in interpersonal contexts are applicable to the online decision support technology domain; hence, they constitute the major theoretical foundation of the present study. As an exploratory research endeavor, this study was conducted in the specific context of recommendation agents (RAs) to assist consumers in online decision making. RAs are Internet-based software programs that carry out a set of operations on behalf of users and provide decision advice based on users' needs, preferences, profiles, and previous shopping activities [2]. RAs are becoming increasingly prevalent in a wide range of Web sites because they help buyers and sellers reduce information overload [32, 38] and improve the quality of purchase decisions [22, 24]. RA technologies have been implemented by a variety of organizations, including Yahoo and Amazon.com, to provide value-added services to consumers. Demonstrating the importance of RAs, the Economist (June 4, 2005) reported that eBay paid $620 million for Shopping.com, an online shopping comparison site that provides recommendation services. RAs facilitate consumers' decision making by taking over some of their decision tasks (e.g., narrowing down the consideration set with products that fit consumers' preferences). However, consumers may have several trust-related concerns, such as</page><page sequence="3">ATTRIBUTIONS OF TRUST IN DECISION SUPPORT TECHNOLOGIES 25 1 [28, 66] : Does an RA have the ability to understand their needs? Is it aware of all of the products available in the market? Is it biased favorably toward certain manufacturers or vendors? These concerns can diminish customers' trust in RAs and, accordingly, their utilization. In online environments, these concerns are not uncommon, due to the lack of proven guarantees that e- vendors or RA providers will not engage in op- portunistic behaviors [18]. Consequently, consumer trust is one of the most important factors determining the effectiveness of RAs, as revealed in a focus group study [1]. Trust evolves with the interaction between users and technologies (e.g., [35]). In this study, we focus on the early stages of trust formation when consumers use and interact with an RA for the first time, a time during which users' perceptions of the uncertainty and risks inherent in RA use are particularly salient [42]. Sufficient trust is needed to overcome these perceptions; otherwise, users may easily switch to other Web sites or RAs by a single mouse click [66]. Hence, while the evolving and ongo- ing nature of trust is equally important, in this study, the focus is on trust formation at the early stage of interaction with RAs. In this study, we combined a qualitative approach with quantitative analysis in a multimethod approach [45]. We collected both qualitative and quantitative data. To elicit users' trust attributions, qualitative data (i.e., written protocols provided by participants to justify their extent of trust in an RA) were collected in a laboratory setting. In addi- tion, participants' trust in the RA were measured using Likert-type scales to examine, by utilizing a structural equation modeling analysis, the causal strengths of the trust attributions identified from the written protocols. Trust in Online Recommendation Agents The recent literature on information systems has discussed four general approaches to defining trust [18, 41]: a belief or a collection of beliefs [8], emotional feelings [27], an intention [39], and a combination of these elements [41]. Several comprehensive reviews on the trust literature have already been published (e.g., [18, 39, 41]). In particular, we focus on trusting beliefs in this study (i.e., the first approach) because they have been identified as important mediators that influence trusting intentions [34,41]. Trusting beliefs concerning a decision support technology include one's perceptions about the competence, benevolence, and integrity of the technology. According to McKnight et al. [41], in the context of RAs, competence belief 'refers to the consumer's perception that an RA has the skills and expertise to perform effectively in specific domains, benevolence belief is the consumer's perception that an RA cares about him or her and acts in his or her interest, and integrity belief is the perception that an RA adheres to a set of principles (e.g., honesty and keeping promises) generally accepted by consumers. The nomological validity of trust in RAs, as defined above, has been confirmed by Wang and Benbasat [66], who tested an integrated trust-TAM (technology acceptance model), and by Komiak and Benbasat [27]. During initial interactions with an RA, consumers form a certain level of trust in the RA which influences their intentions to</page><page sequence="4">252 WANG AND BENBASAT adopt it. There exists ample theoretical and empirical evidence that people respond socially to RAs and that they perceive RAs as exhibiting human-like characteristics (e.g., benevolence and integrity). Although human properties do not intrinsically exist in RAs, the salient fact is that they are perceived as such by those who interact with them [5 1].1 Hypotheses Development Trust is a social construction that originates from interpersonal relationships [35]. Due to its relationship-based nature, it has been argued that trust is multifaceted and that the formation of trusting beliefs involves conjunctive underlying reasons [23, 3 1 ] . The multiple reasons can complement each other to justify the extent of trust in a target [31]. Therefore, trust in an RA can be attributed to multiple causes. Drawing on research from a variety of disciplines (e.g., psychology, sociology, economics, marketing, and organizational science), the theories on trust formation shed light on the reasons why trust is formed in various contexts [14, 18, 35, 43, 68]. Based on a comprehensive review, we identified six of the most prominent reasons - dispositional, institutional, heuristic, calculative, interactive, and knowledge-based reasons.2 A summary of these reasons and the related literature is reported in Table 1. Their association with users' extent of trust in RAs will be discussed extensively in this section. In this study, we focus on the formation of trust in an RA in the early stage dur- ing which users interact with the RA for the first time. Many previous studies have investigated initial trust formation in which users do not have any experience with trust targets (e.g., [43]). Initial trust develops through nonexperiential influences (e.g., dispositional and institutional). The trust formation in an early stage extends past the initial stage and is also influenced by experiential factors.3 In the context of system troubleshooters' trust in their supervisors, McKnight and Chervany [40] found that both experiential and nonexperiential factors influence trust. Therefore, the trust reasons that are investigated in this study relate to both experiential (e.g., interactive and calculative) and nonexperiential factors. Furthermore, people often experience ambivalence in their evaluation toward a trust target [9, 50]. Lewicki et al. argued that simultaneous positive and negative cognitions and assessments exist during the formation of a trust relationship: "Relationships are multifaceted or multiplex, therefore enabling parties to hold simultaneously differ- ent views of each other - views that may be accurate but, nonetheless, inconsistent among them" [36, p. 442]. Lewicki et al. contended that although parties may pursue consistency and the resolution of inconsistent views, the more common state of trust formation is "not one of balance but, rather, of imbalance, inconsistency, and uncer- tainty" [36, p. 444]. Therefore, when users interact with an RA, the ambivalence in their cognitive assessments will produce both positive and negative reasons. Positive reasons will be associated with a higher level of trust and negative reasons will be associated with a lower level of trust.</page><page sequence="5">2 &amp; &lt;D C c/5 a I C/5 'S S | CO &lt; í * I ¡i - r " it • Sá «a •c « -o &gt;&gt; -o S i O û) 0 O &lt;D ^ «i ed o co -S co &gt;^ o i-i o CO co m -a &lt;ù | i-i i- o CO x5 co m -a g« &lt;ù i i* IN i -fi •*3 &gt; 3 § 2 'S I -9 •*3 3 T3 2 'S TD I s1" ' Jii o g .è* * 03 w ^ Si &lt;D w cd eö •£ OC o -o -5 &amp; - S o g ffl » -o í ^ ; 5 « - ■g il Hlil ¡lesili ■a « « g H o » » S 1 Ë S 8. I if S!| S .8-8 3 ! to 3 h i to bû h ■a s^s ■a I * ^ -s =■ I | l'I" pj E c ^ LO ^" t« « « r f„ ill 1 là 2-gS m ¿5 co T-, s^ è i i lì 1 I iii f I 1 11 I I "¡11 253</page><page sequence="6">254 WANG AND BENBASAT Dispositional Reason A dispositional reason is based on an individual's general predisposition to trust other parties [ 1 8, 39, 43, 53]. Dispositional factors play an important role in the early stages of trust relationship formation when no previous interaction history exists to judge the other party's trustworthiness [18, 39]. Accordingly, in the absence of previous experience with a decision support technology, users' general tendency to trust oth- ers influences their cognitive assessment of trust in the technology [34]. Evidence from both laboratory and field studies indicates that people differ significantly in their general tendencies to trust technologies [28, 29]. Some individuals are inclined to trust a technology, whereas others may be unwilling to trust it regardless of the circumstances that would support doing so. Therefore, higher trust in an RA can be attributed to a dispositional tendency to trust a technology (i.e., a positive dispositional reason) whereas lower trust can be attributed to a dispositional unwillingness to trust the technology (i.e., a negative dispositional reason). Hypothesis 1: A dispositional reason (positive or negative) is associated with the extent of trust (higher or lower) in RAs.4 Institutional Reason Institution-based trust is associated with societal structures (e.g., legislation, rules, guarantees, and third-party assurances) that people believe will make an environ- ment trustworthy [18, 41, 48, 55, 68]. The institutional safeguards produce a sense of security that leads to the increase of trust in others in a particular circumstance (e.g., Web-based environments). In contrast, in the absence of institutional safeguards, trust in RAs will be inhibited because there is a lack of external assurance that the RA will not engage in opportunistic behavior in online environments, especially when users interact with the RA for the first time. A widely studied institutional safeguard is structural assurance [41, 43]. Structural assurance is an assessment of trust based on contextual conditions, such as regula- tions, guarantees, promises, legal guidelines, or protective procedures [43, 68]. In the context of this study, for instance, an e- vendor who provides the RA may or may not provide guarantees for the quality of its services. Even when such guarantees are provided, consumers may or may not be aware of them. As a result, we contend that when interacting with RAs, trust in RAs can be attributed to institutional factors. Hypothesis 2: An institutional reason (positive or negative) is associated with the extent of trust (higher or lower) in RAs. Heuristic Reason Trust may arise from heuristic cues, such as first impressions, and be transferred from other sources (e.g., [25, 43]). A heuristic reason does not involve the users' systematic processing of their experiential interactions with an RA or information about an RA</page><page sequence="7">ATTRIBUTIONS OF TRUST IN DECISION SUPPORT TECHNOLOGIES 255 per se. According to the theory of swift trust, heuristic cues can generate trust quickly even prior to having firsthand experiences [25, 44]. Particularly, when interacting with an RA, the look and feel of the technology influ- ence users' trust in it [28]. A pleasant and friendly interface indicates that an RA has been designed to "care" about users' feelings and the effort users would spend using the RA. In addition, it also illustrates that the RA "invests" effort in developing the RA-user relationship [18]. In contrast, an unfriendly interface may trigger negative cognitions leading to trust reduction. Hence, consumers will utilize heuristic cues, including their impressions about the interface of an RA, when forming their trust in it. In addition, trust can be transferred from one entity to another [14, 59]. When con- sumers interact with an RA for the first time, they identify other entities and heuristic cues closely associated with the RA (e.g., a general impression about the Web site or the reputation history of the e- vendor) to infer its trustworthiness. A trustworthy source will increase the consumers' trust in the RA, whereas an untrustworthy source may be associated with lower trust. Hypothesis 3: A heuristic reason (positive or negative) is associated with the extent of trust (higher or lower) in RAs. Calculative Reason The calculative reason refers to the consideration of the costs or the rewards associated with a trustee acting in an untrustworthy way [14, 35]. If the benefits of acting in an untrustworthy way exceed the costs of being caught and "penalized," then consumers may infer that the trustee could not be trusted. Gefen et al. [18] found that calcula- tive-based beliefs affect consumer trust in e- vendors. In the context of this study, RAs might be designed to provide biased recommendations. If users can identify or are suspicious of such a possibility, they might stay away from particular e- vendors and their trust in RAs might be lower. On the trustor's side, Tan and Thoen [60, 61] provided a different perspective on the calculative reason. They contended that individuals will judge their own gain from the trusting relationships with others as well as the potential risks involved. The calculative reason reflects mental states comparing the gain and the risks with some thresholds, or making trade-offs between gains and risks. These considerations are especially relevant in the context of decision support technologies, which benefit users by facilitating their decision making. In addition to risk considerations that are widely recognized as salient factors influencing trust formation [39], in a task-based context, utility assessments are among the most important components of relation- ship building and the parties who present the highest utility and perceived value will be characterized as trustworthy [3], Users will arrive at a positive overall evaluation of an RA when they perceive it to be useful, which in turn reinforces and enhances their trust in the RA. In sum, there are many calculative factors influencing users' trust when using deci- sion support technologies, including RAs. The relative benefits and costs of an RA</page><page sequence="8">256 WANG AND BENBAS AT behaving in an untrustworthy manner will be judged via the calculative consideration. When the benefits outweigh costs (i.e., a negative reason), trust in RAs will be lowered. Similarly, users will assess their own benefits and costs of interacting with the RA, which will increase or reduce their trust in the RA accordingly. Therefore, we posit: Hypothesis 4: A calculative reason (positive or negative) is associated with the extent of trust (higher or lower) in RAs. Interactive Reason Zucker [68] noted that people assess another party's behavior and performance during their interactions, thus forming trust based on their interactive experiences with the RA. Particularly, users' expectations about the behavior and performance of the RA, their control over the RA, and the verification of the recommendations from the RA will influence their assessment of trust in the RA [29]. Expectation Consumers use their expectations as a benchmark to judge the trustworthiness of RAs [29]. As Kramer stated, "individuals' judgments about others' trustworthiness are anchored, at least in part, on their a priori expectations about others' behavior" [29, p. 576]. Consumers trust an R A when it behaves as they expected [17]. When an RA's actions do not conform to their expectations, trust in the RA will be lower [35]. The self-efficacy theory [4] posits two kinds of expectancy beliefs: outcome expecta- tions (beliefs that certain outcomes will be generated) and efficacy and process expecta- tions (beliefs about whether or not an RA can effectively perform the behavior necessary to produce a desired outcome). Similarly, consumers form two types of expectations when using RAs. First, they form outcome expectations, which are directed to the final recommendations provided (e.g., "I think the RA should recommend several Olympus cameras"). When the recommendations match their expectations, trust emerges; other- wise, it might decline. The second type of expectation includes process and behavior expectations, which concern the expected "activities" that an RA conducts in order to achieve certain outcomes (e.g., "The RA should ask me about my brand preferences"). Consumers may use their own knowledge to generate expectations regarding RA be- havior. When an RA behaves in an expected way, trust in the RA will be higher. Control In addition to expectations, users' control over a technology is another source induc- ing trust. In organizational contexts, Das and Teng [13] and Luhmann [37] proposed a theory concerning the relationship between trust and control. Control mechanisms can influence trust by reducing the level of uncertainty: "the attainment of desirable goals becomes more predictable" [13, p. 493]. This reduction of uncertainty applies to the early stage of trust formation with an RA when the user faces many uncertainties about using the RA. Control over an RA decreases the likelihood that consumers will be misguided by the RA, and thus the consumers are more likely to trust the RA [28].</page><page sequence="9">ATTRIBUTIONS OF TRUST IN DECISION SUPPORT TECHNOLOGIES 257 Consumer control over an RA may arise from the choices that are provided by the RA, the tendency of the RA to influence the consumers' decision making, the consumers' opportunity to express their needs, and similar factors as suggested by Silver [56]. Verification The potential to verify the performance of an RA using other resources can facilitate consumers' trust in the RA [15]. In the early stages of a relationship, when consumers lack the knowledge to check the credibility of the RA and when they do not have an interaction history to rely on, they may consult a trusted third party [28]. For example, by contacting their friends or independent product reviewers, consumers can verify if the recommendations provided are good and if the RA's claims are true. In contrast, the absence of a third party to validate the advice provided by the RA makes the verification process difficult, thereby reducing trust [15]. An examination of the above factors demonstrates that users' interactive experiences with an RA constitute a rich foundation when forming trust in the RA. This reflects some distinctive aspects of human-computer interactions [33]. Unlike interacting with a person, users can freely test, manipulate, and have control over a technological arti- fact, as long as these actions are allowed, and then exercise their trust judgment [28]. However, when the interaction with an RAis very restricted (i.e., a negative interactive reason), users may have a negative assessment of the RA, leading to a lower level of trust. Therefore, trust in an RA can be attributed to an interactive reason. Hypothesis 5: An interactive reason (positive or negative) is associated with the extent of trust (higher or lower) in RAs. Knowledge-Based Reason Trust develops when one gains knowledge about a trustee [18], allowing one to inter- pret and predict a trustee's behavior [14, 35]. A knowledge-based reason refers to the deliberation on the information about a trustee [18, 35]. In addition to informational sources, knowledge about the trustee accumulates when a trustor is more familiar with the trustee. However, because we focus on the early stage of trust development before users have become familiar with an RA, the information and explanations about the RA are the primary triggers of the knowledge-based trust reason. It is worthwhile to note that the knowledge-based reason is different from an inter- active or heuristic reason. As opposed to an interactive reason, it is not necessary to interact with an RA to obtain explanations or information about the RA, which can be provided as stand-alone facilities. For example, explanations about the nature of the product database that an RA searches and about the strategy that the RA uses to generate recommendations can be delivered to users before they interact with the RA. Nevertheless, explanations and information about an RA can also be gained from interacting with the RA. Then, there are two possible triggers of trust. One is the processing of the explanations and information contents, and the other is the way to get the explanations and information (e.g., whether or not the explanations can be</page><page sequence="10">258 WANG AND BENBASAT easily retrieved). The former mainly helps the user understand the inner workings of the RA. Therefore, it will trigger the knowledge-based reason. In the latter case, the user mainly focuses on the interaction with the RA. Hence, the interactive reason will be involved. Furthermore, although a heuristic reason may also be triggered by informational sources, it does not center on the RA per se (i.e., information about other parties that are associated with the RA). In contrast, a knowledge-based reason involves a central processing of explanations and information about the RA per se. Information asymmetry is a major obstacle to trust due to the agency relationship between consumers and RAs [67]. In the absence of adequate explanation facilities about the RA's actions (i.e., its procedures for generating recommendations) and its "motives" for eliciting users' requirements, users are unable to ascertain RAs' compe- tence and "motivation" [67]. Explanations are important components of decision sup- port technologies, including RAs, because by making the performance of technologies transparent to their users, they improve user trust in the systems [20, 34, 67]. In particular, the provision of explanation facilities and information about an RA in- dicates its openness and willingness to share information [13, 67]. Information sharing reduces behavioral uncertainty, and it in turn further increases trust [67]. Deliberation on the information and explanations prompts trust due to a positive knowledge-based reason. In contrast, when limited information and explanations are provided (i.e., a negative reason), trust in RAs will be lower. Hypothesis 6: A knowledge -based reason (positive or negative) is associated with the extent of trust (higher or lower) in RAs. Research Method To investigate the causal strengths of the six hypothesized reasons about trust in RAs, we conducted a laboratory experiment to gather qualitative and quantitative data. After the experimental tasks, the participants' trusting beliefs (competence, integrity, and benevolence) were captured using Likert-type scales. Moreover, participants were asked to provide written answers to several open-ended questions to justify their trust levels (to gather written protocol-based qualitative data). The written protocols were coded to identify participants' trust attributions, which were mapped to the six trust reasons. Then, we conducted a structural analysis to test the causal strengths of the trust reasons. Combining a qualitative approach with quantitative analysis in a multimethod approach minimizes the threat of monomethod variance, goes beyond mere self-reports, and increases the robustness of the results [45]. Description of the Experimental Recommendation Agents An RA for digital cameras was developed for this study. Building a new RA, rather than using one that is currently available from a commercial Web site, ensures that it would be novel to all the participants, thereby allowing the study to remain focused</page><page sequence="11">ATTRIBUTIONS OF TRUST IN DECISION SUPPORT TECHNOLOGIES 259 on trust formation in an early stage. The RA was similar to those used in other studies (e.g., [54]) and in leading commercial applications (e.g., MyProductAdvisor.com). To elicit participants' needs and preferences, the RA used one of the most popular approaches, that is, RA-user dialogues [54], in which the participants responded to RA's questions regarding their needs and product preferences.5 After the RA elic- ited participants' requirements in the consultation dialogue, most of the participants received several pages of recommended products, with each page containing five products.6 The RA implemented in this research is a typical application of an online decision support technology [66]. For a different type of technology, the relative causal strengths of the reasons about trust in the technology may vary. For example, although the RA provides personalized advice and solutions that are similar to an infomediary Web site, it requires more interaction between the RA and the user. Thus, users need to supply their needs and requirements via their interaction with the RA, and they may adjust their inputs after examining the recommended choices. Accordingly, the interactive reason may be more influential in forming trust in RAs than infomediary Web sites. This study focuses on decision support technologies, but the examination of different types of technologies, though interesting, is beyond the scope of this study.7 Moreover, the RA was provided by the Web site that offered the products rather than a third party. We informed the participants about this prior to the experimental tasks. When an RA is provided by the vendor rather than a third party, trust in the RA would be a more salient issue because a third party is less likely to be biased in providing recommendations.8 The experimental RA was employed to conduct a series of experiments. One ex- periment was to investigate the extent to which trust in an RA can be enhanced by explanation facilities. The results were reported in Wang and Benbasat [67]. The data for the present study were originally collected in the experiment for that earlier study. The ability to explain knowledge and reasoning, referred to as explanation facilities, has been recognized as one of the critical components of intelligent and knowledge- based systems since their inception [20]. Three types of explanation facilities (how explanations, why explanations, and guidance) were made available using a 2 x 2 x 2 full factorial design, in which each explanation type was either available or not. The design generated eight experimental RAs with different combinations of explanation facilities to produce an adequate level of variance in the three trusting belief scores: average trust scores ranging from 3.0 to 8.0 (on a nine-point scale) with a mean of 5.9 and standard deviation of 1.1 (for details and findings, see [67]). Experimental Tasks, Procedures, and Incentives One hundred and twenty students were recruited from a large North American uni- versity. In the experiment, a research assistant first trained the participants on how to use and navigate the interface assigned to them using a tutorial RA with the same features as those of the experimental RA randomly assigned to the participants.9 During the training session, no participants reported that they had used the RA before. Next,</page><page sequence="12">260 WANG AND BENB ASAT we asked each participant to complete two tasks with the assistance of an RA; first, to choose a digital camera for a good friend and, second, to select another one for a close family member. The order of the two tasks was counterbalanced. No time limit was placed for the completion of either of the tasks. We used two tasks instead of one in order to ensure that the participants interacted with the RA sufficiently to offer informed judgments about the RA.10 After they completed the two tasks, we asked them to respond to a questionnaire, which included three scales to measure their competence, benevolence, and integrity beliefs in the RA, and to answer the following: (1) Do you believe in the virtual advisor's11 competence? Why? (2) Do you believe in the virtual advisor's benevolence? Why? and (3) Do you believe in the virtual advisor's integrity? Why? These three questions correspond to the three belief components of trust in RAs, as discussed in the second section (i.e., competence, benevolence, and integrity). The definitions of the three trusting beliefs (from Merriam-Webster's Collegiate Diction- ary, 11th edition) were also provided to the participants. The participants' answers to these questions were later analyzed to identify their trust attributions. Each participant was rewarded $15 for his or her participation. To motivate the participants to treat the experiment as a serious online shopping session and to increase their involvement, we offered the top 25 percent of the performers an ex- tra $25, and offered the participant with the best performance another extra $200. The main criterion for the judgment of performance was the extent to which the participants' justifications were appropriate and convincing to support their choice of a particular digital camera. The use of extra incentives to increase involvement is a well-accepted and commonly used method for experimental studies to increase experimental realism [6]. Coding of Written Protocols A content analysis of the written protocols (i.e., participants' answers to the essay questions) was employed. Before coding the protocols, each written protocol was first broken into episodes (units of coding) in which each episode contained at most one trust attribution. Then, we mapped the trust attributions identified from the written protocols to the six reasons. The first author and a research assistant (a Ph.D. candidate in management information systems) coded the protocols independently. The research assistant was familiar with the literature on trust and was provided with the definitions of the different types of trust attributions. We conducted a series of pilot tests to ensure that (1) the coding scheme covered all of the reasons involved in the formation of trust in RAs, and that (2) the two judges shared a common and valid understanding of the six reasons. A high intercoder agree- ment will not be reached without a common and proper understanding of these reasons. The pilot analysis revealed that the participants' attributions contained both positive and negative ones. For example, the provision of a certain type of explanation about RAs (e.g., explanation on the line of reasoning of an RA) triggered a positive cognitive deliberation that enhanced trust, while the lack of other explanations prompted a nega- tive cognition that inhibited trust. Accordingly, for each episode, each judge decided on</page><page sequence="13">ATTRIBUTIONS OF TRUST IN DECISION SUPPORT TECHNOLOGIES 261 (1) which reason it related to, if any, and (2) whether it could be mapped to a positive or negative reason. Some episodes were coded as "no reason involved in." Six rounds of pilot coding were conducted. For each round, the protocols from three randomly chosen participants were analyzed, and the discrepancies between the two coders were resolved by a meeting of the coders. Intercoder agreements steadily increased after each round of pilot coding. In the last round of pilot coding, the raw agreement rate reached a high rate of agreement at 0.85 [30]. For each participant, the coding results provided the number of episodes related to each of the 12 categories of reasons (six positive ones and six negative ones). On average, each participant provided about six episodes for the three questions. Around 65 percent of the episodes were related to positive reasons, and 35 percent were related to negative reasons. Examples from the written protocols are listed in Table 2. To assess the reliability of the coding scheme and to ensure the validity of the analysis, Cohen's kappa coefficient was used to measure the intercoder agreement [11]. The kappa coefficient of coding of all episodes was 0.79, indicating a high agreement between the two coders.12 The close agreement demonstrates that the at- tributions in the coding scheme are not only theoretically independent of one another but also exhibit high discriminant validity. By aggregating the attributions identified in all the protocols, the distributions of positive and negative reasons are summarized in Tables 3 and 4. Findings of the Structural Analyses To test the hypotheses, structural analyses were conducted to examine the rela- tive causal strengths of the trust reasons. For the structural analyses, we used partial least squares (PLS), as implemented in the PLS-Graph version 3.0. PLS is especially suitable for studies in the stage of theory building [5]. It does not require the normal distribution of data for estimating parameters. In this study, the number of episodes for a trust attribution was not normally distributed. PLS analyses used each participant as a unit of analysis, and the episodes for the three trusting beliefs (competence, integrity, and benevolence) provided by a participant were combined. The six positive reasons and the six negative reasons were modeled as exogenous variables, and trust was modeled as an endogenous variable. For each participant, the input value of a reason (positive or negative) was the total number of the episodes signifying the reason (i.e., instances of a positive or negative reason). The multi-item trusting belief measures of trust collected after the two experimental tasks were completed were modeled as reflective indicators of trust in an RA. The measures for the three trusting beliefs in RAs that were developed and validated by Komiak and Benbasat [27] were adopted for the current study. The validity and reliability of the measures were confirmed by Wang and Benbasat [66]. These measures have good reliabilities (Cronbach's alphas &gt; 0.70) and satisfactory discriminant and convergent validity. In an earlier analysis, we used PLS to assess the psychometric properties of the trust measures.13 An examination of the item loadings, composite reliability of constructs, and average variance extracted (AVE) all reveal strong positive loadings</page><page sequence="14">262 WANG AND BENBAS AT Table 2. Examples of Episodes Related to Positive/Negative Reasons Reason/source Examples Dispositional reason Positive: "I think that I do believe in the competence of the [18, 29, 39, 43, 53] virtual advisor because, well, I trust high-tech things." ". . . since it is a machine software program, I don't have to worry about too many things." Negative: "However, in most websites, electronic advisors might be biased." "I would prefer speaking to a real human being and explaining my needs to him/her to using a computer program." Institutional reason Positive: ". . . and there is a money back guarantee." [18, 39, 41 , 43, 47, ". . . as well there is guarantee for receiving the product." 48, 55, 68] Negative: "Also, there is no guarantee when purchasing the product through Internet shopping." ". . . because there is always some risk when purchasing products online." Heuristic reason Positive: ". . . and because I had a good overall [14, 18, 25, 28, 44] impression of the website." "Yes, because it is user-friendly and . . ." Negative: "I would have some drawbacks on this because the online store doesn't really have a long reputation history." "But a cartoon character might have been nicer; this virtual advisor is not 'physically attractive.'" Calculative reason Positive: ". . . it has no reason to hide any information [3, 14, 18, 60, 61] from me or to try 'pushing' a certain product to make a quick sale." 'The advisor does provide a useful service." Negative: ". . . but it wasn't as useful to the extent of a salesperson." Interactive reason Positive: "Yes, because his recommendations match my [13, 28, 29, 35, 37, expectations and prior knowledge of digital cameras." 43, 58, 68] ". . . you can always change your answers if the camera they have chosen does not suit your needs." Negative: "It would be better if the advisor can provide some outputs of the cameras, so that the customers can really see the qualities of the cameras." "I want to consider more choices when I am buying a digital camera." Knowledge-based Positive: "Yes, the how explanations presented all his reason reasonable reasoning in choosing the cameras." [13, 14, 18, 28, 35, 67] ". . . because the virtual advisor provided all the important information about the product." Negative: ". . . but I would like more explanations, so that people like me (who don't know anything about digital cameras) would benefit more." "I need more information about the products and their differences."</page><page sequence="15">ATTRIBUTIONS OF TRUST IN DECISION SUPPORT TECHNOLOGIES 263 Table 3. Percentage Distribution of Positive Trust Reasons (n = 120) (in percent) Reason Competence3 Benevolence3 Integrity3 Average6 Dispositional 3.1 6.9 8.3 6.1 Institutional 0.4 1.6 3.3 1.7 Heuristic 7.0 8.5 10.0 8.5 Calculative 26.5 25.2 13.3 21.8 Interactive 27.6 41.9 43.6 37.5 Knowledge based 35.4 15.9 21.6 24.5 Total 100.0 100.0 100.0 100.0 Notes: a The percentage number = (total quantity of episodes related to a particular reason about a trusting belief from all participants)/(total quantity of episodes related to all reasons about a trust- ing belief from all participants). b "Average" = (total quantity of episodes related to a particular reason about all three trusting beliefs from all participants )/(total quantity of episodes related to all reasons about all three trusting beliefs from all participants) Table 4. Percentage Distribution of Negative Trust Reasons (n = 120) (in percent) Reason Competence Benevolence Integrity Average Dispositional 12.5 21.1 27.4 19.1 Institutional 0.4 0.8 2.0 1.5 Heuristic 2.2 14.2 4.8 6.1 Calculative 9.4 12.9 6.8 9.9 Interactive 47.9 32.0 29.9 38.2 Knowledge based 27.6 19.0 29.1 25.2 Total 100.0 100.0 100.0 100.0 that are significant at the 0.001 level, thus indicating high individual item reliability. No item loads higher on another construct than it does on the construct it is designed to measure, and the square root of each constructs AVE is greater than the correla- tions between the construct and others. Therefore, the trust belief measures have good discriminant and convergent validities. The measurement items for the trusting beliefs are shown in the Appendix. The PLS results reported in Figure 1 reveal the causal strengths of the positive and negative reasons on trust in RAs. We found that the positive dispositional and knowl- edge-based reasons were significantly associated with higher trust, but the negative ones were not significantly associated with lower trust. Therefore, HI and H6 were partially supported. For institutional and heuristic reasons, either positive or negative reasons did not exert any significant effects on trust. Therefore, H2 and H3 were not supported. Regarding the calculative and interactive reasons, as we posited, the positive reasons were significantly associated with higher trust and the negative reasons were significantly associated with lower trust. Hence, both H4 and H5 were supported.</page><page sequence="16">264 WANG AND BENBASAT Dispositional Dispositional Reason ' /[ Reason Institutional ' ?'*' Institutional Reason ' ' / / Reason Heuristic Ì N' As Heuristic Reason n.s.^N^ S^Z ^^7*^ ^^--n-s.-"" Reason V Z / ^Üü3r Trust m RA yír^ _ Calculative )^0A9"'//K^ ^ (ä2 = (K50) ^^&gt; ^ niR- U.l» Í Calculative Reason y"/ ^ ^ '' niR- U.l» Reason ^ ^ 0.30"/ ' -0.26" p ; Interactive / / ' N, Interactive Reason Reason ' ., ' V / 0.32 ., ' V J Knowledge- 1/ " '| Knowledge- Based Reason &gt;[ Based Reason Figure 1. PLS Results Notes: Factors on the left-hand side are positive reasons and those on the right-hand side are negative reasons, n.s. = nonsignificant; * p &lt; 0.05; ** p &lt; 0.01. Discussion Findings This study was aimed at revealing the underlying reasons about the extent of trust in decision support technologies. Conjunctive reasons are involved when forming trusting beliefs [23, 31]. By extending theories of trust formation in interpersonal and organizational contexts to that of decision support technologies, we identify six reasons why users form a certain level of trust in a technology in the early stage of their use of the technology. The causal strengths of the positive and negative reasons are revealed from a structural equation modeling analysis. The results show that four reasons (i.e., knowledge based, interactive, calculative, and dispositional) influence users' extent of trust in RAs, while institutional and heuristic reasons do not. Among the four influential reasons, three (i.e., calculative, interactive, and knowledge-based) are triggered by a trustor's experiential use of technology. Hence, we conclude that trust in an RA can be mainly attributed to the experiential use of an RA. Furthermore, we found that the causal strengths of positive and negative reasons are not entirely asymmetric. Overall, the positive reasons are more influential than the negative ones. Particularly, for the dispositional and knowledge-based reasons, only the positive reasons are associated with higher trust but the negative reasons are not associated with lower trust. This is different from earlier observations in the interpersonal and organizational contexts that negative events and cognitions are generally more influential in people's judgments than positive events and cognitions [29, 35]. Our finding shows that trust can be robust, especially when it is built upon</page><page sequence="17">ATTRIBUTIONS OF TRUST IN DECISION SUPPORT TECHNOLOGIES 265 an experiential use of the technology (e.g., [40, 43]). This is probably because people consider behavioral experiences as reliable and readily accessible in their memory [43], thus exerting stronger effects on the formation of trusting beliefs. Limitations Before discussing the implications of this study, we first consider its limitations. The results are based only on a single type of Web-based decision support technol- ogy - namely, RAs - which may limit the generalizability of the study's findings. The reasons about trust in different types of technologies as well as their relative causal strengths may vary. The literature on trust shows that a need for trust requires some level of risk [39]. In this study, we focus on decision support technology because of decisional risks involved when delegating some decision tasks to the technology. When using a different type of technology for a certain task, the risks involved in using the technology may be different, thus influencing consumers' involvement in the task. According to the elaboration likelihood model (e.g., [49]), under different levels of motivation and involvement, the extent to which a person scrutinizes and examines a trustee will differ. When the involvement is low, people will rely more on heuristic cues; otherwise, they will base their judgment more on their substantial interactions and the systematic processing of information about the trustee. Our experimental task of using RAs for online shopping may have obtained a relatively high level of par- ticipant involvement. As a result, consumers' cognitive evaluations may rely more on their interaction with the RA and their knowledge about the RA than on the heuristic cues. This may explain why the heuristic reasons are not influential. Future research would therefore be required to understand why trust is formed in different types of decision support technology for various contexts. The findings of this study may also be limited in that the experimental tasks did not involve real purchases, although the participants were instructed to treat the experimen- tal tasks as if they were real. The participants were only asked to decide what camera models to buy with the support of an RA. Therefore, the participants focused primarily on the RA rather than on the e- vendor or other environmental and institutional factors that could be more relevant when a real purchase is involved. This may explain why the institutional reason did not appear to influence trust in this study. Future research is therefore needed to examine the trust attributions in a real shopping environment where consumers' concerns regarding the vendor may be stronger and, as a result, where the causal strengths of trust attributions may differ. Furthermore, the written protocol method has its limitations. In this study, we used retrospective written protocols (i.e., protocols were collected after the experimental tasks). However, this methodology requires participants to recall experiences after completing the experimental tasks, which might have allowed them to systematically reorganize their perceptions and cognitions [62]. Moreover, the essay questions used to elicit the written protocols may have caused the participants to act more self-reflectively than usual to justify their trusting beliefs [19], which may systematically increase the quantity of episodes. Nevertheless, this study is not solely based on the quantity of</page><page sequence="18">266 WANG AND BENBAS AT episodes to judge the influence of trust attributions. Instead, a structural model was tested to reveal the relative causal strengths. It was not evident that participants might have overjustified their trust with a particular reason. Therefore, the threat from using a written protocol approach should not be a concern. Implications for Trust Research This study makes several contributions to the research on trust. Specifically, the empiri- cal investigation of the underlying reasons why trust is formed in an online decision support technology is its major contribution to research and to the trust theory. Prior research on trust, in contrast, has primarily focused on interpersonal and organizational trust while largely neglecting trust in technological artifacts [27, 66, 67]. Particularly, the present study identified some distinctive features of trust formation in online decision support technologies, RAs for e-commerce to be specific. We found that in the early development of trust in RA technologies for online shopping tasks, users mainly rely on their experiential interactions with the technologies. This finding likely results from the limited cues available to judge the trustworthiness of RAs in online environments as compared to a human advisor's (e.g., a salesperson's expressions and tone, which can be used to judge them in traditional shopping environments) [66]. In addition, as Bengtsson et al. suggested, "they [computers] are unable to supply the kind of contingent and fully synchronous interaction that is present in FtF [face- to-face] conversation" [7], whereas "humans have more behavioral resources at their disposal to achieve an appealing and credible demeanor, and they are better able to adapt their conversations if there are indications that their image is suffering" [7], Because computers, to some extent, lack compensatory and adaptive responses, they are at a "disadvantage" within the context of human-technology interaction and, as a result, they may not gain the users' trust. As a substitute, consumers may interact more freely and extensively with an RA, inasmuch as there are no strict social routines that limit one's interactions with an RA [28, 33]. This may offset the shortcomings of human-computer interactions in terms of adaptability of technology. Therefore, differences may exist in the formation of interpersonal versus technological trust that are induced by different cues and interactions; presumably, a user considers an RA as a more objective advice-giving entity that exhibits fewer social cues on which to base trust than a person who advises the user [28]. This also explains why the heuristic reason is not influential in the formation of trust in RAs. Moreover, our protocol analysis results confirm that simultaneous positive and nega- tive trust attributions exist in the early formation stage of trust in RAs and that their causal effects on trust are not totally asymmetric. Trust can be robust, especially when it is built upon experiences [43]. Previous trust studies primarily "discuss asymmetry in the consequences of trust versus distrust (rather than the determinants of trust)" [57, p. 20] and explore the asymmetric effects of consumer judgments on trust versus distrust (e.g., [10]). Few studies have specifically examined the asymmetric effects of positive versus negative trust reasons. Our research represents an early research</page><page sequence="19">ATTRIBUTIONS OF TRUST IN DECISION SUPPORT TECHNOLOGIES 267 effort in this regard and more future research is needed to systematically examine such asymmetric effects. Implications for Practice By revealing the reasons why users trust online decision support technologies, this study provides several important practical design guidelines and implications (for RAs, in particular). The most influential reasons are knowledge based, interactive, and calculative, each of which can be engendered by different technological features and functionalities as discussed next. Knowledge-Based Reason Explanation facilities need to be embedded in online decision support technologies to effect a knowledge-based reason [67]. For instance, an RA should inform consumers about how and why it performs in a certain way so that users can better understand its behavior. It should also provide consumers the necessary knowledge and guidance (i.e., the trade-offs among different product attributes) for them to make informed choices in their interactions with the RA [67]. The data from this study reveal to what extent trust in an RA is enhanced when it provides explanation facilities (results are reported in Wang and Benbasat [67]). A comparison of the subjects in the "with" explanation treatment to those in the "without" shows that when explanations are available, the number of positive episodes of the knowledge-based reason increased fourfold (i.e., from 3.5 to 16.5), while the number of negative episodes dropped from 17 to 1. This confirms the effects of explanation facilities on promoting trust in RAs due to a knowledge-based reason. There are also other means to prompt the knowledge-based reason. For example, as revealed in the written protocols, feedback about an RA from other consumers can be a source of additional information. Likewise, providing a discussion forum about an RA can assist a consumer in eliciting feedback about the RA from other consumers. These additional functionalities may help supply additional information about the RA and, consequently, prompt trust in the RA. Interactive Reason Consumers' expectations about RA design and performance can influence trust in the RA due to an interactive reason. For instance, as inferred from the written protocols, participants with different levels of product expertise expected an RA to behave in different ways. Those with low levels of expertise expected the RA to ask questions related to the basic features of cameras, whereas those with high levels of expertise expected the RA to elicit their requirements on both basic and advanced features. Accordingly, RA-user dialogues should be designed in a personalized manner, as suggested by Komiak and Benbasat [27], and in an adaptive fashion in order to pre-</page><page sequence="20">268 WANG AND BENBASAT sent different types of questions and different levels of sophistication expected by the different segments of the user population [54]. Another approach to foster trust in an RA based on an interactive reason is that the decision strategy used by an RA should enable it to find suitable products in a way expected by the consumers. Consumers use different decision strategies (e.g., elimination-by-aspect, additive compensatory) to choose from multiple product al- ternatives (e.g., [63, 64]). The strategy employed by an RA should therefore be flex- ible so as to cater to different consumers' decision strategy preferences. Otherwise, an RA that employs a decision strategy not congruent with what consumers prefer and expect leads to expectation disconfirmation. To increase flexibility, providing multiple strategies and enabling users to control the decision strategy are therefore desirable [56]. Calculative Reason One of the key factors associated with the calculative reason is the gains from the RA use (perceived usefulness). A major benefit of using RAs is the facilitation of con- sumers' decision making. However, one weakness of current RA applications is that consumers' abilities to evaluate the RA's product recommendations are limited. One solution is to embed multimedia technologies into RAs in order to facilitate consum- ers' product evaluation and enable them to express their preferences better. Virtual reality technology combined with RAs can enable consumers to not only express their preferences and obtain recommendations but also evaluate the product recommenda- tions and reconfigure their preferences. These tools can facilitate consumer decision making and deliver more benefits to users [26]. With these features and capabilities incorporated into RAs, recommendation services for customers will be more effective and more likely to earn users' acceptance. Conclusions As Web-based decision support technologies become increasingly available to boost customers' online experiences, trust in technologies plays a central role in the effec- tiveness of these technologies. Trust influences consumers' use of technologies [28, 66] and their perceptions of the Web store that provides these technologies [65]. This research reveals the reasons why users trust decision support technologies. Theories on interpersonal and organizational trust apply to the context of RAs, but the formation of trust in technology also involves distinctive elements of human-computer interac- tion. Understanding trust formation in RA-like technologies will assist researchers and practitioners in identifying the design that augments consumers' trust, which will consequently increase the effectiveness and acceptance of decision support technolo- gies. Future research is therefore recommended on RA designs that can enhance us- ers' trust in RAs. With enhanced trust in technologies, organizations can attract more customers and facilitate their relationship building with customers [52].</page><page sequence="21">ATTRIBUTIONS OF TRUST IN DECISION SUPPORT TECHNOLOGIES 269 Acknowledgments: The work described in this paper was partially supported by grants from the Natural Sciences and Engineering Research Council of Canada (NSERC), and the Research Grants Council of Hong Kong S.A.R. (Project No. CityU 1498/06H). The authors are grateful to special issue guest editor Paul A. Pavlou and the anonymous reviewers for their valuable comments on earlier versions of this paper. They also thank Lei Zhu and Lingyun Qiu, who assisted in coding the written protocols, and Kevin Chen, who helped build the experimental Web site and decision aids. Notes 1 . A detailed discussion on people's social responses to technology is provided in the online Supplement A (http://wang.CityU.net/PaperSupplement/JMIS07/default.html#A). 2. These reasons have also been referred to as bases and antecedents of trust, trust processes, or even trust per se. The trust literature has a divergence of views in this regard. For example, Gefen and Sträub [17] and McKnight and Chervany [40] summarized a set of antecedents of trust, such as calculative-based and knowledge-based trust. Similarly, Kramer [29] suggested several bases of trust that promote the emergence of trust, and Zucker [68] proposed several modes of trust production. Nevertheless, it has been acknowledged that trust is difficult to measure directly and that people use indirect measures (e.g., these antecedents as indices or signals) to determine the level of trust [68, p. 54]. As a result, these reasons, antecedents, and bases of trust have been referred to as different types of trust as well. 3. Trust development can go beyond the initial stage when users interact with an RA many times. In such a stage, other reasons may explain the extent of trust, such as the identification- based reason suggested by Lewicki and Bunker [35]. Because we focus on trust formation in early stages, reasons that justify the late stages of trust formation are excluded. 4. As introduced in the second section, trust in RA includes three belief dimensions (i.e., competence, integrity, and benevolence). Given the limited work in the area of trust formation in decision support technology, it is difficult to posit the causal effects of a trust reason on a particular trusting belief. Therefore, the hypotheses were not formulated in the belief dimen- sion level. 5. Screenshots of the RA interfaces are provided in the online Supplement B (http://wang. CityU.net/PaperSupplement/JMIS07/default.html#B). Figure 1 provides a Screenshot of the RA-user dialogue and Figure 2 provides a Screenshot of product recommendations arising from the RA-user dialogue. 6. The product database used in the recommendation agents contained 134 products. Most of the product models available in the market at the time of the experiment were included. The exact number of recommended products depended on the requirements elicited from the participants. 7. More discussion is provided in the Limitations section. 8. Future research is needed to investigate whether or not the ownership of an RA makes a difference in users' trust attributions as well as the relative effects of the attributions. 9. The training session also helps deal with potential novelty effects. At the same time, to ensure that the trust formation examined in this study is still in the early stage, the training materials used PowerPoint slides showing Screenshots of the tutorial RA, but the participants did not have real interactions with the RAs during the training session. 10. Our pilot test revealed that many participants were not very confident in evaluating an RA after completing only one task. After two tasks, the participants were able to form a certain level of initial trust in the RA. 1 1 . We used the term virtual advisor to refer to the RA because participants m our pilot test suggested that this term would be easier to understand than the term recommendation agent. 12. Landis and Koch [30] suggested a kappa ranging from 0.61 to 0.80 as a benchmark ot high agreement. 13. Detailed measurement validation can be found in Wang and Benbasat [66], which provided nomological validity for the measures of trust in RAs and examined an integrated trust-TAM for user acceptance of RAs.</page><page sequence="22">270 WANG AND BENBAS AT References 1. Andersen, V.; Hansen, C.B.; and Andersen, H.H.K. Evaluation of agents and study of end-user needs and behaviour for e-commerce. Research Report, Riso National Laboratory, Roskilde, Denmark, 2001. 2. Ansari, A.; Essegaier, S.; and Kohli, R. Internet recommendation systems. Journal of Marketing Research, 37, 3 (2000), 363-375. 3. Atkinson, S., and Butcher, D. Trust in the context of management relationships: An em- pirical study. S.A.M. Advanced Management Journal, 68, 4 (2003), 24-33. 4. Bandura, A. Self-Efficacy: The Exercise of Control. New York: Freeman, 1997. 5. Barclay, D.; Thompson, R.; and Higgins, C. The partial least squares (PLS) approach to causal modeling: Personal computer adoption and use as an illustration. Technology Studies, 2, 2 (1995), 285-309. 6. Benbasat, I. Laboratory experiments in information system studies with a focus on individuals: A critical appraisal. In I. Benbasat (ed.), The Information Systems Research Chal- lenge: Experimental Research Methods, vol. 2. Boston: Harvard Business School Press, 1989, pp. 33^7. 7. Bengtsson, B.; Burgoon, J.K.; Cederberg, C; Bonito, J.; and Lundeberg, M. The impact of anthropomorphic interfaces on influence, understanding, and credibility. In R.H. Sprague Jr. (ed.), Proceedings of the Thirty-Second Annual Hawaii International Conference on System Sciences. Los Alamitos, CA: IEEE Computer Society, 1999 (available at http://csdl2.computer. org/comp/proceedings/hicss/1999/0001/01/0001 105 1 .PDF). 8. Bhattacherjee, A. Individual trust in online firms: Scale development and initial test. Journal of Management Information Systems, 19, 1 (Summer 2002), 211-241. 9. Cacioppo, J.T., and Berntson, G. Relationship between attitudes and evaluative space: A critical review, with emphasis on the separability of positive and negative substrates. Psycho- logical Bulletin, 115, 3 (1994), 401-423. 10. Cho, J. The mechanism of trust and distrust formation and their relational outcomes. Journal of Retailing, 82, 1 (2006), 25-35. 11. Cohen, J. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20, 1 (1960), 37^6. 12. Corritore, C.L.; Kracher, B.; and Wiedenbeck, S. On-line trust: Concepts, evolv- ing themes, a model. International Journal of Human-Computer Studies, 58, 6 (2003), 737-758. 13. Das, T.K., and Teng, B.-S. Between trust and control: Developing confidence in partner cooperation in alliances. Academy of Management Review, 23, 3 (1998), 491-513. 14. Doney, P.M., and Cannon, J.P. An examination of the nature of trust in buyer-seller relationships. Journal of Marketing, 61, 1 (1997), 35-51. 15. Donnelly, P. Take my word for it: Trust in the context of birding and mountaineering. Qualitative Sociology, 17, 3 (1994), 215-241. 16. Dzindolet, M.T.; Peterson, S.A.; Pomranky, R.A.; Pierce, L.G.; and Beck, H.P. The role of trust in automation reliance. International Journal of Human-Computer Studies, 58, 6 (2003), 697-718. 17. Gefen, D., and Sträub, D. Managing user trust in B2C e-services. e-Service Journal, 2, 2 (2003), 7-24. 18. Gefen, D.; Karahanna, E.; and Sträub, D.W. Trust and TAM in online shopping: An integrated model. MIS Quarterly, 27, 1 (2003), 51-90. 19. Gould, S. An interpretive study of purposeful, mood self-regulating consumption: The con- sumption and mood framework. Journal of Psychology &amp; Marketing, 14, 4 (1997), 395-426. 20. Gregor, S., and Benbasat, I. Explanations from intelligent systems: Theoretical founda- tions and implications for practice. MIS Quarterly, 23, 4 (1999), 497-530. 21. Grenci, R.T., and Todd, P.A. Solutions-driven marketing. Communications of the ACM, 45, 3 (2002), 65-71. 22. Häubl, G., and Trifts, V. Consumer decision making in online shopping environments: The effects of interactive decision aids. Marketing Science, 19, 1 (2000), 4-21. 23. Hewstone, M. Causal Attribution: From Cognitive Processes to Collective Beliefs. Cam- bridge, MA: Basil Blackwell, 1989.</page><page sequence="23">ATTRIBUTIONS OF TRUST IN DECISION SUPPORT TECHNOLOGIES 27 1 24. Hostler, R.E.; Yoon, V.Y.; and Guimarães, T. Assessing the impact of Internet agent on end users' performance. Decision Support Systems, 41, 1 (2005), 313-323. 25. Jarvenpaa, S.L., and Leidner, D.E. Communication and trust in global virtual teams. Organization Science, 10, 6 (1999), 791-815. 26. Jiang, Z.; Wang, W.; and Benbasat, I. Multimedia-based interactive advising technology for online consumer decision support. Communications of the ACM, 48, 9 (2005), 92-98. 27. Komiak, S.Y.X., and Benbasat, I. The effects of personalization and familiarity on trust and adoption of recommendation agents. MIS Quarterly, 30, 4 (2006), 941-960. 28. Komiak, S.Y.X.; Wang, W.; and Benbasat, I. Trust building in virtual salespersons versus in human salespersons: Similarities and differences. e-Service Journal, 3, 3 (2004-5), 49-63. 29. Kramer, R.M. Trust and distrust in organizations: Emerging perspectives, enduring ques- tions. Annual Review of Psychology, 50 (1999), 569-598. 30. Landis, J.R., and Koch, G.G. The measurement of observer agreement for categorical data. Biometrics, 33, 1 (1977), 159-174. 31. Leddo, J.; Abelson, R.P.; and Gross, P.H. Conjunctive explanations: When two reasons are better than one. Journal of Personality and Social Psychology, 47, 5 (1984), 933-943. 32. Lee, B.-K., and Lee, W.-N. The effect of information overload on consumer choice quality in an on-line environment. Psychology &amp; Marketing, 21, 3 (2004), 159-183. 33. Lee, J.D., and See, K.A. Trust in automation: Designing for appropriate reliance. Human Factors: The Journal of the Human Factors and Ergonomics Society, 46, 1 (2004), 50-80. 34. Lee, M.K.O., and Turban, E. A trust model for consumer Internet shopping. International Journal of Electronic Commerce, 6, 1 (Fall 2001), 75-91. 35. Lewicki, R.J., and Bunker, B.B. Trust in relationships: A model of trust development and decline. In B.B. Bunker and J.Z. Rubin (eds.), Conflict, Cooperation, and Justice. San Francisco: Jossey-Bass, 1995, pp. 133-173. 36. Lewicki, R.J.; McAllister, D.J.; and Bies, R.J. Trust and distrust: New relationships and realities. Academy of Management Review, 23, 3 (1998), 438-458. 37. Luhmann, N. Trust and Power. New York: Wiley, 1979. 38. Maes, P. Agents that reduce work and information overload. Communications of the ACM, 57,7(1994), 31-40. 39. Mayer, R.C.; Davis, J.H.; and Schoorman, F.D. An integrative model of organizational trust. Academy of Management Review, 20, 3 (1995), 709-734. 40. McKnight, D.H., and Chervany, N.L. What builds system troubleshooter trust the best: Experiential or non-experiential factors? Information Resources Management Journal, 18, 3 (2005), 32-49. 41. McKnight, D.H.; Choudhury, V.; and Kacmar, C. Developing and validating trust mea- sures for e-commerce: An integrative typology. Information Systems Research, 13, 3 (2002), 334-359. 42. McKnight, D.H.; Choudhury, V.; and Kacmar, C. The impact of initial consumer trust on intentions to transact with a Web site: A trust building model. Journal of Strategic Information Systems, 11, 3^ (2002), 297-323. 43. McKnight, D.H.; Cummings, L.L.; and Chervany, N.L. Initial trust formation in new organizational relationships. Academy of Management Review, 23, 3 (1998), 473-490. 44. Meyerson, B.; Weick, K.E.; and Kramer, R.M. Swift trust and temporary groups. In R.M. Kramer and T.R. Tyler (eds.), Trust in Organizations: Frontiers of Theory and Research. Thousand Oaks, CA: Sage, 1996, pp. 166-195. 45. Mingers, J. Combining IS research methods: Towards a pluralist methodology. Informa- tion Systems Research, 12, 3 (2001), 240-259. 46. Muir, B.M. Trust between humans and machines, and the design of decision aids. Inter- national Journal of Man Machine Studies, 27, 5-6 (1987), 527-539. 47. Pavlou, P.A. Institution-based trust in interorganizational exchange relationships: The role of online B2B marketplaces on trust formation. Journal of Strategic Information Systems, 11, 3-4 (2002), 215-243. 48. Pavlou, P.A., and Gefen, D. Building effective online marketplaces with institution-based trust. Information Systems Research, 15, 1 (2004), 37-59. 49. Petty, R.E., and Cacioppo, J.T. The elaboration likelihood model of persuasion. Advances in Experimental Social Psychology, 79(1986), 123-205.</page><page sequence="24">272 WANG AND BENBASAT 50. Priester, J.R., and Petty, R.E. The gradual threshold model of ambivalence: Relating the positive and negative bases of attitudes to subjective ambivalence. Journal of Personality and Social Psychology, 71, 3 (1996), 431-449. 51. Reeves, B., and Nass, C. The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places. New York: Cambridge University Press, 1996. 52. Reibstein, DJ. What attracts customers to online stores, and what keeps them coming back? Journal of the Academy of Marketing Science, 30, 4 (2002), 465-473. 53. Rotter, J.B. Generalized expectancies for interpersonal trust. American Psychologist, 26, 5 (1971), 443-452. 54. Russo, J.E. Aiding purchase decisions on the Internet. Paper presented at the Winter 2002 2002 SSGRR International Conference on Advances in Infrastructure for Electronic Business, Education, Science, and Medicine on the Internet. L'Aquila, Italy, July 29- August 4, 2002. 55. Shapiro, S.R The social control of impersonal trust. American Journal of Sociology, 93, 3 (1987), 623-658. 56. Silver, M.S. Systems that Support Decision Makers: Description and Analysis. Chichester, UK: John Wiley &amp; Sons, 1991. 57. Sirdeshmukh, D.; Singh, J.; and Sabol, B. Consumer trust, value, and loyalty in relational exchanges. Journal of Marketing, 66, 1 (2002), 15-37. 58. Sitkin, S.B., and Roth, N.L. Explaining the limited effectiveness of legalistic "remedies" for trust/distrust. Organization Science, 4, 3 (1993), 367-392. 59. Stewart, K.J. Trust transfer on the World Wide Web. Organization Science, 14, 1 (2003), 5-17. 60. Tan, Y.-H., and Thoen, W. Toward a generic model of trust for electronic commerce. International Journal of Electronic Commerce, 5, 2 (Winter 2000-1), 61-74. 61 . Tan, Y.-H., and Thoen, W. Formal aspects of a generic model of trust for electronic com- merce. Decision Support Systems, 33, 3 (2002), 233-246. 62. Todd, P., and Benbasat, I. Process tracing methods in decision support systems research: Exploring the black box. MIS Quarterly, 11,4 (1987), 493-512. 63. Todd, P., and Benbasat, I. An experimental investigation of the impact of computer based decision aids on decision making strategies. Information Systems Research, 2, 2 (1991), 87-115. 64. Todd, P., and Benbasat, I. The use of information in decision making: An experimental investigation of the impact of computer-based decision aids. MIS Quarterly, 16, 3 (1992), 373-393. 65. Urban, G.L.; Sultan, F.; and Quails, W.J. Placing trust at the center of your Internet strat- egy. Sloan Management Review, 42, 1 (2000), 39-^8. 66. Wang, W., and Benbasat, I. Trust in and adoption of online recommendation agents. Journal of the AIS, 6, 3 (2005), 72-101. 67. Wang, W., and Benbasat, I. Recommendation agents for electronic commerce: Effects of explanation facilities on trusting beliefs. Journal of Management Information Systems, 23, 4 (Spring 2007), 217-246. 68. Zucker, L.G. Production of trust: Institutional sources of economic structure, 1840-1920. Research in Organizational Behavior, 8 (1986), 53-111.</page><page sequence="25">ATTRIBUTIONS OF TRUST IN DECISION SUPPORT TECHNOLOGIES 273 Appendix Trust Measures Trusting Belief-Competence 1. This virtual advisor is like a real expert in assessing digital cameras. 2. This virtual advisor has the expertise to understand my needs and preferences about digital cameras. 3. This virtual advisor has the ability to understand my needs and preferences about digital cameras. 4. This virtual advisor has good knowledge about digital cameras. 5. This virtual advisor considers my needs and all important attributes of digital cameras. Trusting Belief-Benevolence 1 . This virtual advisor puts my interest first. 2. This virtual advisor keeps in mind my interests. 3. This virtual advisor wants to understand my needs and preferences. Trusting Belief-Integrity 1. This virtual advisor provides unbiased product recommendations. 2. This virtual advisor is honest. 3. I consider this virtual advisor to be of integrity.</page></plain_text>