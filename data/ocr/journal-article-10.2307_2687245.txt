<plain_text><page sequence="1">EDITOR MEDIA Warren Page HIGHLIGHTS Department of Mathematics New York City Technical College 300 Jay Street Brooklyn, NY 11201 with Donald W. Bushaw Washington State University Pullman, WA 99164 Annie and John Selden Tennessee Technological University and MERC Cookeville, TN 38501 This column allows readers to monitor a broad spectrum of publications, professional activities, and instructional resources. Readers are encouraged to submit items that will be of interest to colleagues in the mathematical community. Send all contributions to Warren Page. Mathematical Recreations: The Anthropomorphic Principle, Ian Stewart. Scientific American 273:6 (December 1995) 104-106. Murphy's Law states that if something can go wrong, it will. An immediate corollary is that your toast will fall off the table and, when it does, it will fall buttered side down. A recent mathematical analysis of buttered toast failing off tables has shown that it is not Murphy's Law that plagues us; it is physics. Start with a set of reasonable assumptions: A normal piece of buttered toast is lying buttered side up on the edge of a table of average height when it slips over the edge and falls to the floor. In failing over the edge, the toast is given an angular velocity. Simple calculations show that the toast will rotate through approximately 180? during the fall and, thus, land buttered side down. For the toast to rotate through 360? and land butter up, the table would have to be roughly 10 feet high. Besides resorting to sitting at incredibly high tables, we might also avoid Murphy's Toast Law by swatting our bread sideways as it falls off the table. A horizontal velocity of at least 1.6 meters per second gives a better chance of the toast's landing buttered side up. Consequently, Murphy's Law, at least as it applies to toast, is destined to butter our collective carpets forever. There is no escape. Indeed, the problem is not limited to our planet! On any planet where bipedal creatures remotely like us sit at tables and eat buttered toast, the law will prevail. It can be shown that the height of a bipedal creature has an upper bound that depends upon the prevailing gravitational forces. This bound is just under 10 feet. Assuming that such creatures use tables of roughly half their own height (as we do), toast failing buttered side down would truly be a universal event. DNS-H Posing Mathematical Problems: An Exploratory Study, Edward A. Silver et al. Journal for Research in Mathematics Education 27:3 (May 1996) 293-309. The NCTM Standards call for students to have experience formulating their own problems. Little is known, however, about the problem-posing tendencies of teachers themselves. Are they comfortable posing "what-if-not" questions, which challenge a problem's givens? Will they alter what a problem asks for (change its goal)? In this study, 28 prospective secondary mathematics teachers and 53 middle school teachers considered this scenario: On billiard tables measuring 6 by 4 feet and 4 by 2 feet, a ball is shot at a 45? angle from one corner, eventually landing in a corner pocket. They had 10 minutes to pose questions prior to a 30-minute problem solving phase, in which they predicted the final destination of the ball for VOL 27, NO. 4, SEPTEMBER 1996 317</page><page sequence="2">tables of various sizes. During a final nve-minute period they generated additional questions. In the time allowed, 399 reasonable questions were posed, almost 40% of which involved manipulation of initial constraints, with the remainder involving variations of the goal. The questions, however, did not always have "nice" solutions. A&amp;JS Applying Elementary Probability Theory to the NBA Draft Lottery, Stephen Penrice. SIAMReview 37:4 (December 1995) 598-602. Many professional sports leagues try to balance the abilities of their teams through a "draft." The teams that pick early in the draft get the best choice of new players entering the league. Until the early 1980s, the National Basketball Association (NBA) assigned teams their draft positions by reversing the order of their rankings in the previous year, so the weakest team got to choose first. Some teams were suspected, however, of deliberately losing games late in the season to improve their position in the draft. In 1990 the league began to use a lottery in which weaker teams had better chances of higher draft positions, but with nothing guaranteed. In 1993, the league kept the basic system but changed the odds to improve the weaker teams' chances of winning high positions. Penrice's classroom note shows how to compute the expected value and variance of Xi, a random variable giving the draft position of the team holding position i prior to the draft. It also calculates E(X% ? i)2, which gives a measure of how far from its original position a team might expect to be after the lottery. The author then uses a Pascal program to produce tables using the 1990-93 system and the new system for the 11 NBA teams that do not make the playoffs. This article is a nice application of probability for beginning students, using nothing more advanced than basic concepts of probability and expectation. RNG Historical Development ofthe Newton-Raphson Method, Tjalling J. Ypma. SIAM Review 37:4 (December 1995) 531-551. From the way Newton's method for solving an equation f(x) = 0 is commonly introduced in nrst-semester calculus, one might guess that it originated with Newton's invention of the calculus. The true story is quite different, as this article describes. Iterative methods for solving equations date back to the ancient Greeks and Babylonians, but it is to Frangois Viete that Newton's method can be traced. Viete's method approximated the derivative by a finite differ? ence. However, his method applied only to polynomials and it found individual digits of the solution one by one. Newton studied Viete's method and a year later developed the secant method. It is interesting to note that this derivation is geometrical, involving no calculus, and preceded the development of "Newton's method," which applied only to polynomials. The method is derived algebraically, by retaining the lowest order terms in a binomial expansion. Later, in Principia Mathematica, Newton applied an iterative method to a nonpolynomial equation. The derivation was still geometrical and used no calculus. Joseph Raphson simpli- ned Newton's method for polynomial equations, but it was Thomas Simpson who finally used calculus to derive the general formulation with which we are familiar. The author of this arti? cle therefore suggests that a more appropriate name would be the Newton-Raphson-Simpson method. Transcripts of Newton's notes are included, allowing the reader to calculate along with the master. RNG Regular Dissections of an Infinite Strip, John E. Wetzel. Discrete Mathematics 146:1-3 (November 15, 1995) 263-269. Bro. U. Alfred Brousseau posed this problem in the early 1970s. Given m equally spacedpoints on a line in the plane and n equally spaced points on a parallel line, connect each ofthe m points to each ofthe n points by a straight line segment. Find the number of regions formed by the mn line segments in the infinite strip between the parallel lines. Wetzel derives formulas for the numbers V, E, and F of points of intersection, edges, and faces in the infinite strip in terms of counting numbers Lk(m, n) defined as follows. For Zmn = {(i, j)\l &lt; i &lt; ra, 1 &lt; j &lt; n} (the mxn rectangular lattice) and for k &gt; 2, we let Lk(m, n) denote the number of lines that meet Zmn in precisely k points. (For example, 1/2(3,2) = 9, 1/3(3,2) = 2, and Lfc(3,2) = 0 318 THE COLLEGE MATHEMATICS JOURNAL</page><page sequence="3">for k &gt; 4.) However, there are no known general formulas for Lk(m, n) in terms of familiar counting functions. Perhaps this paper will stimulate interest in finding such formulas. CHJ Problem-Based Mathematics?Not Just for the College-Bound, Lynne Alper et al. Educa? tional Leadership 53:8 (May 1996) 18-21. A Common Core of Math for All, Arthur F. Coxford and Christian R. Hirsch. Educational Leadership 53:8 (May 1996) 22-25. The Interactive Mathematics Program (IMP) and Core-Plus Mathematics Project are two inno- vative NSF-funded high school programs. Each tries to answer the call of the NCTM Standards for curricula that integrate traditional algebra, geometry, and trigonometry-precalculus topics, along with some statistics, within a cooperative, problem solving, calculator-based approach that emphasizes mathematical sense-making. For example, IMP approaches solving systems of linear equations through complex team projects such as Meadows or Mails?, in which stu? dents try to solve a city planning problem. In Core-Plus Math, rather than having work-prep, tech-prep, and college-prep tracks, all high school students take a three-year common core with varied fourth-year choices. For instance, an employment-bound senior might choose a course in computer-aided design or statistical process control. Each year of Core-Plus contains seven connected units and a thematic capstone experience. In a typical sophomore unit, stu? dents investigate rich applied problems such as Matrix Models (managing inventory and sales information for an athletic shoe store). Both projects are currently undergoing intense evalu- ations of effectiveness. Meanwhile, we note that David Smith of Duke has commented that if his first-semester reformed calculus students had completed only three years of Core-Plus, he could eliminate at least a third of his course. A&amp;JS Beyond the Last Theorem, Dorian Goldfeld. The Sciences 36:2 (March/April 1996) 34-40. In May 1995, Andrew Wiles and Richard Taylor published a proof of "Fermat's Last Theorem," thereby correcting a misnomer and making a theorem, at last, of a very famous conjecture. With all the hoopla in the press that accompanied this great achievement, much of what was really accomplished was overshadowed. In this excellent expository article, Goldfeld explains the real significance of the work done by Wiles and Taylor and what is likely to be the "next Holy Grail." Central to the Wiles-Taylor paper was a proposition that proved the bulk of the Shimura-Taniyama-Weil (STW) conjecture. This result establishes a kind of equivalence between elliptic curves with integer coefficients (curves of the form y2 = x3 + ax + b, where a and b are integers) and modular functions (functions that are periodic with respect to rigid motions in space). This link may well be exploited to give a proof of the ABC conjecture. And that, in turn, would provide a mechanism for systematically solving whole classes of Diophantine equations, of which Fermat's (xn +yn = zn) was only one. The ABC conjecture deals with two relatively prime integers, A and B, along with their sum, C. One considers the square-free part of the product, denoted sqp(ABC), where the square-free part of an integer, sqp(Ar), is defined to be the product of the distinct prime divisors of N. The conjecture states that for any value n &gt; 1, the quotient [sqp(ABC)]n/C has a lower bound. The Wiles-Taylor results, along with the work of Goldfeld and others, suggest that a proof of the ABC conjecture may well be close at hand. And with that, Hilbert's tenth problem may finally yield. DNS-H The Axiomatization of Linear Algebra: 1875-1940, Gregory H. Moore. Historia Mathe? matica 22:3 (August 1995) 262-303. How did the fundamental notions of vector space and module come to be isolated and ax- iomatized? The author's answer is not simple, but his story is fascinating and involves many well-known mathematicians. In his 1888 book on Grassmann's geometric calculus, Peano first isolated the abstract concept of a vector space over the reals. He called vector spaces linear systems, and even gave axioms for what are now called inner-product spaces. Thirty years later, Hermann Weyl axiomatized finite-dimensional real vector spaces and inner-product spaces in his book Space, Time, Matter. However, "neither Peano nor Weyl played a decisive role in the VOL. 27, NO. 4, SEPTEMBER 1996 319</page><page sequence="4">diffusion of axiomatic vector spaces." Several years later, Banach, Hahn, and Wiener indepen? dently axiomatized normal vector spaces, Banach's contribution being the most influential. Banach indicated that his use of postulates was for economy; he had to prove a result only once rather than repeatedly for the 10 different kinds of function spaces he was investigat- ing. In 1925, Frechet generalized normed vector spaces to topological vector spaces?vector spaces with a topology (not necessarily metric) in which vector addition and scalar multi? plication were continuous. In the 1930s, amid much active research in topological algebras, Kolmogoroff and von Neumann studied the role of convexity in topological vector spaces. Saunders MacLane explains why Banach's axiomatization of vector spaces was accepted while Peano's earlier one was not: "In the conceptual parts of mathematics, it is not the discovery but the courage and conviction of importance that plays a central role." In his 1871 work on algebraic number theory, Dedekind introduced both the notion of "ideal" and the term "module." His ideals were actually modules over a ring of algebraic integers in the modern sense, whereas his modules were not. A Dedekind module M was any subset of the complex numbers closed under addition and subtraction. His use of a = b (mod M) to mean a ? b is in M was modeled on Gauss's use of a = b (mod m) when m is an integer, which explains the historical connection between the linear algebra term module and the number theory term mod. The modern concept of module (also ring, ideal, and algebra) first appeared in Emmy Noether's groundbreaking papers of the 1920s. Her 1929 paper clearly separated the concepts of linear transformation and matrix (which had often been confused): "A linear transformation is a homomorphism of two modules of linear forms; a matrix is an expression (the representation) of this homomorphism by a definite choice of basis." Van der Waerden's highly influential two-volume textbook of 1930-1931, Moderne Algebra, played a substantial role in making the notion of vector space central. Ten years later, the most influential American book to treat modern algebra, Birkhoff and MacLane's A Survey of Modern Algebra, also prominently treated vector spaces over a field. Maclane said later that he had not understood the importance of the notion of module: "The first real recognition of the central role of a module is the Bourbaki volume on linear algebra (1947)." But Birkhoff was more critical: "Whereas MacLane and I had tried to temper the purism of van der Waerden's 'modern' algebra in our book, Bourbaki was ultramodern. For instance, his book on Linear Algebra discusses vector spaces after modules." Notwithstanding Birkhoff s opinions, MacLane and Birkhoff s 1967 book, Algebra, did present modules before vector spaces. PR Exactly How Did Newton Deal with His Planets?, S. K. Stein. Mathematical Intelligencer 18:2 (Spring 1996) 6-11. In 1687, at the urging of Edmund Halley, Isaac Newton published the Philosophiae Naturalis Principia Mathematica, giving the world a mathematical analysis of the motions of the planets. He derived his formulas using a mix of geometry and limits?but without recourse to his methods of fluxions, known today as calculus, which he had developed some 20 years earlier. This article describes what Newton did and how he did it. Stein uses Newton's reasoning and diagrams, but presents his arguments in contemporary terms. For Newton, geometric quantities were generated in time, with a line, for example, generated by the motion of a point, and a plane by the motion of a line. Stein describes how Newton used the geometry of curves to devise a method for calculating the measure of the force acting on an object in a given orbit, Then he shows how to apply the method to find the laws of centripetal force for bodies that revolve around circles, spirals, and ellipses, discovering in each case that force is inversely proportional to some power of displacement. His development is wonderfully diagrammed and demonstrates the beauty of the kinetic approach to geometry. EAM Percent: A Privileged Proportion, Melanie Parker and Gaea Leinhardt. Review of Educa? tional Research 65:4 (Winter 1995) 421-481. If you teach preservice teachers or have children in middle school, you will want to read this article. In a 1994 study, preservice elementary teachers were able to calculate 23% of 55, but not what percent 16 is of 55, When asked to explain the meaning of 25%, all indicated 320 THE COLLEGE MATHEMATICS JOURNAL</page><page sequence="5">it was 1 part out of 4, but over half did not see the relevance of 25 parts out of 100. Such difficulties with percentage are not new. For 70 years, studies have documented that many students, from seventh graders through preservice elementary teachers, tend to ignore the percent sign, making no distinction between 1/2 and 1/2%. Many commonly use a faulty algorithm as well, removing the percent sign and placing a decimal point to the left of the numeral, thereby obtaining 0.55 for 55% and 0.110 for 110%. For many years, students were asked to find one of A, B, or C, given the other two in A% of B = C, and they learned a separate rule for each case. In the late 1950s, proportionality became the emphasis, and students were taught to convert "25 is 10% of what?" into 25/x ? 10/100. Unfortunately, this often turned into the mnemonic "is over of equals percent over 100," with the is being 25 and the of being the unknown. Why is percentage so hard to learn? It has a long history, variously emphasizing its additive, part-whole, multiplicative, and ratio interpretations. In third-centuiy B.C. India, interest was additive?from "12 coins on every 100 coins" one could easily figure 24 coins on 200 or 36 on 300, a method favored by students today. In nfteenth-century Europe, the currency unit was explicitly stated with the interest, as in 6 florins per 100. By about 1650, this had become 6 fl. percent. With the emergence of statistics for collective data in the 1700s, percentage became abstracted for making standardized comparisons. From 1830 to 1860, arithmetic texts under- went massive content rearrangement, including more topics under percentage with specific rules for each problem type, but with little indication how the topics were related. Today's texts treat percentage in much the same way. Percentage has many interpretations. It can indicate the size of a subset (50% of the class) or describe increase/decrease comparisons (the new price is 25% of the original price; the price was increased by 25%; the number of girls is 25% of the number of boys; the number of boys is 300% more than the number of girls). The conciseness of the language of percentage also can confuse students. Often referents must be inferred. For example, the statement "the unemployment rate is currently 8%" gives neither the number of unemployed nor the total workforce. Whereas percent evokes a multiplicative schema, percent more than evokes an additive schema, so "A is 20% more than B" may need to be interpreted as "A is 120% of B." Addition and subtraction are inverse operations, yet when a price is increased by 5%, and then decreased by 5%, one does not get back to where one started. (Recently two university bookstore clerks asked us to explain this seeming paradox.) A&amp;JS The Magic of 3 x 3, Martin Gardner. Quantum 6:3 (January-February 1996) 24-26. Everyone is familiar with this 3x3 magic square 2 9 4 7 5 3 6 1 8 which consists of the first nine positive integers. Other interesting magic squares have been discovered. Some of these are made of consecutive composites, primes in arithmetic progres? sion, and palindromes. In 1987, the author offered $100 to anyone who found a 3 x 3 magic square made with consecutive primes. Using a Cray computer, Harry Nelson of Lawrence Liv- ermore Laboratories produced such a square and claimed the prize. Gardner now offers $100 to anyone who can find a 3 x 3 magic square of distinct squares. This question was originally asked by Martin LaBar in this journal [CMJ (1984), p. 69]. If such a magic square exists, its numbers are most certainly large. John Robertson has shown that the task is equivalent to finding an elliptic curve of the form y2 ? x3 ? n2x with three rational points, each the double of another rational point on the curve, having x-coordinates in arithmetic progression. CC CORE Mathematics at the United States Military Academy: Leading into the 21st Cen? tury, David C. Arney et al. Primus 5 (1995) 343-367. The United States Military Academy at West Point was founded by an Act of Congress on March 16, 1802. As the first scientific and technical school in the United States, it was modeled after successful schools in Europe, particularly the Ecole Polytechnique in Paris. Since its VOL. 27, NO. 4, SEPTEMBER 1996 321</page><page sequence="6">initial purpose was to train military engineers, the curriculum, especially in mathematics, was heavily weighted toward "applied" subjects. Nevertheless, the Academy has always been at the forefront of pedagogical and technological innovations in the classroom. As it heads into the next century, it has renewed its search for the best ways to educate future military leaders in the mathematics necessary for problem solving. This article outlines the entire mathematics curriculum at the Academy, including the Core Curriculum and the Interdisciplinary Lively Applications Projects (ILAP). The Core Curriculum consists of four courses: Discrete Dynamical Systems, Calculus I, Calculus II, and Probability and Statistics. The ILAP program enables the students to investigate substantial problems from other disciplines while acquiring mastery of these core courses. Among the projects conducted in the past two years were ones dealing with smog in the Los Angeles basin, pollution levels in the Great Lakes, aircraft ranges under various flight strategies, and oxygen consumption and lactic acid production. Although the Academy's specialized mission differs from that of most other colleges and universities, its mathematics curriculum manages to incorporate many of the elements of today's mathematics reform movement in a way worthy of emulation by other institutions. XJK Counting Pairs of Lattice Paths by Intersections, Ira Gessel et al. Journal of Combinatorial Theory Series A 74:2 (May 1996) 173-187. Consider a walk onanrx(n-r) rectangular lattice that begins at the lower left corner (say, the origin), proceeds by unit steps either right or up, and ends at the upper right corner?the point (r, n ? r). The number of such walks is (?). Now suppose we ask for the number of ordered pairs of such walks that intersect in exactly k points. A point counts as an intersection point if it lies in the intersection of the sets of vertices of the two walks and is neither the initial nor the terminal vertex. The number of such ordered pairs of walks is denoted by N?,r. Note that N^ = ("). At the opposite extreme, it is known that A^'r = ^ (n~x) (?Zi)- (This is an interesting exercise.) In this paper, the authors establish two explicit formulas for N?,r. As a consequence, they obtain the equality N?,r = 2Nq,t; that is, there are twice as many walks having one intersection as having no intersection. The article ends with a number of related results, including a probabilistic variant of the problem. OU Incompatible Goals: Narratives of Graduate Women in the Mathematics Pipeline, Frances K. Stage and Sue A. Maple. American Educational Research Journal 33:1 (Spring 1996) 23-51. This study looks at the experiences of seven women, aged 27 to 47, who opted for Ph.D. programs in education, rather than mathematics, after successfully completing bachelor's or master's degrees in mathematics. Two had even begun Ph.D. work in mathematics. Most remembered a particular person responsible for their consideration of a mathematics major. They enjoyed problem solving, found mathematics challenging, and had done well in it. As they progressed through undergraduate and graduate mathematics, however, their earlier positive experiences, including group problem solving and caring teachers, were replaced by perceptions of isolation, competitiveness, and an uncaring faculty. The authors, whose research interests include college student experiences, question the wisdom of counseling women into the "hard sciences" without special preparation, and they suggest that faculty members could help by establishing support networks. A&amp;JS Remembering Olga Taussky Todd, Chandler Davis. Association for Women in Mathematics Newsletter 26:1 (January-February 1996) 7-9. This is a remembrance of Olga Taussky Todd?the woman and the mathematician. The salient points of her life and work are discussed. She was born in 1906 in the Moravian part of the Austro-Hungarian empire. After leaving in 1934 and spending a year at Bryn Mawr, she went to Girton College Cambridge. In 1938 she married John Todd, a classical analyst. After World War II, the couple moved to the United States and were associated with the National Bureau of Standards, first in Washington, DC, and then in Los Angeles. In 1957, the Todds joined the faculty of Caltech, he as professor and she as research associate. Eventually, in 1971, she 322 THE COLLEGE MATHEMATICS JOURNAL</page><page sequence="7">also received the rank of professor. She is most identified with the area of "real and complex matrix theory," or "linear algebra and applications." Many of her doctoral students have had major roles in the burgeoning of matrix theory. A warm human being, she always tried to ease the way of younger women in mathematics. LSG Editor's note. The May 1996 Media Highlight " Furman University Electronic Journal of Undergraduate Mathematics" was written by Warren Page. Media Correspondents CC Curtis Cooper; RNG Raymond N. Greenwell; LSG Louise S. Grinstein; CHJ Charles H. Jepsen; VJK Victor J. Katz; EAM Elena A. Marchisotto; PR Peter Ross; A&amp;JS Annie and John Selden; DNS-H David N. Seppala-Holtzman. jfflr</page></plain_text>