<plain_text><page sequence="1">1973] 379 Combining Possibly Related Estimation Problems By B. EFRON and C. MoRRIs Stanford University The RAND Corporation [Read before the ROYAL STATISTICAL SOCIETY at a meeting organized by the RESEARCH SECTION, on Wednesday, May 9th, 1973, Professor J. DURBIN in the Chair] SUMMARY We have two sets of parameters we wish to estimate, and wonder whether the James-Stein estimator should be applied separately to the two sets or once to the combined problem. We show that there is a class of compromise estimators, Bayesian in nature, which will usually be preferred to either alternative. "The difficulty here is to know what problems are to be combined together- why should not all our estimation problems be lumped together into one grand melee ?" GEORGE BARNARD commenting on the James-Stein estimator, 1962. Keywords: EMPIRICAL BAYES; JAMES-STEIN; SIMULTANEOUS ESTIMATION; COMBINING ESTIMATES; PARTIAL EXCHANGEABILITY 1. INTRODUCTION SUPPOSE that the statistician wishes to estimate parameters 01, 02,.., 6k where each Gi is the mean of an independent normal variate xi, xi I Oi -X(0j, D)- (1.1) James and Stein (1961) have shown that for k k 3 the estimator Si= [1- (k2) ]xi (1.2) S= 1 x2, is uniformly better than the maximum likelihood estimator 8? = xi when the loss function is the sum of squared errors. The James-Stein estimator seems to do the impossible. The estimate of each 6i is made to depend not only on xi but on the other xj, whose distributions seemingly are unrelated to Oi, and the result is an improvement over the maximum likelihood estimator no matter what the values of 01, 02, ..., I k The reaction of the statistical community to this tour deforce has been generally hostile, the usual suggestion being that this is some sort of mathematical trick devoid of genuine statistical merit. Thus we have the "speed of light" rhetorical question, "Do you mean that if I want to estimate tea consumption in Taiwan I will do better to estimate simultaneously the speed of light and the weight of hogs in Montana ?" (For a recent example see Kempthorne's discussion following Lindley and Smith, 1972.) Of course saying that the parameters are unrelated does not make them so, no matter what names are attached to them. If they all happen to be near 0 then ipso facto they are related in that sense, and it is exactly in this situation that the estimator</page><page sequence="2">380 EFRON AND MORRIS - Combining Possibly Related Estimation Problems [No. 3, (1.2) does much better than the m.l.e. The most favourable case occurs for l = 02 = ...= 6k = 0, when the risk of (1.2) is only 2/k that of the m.l.e. If the parameters do not happen to lie anywhere near 0 then the denominator S in (1.2) tends to be large and 8i is nearly equal to 8?, so there is no noticeable reduction in risk. We see that the James-Stein estimator combines possibly related estimation problems, the possible relation being proximity to the origin while the degree of combination depends on the strength of the relationship evidenced by the data. In spirit this is not much different from the less controversial "preliminary test estimator" which first tests the hypothesis Ho: 01 = 02...= Ok = 0, and estimates 6* either with 0 or xi as the test is accepted or rejected. The James-Stein rule, more exactly the "plus-rule" version where the bracketed factor in (1.2) is not allowed to be negative, makes a smooth transition between complete acceptance and complete rejection of IIO, and its risk function is uniformly better than that of the preliminary test estimator, as shown in Sclove et al. (1972). Other possible relations besides proximity to the origin can be exploited using variations of (1.2). In the discussion following Stein's (1962) paper Lindley suggested the estimator which pulls in toward the grand mean x = k-1 E4 xi instead of 0, = (k-3) D) (1.3) where S'= X* (x, - .)2. This estimator, which also dominates the m.l.e. for k,&gt; 4, does well when the Oi lie near each other, not necessarily near 0, the risk of (1.3) being 3/k times that of the m.l.e. if 01 = 02 .. = Ok- Table 1 gives an example of the use of K. At the beginning of the 1970 season the performances "at bat" of fourteen baseball players were observed for each player 45 times, and their batting averages (number of "hits" divided by 45) computed. TABLE 1 Application of the James-Stein estimator to predicting batting averages of 14 baseball players 1 2 3 4 5 6 7 83 0-400 0-378 0.356 0-333 0.311 0-311 0-289 i 0-303 0.299 0.294 0.289 0.284 0.284 0.278 6i 0.346 0.298 0.276 0.221 0.273 0.270 0.263 8 9 10 11 12 13 14 30 0.244 0.222 0-222 0.222 0.222 0.200 0.178 3S 0.268 0.263 0.263 0.263 0.263 0.257 0.251 Oi 0.269 0-264 0.256 0-304 0-264 0-285 0-319 These appear in row 1 of the table as 8?, the m.l.e. of the batters' true binomial probabilities of getting a hit. An arcsine transformation was performed on each 8*, (1.3) applied to the transformed data, and the inverse of the arcsine transformation</page><page sequence="3">1973] EFRON AND MoRRis - Combining Possibly Related Estimation Problems 381 performed to give the estimates Si in row 2. Each player's batting average for the remainder of the 1970 season, based on about 400 more at bats, is given in row 3 as the true value Oi of his binomial probability (though in fact it is only a more accurate estimator of the same quantity). It can be seen that S' is closer to Oi in all 14 cases, the ratio of sum of squared errors of &amp; to 8? being 0-20. The details of this example will be included in a forthcoming paper on applications of Stein's method, but the fact that the benefits of the Stein approach can be substantial in actual statistical practice, even without a knowledge of American baseball, should be obvious. One major criticism of the James-Stein result concerns the use of the sum of squares loss function. A Oi that is moderately far away from all the other Oj may be grossly misestimated by Si, the maximum risk for estimating a single parameter being about Vk times that of 89. In such a situation, it may be meagre consolation that the total sum of squares risk for all k parameters is smaller for (1.2) than for the m.l.e. However, in Efron and Morris (1972) we have shown that a simple modification of (1.2) or (1.3) sharply reduces the maximum single parameter risk at the cost of only a modest increase in total risk. Barnard's quoted comment on Stein (1961), really a more sensible statement of the "speed of light" criticism, raises another challenge to the James-Stein estimator. In practice the method's success in gaining a large advantage over the m.l.e. hinges upon the choice of estimation problems the statistician chooses to combine together in (1.2) or (1.3). In this paper we consider a simplified context where it is possible to answer Barnard's question. We will assume that the k parameters can be divided into two natural groups of size k1 and k2, where k1 + k2 = k. In the baseball example, for instance, there might be k1 left-handed batters and k2 right-handers. We wonder which is better, to apply the James-Stein estimator separately to the two groups, or to apply it once to the combined problem. Our answer is "usually, neither" (though if k1 and k2 are both large, say &gt; 10, the separate estimator will usually be preferable to the combined one). Instead we produce a class of compromise estimators which perform better than either extreme in reasonable situations. Essentially the compromise consists of letting the data decide to what degree the two groups should be combined for the purpose of James- Stein estimation. Once again we will be combining possibly related estimation problems, though now the individual problems themselves each involve several parameters. Our previous papers concerning Stein-type estimators, Efron and Morris (1972, 1973), are written from an empirical Bayes point of view. We assume that the parameters Oi are independently drawn from a normal prior distribution, X(O, A), with the "second-order parameter" A unknown to the statistician. Empirical Bayes estimators are formed by estimating A from the data x1, x2, . . ., Xk, and then using the ordinary Bayes estimators [1 - D/(A + D)] xi for the 6i, but with A replaced by its estimate. In particular using A = S/(k -2)- D, which makes D/(A + D) unbiased for D/(A + D), results in (1.2). (A more realistic approach takes the prior to be X(M, A), with M also unknown, but if we are content to estimate M with x this theory is equivalent to the simpler one with M = 0.) Lindley suggested this approach to understanding Stein's (1962) result. His rationale there was purely Bayesian, starting from a vague "prior prior" on A. Our papers, however, stray from the Bayesian straight and narrow by calculating risk functions R(A, 8) as afunction of A for various estimation rules S = (81, 825 Sk.).</page><page sequence="4">382 EFRON AND Mopaus - Combining Possibly Related Estimation Problems [No. 3, including (1.2) and (1.3). R(A, 8) is a Bayesian risk function in the sense that it is defined to be the average of the usual risk functions R(O, 8), 0 = (01, 02, *.. 60)0 over the independent X(O, A) priors on the 6i, but it is also a risk function in the Wald sense. That is, the performance of a fixed rule 8 is examined against a whole family of prior distributions and not just against the "right" prior, i.e. the one for which it is Bayes. The James-Stein rule has a particularly simple R(A, 8) risk function. Expressed in terms of the Relative Savings Loss, RSL(A, R(A, 8)-AD/(A + D) (1.4) (which is just the regret at using the empirical Bayes rule S instead of the actual Bayes rule, divided by the corresponding regret if the m.l.e. is used instead of the Bayes rule), (1.2) has RSL(A, 8) = 2/k for every value of A. This is easy to remember and furnishes a convenient par value for the comparison of other empirical Bayes estimators. The empirical Bayes approach is shown in Efron and Morris (1973) to reduce the k-dimensional problem of estimating (O., 02, ..., ok) from (x., x2, ..., Xk) to the one-dimensional problem of estimating A, or more precisely D/(A + D), from S. The mysteries of multi-parameter estimation are in this way put into a much more familiar context. The same method is used in this paper. Once again the parameters are assumed to have independent normal prior distributions, but with possibly different variances in the two groups, X(O, A1) and X(O, A2). By restricting attention to a useful class of estimation rules introduced in Section 2 we reduce the k1 + k2 para- meter problem to one that depends upon the single parameter b_ (A1 + D)/(A2 + D). All of our compromise rules are the Bayes rules against various pr ors on b. However, once again we hedge our Bayesian bets with classical caution: having obtained such a rule by standard Bayesian methodology we compute its risk function as a function of b. Perhaps the most interesting implication of the James-Stein estimator is that "complete ignorance" is a difficult concept to express in multiparameter problems. The uniform prior over k-dimensional space is too ignorant. It forces the statistician to ignore useful evidence that the parameters lie near one another. It is now clear that compound priors obtained by putting a vague prior prior on A give a more useful expression of complete ignorance, see Strawderman (1971) and Lindley and Smith (1972), as well as Efron and Morris (1973). For the "two groups" problem considered here there is a similar though milder surprise. The vague prior on b results in using the James-Stein rule separately on the two groups. For the case k. = k2 = 4 non-vague priors are presented which almost, though not quite, dominate this procedure. The resulting rules also compare favourably with the James-Stein rule applied to the combined k1+k2 parameter problem. That such rules exist is the main point of this paper, and indicates the existence of still more useful representations of complete ignorance in multiparameter problems. (The numerical evidence is not as scanty as it appears here. Lack of space forced the deletion of similarly favourable results for several other values of k, and k2.) The last Section of the paper discusses briefly the case of several groups of parameters.</page><page sequence="5">1973] EFRON AND MORRIS - Combining Possibly Related Estimation Problems 383 2. A USEFUL CLASS OF ESTIMATORS We consider the problem of estimating k+ = k1 + k2 parameters 0 = (01, 02), 01 = (011 012, ..., 01k1)9 02 = (021, 022, ..., 02k,) Corresponding to each 6ij there is a single normal observation xj, x*j Ijsij -AX(Ojj,Di) independently i= 1,2, j= 1,2,...,ki. (2.1) For now we will also assume Di = D2= D (2.2) say, with D known. (These assumptions are appropriate if we start with n independent observations for each Sj, Y _jX(Aij, a2), h = 1,2, ..., n. Then Xi = hl Yijkln and D = ar2ln. In Section 7 we will show that our theory continues to hold if &amp;2 is unknown but is estimated from Ei E Zh(Yijh- Xj)2. We will also allow n to depend on i, but not on j, so that Di = -2lni.) We will be interested in squared error loss functions for 0, and 02 separately and also for 0 = (01, 02). If 8 = (81, 82) is an estimate of 0 then these loss functions are Li(0A) =Tk||S-o0i|2, i= 1,2 and (2.3) L(0, 8) =I1 -I0 112. For any estimation rule 8(x) these define the group risk functions Rz*(fO S*) Ee L*(Oiq 8*(x)) and also the total risk R(O, 8)--Ee L(6, 8(x))., Notice that 8i(x) may depend on all of x = (x,, x2) and not just on xi, so that the risk for group i can depend on all of 0. Our estimation rules will be of the form Si= [1 D(k+ - 2) P(V)] x (2.4) where Si=-|X,112, S=-S1 +S2, VJ = SJ/S+, k= k1 + k2. The combined James- Stein rule for all k+ co-ordinates, S(0omb) has pi(V)- 1, while the choice p(V) = ki-2 1 (2.5) k+ 2' (2.5 defines iSeP), the "separate" James-Stein rule for the ith group. The authors have used rules of the form (2.4) for another purpose in their 1972 paper, where the function pi is called the "relevance function". That name is appropriate here too since the choice of pi(V*) determines the relevance of the two groups of observations xl and x2 to the estimation of Oi.</page><page sequence="6">384 EFRON AND MORRIS - Combining Possibly Related Estimation Problems [No. 3, We could have defined the rules (2.4) to be 1-D(k - 4) 126 3*i = Xi+ [l- ( 5+, (V)] (xij-), (2.6) X-j=Ex j/k, S* = E(Xij-?)2, S =SI+?S, V*=SI/SI. :1=1 j=1 Effectively this amounts to reducing the dimension of each group from ki to ki -1, applying the simpler form (2.4) to a k+-2 dimensional problem. The two missing co-ordinates 61 and 62 are estimated by their maximum likelihood estimates xl and x2. This point is discussed in Section 2 of Efron and Morris (1973), and will be mentioned again in Section 7. For now we assume that the two groups of observations have already been individually centred about appropriate origins so that the form (2.4) is reasonable. Actually a rule of form (2.4) should not be used unless 0 &lt; D(k+ -2) (V) &lt; I (2.7) A rule which does not satisfy (2.7) can be improved by forcing it to do so, as discussed in Section 6. However, in searching for a good choice of pi(VI) it is convenient to ignore this restriction at first. We do not require k1, k2&gt; 3 in the theory which follows, only k+ &gt; 3. However, Stein's rule applied separately to group i requires k* &gt; 3, and we will implicitly assume this when referring to it. 3. THE BAYES RULES We now assume that 0 = (01, 02) has the prior distribution 61j A- (O, Ai) independently i = 1, 2, j = 1, 2, ..., ki. (3.1) Under (3.1) and (2.1)-(2.2) the conditional distribution of 0 given x is o j I xij ((l-Bj)(1 -B) xij, D(1 -BO) independently i = 1, 2, j = 1, 2, ...,ki, (3.2) where D Bi= Ai=D[B7'-1]. (3.3) If B1 and B2 (equivalently A1 and A2) are known to the statistician then the Bayes rule under squared error loss is 8* = (8*, 8*), b.=[1 -Bil xi&gt;. (3.4) In Efron and Morris (1972) the authors have discussed the empirical Bayes situation where the Bi are unknown but are estimated from Si= - X, 112. The marginal distribution of Si under (3.1) and (2.1)-(2.2) is D S ,-.-2 independently i = 1, 2. (3.5)</page><page sequence="7">1973] EFRON AND MORRIS - Combining Possibly Relited Estimation Problems 385 For any rule 6(x) = (1(x), 82(x)), define the Bayes risk for the ith group to be R?({B1, B2}, S) EB,,B, Ri(O, Si), (3.6) EBI,B2 indicating expectation with respect to the distribution on 0 given by (3.1) with Ai= D[B'-1 1]. The minimum Bayes risk is that of 8*, and by (3.2) equals R({B1, B2}, 8*) D(1 -Bj). (3.7) This compares with R&amp;({B1, B2}, )- = D for the maximum likelihood estimator 8 = xi. Under (3.1) the statistician saves D - D(1 - B) -DBi by using 8* instead of 80. Finally, the "relative savings loss" of 8i for group i is defined to be RSL%({B1, B2}, i) R.({B1, B2}, 8) - Ri({B1, B2}, 8X ) R-t.{B1, B}, SQ) - R-z.{B1, B2},9~ -[R,({B1, B2}, 8%) - D(1 - B%)]/DBj. (3.8) In what follows it will be convenient to consider only the case i = 1. Of course the results for i = 2 can always be obtained by interchanging the labels of the two groups. Lemma 1. The rule 81 = U - A(Sl9 S2)] Xl (3x9) has RSL,({Bl, B2},1) = "E L B1 J ' (3.10) where EB,)B2 indicates expectation with respect to D BX2 independent of S2 (3.11) (This is (3.5) with k1 increased to k,+2.) Lemma 1 is a special case of Lemma 2 (see Efron and Morris, 1973). It expresses RSL1, the cost of using (3.9) instead of the actual Bayes rule (3.4), in terms of how well fl(S1, S) estimates B,. We can then work formally with the problem of estimating B. with loss function B, ] (3.12) on the basis of (Si,S2) distributed as in (3.11), instead of with the original high dimensional estimation problem. Lemma 1 remains true, with the obvious changes in (3.11), if we drop assumption (2.2) so that D1- D2. It follows from Lemma 1 that the separate James-Stein rule for group 1, b(sep) = [1 -fi(sep)] X f(sep) -D(-)S.3 has RSL1({B1, B2}, S(seP)) 2 (3.14) Ic1k see Theorem 1 in Efron and Morris (1973). It also follows that ifB1 = B2 the combined James-Stein rule 5(comb) [1 .fi((comb)]xl, (comb)_ D(k -2)/S+ (3.15)</page><page sequence="8">386 EFRON AND MoiRIs - Combining Possibly Related Estimation Problems [No. 3, has RSL,({Bl, B1}, &amp;(comb)) = 2 (3.16) + which of course can be considerably smaller than 2/kl. However, if B1JB2 is either very large or very small then RSL1({Bl, B2}, 8(comb)) will be large compared to 2/kl. As expected, the combined rule does better than the separate rule if B1 = B2 and worse if B, is much different from B2. Table 2 compares RSL, for 8(comb) and 8(sep) as a function of b--B2 = A2+D (3.17) for the case kl =k = 4. We see that 8(comb) is preferred for l &lt; b &lt;4 approximately, and 8(sep) preferred otherwise. TABLE 2 RSL1({B1, B2}, 81) as a function of b = B2/B1 for the separate and combined James-Stein rules, and also for the Bayes rules 8(3), 8(4), 8(5) described in Table 3. k = k= 4 b = B21B, + 1 1 1i I i i 1(sep) 05 0*5 0S 5 0*5 05 05 ,(comb) 0-92 0-86 0.76 0*62 0*46 0*31 0.32* Rule 3 050 0 50 0.48 043 037 0.32 Rule 4 051 0-50 049 045 0.39 0.33 Rule 5 0.93 0-87 0-79 0.66 0-52 0.38 b = B2/B1B, 1 2 4 8 16 32 64 (sep) 05 0-5 0S 5 0.5 0-5 0.5 0-5 3(comb) 0 25 0.33 0 56 0 90 1V28 1V63 1.91 0.26* 0.32* 0.48* 0.66* 0.80* 0.89* 0.95* Rule 3 0.30 0-37 0.51 0.70 0-89 1.06 1419 0.31* 0.38* 0.50* 0.66* 0.79* 0.89* 0.94* Rule 4 0.31 0 34 0 44 0.60 0.80 1.01 1.20 0.35* 0.45* 0.59* 0.73* 0.84* 0.91* Rule 5 0.29 0.27 0.33 0.46 0.63 0.84 1.06 0.28* 0.34* 0.46* 0.62* 0.76* 0.86* (The starred numbers are for the modified version of each rule as described in Section 5. Where there are no starred numbers the two versions agree.) Table 3 shows that the region of preference in terms of RSL1 is actually (0-29, 4 80). It is clear from Table 3 that 8(comb) compares more unfavourably with 8(sep) as k, = k2 increases. Not only does the region of preference for 8j(comb) grow narrower,</page><page sequence="9">1973] EFRON AND MoRRis - Combining Possibly Related Estimation Problems 387 but also the difference in RSL, at b = 1, 2/k1 - 2/kk = l/k,, becomes smaller. 8(comb) should not be used for k1 = k2 greater than 10. However, this does not necessarily indicate the use of 8(sep) as we shall see. TABLE 3 Range of b = B2/B1 for which 8(comb) is preferred to 8(sep) in terms of RSL1, RSL. Various choices of kc = k2. RSL is as defined in Section 4 k, = k2 RSL1 RSL 3 (0'22, 8'50) (0-13, 7'80) 4 (0-29, 4 80) (0 22, 4'45) 6 (0 38, 293) (0 35, 2 83) 8 (0 45, 2 40) (0 43, 230) 10 (0 50, 2'10) (0 49, 2 05) 15 (0'57, 1 76) (0 57, 176) 20 (0 62, 1'62) (0 62, 162) oo (1, 1) (1, 1) The fact that RSL,({Bl, B2}, S(comb)) &gt; 1 for large values of b shows that 8(comb) does not dominate the m.l.e. when we consider just the first four components 01. The situation is not as serious as it looks though. If b = 64 for example then B1 6 1 and Rl({B1,B2}, Scomb)) = D{1-B(I-RSL)}, D{l+( 14)0-91} = D(I-014), as com- pared with the value D for the m.l.e. In reading Table 2 it should be kept in mind that the entries for large values of b, say b &gt; 8, are not very important compared to those for small values of b. As a rule of thumb, if RSL,({Bl, B2}, 8) &lt; 1 + 0 I b nothing too bad can happen since then Rl({Bl, B2}, 61) &lt; D(l 1). The fact observed in Table 2 that RSL,({Bl, B2}, 81) is a function only of b holds for every rule of the form (2.4): Corollary 1. For rules of the form (2.4) we have RSL1({B1, B2}, 8)&lt; = 1+ (1 -k~-) L"'){[V1+bV2] p1(_)- 1}2 (3.18) which depends only on b = B2/B1, the density of V1 under (3.11) being = Vlk/2 Vk2/2-1(V +bV2)-(k+/2+1) bk2/ ]{(k+ + 2)/2} ]P{(k1 + 2)/2} P(k2/2) (.9 (proof given in Appendix). For rules of the form (2.4) we will use the simpler notation RSL,(b, 61) in place of RSL1({Bl, B2}, 81). We now consider rules that are Bayes against two-stage priors defined as follows: choose B1 and b-B2/B1 according to the improper prior g(Bl, b) = f(b)/B1, 0 &lt; B1, b &lt; oo, (3.20) where f(b) is an arbitrary density function. (We will also allow b to have a discrete distribution, but will write all formulae as continuous integrals for convenience.) Then 0 is chosen according to (3.1) with Ai = D[Bj -1]. The prior g puts mass outside of 0&lt; B1, B2 &lt; 1, but this does not affect the formal estimation problem (3-1l)-(3-12).</page><page sequence="10">388 EFRON AND MoRius - Combining Possibly Related Estimation Problems [No. 3, The theorem which follows will show that if we let f be the distribution putting all of its mass at b - 1 the resulting Bayes rule is 8(comb). To obtain 8(SeP) we need to takef(b) proportional to the improper density 1/b, 0 &lt; b &lt; oo. The rules, 8(3), 8(4), 8(5) evaluated in Table 2 are Bayes against discrete mass functionsf(b) putting probability mass on the points b = 1, 6, 4, 1,4,16,64,256 as described in Table 4. The eccentric looking values off(b) for 8(3), which will be explained further in Section 4, TABLE 4 Relative probability mass function f(b)/f(1) for 8(3), 8(4), 8(5) Mass points b Rule 256 84 18 1 1 4 16 64 256 6j3) 0.0996 0*1969 0.2353 0.4800 1 0.1200 0.0147 0.0031 0.0004 0.05* 0.1* 0.125* 0.3* 1* 0.3* 0.125* 0.1* 0.05* 1SW 00996 01969 0.2353 04800 1 0.2800 0.0147 0-0031 0.0004 6(5) 0 0 0 0 1 1.2 0 5 0.1 0 05 The starred numbers for 6j3) are explained in Section 4. were chosen to give a low value of RSL,(b, 8(3)) at b = 1 without letting it get too large anywhere in the range Aib &lt; 4. As noted earlier it is not as important to control RSL1(b, 81) for large values of b, while for b &lt; I&amp; the data will almost always give strong evidence that the two groups are very dissimilar and should not be combined. (If desired additional mass points could be located at 1o124, 4096' etc. to hold RSL1(b, 8(3)) near 0 5 for arbitrarily small values of b and with negligible effect on the values given in Table 1.) Notice that SW) compromises quite favourably between VSWeP) and 8(comb), and might well be preferred to either in reasonable situations. The distribution f(b) for 8(4) is identical to that for 8(3) except that f(4) is sub- stantially increased. This decreases RSL1 for b &gt; 2 at the expense of the values for b &lt; 1. The distribution f(b) for 8(5) puts probability 1 on b &gt; 1, an assumption that is reasonable in some situations as shown in Section 8. These rules all have the form (2.4), and their Bayesian nature guarantees, by the theorem which follows, that no rule of the form (2.4) can have a uniformly lower value of RSL1(b, 81). The authors have explored a wide variety of continuous and discrete forms for f(b). The discrete forms performed quite satisfactorily and since they have computational advantages they are featured in the numerical examples of this paper. Theorem 1. The formal Bayes rule for estimating B1 under loss function (3.12), distributional assumptions (3.11) and prior distribution (3.20) is Br(S1, So = D(k 2) P (V1), (3.21) where P1(V1) = bk81[[V1 + bV2]-b+12 f(b) db { bkul2[V1 + bV2-(k+12-1) f(b) db. (3.22)</page><page sequence="11">1973] EFRON AND MoRRis - Combining Possibly Related Estimation Problems 389 Among all rules of the form (2.4), the choice of Pi = p* minimizes IfRSL,(b, 81) f(b) db. (3.23) (Proof given in Appendix.) Several points are worth mentioning in connection with Theorem 1. Because of (3.23) no rule of the form (2.4) can dominate 8*= [1 D(k+ 2)*(v] If RSL1(b', /0 &lt; RSL1(b', 8 ) for some b' then there exists b" such that RSL1(b", S )&gt;RSLj(b',g8*). This implies the seemingly stronger statement that if R1(O', 81) &lt; R1(O', 68) for some 0' then there exists 0" such that R1(0', 68) &gt; R1(0', 8r), i.e. that 8* is admissible for estimating 01 among rules of the form (2.4). The rule 8* is formal Bayes for our original problem of estimating 01 with loss function L1(01, 81) having observed x*j -X A'(6i, D), as well as for the derived problem (3.11)-(3.12). The prior distribution in this framework is given by (3.1) and the density (1/B) f(b) on (B1, b). See Section 4 of Efron and Morris (1973). It is not necessary forf(b) to have a finite integral for the rule given in Theorem 1 to be unique formal Bayes against (1/B1) f(b) for the estimation problem (3.11)-(3.12). A sufficient condition (see Zidek, 1970) is that Jblks[1 + b]-Ak++lf(b) db &lt;oo. (3.24) However, if f is improper the admissibility result quoted above does not necessarily hold. For most choices off(b) the expression (3.22) for p* cannot be evaluated explicitly. An exception is f(b) = , a, bl, I1k- &lt; l&lt;, k-2, (3.25) the restrictions on I being necessary to satisfy (3.24), for which a standard F-density integral gives P1(V1) =1k{1 k+2 (2 kI 21) (VJ/V2)/ al_i(VJIV2Y} (3.26 where the constants al are defined in terms of the constants a, by al al -(2k2+ l r(2k -l2 (3.27) Priors of type (3.18) with f(b) given by (3.25) were used by Portnoy (1969) for estimating variance components. If f(b) is discrete, putting probability f* at b = bi, i = 1, 2, ..., I, then obviously pi*(V1) has the closed form expression I I I p~(V1) = f r btkj [V1 + b* V2&gt;i2k+/Ef biiks[V1 + bi V2]Ak+. (3 .28) i=l 1=</page><page sequence="12">390 EFRON AND MoRRis - Combining Possibly Related Estimation Problems [No. 3, This formula is computationally convenient, and with I = 9 the values of fi and bi could be manipulated to give RSL functions closely matching those of any continuous f(b) the authors chose. 4. TOTAL RISK IN THE CASE k1 = k2 The Bayes theory of Section 3 was developed for RSLj, the relative savings loss for the first group. Here we consider the total RSL for all k+ k1 + k2 components, the total RSL defined as in (3.8) to be R B 52} ) R({B1, B2}, 8)- R({B1, B2}, *) (4.1) RSL({B, B2}, R({B1, B2}, 50&gt;.- R({B1, B2}, 5*8(41 We will only consider the symmetric case where k1 = k2 and the prior distribution of b is invariant under the mapping b - 1/b. For the case where b has density f(b) the symmetry condition is f(b) = b-2f(b-). (4.2) For any rule S = (5k, 82) the total RSL is a simple function of RSL, and RSL2, RSL({B1, B2}; 5) = RSL,({Bl, B2}; 81) + b RSL2({B1, B2}; 8 (43) 1+b This is verified by rewriting (3.8) as Ri({B1, B2}, 5S) = D(1 - B) + DB* RSLi({B1, B2}, S). (4.4) Then by (2.3), 2 k. R({B1, B2}, 8) = 4 i RSLi({B1, B2}, 8), (4.5) and also the total risk of 8* the Bayes rule and 80 the m.l.e. are 2 R({B1, B2}, 85) = D(1 - B), R({B1, B2}, 80) = D. (4.6) i=1 Equation (4.3) follows from the definition (4.1). Corollary I shows that for rules of the form (2.4) both RSL,({Bl, B2}, 81) and RSL2({B1 B2}, 82) depend only on b and therefore by (4.3) so does RSL({Bl, B2}, 8). We can use the simpler notation RSL(b, 8) for rules of this form. Theorem 2. The rule which minimizes f* RSL(b, 8) f(b) db among all rules of the form (2.4) is D = 1- (k+ - 2) P*(Vi) xi, i = 1, 2, (4.7) where p*(V1) is defined by (3.22) but withf(b) replaced by fi (b) = f(b) (4.8) 1 +b' 48 (Proof given in Appendix.) Notice that f+(b) does not have the symmetry property (4.2) and as a matter of fact gives less weight to large values of b thanf(b) which is symmetric. We can expect the rule (4.7)-(4.8) to have a larger value of RSL,(b, 81) than the rule defined by</page><page sequence="13">1973] EFRON AND MoRRis - Combining Possibly Related Estimation Problems 391 (3.21)-(3.22) when b is large, and a smaller value when b is small, and vice versa for RSL2. By (4.3) this is a favourable trade-off if we are only interested in the total RSL. The special form of f+(b) forces p* to have a certain symmetry: Theorem 3. The function p* defined in Theorem 2 satisfies Vl p*(Vl) + V2 p *(V2) = 1 (4.9) for all V1 and V2 = 1 -I1, 0&lt;V1, 1, so that P*(0.5) = 1. (4.10) Moreover, p*(V1) is non-increasing and &gt; 1 for V1 &lt; 0 5 and p*(Vi) &lt; 1 for V1 &gt; 0 5. If b-Ak, f+(b) db &lt; oo then p*(l) = 1. (Proof given in Appendix.) It is interesting to notice that 8(comb) is a Bayes rule in the sense described in Theorem 2, while 5(seP) is not, not even if we letf(b) be improper. Choosingf to put mass 1 at b = 1 gives 8(comb) as before. The discussion in Section 3 shows that we would need f+(b) oc b-1 to get 8(sep), but this would imply f(b) cc 1 + b-, which does not satisfy (4.2). Notice also that 8(sep) has p(j) = (k1 -2)(k -1) &lt; 1, by (2.5) with k, = k2, so that (4.13) is violated. Fig. 1 shows RSL(b, 8) and p(Vl) for 8(sep), 8(comb) and 8(3). The rule 8(3) is Bayes, in the sense of Theorem 2, against f(b) as given by the starred numbers for 6S" in Table 4. This gives f+(b) proportional to the unstarred numbers shown for 8 3)* RSL P 1.0 5 6~(comb) 0-84 / - ~ %\,6(hyp) 0-6 ; _ 2 | \ {js3 /O 8(comb) 0-2~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 02 1 1 2 4 8 16 32 64 128 b 0 0-2 04 0-6 0'8 10 V 1I 1 1 1_ 1 1 z 4 8 1 6 3 2 64 1 28 FIG. 1. RSL(b, 6) and p(V1) for 5(sep, )(COmb) and 5(3), k, = k2= 4. Also shown is RSL(b, 8(hyp)) for the hypothesis testing rule.</page><page sequence="14">392 EFRON AND MORRIS - Combining Possibly Related Estimation Problems [No. 3, In plotting Fig. 1 we have made use of the fact that RSL(b, 6) = RSL(1/b, 8) in the symmetric case for rules of the form (3.4) with Pl= P2. Once again it seems fair to say that S(3) would be preferable to both 8(sep) and S(comb) in many reasonable situations. It does not do too much worse than S(comb) at b = 1, while protecting the statistician against large or small values of b almost as well as 5(sep). Fig. 1 also shows a plot of RSL(b, S(hyp)) where S(hyp) is an "hypothesis testing rule" equalling 5(comb) if V1 e (VL, VU) and 8(sep) otherwise. Here VL and vU = 1 -VL have been chosen so that RSL(b, S(hyp)) = RSL(b, 8(3)) at b = 1. The extremely poor performance of 5(hyp) discourages further interest in rules of this type. 5. FORCING A RULE TO DOMINATE THE MLE In one sense it is unfair to compare S(sep) with S(comb). The former dominates the m.l.e. for the estimation of 01 and 02 separately, while the latter is only guaranteed to dominate the m.l.e. for the combined estimation of 0 = (01, 02). It is safer to use S(sep) than S(comb) if we are interested in RSL1 and RSL2 separately. We now give a corollary of a result due to Baranchik (1964), which gives a sufficient condition for a rule of the form (2.4) to dominate the m.l.e. for the estimation of 01. Theorem 4. If (i) V1 p1(Vl) is a non-decreasing function of V1 and (ii) pl(l) &lt; 2(k1 - 2)/(k+ -2), then the rule defined by (2.4) has R1(0, 51) &lt; D for every value of 0. (Proof given in Appendix.) Notice that S(comb) has p(comb)(Vi) = 1 satisfying (i) but not (ii). We can easily modify any such Pi function to satisfy the conditions of Theorem 4 by defining p'(V1) = min pl(V1), 2(k1-2) (5.1) pi 1) ~~V1(k+ - 2)1 The corresponding rule St(x) = [1- S ?2 (VD1 (5.2) will have R1(0, S') &lt; D for all 0 by Theorem 4. Condition (ii) is necessary: if pl(l)&gt; 2(k1 - 2)/(k+ -2) then it is possible to show that R1(0, 81)&gt; D when 01 is made sufficiently large in magnitude while 02 is held fixed. Condition (i) is not necessary, though no convenient weaker condition is known. (See Section 3, Efron and Morris, 1973.) The starred figures in Table 2 give RSL1(b, 81), with 5' obtained from S by (5.1)-(5.2) for each of the five rules. In the case of 5)"W and SP4)" condition (i) is not satisfied. Nevertheless the numerical results indicate that the modification is still effective in holding RSL, below 1. 6. PLUS RULES So far our theory has completely ignored the restriction B.e(0,1], i= 1,2. (6.1) Our rules have fi &gt; 0 but not necessarily Bi &lt; 1. We can improve any rule Si = [1 -i(Sl, S2)] xi by defining -ft+(Sl, S2) min {f3(S1, S2), 1}. (6.2)</page><page sequence="15">1973] EFRON AND MoRRis - Combining Possibly Related Estimation Problems 393 The rule Si+ = [1- -l+(S11S2A Xi = [1 - A(S1,S2)]+x, (6.3) where Lf(x)]+- max {f(x), 0} as usual, will have RSLi({B1, B2}, 8+) &lt; RSLi({B1, B2}, Si), i = 1,2, with strict inequality if PB,,B,{Bi&gt; 1}&gt;0, and likewise for the total RSL. This is obvious from Lemma 1. An explicit formula for the improvement is given as Corollary 3 of Efron and Morris (1973). RSLi({Bl, B2}, Si) - RSLi({Bl, B2}, S ) E B;2 L(i{([As-l]++ -Bi))2-(1-Bi)2}. (6.4) We can use (6.4) to obtain a useful comparison of the "plus rule savings" between rules of the form Si = [1- r(S+) p(Vj)] xi (6.5) and the corresponding rules without the pi function, 851) = [1-r(S+)] xX. (6.6) Notice that for -r(S+) = D(k - 2)/Se we have C)_ = (comb). Once again we will state results only for the case i = 1. Lemma 2. Let Q)= p1(V1) Vikt V0ka+-l dV, /{r(2kl +1) P(ik2 +j)/P(-k2 +1+ 1)}. (6.7) Then if AjD&gt;Y, 1, j = 0, 1, 2, ... (6.8) we have for all values of b- B2/B1 &lt; 1 that RSL1({Bl, B2}, 81) - RSL1({Bl, B2}, S+) &gt; RSL1({Bl, B2}, Sjc)-RSL1({B1, B2}, S(C)+). (6.9) For b = 1 (6.9) holds if ,iO) &gt; 1. The inequality (6.9) is strict under the additional condition that PB1,B2{r(S+) P1(V1) &gt; 1} &gt; O. (Proof given in Appendix.) Theorem 5. In the symmetric case of Section 4, k1 = k2, b 1/b, the rule 8 * given in Theorem 2 has RSL1({B1, B2}, 8) -RSL1({Bl, B2}, 8* +) &gt; RSL1({B1, B2}, 8(comb)) - RSL1({B1, B2}, ob(comb)+) (6.10) for b_ B2/B, &lt; 1, with strict inequality if 8* is not identical to 5(comb). (Proof given in Appendix.) Theorem 5 states that rules such as SM gain more from the plus rule modification, in terms of RSL1, than does 8(comb) as long as b &lt; 1. By continuity this must also be true for small enough values of b &gt; 1. We have already mentioned that reducing RSL1 is more important for values of b (1 than for large b, so it seems fair to conclude that the overall comparison of RSL1 between 6(3) and 6(comb) will become more favourable to the former after the plus rule modification.</page><page sequence="16">394 EFRON AND MoRRis - Combining Possibly Related Estimation Problems [No. 3, Theorem 5 holds true for total RSL in the case b = 1. It seems likely to the authors that this holds for all values of b, but the result has not been verified. Using any constant except k-2 in the definition of the James-Stein estimator uniformly increases the risk. However, this clear-cut situation no longer holds when we consider plus rule versions of the James-Stein estimators. In Section 6 of Efron and Morris (1973) the authors suggest that the constant k - d, with d = 066 for k &gt; 4, is superior in the plus rule context. Essentially the larger constant takes greater advantage of the fact that the parameter Bi cannot assume values greater than 1. We can modify our Bayes's theory in this direction by replacing (3.22) with the prior g'(Bl, b)_ Bf-dI2 f(b), 0 &lt; BJ, b &lt; oo. (6.11) This gives the formal Bayes estimator B'(S1, S2) = D(k+-d) p'(V) (6.12) + pi 1' where p'(Vl) is given by (3.22) with k+ replaced by k+ = k++2-d. (6.13) For d&lt; 1 it is also true that pl (V1) &lt; p (V1) (6.14) so that B (S1, S2) &lt; B'(S1, S2). (6.15) The proof of (6.14) is similar to the argument used in the proof of Theorem 3. On the basis of the numerical results in Efron and Morris (1973) the authors suspect that the plus rule version derived from (6.12), 1+ = [1- Dk(k-d) p(V)] xi (6.16) with d = 066 will have generally better RSL1 characteristics than S*, but no direct theoretical or numerical evidence has been obtained. 7. IF THE VARIANCE D IS UNKNOWN We usually will not know the variance D of xij given Oij, xij| I ij -X(Oij, D), (7.1) but instead will have available an independent estimator of D, say W DX2, (7.2) W independent of (S1, S2). In this case an empirical Bayes rule of the form e r D(k+ -2) PA xi(73</page><page sequence="17">1973] EFRON AND MORRIS - Combining Possibly Related Estimation Problems 395 can itself be estimated by S=[1- S PADV* | Xi, (7.4) where D = W/(n + 2). (7.5) Lemma 1 easily yields the relationship RSLi(b, &amp;i) = +2 + n +2 RSLi(b, S.). (7.6) This holds for i = 1, 2 and therefore by (4.3) for the total RSL. For the purpose of choosing a good pi function it does not matter whether or not D is assumed known; any RSL comparison between rules of the form (7.4) is the same, up to the simple linear transformation, given by (7.6), as the RSL comparison for the corresponding rules (7.3). Next consider the model (2.1) where we drop the assumption D1 = D2. Instead let Di = D/ni, i = 1, 2, where the ni are known constants. In this situation we modify (2.4) by defining Si ni jj x,112. With only this change Corollary 1 remains true as stated. The rule S(3), for example, will still have RSL1(b, S(3)) as given in Table 2. If D is not known but can be estimated as in (7.2)-(7.5) then formula (7.6) applies. These results all follow from Lemma 1. Finally, suppose we replace (3.1) with the more flexible model OijX(Mi,AA) independently i= 1,2, j= 1,2, ..,ki, ki4. (7.7) An appropriate class of rules is 8, -=i + [1- 3(S1, S (Xjj -X), (7.8) where kCi k- T= EXjl/k,, SX = (Xj -i)2. (7.9) :1=1 = An elementary calculation gives 1 k.-I RSLi({Bf, B2}, Si) J -+ RSLi({B1, B2}, 6i). (7.10) Here Si = [1-f3(S1, S2)] Xi and RSLi is calculated under assumptions (3.1) by Lemma 1. For example if k1 = k2= 5 the rule 5(3)' has RSL1 = 0 2 + (0 8) (0 03) = 0 44 at b = 1. Expressions (7.6) and (7.10) combine to give 1 k.-I E2n1 RSLi(b, ) + k[n +2RSLi(b, S)J (7.11) for rules of the form sj= x [i+ 1- (k ) Ip-(VD)]k(xfj-x-2(7.12)</page><page sequence="18">396 EFRON AND MoRRis - Combining Possibly Related Estimation Problems [No. 3, 8. ONE-SIDED RULES Any rule of the form (2.4) can be uniformly improved if the statistician knows that b &gt; 1 or knows that b &lt; 1. Once again we state only the case i = 1. Theorem 6. Let 6' and S&gt; be the rules obtained from (2.4) by replacing pl(V1) with p&lt;(V1)=_max{l, pl(V1)} and p&gt;(V)= min{l, pl(V1)} respectively. Then RSLl(b, 81) &gt; RSL,(b, S&lt;) for b &lt; 1 and RSL,(b, 81)&gt; RSL1(b, S&gt;) for b&gt; 1. The inequalities are strict if Pi and p&lt;, respectively Pi and p&gt;, are not identical. (Proof given in Appendix.) Since p(V1) &gt; pl(Vl) it follows from (6.4) that RSLl(b, SD9 - RSL1({B1, B2}, S&lt;+) &gt; RSL1(b, 8) - RSL1({B1, B2}, S+). Combining this fact with Theorem 6 gives RSL1({Bl, B2}, S1t)&gt; RSL1({Bl, B?J, S&lt;+) (8.1) for b &lt; 1. A similar result cannot be stated for S&gt;. The natural way to incorporate knowledge that b&gt; 1, say, into our Bayes theory is to choose f(b) = 0 for b &lt; 1 in (3.22). Then it is easy to verify from (3.22) that pi(Vl) &lt; 1 for all V1 so that Theorem 6 yields no improvement. Likewise f(b) = 0 for b&gt; 1 implies p*(Vl) &gt; 1. The rule 8(5) described in Table 4 is designed for the case b &gt; 1. Table 2 shows that RSL1(b, Wj5)) is indeed small for b&gt; 1. A very simple case where the assumption b &gt; 1 is usually warranted arises when we want to estimate just one group of parameters 0 = (01, ..., Ok) on the basis of independent observations xiJ I .iA-(Oi, 1). As we have already seen in (2.6) it is usually suggested that the average 6 = OJlk be estimated by its maximum likeli- hood estimator x El' x/k and then the James-Stein estimator applied to the residuals xi - x to estimate 6i -0. This procedure prevents a large value of 0 from eliminating the Stein savings in the k-I dimensional residual space, but has an RSL of 1 for the co-ordinate 0. We can consider this as a "two groups" problem with ki = 1 and k2 = k-1. Here the assumption b &gt; I is a Bayesian statement of belief that usually kG02 will be greater than Ek=1 (6, - 0)2/(k -1). This is intuitively plausible and can be formally derived from a components of variance model. In the case k = 8, k1c = 1, k2 = 7, Table 5 compares RSLl(b, 61) for Sj(comb) and a one-sided Bayes's rule. For b &gt; 1 the compromise seems quite favourable compared with either S(comb) or with the m.l.e. 60 which has RSL,(b, S)=_ 1. 9. SEVERAL GROUPS Suppose there are I&gt; 2 groups of parameters Oi = (O1i, i2' 0ik) which we are trying to estimate on the basis of observations xi = (xil, xi2, ..., Xik)9 xij I Oij -AX(O4, D) independently, i = 1,2, ..., I, j= 1,2,...,ki. (9.1) The theory of Section 3 applies directly to the estimation of, say, 01 with rules of the form SI.= [1- D 2 P'(V')1xi, (9.2)</page><page sequence="19">1973] EFRON AND MORRIS - Combining Possibly Related Estimation Problems 397 where now I I I k+= k S S and V1= S/S+. i=1 i=l i=1 Lemma 1, Corollary 1 and Theorem 1 remain true as stated by replacing k2 with k[2, = k+-kl, S2 with S[2] = S+-S1 and V2 with V,21 = 1-V1. TABLE 5 RSL1(b, S,) for the m.l.e., 8(comb), and a one-sided Bayes rule (b &gt; 1), k1 = 1, k2 7 Rule 64 32 16 8 4 - I 6?=x,them.l.e. 1 1 1 1 1 1 1 18(comb), Stein's rule, 0-96 0-93 0-87 0.76 0.58 0-38 0-25 all 8 co-ordinate Bayes's rule, f(b) = 1, 0.98 0-96 0-92 0-86 0-76 0-64 053 0 05, 0 05, 0 05 for b = 1, 4, 16, 64 Rule 2 4 8 16 32 64 6? = x, the m.l.e. 1 1 1 1 1 1 IScomb), Stein's rule, 0 53 1-83 5-10 11P63 22-84 39-78 all 8 co-ordinate Bayes's rule, f(b) = 1, 0-48 0 54 0-72 1 01 1-38 1-62 0 05, 0 05, 0 05 for b = 1, 4, 16, 64 There is a potentially serious drawback to rules of the form (9.2). As an example suppose we have 9 groups of 4 parameters each, so I = 9, k+ = 36, k1 = 4, k[2] = 32. If just one of the 9 vectors Oi is much larger in magnitude than the rest, say jj O912j&gt;jjOj12, i = 1,2,..., 8, then we can expect the rule (9.2) to do no better than 5(sep). (In the Bayesian formulation we will effectively be in a situation with b&lt;? 1, and on the basis of Table 2 we expect to have RSL1(b, S1) &gt; 2/k1 = 0 5 for any rule which has small RSL1 for values of b near 1.) Moreover, numerical evidence shows that there is little advantage in having k[2] as large as 32 even if the 1Oi 012 are of comparable magnitude for i = 2,3, ..., 9. The point is that one "bad" group can destroy any possible intergroup savings with rules of the form (9.2), and even ignoring the possibility of a bad group there seems to be little advantage to combining all 32 other parameters into one group. A simple way around this difficulty is to calculate a two-groups rule for each i = 2, 3, .. .J,I 8 i) = [1- D(kSfi) -p2)(Vii))1 xl, (9.3) where k() k1 + k, S() S1 + Si and V(i) = SJS+*), and average them to obtain the estimate Sl, 81-E W*S(*). (9.4) i=2</page><page sequence="20">398 EFRON AND MORRIS - Combining Possibly Related Estimation Problems [No. 3, The WH are non-negative numbers satisfying 11=2 W = 1, and could be chosen to reflect the statistician's a priori assessment of how similar 0% will be to 01. Jensen's inequality gives R1(09 b1) &lt; E Wi R1(0e, s(i)) (9.5) i=2 so the risk function of 81 cannot be worse than that of the worst 6)1 . Roughly speaking, a single bad group affects the estimation of 01 only in pro- portion to its weight W' when using rules of the form (9.4). This seems a useful improvement over (9.2), but no specific calculations have been carried out. 10. CONCLUSIONS We can now give a partial and tentative answer to Barnard's question. In the two-groups situation,with k, and k2 as small as 4, 8(comb) should not be used unless the statistician is certain that the two groups are similar. ("Similar" means that b = B2/B1 is nearly unity in the Bayesian formulation, and for a non-Bayesian can be interpreted to mean that [koj - 1)2] / [ )2/k] is nearly unity.) Bayes's rules such as 5(3) are preferred otherwise, the preference becoming stronger as k1 and k2 grow larger. These rules can be chosen to dominate 8(sep) over a very wide range of b values, so that they are preferred over S(sep) in most situations. REFERENCES BARANCHIK, A. (1964). Multiple regression and estimation of the mean of a multivariate normal distribution. Tech. Report No. 51, Department of Statistics, Stanford University. BLACKWELL, D. and GIRSHICK, M. A. (1954). Theory of Games and Statistical Decisions. New York: Wiley. EFRON, B. and MORRIS, C. (1972). Limiting the risk of Bayes and empirical Bayes estimators- Part II: The empirical Bayes case. J. Amer. Statist. Ass., 67, 130-139. (1973). Stein's estimation rule and its competitors-an empirical Bayes approach (in press). J. Amer. Statist. Ass. 68, 117-130. JAMES, W. and STEIN, C. (1961). Estimation with quadratic loss. In Proceedings of the Fourth Berkeley Symposium, University of California Press, Vol. 1, pp. 361-379. LINDLEY, D. V. and SMITH, A. F. M. (1972). Bayes estimates for the linear model (with discussion). J. R. Statist. Soc. B, 34, 1-41. PITMAN, E. S. G. and ROBBINS, H. (1949). Application of the method of mixtures to quadratic forms in normal variables. Ann. Math. Statist., 20, 552-560. PORTNOY, S. L. (1969). Formal Bayes estimation with application to a random effects analysis of variance model. Tech. Report No. 2, Department of Statistics, Stanford University. SCLOVE, S., MORRIS, C. and RADHAKRISHNAN, R. (1972). Non-optimality of preliminary-test estimators for the mean of a multivariate normal distribution. Ann. Math. Statist., 43, 1481- 1490. STEIN, C. (1962). Confidence sets for the mean of a multivariate normal distribution (with discussion). J. R. Statist. Soc. B, 24, 265-296. (1966). An approach to the recovery of inter-block information in balanced incomplete block designs. In Festschrift for J. Neyman (F. N. David, ed.), pp. 351-366. New York: Wiley. STRAWDERMAN, W. (1971). Proper Bayes minimax estimators of the multivariate normal mean. Ann. Math. Statist., 42, 385-388. ZIDEK, Z. V. (1970). Sufficient conditions for the admissibility under squared error sloss of formal Bayes estimators. Ann. Math. Statist., 41, 446-456.</page><page sequence="21">1973] EFRON AND MoRRis - Combining Possibly Related Estimation Problems 399 APPENDIX Proof of Corollary 1 We apply Lemma 1 with A1(S1,S2) = D(k- 2) p(V) = D(k 2)(Vl +bV2)pl(V). (Al) Sl + S2 S + bS2 1 2 Notice that under (3.11) we have S1 + bS2 -(DIBO Xk++2 independent of VJ1bV2 = SJ/bS2, and hence independent of (V1, V2). Therefore RSL1({B1, B2}, 1 = -(i) X2 (Vi + bV2) pl(Vl)-1 } =kE-2 pB2V 2(V+b k~+ 2 + = EL B { I+ (V1 + bV2) Pi(V1)] -2 k-2 (V1 + bV2) pl(Vl)] + 1) =EB1,B2 +k [(V + bV2)P1(1)]2-2 +k [(V + bV2) pl(V,)] + I 1- E(1) ~ ~ 1(V)-1}2. k+ ( k+) EBLYBa{[Vl + bV2] pl(Vl)-12 Equation (3.19) is obtained by noting that (V/bV2) - ((k + 2)/k2) Fk1+2,k2. Proof of Theorem 1 The proof of (3.21)-(3.22) is by means of a standard Bayesian calculation: from (3.11) and (3.20) the conditional density of B1 and b given (S1, S2) is g(Bl, b I S1, S2) xC Bik+ bik2 exp {- B1(S1 + bS2)/2D}f(b) (A3) and the unique formal Bayes estimator under loss function (3.12) is Br(Sl, S2) =BbIS1, -g(B bIS,S2)dB, db. (A4) 0B,j B2 1 But f Bg(B1, b j S1, S2) dB1 = f Btk+-lb2ka exp {- B1(S1 + bS2)/2D} dB, f(b) (S 2D V Ff(k+-1+1)bik2f(b) -( 1 )k+-+l?V 2D b kV-2 l I(Fk, - 1+ 1) b2 f(b) (A5) Applying (A5) with 1= 1 and 2 to (A4) gives (3.21)-(3.22). To obtain (3.23) we need the following result, which can be obtained easily by differentiation: let Z be a random variable with mean ju and variance a2, and suppose we wish to minimize E[oZ+ o -y]2 over oz for 3, y fixed constants. The the best choice of oz is oz - U2 + (p __ aUq (A6)</page><page sequence="22">400 EFRON AND MoRRis - Combining Possibly Related Estimation Problems [No. 3, and E[]= U2+(P+ )2 (A7) By Corollary 1, minimizing fl RSL(b, 61) f(b) db is equivalent to choosing Pi to minimize | E1 -[V+by2] p1(V 1}2f(b) db (A8) which, reversing the order of integration, equals f f{ [ i1+ b] p1(V1) -+ f(b 1) V2 h(V) db dV1. (A9) Here h(V,) is the marginal distribution of V, and f(b I V,) the conditional distribution of b given V1 obtained from the joint density f(b) hb(Vl), hb(Vl) as given in (3.19). Applying (A6) to the inner integral in (A9), with b in the role of Z, pl(Vl) = c, VJ1V2 and V1-1 = y, shows that the optimum choice of pl(Vl) is *(V ) l V1+ V2 ,1V) (AlO) p(1 =2 Vr2(V1) + [V1 + V2 (V1)]2 where tk(Vl) and cr2(Vl) are respectively the mean and variance of b under the conditional distribution f(b f V1). Finally, we show that (AIO) is identical to (3.22). Letting Ih(Vi) = f bbk++j[V +b2]-*k++l) f(b) db, the right-hand side of (3.22) equals [V1o+ 2I1]/[VZ2 Io+2VE 2I4+ V2I2]. But IIO= (V) and I2/10 = ar(11) + p2(V1), showing that this last expression equals the right-hand side of (AIO). Proof of Theorem 2 By (4.3) JRSL(b, 8) f(b) db = J RSL,(b, 61) 1)b db + J RSL2(b, S6) Ib db. (A+1) By changing variables from b to 1/b in the last integral and making use of (4.2) we can rewrite this as jRSL(b, 6) f(b) db = RSLI(b,. 61) +b + RSL2(1/b, 62) f+b db. (A 2) But for any rule 62 we have RSL2((B1, B), 62) = RSL1((B2, B1), 82) and hence RSL2(l/b, 82) = RSL1(b, 62), SO that {RSL(b, 6) f(b) db= { RSL1(b,60f+(b)db + RSL1(b,62)f+(b)db. (A13) It is now obvious from Theorem 1 that the rule defined by (4.7)-(4.8) minimizes both integrals on the right-hand side of (A13) and therefore minimizes RSL(b, 6) f(b) db.</page><page sequence="23">1973] EFRON AND MoRRis - Combining Possibly Related Estimation Problems 401 Proof of Theorem 3 By changing variables from b to 1/b we see that biki[V2 + bV1]-(2k++i) f+(b) db = bik[V1 + bV2]-(Jk++j) bj+1 f+(b) db, (A14) where we have used k1 = k2 and f(1/b) = b2f(b). Applying (A14) to formula (3.22) gives V1 p*(V) + V2 P*(V2) - V{J| bikkV2 + bV2] -k+ f+(b) db/ f bik.[V1 + bV2]-(ik+ -1) f+(b) db} + V2{f blk4v1 + bV2]Ak+ bf+(b) db/ fbJk2[v1 + bV2]-(Ik+-1) f+(b) db} - 1. (A15) To show that p*(V,) &gt; 1 for V1 2 use (3.22) to write P*(Vl) = V1+ V2{f bi+l[V1+bV2]Ak+ f+(b) db/f b2k4V1+bV2]ik+ f+(b)db} -V1+ v2{ bHvl,k+(b) db/ Hvi,k+(b) db} (A16) where Hv,,k+(b) = b ka[V1 + bV2]-ik+ f+(b). Equation (4.10) implies that fbHo.5,k+(b) db/ f HO.5,k+(b) db = 1. Finally, notice that for b &lt; b' Hvi,k+(b VI + bV Ik+ {b' f+(bt)(A7 Hvl,k+(b) LVI + YV2] b Jf+(b) and differentiation of (A17) shows that Hv1,k+(b')/Hv1,k+(b') is non-decreasing in V1. This proves that fbHv,,k+(b) db/f Hvi,k+(b) db is non-decreasing in V1 and hence is &lt; 1 for V1 &lt; 0'5, so the derivative of (A16) is positive for V1 (0 5. Then (4.9) implies that p*(V2) &lt; 1 for V2 , 0'5. If fgoo b- 24f+(b)db &lt;oo then limp*(V,)&lt;oo by (3.22), and (4.9) implies that p*(I) = 1. Proof of Theorem 4 We have k+2 (V k k1-2 k+_2 (Vj( PAVD - ~ ~~ VI pi(VI) (A18) 1 - SI jk1-2 ''</page><page sequence="24">402 EFRON AND MoRRis - Combining Possibly Related Estimation Problems [No. 3, For any fixed value of S2 the term in curly brackets is a non-decreasing function of S1 approaching a limit &lt; 2 as S1 approaches infinity. Baranchik's theorem, as amended by Strawderman (1971), then says that E0{L1(Ol, S1) I S2} &lt; D for all 01, and the theorem follows by integrating over the possible values of S2. Proof of Lemma 2 By (6.4) it is sufficient to show that E(1)[[-T(S+) p1(V1)- 1]+ + (1-B1)]2 E(1)[[[T(S+)- 1]++ (1 -B1)]2 (A19) for b &lt; 1. For b &lt; 1 we have B2&lt; B1 and the distribution of S2, (D/B2) Xk2, can be written as a mixture of distributions (D/BO) X+2j with negative binomial mixing weights j-1 ci = bik2(1 -b)jfl (fk2 + I)/j!, 1=0 see Pitman and Robbins (1949). Since SI (D/B) X1+2 for the purposes of E('), conditioned onj we have S+ - (D/B1) Xk++23 independent of V1'%-Be (k+ 1, 1k2+1). Let E(S+,j) indicate the conditional expectation with S+ fixed and V1 having this last distribution. Then E(S+,i)[() pl(V) - 1 ]+ + (1 - B)}2 &gt; L(S+,j){[T(S+) ,1j) - 1 ]+ + (1 -B&amp; &gt; f(s+,i){[T(S+) - 1 ]+ + (1 - B)}2, (A20) the last line following from Jensen's inequality, the second from the fact that {[pl] + +(1- B)}2 is non-decreasing in Pi' This proves (6.9). For b = 1 all the c= 0 except co, completing the proof of the lemma. Proof of Theorem 5 We use Theorem 3 to verify condition (6.8) of Lemma 2. Define Ai = p*(Vi) -1, i = 1,2, h(j) = Vik, Vik+1-l and hi) = VtkI+ l Viki. By Theorem 3 A1&gt;0,A240 for V1 &lt; 0 5. We have )^ A1+ h)5 A2 = (V1V2)jk+-1 V{ [(p) V Al1 + V2A2]. (A21) Equation (4.9) can be written as V1A,1+V2 A2 = 0, so (6.10) shows that h(j) A1 +hj) A2 &gt; 0 for V1, 0'5. Equation (7.8) follows by writing the right-hand side of (6.7) as 1 + f(hI') A1 + h U) A.) dV1/ [r(k1/2 +1) r(k2/2 +,j)/I(k+/2 +j+ 1)] Proof of Theorem 6 If b &lt; 1 then {[V1 + bV2] p(V1) - 1}2 &gt; {[V1 + bV2] p1&lt;(V1 - 1}2. If b&gt;1 then {[V1 + bV2] pl(V)-1}2 - {[V1 + bV2] p1(V) -1}2 The theorem follows from Corollary 1.</page><page sequence="25">1973] Discussion of the Paper by Professor Efron and Dr Morris 403 DISCUSSION OF THE PAPER BY PROFESSOR EFRON AND DR MORRIS Professor D. V. LINDLEY (University College London): In proposing the vote of thanks tonight, the Research Section Committee suggested that I might discuss the background to the paper and explain why the results in this field are so important to all statisticians; from those who compile official statistics, to potential or actual contributors to the more mathematical journals. It is a pleasure to do this in the context of a paper which significantly adds to our knowledge of the field. Almost certainly the most widely-used group of statistical techniques is that based on a linear model, embracing least-squares and analysis of variance concepts. These useful tools are supported by a body of theory that goes back to Gauss. The theory and the practice march happily hand-in-hand and the whole corpus of knowledge forms one of the most satisfactory parts of statistics. The older parts of this theory assumed the variances to be known, or at least estimated on the basis of large samples. It was a major contri- bution of Student's, enormously extended by Fisher, to show how the unknown variance case could be handled. It is an over-simplification, though broadly true, that this work demonstrated that it does not make much difference if the variances are unknown: the t-distribution is nearly normal, at least for the moderate sizes of samples that are the most useful field for the application of the ideas. Let us therefore consider the linear model with known variances. The theory has data y, having expectations, E(y) = AO, known linear functions of unknown parameters, 0. By a well-known linear transformation on y, we can consider data, part of which contains xi having E(xi) = 6i, the rest having zero expectations and being irrelevant. With a normality assumption we are almost at Efron and Morris's (1.1), except that the dispersion matrix will not necessarily be a multiple of the unit matrix. This can be corrected by a further transformation but then we shall lose the squared-error loss. However, we shall have a quadratic form instead, and both the least-squares theory and the new results to be discussed are unaffected by this; so essentially we have (1.1). Now comes the crunch-and notice it applies to the general linear model. The usual theory says xi is the best estimate of Oi, but Stein showed that there is another estimate which is, for every set of 0's, better than it, when judged by the squared-error criterion,, except when only one or two parameters are involved. In other words, using standard criteria, the usual estimate is unsound. Further calculation (described in the paper) shows that it can be seriously unsound; with 10 parameters, quite a small number by the standard of present-day applications, the usual estimate can have five times the squared error of Stein's estimate. And remember-it can never have smaller squared error. (Just to digress for a moment, there is a way out. You can demand unbiased estimates. For xi is the only unbiased estimate of 0? based on sufficient statistics. It does not require any theory to find the best in a class of one. But why use the concept of unbiasedness? This question has been asked before, but a satisfactory answer has not been received. Invariance is another way out.) Returning to the general theme; the result of Stein's undermines the most important practical technique in statistics. Though as Bartlett, Kempthorne and Novick have else- where pointed out, some statisticians, notably in genetics and education, have recognized this for almost 50 years. But next time you do an analysis of variance or fit a regression surface (a line is all right!) remember you are, for sure, using an unsound procedure. Unsound, that is, by the criteria you have presumably been using to justify the procedure. Worse is to follow, for much of multivariate work is based on the assumption of a normal distribution. With known dispersion matrix this can again be transformed to the standard situation of (1.1) and consequently, in all cases except the bivariate one, the usual estimates of the means of a multivariate normal distribution are suspect. Though it is tangential to tonight's paper, it is relevant to remark that the usual estimates of the variances and covariances are also suspect. Only here we have not a better estimate to use. How does one make inferences about a dispersion matrix?</page><page sequence="26">404 Discussion of the Paper by Professor Efron and Dr Morris [No. 3, These arguments surely justify us in thinking that this work is important and that we should take a further look at the problem. But they do not mean that we should accept Stein's estimator, for its use also poses problems, both of practical and theoretical interest. For one thing, the estimate of 0 uses x; for j# i, surely a curious procedure. With his remarkable ability for putting his finger on a key point, Barnard has made the suggestion quoted by the authors-why not lump all our problems together? Tonight's paper helps enormously in our understanding of this dilemma. But there is another way out. The authors reveal quite clearly that their estimates are not obtained by completely wild guesswork, but are organized by being produced within a Bayesian argument. I suppose they, and others, would say that the only guesswork goes into the prior-for example (3.20). Having used the Bayesian tool they throw it away and look at various forms of risk function. However, this treatment of the Bayesian argument ignores an important point. Probability distributions over parameter space have an operational meaning; they are not just tools to be discarded at one point of the manufacturing process, but are always with us. On the theoretical level, Efron and Morris write as if Savage, Ramsey or de Finetti had never existed. On the practical level, they forget that in any application 6i refers to a physical thing, like hogs in Montana. According to this Bayesian argument, the probabilities for the O's would be assessed in practical terms (including, for example, one's knowledge of hogs). If that for 01 were independent of that for 02, then the problems would not get drawn together by the Bayesian argument: if dependent, then they might be. (The common thread in Efron and Morris's treatment is their B1 and B2, united in (3.20).) Thus, I feel that one baseball player's performance tells me some- thing about another's, but probably nothing about hogs in Montana. My argument is, that by ignoring the meaning of the parameter distribution, Barnard, the authors and others have created their own difficulties. Even though the coherence feature is ignored, the authors do add significantly to our knowledge of sensible procedures. For example, they reinforce the view that expressing "vague prior" knowledge by using a uniform, invariant, prior distribution is unsatisfactory. Their remarks that "''complete ignorance' is a difficult concept to express in multi- parameter problems" and that "the uniform prior ... is too ignorant" are very apposite. One way of regarding their calculations is to look upon them as investigations of the robustness of Bayesian procedures. For example, the superiority of 8(3) to 83(comb) can be interpreted as showing that someone who does combine his problems, because his prior tells him to, is liable to go astray if they are not in reality so connected. As Box has pointed out, we have to think carefully about those aspects of our prior that influence the subsequent analysis. However, this robustness study-if that is what it is-is based on risk considerations: that is, on integrations over sample space. Now a Bayesian does not regard this as a sin. He does such an integration when he carries out a preposterior analysis. However, in that case he knows what his sample space is. Efron and Morris never mention theirs. Without any apology or explanation they integrate over all samples of the same size (namely one). Why? I do not expect them to be able to answer that question, but they might be able to say what would have happened with a different sample space; would their conclusions have persisted? I know little about baseball, but had it been cricket, the fixed variable in Table 1 would have been time, and during that time the number of innings would have been a random variable. Obviously more complicated spaces can be imagined. To a Bayesian concerned only with posterior analysis, such considerations are irrelevant. A final point is concerned with the estimation procedure itself. In most practical situations it seems to me that a point estimate on its own is an inadequate answer to the real problem. A more satisfactory response is to give the posterior distribution, the location of which is all that the point estimate usually provides. Would the authors go this far? If not, would they feel it necessary in some way to add to the mere statement of the point estimate-for example, by providing something analogous to a standard error? I mention this because, in a recent paper with Smith, we used modal estimates. More</page><page sequence="27">1973] Discussion of the Paper by Professor Efron and Dr Morris 405 detailed calculations show that it can happen that the joint mode for several parameters can be misleading in regard to the distribution of one of them. For example, estimating A using X(x, - x)2 may not be as good as using 1(85-8')2, a substantially smaller quantity. The topic of tonight's paper is important to all of us. The authors have added significantly to our knowledge of the field and it is a pleasure to propose a well-deserved vote of thanks. Dr. J. B. COPAS (University of Essex): Tonight's paper brings to light a most intriguing aspect of Stein estimation. I was very happy to see the authors' data example, since it happens to be of a very similar form to some chemotherapy trial data which I myself have considered with a view to applying Stein-type estimates (Copas, 1972). The published estimates on our two sets of data are therefore tantamount to applying the "separate James-Stein rule" of tonight's paper, and so it was with considerable interest that I read that by combining these two groups by using the authors' 68), it would be possible to improve the accuracy of my estimates. I confess to being a little hesitant in allowing the fortunes of baseball players to influence the success rates of drugs, but the authors seem to be telling us to lay aside such fears and to leave all to their Bayesian wizardry. The data to which I am referring were the response frequencies in 10 control sets of patients who were all given the same treatment, but each set was part of a clinical trial undertaken in a different institution. There are thus 10 binomial observations, each with n = 23 (to make a slight simplification). The angular transformation gets the data into the form of the paper, leading to 10 observations in the range 11.970 to 39.760 with VD = 5.97?. Now when the angular transformation is applied to the baseball data to give the second group, we will be in the situation of unequal D's, as in the baseball data we have n = 45. This is discussed in Section 7, and the authors' advice is equivalent to an appropriate rescaling. Thus I used the transformation 1(45/23) sin-' Vp giving the 14 observations of the second group in the range 34-9 to 54-87 with the same D as before. Now b, which is the ratio of the marginal variances of the two groups, is clearly not too far away from one, and so 80) will presumably conclude that the groups are rather closely related, and so give estimates similar to those of the combined James-Stein pro- cedure. Thus the batting averages will pull up nearly all of the drugs estimates, while the drugs data will tell us that the batters have been unusually lucky and that their averages should all be pulled downwards. Now in combining different groups such as these, it will nearly always be necessary, as it is here, to transform the data to match the variances. Unfortunately, however, such transformations are not unique, and the baseball data would be equally well suited for this purpose if transformed by the alternative form of the angular transformation, leading to -4(45/23) sin-' (1 -2p). This gives the second group in the range 28.040 down to 8.070. The estimates for the drugs will now be pulled downwards, in the opposite direction to before, whereas the batting averages will still be lowered owing to the inversion in the second transformation. The point to be made is that when the problems are entirely different in nature, it will never be clear which transformations to use. Exactly the same difficulty occurs, of course, with the James-Stein estimate itself. We are told that the problems are related if their parameters are near zero. But how close a parameter is to zero depends on the location of the measurement scale; for instance, is 0 the speed of light, or is it the small correction from the conventional value of 3 x 1010 ? All of these estimates depend critically on such choices of location.</page><page sequence="28">406 Discussion of the Paper by Professor Efron and Dr Morris [No. 3, I simply do not believe that the efficacy of combining two different pieces of information can be dictated by a mathematical formula without reference to the contexts. It is nonsense to let baseball scores effect the inference from a clinical trial, and I can only suppose that, although my value of b is near one, I have somehow hit upon an exception to what the authors describe as "all reasonable situations". Now the intriguing thing about all this is why does the mathematics suggest that we should carry out these calculations in circumstances when they are clearly nonsensical. The claim of the Stein estimate is that it has a uniformly lower total risk than the m.l.e. As the authors' own work elsewhere has shown, however, a small modification to the Stein estimate would be used in practice, to guard against the disastrous estimates given to components whose parameters happen to be far removed from the majority. The result is then the situation typical of compound decision problems. The risks are essentially redistributed amongst the components such that most do well but a few do badly; the aggregate risk is very small in one part of the parameter space, but slightly larger in another. The question is, how much of the parameter space is in the favourable region, and what importance should be attached to it? With Stein's estimate, for instance, the risk depends essentially on the size of 1 02. If the Efron-Morris modification is being used, the rule will be better than m.l.e. if 102 &lt; Kand slightly worse if Z 02 &gt; K, where Kis some constant. If the problems have no connection, and 0 is regarded simply as some member of k-dimensional space, then nearly all possible 0's will of course be in the unfavourable category. In the mathematical analysis, however, E 02 itself is usually taken as the parameter of the problem, or, in the Bayesian model in tonight's paper, the 6i's are collapsed onto parameter A. Now the use of Z 0 implies a kind of exchange rate between the components, and corresponds to a scientifically meaningful quantity only when there is a physical connection between the contexts of the components. Similarly, some con- nection is being assumed in the Bayesian model, since as soon as a prior distribution is put onto A, the 6i's become correlated. In the same spirit, it is clear that the "preliminary test estimate" mentioned in tonight's paper rests on testing a hypothesis which only has scientific meaning if the contexts of the problems are related in some appropriate way. The same situation holds, I feel, when it comes to the combining of the two groups of problems that Efron and Morris have been discussing tonight. If the groups have no connection between them, then the parameter b is a purely formal construct. If A1 and A2 can take any values whatsoever, then one should not be surprised if the value of b were very extreme, which is the circumstance in which these procedures perform relatively badly. To look on the positive side, however, there is a circumstance in which I find the authors' compromise rule appealing, and that is the case when the groups do have some similarity of context, and yet there is real doubt as to whether complete comparability can be assumed. For example, in the case of the left-handed batters and the right-handed batters suggested in the paper, I might perhaps be excused wondering whether this distinction is really of any importance. It would then be perfectly sensible for me to look at the data to see if there is a difference, and that is precisely what, in a more formal way, the authors' procedure is doing. Now in the problem being considered, it will be virtually impossible to formulate a realistic prior distribution which one could hold with any conviction, and so I fully agree with the authors' approach of examining the value of the risk functions. In multiparameter problems, however, this job is very difficult, and I am not convinced that the device of collapsing the entire k+-dimensional parameter space onto the one parameter b is a valid one. Whereas this may be an acceptable way of producing the estimate in the first place, it is surely too restrictive as a framework for evaluating the resulting risk functions. For instance, we note in (3.1) that the means of the two normal distributions describing the parameters are assumed to be both zero, or what is the important thing, they are assumed to be equal. Thus the two groups are assumed to cluster over each other. This is surely an unduly favourable situation, and evidently does not hold for the drugs and baseball</page><page sequence="29">1973] Discussion of the Paper by Professor Efron and Dr Morris 407 groups mentioned above. I notice that this condition is relaxed in (7.7), but the discussion there relates to the estimate (7.8). Unfortunately (7.8) does not answer the question being posed, since the combined James-Stein rule is no longer a special case; the points are being pulled into their respective group means and not towards the grand mean. The effect of combining the groups is in the strength of this pull rather than in its direction. In other words, it is not treating the combined set as one of Barnard's grand melees. It seems to me that if all the points are pulled towards the grand mean, then the combined James-Stein rule depends more critically on the difference between the group means than on the ratio of the marginal variances. As an example, take the case when k, and k2 are equal, and large. Then, if we centre on the grand mean and take the group distributions as N(- M, A1) and N(M, A2), the combined rule will be essentially oij ( 1-D +2+A) xii, where A = '(A1 + A2). Now to make the relative regret depend on the Ai's only through the authors' parameter b, we must also scale the value of M relative to the sizes of Al and A2. To do this, define V{(Al + D) (A2 + D)} =(B D B2) D-. Then a straightforward calculation gives the total risk as RSL = (1-b)2 + 4b012 + 4(1 + b2) obi(1 + b)-1 (1+ b +2cabJ)2 If c, = 0, which is the assumption made in the paper, this is Vl+b} which rises slowly from zero as b departs from 1 in either direction, this being in line with Table 2. However, it rises sharply if a increases from zero, namely if the group means start to move away from each other. As an illustration, if D = 1, A1 = A2 = 4 and M = 2, which is rather similar to the configuration of the drugs and baseball groups, we have RSL = 4. If D = 1, A1 = 4 and M = 0, on the other hand, A2 has to fall right to zero before the same value of RSL is reached. I am sure the authors will agree with me that not all the mysteries of this intriguing topic have been resolved, but I congratulate them on their contribution to its discussion. I have much pleasure, therefore, in seconding the vote of thanks. The vote of thanks was carried by acclamation. Professor JAMES M. DicKEY (University College London): Suppose one is determined to act as a literally honest sampling-distribution statistician. Before observing and plotting a list of numbers, such as the authors' 14 baseball players' preliminary batting averages, one chooses a joint estimator or predictor, say the modified Stein-like 873, having all the latest features. But then one looks at the data and finds that they spell "Apparently Something Other"; for example, the data may show a partition into groups not anticipated by 873, or there may be a strong apparent dependence on a covariate, such as the players' defensive field positions, or et cetera, et cetera. I suppose, in practice, one would then abandon his prechosen estimator 873, and fabricate yet another, more suitable, 874. If this is true, then of what practical importance is the "risk" for 873, either the traditional function of 0 or its "Bayes" version integrated over 0, both involving integrals of the "loss" over the sample space? Should not one be calculating, if one insists on a risk, the integral over the sample space minus all the 15</page><page sequence="30">408 Discussion of the Paper by Professor Efron and Dr Morris [No. 3, "Apparently Something Other" subsets on which one will not use 873. But, perhaps, the risks that have been calculated are intended as approximations to integrals over the deleted sample space? The Bayesian view is that risks are useless for analysing specific already-observed data, even the "Bayes" risk (misnamed "Bayesian" in the paper). The posterior expected loss, involving integration only over 0, may be of interest for specific data when estimation is necessary; and the preposterior expected loss (the "Bayes" risk), is useful for planning experiments. But when the traditional statistician would "estimate", the Bayesian statistician would prefer to describe the posterior uncertainty by descriptive statistics about the posterior distribution. Better yet, he would investigate and report interesting aspects of the dependence of the posterior distribution on the choice of prior distribution, for prior distributions which model real persons' plausible flesh-and-blood uncertainties. The search for a "completely ignorant" prior distribution can be seen as a modern analogue of the search for the Holy Grail. In particular, constant-ordinate prior densities over high dimensional Euclidean space embody unrealistic prejudices for extreme 0 values, because of the surface-volume effect. Savage's "precise measurement" or "stable estimation", the mathematics of which applies for any approximating prior density, is not relevant to practice with a constant-ordinate approximating prior density in high dimensions. For suppose a prior density is approximately constant over an n-dimensional ball of radius 2r. A corresponding "precise-measurement" n-variate Normal (m, a2 I)_ proportional likelihood function will need to be centred at an m-value within a con- centric ball of radius 2-r - 2u. But the approximate prior probability that the true 0 lies outside the concentric ball of radius 2-r - , is arbitrarily large for large enough n. Hence, for such n, it is a priori improbable that "precise-measurement" will apply. Oscar Kempthorne once asked me, "What about the data? Don't you Bayesians need the data ?" In data-analytic practice, everyone worth his salt seems to choose the statistical sampling model (if any), used in reporting the data, by looking at the data. A Bayesian need only choose the likelihood function. But how can a Bayesian use the data to choose a likelihood function for reporting the same data? As follows. An open-minded person will have an infinite-dimensional parameter space as the full support-set of his actual prior uncertainty. No zero probability-density ordinates. But sometimes the predictive distributions can be usefully approximated as a mixture over a finite-dimensional para- meter, say 0, P(y) = JP(y I 0) dP(O). Now the data y can be used to decide whether the dimensionality can be reduced further, say to 7 alone, -q = vq in 0 M (4, -q), by calculating the posterior distribution of the nuisance parameter P(-q I y), and its effect in posterior predictive distributions of future data y', P(y' Iy)= P(Y' I Y) dP(@ny) P(Y' I o, y) (?) where P(Y I , Y) = JP(Y' 1 X,dP(4 ? , Y). The data analyst should try out enough departures from his eventual choice of likelihood function to guarantee to his scientific-report readers that their own superparametrized uncertainties would lead to approximately the same conclusions. At the least, he must clearly state the likelihood function assumed. Professor M. STONE (University College London): A very attractive feature of tonight's paper is the way the authors have navigated successfully on a blend of statistical essences,</page><page sequence="31">1973] Discussion of the Paper by Professor Efron and Dr Morris 409 thereby "hedging" their "Bayesian bets". While I have yet to explore at ground level the territory their elegant piloting has introduced us to, the fields and hedgerows already look interesting to say the least. However, I would now like to press the claims of a quite different approach in which neither normal distributions nor unspeakable priors play the slightest role-and even parameters are kept at bay. It is suggested by the remarks of Mosteller and Tukey (1968, p. 109) on the topic of "cross-validation". I hope the present authors will excuse the fact that what I will say rudely ignores their development and backtracks several years. For data consisting of r replications in each of k groups {xi,, i = 1, ..., k, g = 1, ..., r}, the problem is, we suppose, to provide an estimate of the "location" of each group. Very specifically, we would like to choose A in the estimator xi= Axi+(1 -A) (k-l)-1(91+ .** +Xi-1+Xi+l+ *.. + 7k). Least-squares estimation, that is, the choice of A to minimize .E(xij-x ?)2, yields A = 1 and x xi, leaving us vulnerable to attack by the statistically sophisticated. So we invoke cross-validation and choose A to minimize C = -{Xi X( Aj)}2 where x(-j) = Aci(-j) + (1 - A) (k - I)-'(x1 + ... + xi+I + gi+1+ ... + gk) with gi(-j) denoting the average of the ith group omitting xie. Elementary algebra yields xi = x+ i - ) (Xic) k-I~~~~~~~~~~~~~~~* =9+ k(r -1) F-kr + kl -}x-) ( where F = MSbIMSW with customary notation. When F is large, this is approximately ((r-1) F( ) When D is estimated unbiasedly, the Lindley estimator (1.3) takes the form x+ 1l_ (k -3) )(-x (* The coefficient of t - x in (*) is less than that in (**) if and only if F&gt; (k-l) (k - 3)/ {k(k- 3 + 2r)} which is less than unity. However, the coefficient in (*) is negative if and only if F&lt; 1 + {k(r - 1)}-1 which exceeds unity. So we see that (*) is a "bigger shrinker" than (**). The inefficiency of {x} compared with (*) may be plausibly defined as (CA-l/Cmin) -1 and is found to equal (k - l)/{k(r - 1) F}. This is approximately independent of k and approaches zero as (r- 1) F increases. Cross-validatory estimation has, I believe, several advantages. (To be encouraging, I will keep quiet about its obscurities!): (i) It is model-free without being artefact-prone. (ii) In view of (i) it is unnecessary to test any assumptions- there are none! (iii) It is easy to understand and manipulate. (iv) It is a bootstrap technique which, when applied forcefully, gives its user the interesting sensation of statistical levitation. Mr A. P. DAWID (University College London): Once the maximum-likelihood estimator reigned supreme, but now the king is dead. What a golden opportunity to set up a democratic judicial system to deal with each case truly on its own merits. But what do the people do? They put up rival champions and battle for the succession. What is good about a James-Stein type estimator? It is true that it beats the maximum- likelihood estimator, which is therefore bad; but what merit is there in being better than something bad? Barnard's question is misconceived, since we can quite happily abandon</page><page sequence="32">410 Discussion of the Paper by Professor Efron and Dr Morris [No. 3, maximum likelihood without favouring James-Stein. It is the same misconception that judges an admissible estimator to be "good", when all one is entitled to say is that inadmissibility is bad. If, in the set-up of Section 1 of the paper, one truly feels that the different 0's, referring to completely different problems, cannot reasonably be lumped together, there is no compulsion to do so. Independent, proper prior distribution for the O's will lead to an admissible procedure that keeps the problems separate. This procedure may not dominate maximum likelihood, but since when has that been a criterion of estimation? The unknown Norman archer who got King Harold in the eye did not become the next king of England. If, together with independence, the prior distributons were supposed to have long tails, like the Student distribution, the estimate based on data a long way from the prior mean would approximate the maximum-likelihood estimate, as is the case with the estimates considered in tonight's paper. I do not want to attack the various procedures investigated in this very thorough and mathematically interesting paper. They are clearly of value in certain circumstances, and a knowledge of their properties is important, if only so that one might decide that they are all inappropriate to a particular problem. But there is clearly a great deal of arbitrariness involved, not only in the choice between the procedures, but, more important, in the framework in which they are embedded. An estimator which is derived by means of an argument which involves prior distributions will not necessarily be sensible unless the prior reflects a sensible state of information. Now I am all in favour of arbitrariness, so long as it is recognized and can be allowed for. I should be very sorry if the estimators "recommended" in this paper are taken up and applied blindly in inappropriate situations. Two more points. Firstly, what is the authors' justification for focusing attention on relative savings loss (RSL) rather than Bayes risk R? From (3.8) and (3.10), (3.9) has Rl{(B1, BA) 81} = D [(1 - B1) + A(B )B AS1, S2) - B1))] and this (or, equivalently, just EA(i,EB{(S(Sl, S2) - BJ)2/B1}) would be just as easy a loss function to work with as RSL1, and more justifiable. The Bayes estimator for a prior on (B1, B2) in the original problem would be (3.9) with A, the Bayes estimator in the new problem (using R1) for the same prior. Using RSL1 leads to a modulating factor B1, as is transparent in the second paragraph following Theorem 1. Lastly, I should like to add Zidek (1969) to the list of references. Theorems 1 and 2 are effectively special cases of his results. Dr A. F. M. SMrrH (University of Oxford): What are we to make of the James-Stein estimator? Should it be taken seriously as a practical estimation procedure? Does it have anything useful to tell us about methods of statistical analysis in general multi- parameter situations? The following preliminary observations seem to be relevant: first of all, a parametric model is usually a representation of some actual real-world situation (albeit in a schematic and idealized form); it follows that parameters have an interpretation, a semantic content, which derives from this underlying real-world situation; moreover, by virtue of the relationships which exist between elements of the actual situation that one is modelling, there will be relationships between certain groups of parameters (and the more parameters that are involved, the more likely this is to be the case). An important prerequisite for any analysis of a multiparameter problem would seem therefore to be the assessment of the nature and degree of such relationships. The search for methods of incorporating such an assessment has motivated the work of Lindley and Smith (1972) and Smith (1973a, b). In their introduction, the authors also refer to the notion of "relationship" between parameters, but they seem willing to regard parameters as "related" merely because they</page><page sequence="33">1973] Discussion of the Paper by Professor Efron and Dr Morris 411 are thought to have approximately equal numerical values. It is in this context, rather than that of realistic assessment as outlined above, that the James-Stein estimator lays claim to serious consideration. No wonder then that it is seen as "some sort of mathematical trick" and no wonder that to be a better physicist it seems necessary to acquire a rudimentary knowledge of pig-farming! Does anyone seriously believe that the (arbitrary) scales of measurement of the observations are more relevant than the (actual) meanings of the parameters? If not, there seems little reason to take the James-Stein estimator seriously. On the other hand, it does seem to me that a careful (necessarily subjective) assessment of the actual relationships between parameters is an essential ingredient in the analysis of a multiparameter problem, and that routine application of "standard" estimators, etc., is misguided. The following examples are intended as illustrations of possible forms of assessment, and of their effect on subsequent inferences. They are relevant both to a consideration of the James-Stein estimator itself, and to the "combination" problem posed by Barnard. We suppose that we have a number of observations xi, independently and normally distributed with means Oi, and variances cr2 (we assume the latter known, but our results are much the same in the unknown case). The problem of interest is the estimation of the Oi. (i) Parameters unrelated. This assessment requires that we assign priors independently to the Oi. It is easily seen that the estimate of Oi does not depend on xj (j ? i), and that in the particular case where we assign vague (uniform) priors to each Oi we obtain the posterior mean O* = xi. (ii) Parameters exchangeable. We shall not give any details of this case (it is discussed at some length in Lindley and Smith, 1972), but merely note that we are led to 0*= (1-w) xi + wi x, where wi is a weighting factor (cf. the authors' estimator (1.3)). (iii) Parameters lying on a response-surface. It may be that the 0i are in fact treatment effects corresponding to dose-levels, dl &lt;d2 &lt; ... &lt;dk &lt; ..., and that this may lead us to believe that the 0i lie, approximately, on a form of polynomial "response-curve", r(d). In this case, it can be shown (Smith, 1973b) that we obtain an estimate as* = (I1- w) xi + wi rF(di), where ?(d) is an estimate of the response-curve. (iv) The "combination" problem. Suppose that our assessment leads us to group the parameters so that OT C (Or, O2T, *., 2 ); i.e. an obvious relationship holds between the Gi, for any given i. Let us assume, for convenience of exposition, that the relationship is the exchangeable one. Suppose further that we also regard the groups as exchangeable. One way of expressing this is to take xij - N(0ij, or), Oi _ N(,), T2 ' N(p, A2), with a vague (uniform) prior for ,. The resulting posterior mean for Oi, can be shown to be oi*j = (0-Wi) Xij + WAi(- Wi) Xi. + Wt X.., where wi = (a-2 + T- 2)-1 ai 2 Wi = (r7i2 + rnl A2)-1 T72. The "strengths" of our beliefs in the various "within" and "between" relationships are measured by the relative magnitudes of the ri2 and A2; by varying their ratios, one sees how the above form "automatically" deals with the "combination" problem. (v) The "outlier" problem. A deeper form of (iv) arises when we believe that such a grouping of the parameters exists, but are not sure precisely which parameters belong to which groups. The simplest case is that of a single "outlier". I regard all the parameters as exchangeable save one, but I do not know a priori which is the odd man out (e.g. I have reason to believe that an unidentified candidate from among a group taking an examination may well be of a far higher standard than the others, who are roughly comparable. I would not wish to use (ii) in estimating the "outlier's" true score). One way of setting up a prior structure is the following: acting as if Oi were the outlier, take an exchangeable prior for the Oj (j1 i); now, to incorporate the fact that we regard each individual as equally likely to be the outlier, take a mixture, with equal probabilities, of these priors. It can be</page><page sequence="34">412 Discussion of the Paper by Professor Efron and Dr Morris [No. 3, shown (and I hope to report elsewhere the details of this and other studies) that we obtain as posterior mean as* = wi Xi + (1- Wi) X(i), where x(j, is a complicated form of overall mean, and the wi are such that the weighting effect towards xi is relatively greater when xi is itself more "untypical". I should like to end by recording how much I have enjoyed this and other recent papers by the authors. They have posed many interesting problems and have examined them with exemplary thoroughness. Although I disagree strongly with their emphasis, I for one have been much stimulated by their work. Professor A. BIRNBAUM (Statistical Laboratory, Cambridge University): Since this paper is an interesting and original contribution to estimation in just the sense of decision theory, primarily non-Bayesian, it is unfortunate that the introduction implies that the paper has particular significance from the standpoints of non-decision theoretic non- Bayesian inference, whence have stemmed the principal reservations about the com- pounding approach. Surely the most basic point among such reservations is an insistence that there also exist such non-decision theoretic standpoints. If such standpoints may appear more elusive or problematic than formal decision theories, this is insufficient reason for ignoring their existence or nature. It may help to recall that some features of these inference standpoints are represented by simple examples often used to illustrate the intuitive content of concepts such as sufficiency, conditionality (ancillarity) and censoring (Pratt's concept). More simply, such examples can be taken as prototypes of situations in which an investigator's inferences appear reasonably to be not in correspondence with some axiomatic tenet of formal decision theory. (My own most systematic effort in this direction is my 1969 paper, which also discussed the compounding approach briefly.) Professor M. S. BARTLETT (University of Oxford): I rather agree with Dr Stein's remarks (see p. 414) on the details being complicated. This seems to include most of the contributions as well, including his own contribution (though not perhaps Professor Plackett's). It is therefore difficult to see the wood for the trees; I would suggest avoiding the Bayes-non-Bayes additional complication by concentrating on applications with an essentially statistical population context as in the example at the beginning of the paper. Professor Lindley mentioned some of the earlier work, but somewhat vaguely, so may I remind you again of the relevance of Godfrey Thomson's estimators in factor analysis, to which I drew attention in the discussion on Dr Stein's 1962 paper to this Society- regression estimators, as he called them. The example is an extremely special case of his estimators where there is one factor, one test and one specific, so that (before standardization) Xi = Oi+s. We easily obtain the Godfrey Thomson estimators (after de-standardization) 6i 'go (Xi X )+X U0 ax ax or U= {1-(af2/$)} EE - [{(K -3) uI}/{Y(xi - x)2}]J (xi - x) + x, which is the Lindley estimator. The Oi estimates have what I would call interesting rather than important properties, which can be compared with what are called the maximum-likelihood estimates Oi; as indeed Godfrey Thomson and I did at the time. One point I have previously noted is that the information in the two sets is the same as one set is merely a scaling of the other.</page><page sequence="35">1973] Discussion of the Paper by Professor Efron and Dr Morris 413 One's attitude to their relative value will depend on whether one is a statistician studying simultaneously the whole set of Oi or happens to be concentrating on a particular Oi; it thus depends on one's focus of attention, a point I think was indicated in the remarks of the seconder of the vote of thanks. The example I was going to illustrate this by is one already referred to by Professor Barnard, but let me highlight it rather more. Suppose a student is expecting and hoping to get a first-class result in his examination on which perhaps his future career depends; and in fact turns in a very creditable performance. To his chagrin he fails, however, to reach his objective, and is astonished to hear in confidence that with last year's method of assessment he would have succeeded. The examiners' new statistician had, however, adopted a system of assessment which lowered his score towards the overall mean. The focus of attention is obviously now on his individual performance, especially if he decides to sue the examiners for damages. Mr G. N. WILKINSON (Rothamsted Experimental Station): The "inadmissibility" of the multivariate normal sample mean demonstrates, for me at any rate, that "admissibility" is not a relevant criterion of estimators in scientific inference. The authors' own example in Table 1 illustrates the kind of distortion of future inferences that can arise if James-Stein estimators are used to summarize current information. If one graphs, as I have done just now (Fig. 1), the final baseball batting averages against 0:4 0 0 0:3 0 0 O * 0 0O 0 0 0:2 0:2 0:3 0:4 Initial FIG. 1. Plot of final batting averages from Table 1 against (i) initial batting averages 0; (ii) James-Stein estimators 0.</page><page sequence="36">414 Discussion of the Paper by Professor Efron and Dr Morris [No. 3, the initial, one obtains a parabolic-shaped plot which, judged in relation to the 450 line of no empirical change in batting performance, suggests that players of middling initial performance tend to equalize their final batting averages, that initially high scorers fall back in performance somewhat and that initially low scorers tend to improve their performance so much as to surpass that of the initially middling players. A corresponding plot against the James-Stein estimators, however, produces such a severe distortion of this picture that a proper inference is not possible without recall of the initial (maximum- likelihood) estimates. Dr J. A. NELDER (Rothamsted Experimental Station): When is the loss function sensible? If X1, X2 and X3 have units of measurement gm, miles and ?K, it seems pointless to calculate a loss function. If the Xi have the same units, say a weight, but refer to a mixture of elephants, mice and fleas, then presumably the methods of the paper would lead to estimators that recognize the existence of the three groups. I would prefer to look at this process as one of identifying populations, which seems to me fundamental to scientific inference. The populations of the scientist are quite different from the sets or classes of the logician, which can be quite arbitrary. Those of the scientists must be useful for prediction. To me it seems that if the loss function is sensible, then the parameters are exchangeable, and we can act as if the group were a sample from a population. If so, then the likelihood must be modified to take account of this. How this can be done I indicated in my contribution to the discussion on Lindley and Smith's paper. In particular, I regard the estimators Dr Smith has given this evening as maximum-likelihood estimators. Thus maximum likelihood is not, as a previous contributor speculated, dead but very much alive; however, the likelihood must be properly specified. I regard the contrast in the paper between maximum likelihood and other estimators as largely spurious. This paper seems to me to raise in an acute form the problem of how mathematical abstractions are to be usefully made in discussing inferential problems. Inference itself is a problem-solving process in which many false trails are followed, with subsequent back- tracking. The traditional abstractions of mathematics may prove to be much less useful in understanding this process than we currently imagine; indeed they may actually obstruct understanding. I have to confess that I find the introduction to this paper to be situated in an inferential vacuum. The lack of postulated relationships between the x's or the 0's, the lack of any consideration of measurement theory, make it difficult for me to accept the development as necessarily having any relation to problems of inference; we need to look very warily at the basic specification of any deductive system that purports to shed light on inference theory. The following contributions were received in writing, after the meeting: Dr C. STEIN (Stanford University): The results of this paper are very interesting and, like some earlier papers of Efron and Morris, correct a serious practical defect of the James-Stein estimate, but the details are somewhat complicated. Recently, in response to a question of Malcolm Hudson, a student at Stanford, a general approach occurred to me that is likely to be effective in this problem, although I have not yet worked out the details. Let X be a k-dimensional normal random vector with vector mean 0 and covariance matrix the identity. It is not difficult to prove by integration by parts that if g: S1k + is twice continuously differentiable and the expectations exist, then 2 EO 11 X+V logg(X)_0 112= k-EO I 6lV logg(X) II2- - V2 g(X) g(X) 1() where V is the vector of first partial derivatives and V2 = 1(a2/lx2) the Laplacian. It follows that if g is superharmonic (V2 gS 0), X + V log g(X) is minimax. The right-hand</page><page sequence="37">1973] Discussion of the Paper by Professor Efron and Dr Morris 415 side of (1) yields an unbiased estimate of the risk. The James-Stein estimate corresponds to the Newtonian potential g(X) = || X 11|-(-2) (2) which is harmonic except at 0. For the authors' problem where there is a natural decomposition of X into two parts x (XI) (3) a plausible suggestion is to take g of the form g(x) = llx 1- 011 XI 112 (4) with 0 chosen so that g is harmonic except at 0 and c chosen, presumably close to k -2, so as to obtain a good estimated risk function. I have no strong reason for choosing g harmonic, mainly that it seems to work. There is also a semi-Bayesian justification. If we choose a prior density -a that is harmonic, except for singularities, the corresponding g given by g(x) exp(i 11 x- 0 112) T(0) dO (5) is nearly harmonic when 11 X- 0o 112-k &gt;k (6) for all singularities 00 of ir. It then turns out that the unbiased estimate of the risk given by (1) is nearly the posterior risk at all x satisfying (6). These ideas are related to Brown (1971). Dr T. LEONARD (University of Warwick): The estimators described in the first section appear to be reasonable in practice if the shrinkages are taken towards x and there is an underlying assumption of exchangeability about the Oi. It is interesting to compare the results with estimators obtained via a different formu- lation, which is a simplification of that proposed by Lindley (1971). For the model in (1.1) suppose that the OA are independent, given u and A, with Oa - N(,u, A), where u and In A are independent and uniformly distributed over the real line. Then it is possible to show that the relevant solutions of the equations for the joint posterior modes of the Oi are given by x, if ,&gt;i oj=Xi- U(P) (Xi - ), if,A (i- 1, .,k), where U(8) = {1-(1 -4P)i} and B = (k-1) DIS'. These estimates may be compared with those in (1.3), and will usually be regressed further towards x. I feel that it would be useful to compare the James-Stein procedures with those due to Lindley, since both approaches are reasonably well formulated but nevertheless provide different numerical results in many situations. In a recent paper (Leonard, 1972) I obtained estimates for several binomial parameters under exchangeability assumptions. The method employs logistic transformations and a two-stage prior distribution with normality at the first stage. The probabilities are estimated by transformations of the joint posterior modes of the logistic parameters. I applied my method to the data in Table 1, and for comparison purposes assumed prior uncertainty at the second stage of the model, so that the results did not depend upon</page><page sequence="38">416 Discussion of the Paper by Professor Efron and Dr Morris [No. 3, prior parameter values. I found that my estimators were all totally regressed to a central value of 0-279. When compared with the results in Table 1 we see that these estimators are diplomatically superior to those of James and Stein in precisely 7 out of 14 cases. They have roughly the same predictive accuracy since the sum of the absolute deviations is about 031 in both cases. Of course, an advantage of a fully Bayesian approach is that it enables us to introduce prior information if we wish to. This paper by Professor Efron and Dr Morris has done much to indicate the deficiencies of some of the standard sampling theory procedures, and it is to be hoped that some of their estimators will be employed by the practitioner. Professor G. A. BARNARD (University of Essex): I am very sorry indeed that a prior commitment to a conference in Aarhus prevents me from being present at the discussion on this interesting paper; particularly since the authors do me the honour of couching their paper in the form of an extended answer to a question posed by me. I should make it clear that my question was not intended as a criticism of the usefulness of the James-Stein result. I am delighted to see that the authors share with me a some- what eclectic approach to problems of statistics; and as others present will know, I have for some time thought it to be important that the James-Stein result should be com- municated to practical workers, so that when appropriate they can make use of it. The application of the result to particular situations, or collections of situations, is something requiring judgment and further analysis; and the present paper is most valuable in extending our knowledge of how the results can be applied. To welcome the result as practically applicable is, however, not to say one should use it in all cases where it may appear to apply. There are problems, those which might be described as strictly scientific, where considerations of rigour are of supreme importance, and in such cases we may well feel that the lumping together of irrelevant sets of data cannot be justified. Specifically, in estimating a normal mean, we sometimes are in situations where the logic of the problem imposes group invariance properties upon us, and these may have to take precedence over considerations of mean square error, etc. Such matters cannot be treated in the abstract-each scientific problem has to be con- sidered in its context. To put the kind of issue that could arise in a stark form, one might consider whether we would be morally justified in downgrading the score of a child in a public examination, on the grounds that he or she came from a class of pupils whose average score was low. One comment seems to be in order on the example of baseball scores. While sharing what the authors assume is the widespread British ignorance of this noble game, I believe the batter has an opponent called a pitcher. And I believe the games are organized in a kind of league, much like our football leagues. If this is so, then those batters who get high scores early in the season will have done so to some extent because they have been meeting the poorer pitchers-and later in the season they will meet the better pitchers, who have hitherto been attacking their fellow batters. And conversely with those who, early in the season, score low. Thus there will be in any case some convergence towards a mean score, and the example is rather less persuasive than at first sight it might seem. Professor R. L. PLACKETT (University of Newcastle upon Tyne): The authors claim that their baseball example shows the substantial benefits of the Stein approach in actual statistical practice. Another approach is to suppose that the 14 baseball players form a random sample, in which case the prediction for the rest of the season is 0278 for all of them. The ratio of the sum of squared errors is reduced to 017. My conclusion is that Stein's method does quite well, but greater benefits are obtained by simpler methods based on knowledge of American baseball. The authors replied in writing as follows:</page><page sequence="39">1973] Discussion of the Paper by Professor Efron and Dr Morris 417 By undermining faith in invariance arguments, Stein's estimator has upset the Bayesian and frequentist applecarts. The process of picking up the apples seems to have drawn the two camps closer together, or at least established friendly communications, with Professor Lindley's neat essay as a case in point. He catches us on a good Bayesian question, why have we calculated risks for fixed sample sizes only, the answer of course being that we can do the calculations for this case. However, these calculations are appropriate for our application to the baseball problem, for unlike Professor Lindley's cricket example, the number of at bats is fixed (at 45). Life becomes easier if you forget about averaging over the sample space as he and Professor Dickey suggest, and do the analysis as con- ditional on the observed data. The last half of Section 8 of our most recent paper in the Journal of the American Statistical Association, referred to as Efron and Morris (1973), shows how this can be done. We agree that useful confidence intervals for the James-Stein estimator would be an important step forward. Posterior intervals for the relevant Bayesian models seem promising, but we have not yet calculated any of their properties. Yes, the usual estimators of variances and covariances are suspect. Professor Stein's recent unpublished work suggests results similar to the James-Stein estimate for means, though not as clear-cut because of mathematical complexities. In fact, Stein-type effects probably occur in most multiparameter estimation problems, the only thing special about the normal means case being that the mathematical calculations are relatively easy. See Section 9 of Efron and Morris (1973) and also Brown (1966). The Lindley-Smith-Leonard trick of replacing the a posteriori mean with the a posteriori mode as Bayes estimator in complicated problems is appealing, but should be employed with caution in the light of Dr Leonard's remarks. The estimator he presents will have very poor risk properties over a wide range of values of the prior variance A. If A is slightly less than 3D for example, and k is extremely large, then the modal estimator will have risk about triple that of the m.l.e. and four times that of the Lindley estimator (1.3). (Notice that Leonard's a approaches a number slightly in excess of one-quarter in probability under these circumstances, so the modal estimate of each Gj approaches the "totally regressed" estimate x.) Additional numerical work has shown us that the same rule is also bad for small k. The results of Efron and Morris (1973) show that the modal estimator pulls in much too strongly toward the common mean x. Of course, in a situation such as that of the 14 baseball players where the parameters do happen to lie quite near each other, AID - 0 05 (estimated from data not presented tonight, D corresponding to 45 at bats), this is the right thing to do and gives a small loss. But to illustrate the defect of Leonard's rule in a similar baseball situation, suppose it were used to estimate the true batting averages of all regular major league batters (k about 200) after observing them for four full seasons (n about 2,000 at bats for each player). With D diminished by a factor of over 40 due to the increased sample, then A would be about 2D. Leonard's rule would still estimate all players as having equal averages x, the grand mean, in spite of the fact that any standard significance test of equality of the player's averages would be resoundingly rejected. The rule (1.3) would continue to do well in this situation, giving a risk of about two-thirds of that of the maximum-likelihood estimator. The same reasons serve as a chastisement to Professor Plackett. One estimate doth not an estimator make! If he proposes always to use x then he will certainly be in small demand as a baseball prognosticator. Or perhaps he has in mind using x only after the xi pass a homogeneity test. This is the sensible "preliminary test estimator", which we know is dominated by James-Stein rules. The point is that the James-Stein estimator can be expected to dominate substantially x, the maximum-likelihood estimator, and the preliminary test estimator in many examples (several other examples will be presented in a forthcoming paper on applications of Stein's method), because it provides a richer model for the relationship between the ti. The combined estimator x derives from A = var (0) = 0, xi from A = var (06) = oo and the preliminary test chooses between</page><page sequence="40">418 Discussion of the Paper by Professor Efron and Dr Morris [No. 3, these extremes. But estimating values of O&lt;A&lt;oo permits a range of new estimates, e.g. in the form (1.3). So we emphatically disagree with Professor Plackett's conclusion that an extension of classical methods is unnecessary. Dr Wilkinson draws attention to a spurious artefact of the 14 baseball averages: that the three batters who started out worst ended up near the best. Were this relationship repeated after many more observations, it might need substantive explanation, but with such meagre data randomness is an adequate reason. Those knowledgeable of baseball would never suggest that poor initial performances make good final performances more likely. Instead, Stein's estimator adjusts extreme initial performances inwards, partially attributing the extreme values to randomness. And in spite of Professor Wilkinson's complaint, it preserves the "parabolic-shaped plot" since it involves only scale and location modifications of the initial estimates. Professor Copas misquotes our results with considerable gusto. (1) The two groups, in this case the drugs and the baseball players, must be centred about their individual group means. We are specific about this in the paragraph including (2.6). There is both positive and negative impetus for doing so: in most cases the individual centring will improve the performance of our estimators for the same reason that (1.3) is usually an improvement over (1.2), while failure to do so can easily result in systematic biases within each group, as in the Copas example. (2) In this case it is obvious that it is the risk for group 1, the drugs, that is of interest, and not the total risk for both groups. Moreover, it is made clear that it is plausible for b to take any value over a wide range. In these circumstances the mathematics of Section 3 suggest that 61,ep) cannot be much improved upon, not the opposite as Copas states. (But see our reply to Dr Smith, below.) Professor Copas's feigned eagerness for using baseball data to improve his drug estimates suggests that we make life even easier for him. Along with our estimators we are enclosing the volume with Rand's 100,000 random normal numbers. This will provide Professor Copas with all the second groups he will ever need for his analyses. Of course the absurdity of these examples is so apparent to sensible people that we do not expect these rules to be "applied blindly" as Professor Dawid suggests. With the record set straight we can admit that Professor Copas has raised an interesting point. It is true that Stein-type estimators can be made to yield quite different estimates from the the same data by the use of seemingly irrelevant preliminary transformations on the data space. That is what non-invariance means. For instance, the James-Stein plus rule (1.2), with the term in brackets restricted to be non-negative, can be made to estimate 0 by any vector 6 such that 11 6- x jj VI{D(k - 2)} simply by choosing the origin, i.e. the point towards which the estimator shrinks, in different ways. Any choice of the origin-made before observing the data-gives a rule which dominates the m.l.e. in terms of total squared-error risk. A good choice riskwise is one that is close to the actual 0 vector. This requires that the statistician use his prior knowledge of the situation, which may make Bayesians happy and frequentists nervous. In the more complicated situation we have been studying, different groups, different loss functions, etc., judgmental factors multiply. Bad judgments can easily destroy the advantage of Stein-type estimators, and really bad judgment can even lead to absurd results as in the Copas example. Perhaps this confession will soothe those of our critics, including Drs Copas, Dawid, Nelder and Smith, who thought we were proposing a mathematical substitute for common sense and scientific insight. A converse to this statement, which will make frequentists happy and Bayesians nervous, is that too much judgment can be dangerous in a statistical situation. A virtue of the Waldian method is that one writes down as precisely as possible what one intends to do and what the resulting risks will be. This is a great aid to clear thinking even if, as Professor Dickey fairly observes, the data may sometimes force a change of plans. The paper dwells at length on the risk functions of the Bayes risks, against "wrong" as well as "right" priors, for precisely this reason.</page><page sequence="41">1973] Discussion of the Paper by Professor Efron and Dr Morris 419 To answer partially points raised by Professors Lindley and Dickey, our concern for robustness against poor choices of priors leads us to average the estimators over the data as well as over the prior distribution. This same concern justifies studying simple estimators, such as Stein's estimator, for although it can be beaten by a Bayes rule (only slightly), its simplicity permits evaluation of its robustness. Proper application of estimators requires the statistician to understand both their outputs (operating characteristics) and their inputs (the prior distribution). Dr Smith takes the anthropomorphic point of view that parameters are related only if he says they are. This is a reasonable position for a Bayesian, but in no way influences the risk function of any given estimation procedure. To go back to Professor Copas's example, suppose one applies a compromise rule like 6(3) to the estimation of the drug parameters. If B2, the parameter for the baseball players happens to lie near B1, the parameter for the drugs, then 8(3) will do better than S(jeP) even though there is no "real" connection between baseball and chemotherapy. We do not expect B2 to be near B1 in this case, which is the argument for using S(jeP), but if it does it does. (Notice that the pragmatic effect of incorporating the baseball information is probably, in Professor Stone's colourful terminology, the use of a "bigger shrinker" in estimating the drug values since we know that A2, the prior variance for the baseball parameters, is near zero. We can do this directly without the baseball data by using a version of 8(jeP) that is itself a bigger shrinker. This is likely to be a useful change, as described in Section 5 of Efron and Morris (1973) and the last half of Section 6 of the present paper.) The eloquent Dr Dawid does not want an estimator for all seasons, preferring a new one for each. This sounds like a potential case of too much judgment to us. One does not need to be a frequentist to wish to put forward general methods that transcend individual data sets. As to whether James-Stein estimators are interesting in their own right or just as better than m.l.e., the answer is the former and Efron and Morris (1973) show that indeed they enjoy good properties from both the Bayesian and frequentist viewpoints. Our other efforts, published and unpublished, have been aimed at determining when Stein's method works, when it does not, and at developing certain safety features to guard against potential hazards. We think Stein's methods can be used effectively in many real estimation problems, with due regard for the notes of caution sounded tonight and previously. Rather than argue with Dr Nelder, tricky business with a man who can hit you with "inferential vacuum" and "measurement theory" in adjacent sentences, we will ask him some questions. What would have been his inference about 014 on the basis of the top line of Table 1 ? Would it be any different if he could only see 81 instead of all 14 numbers? Or suppose it were known that the left-handed batters were players 3, 4, 10 and 14? These are the kind of questions we have been trying to answer in the admittedly restricted format of squared-error point estimation. (A form particularly favoured by noted inferentialist R. A. Fisher!) Professor Birnbaum is right when he says that the main objections to Stein-type estimators have come from inference statisticians. It would be refreshing to hear their counterproposals for handling specific questions like those posed above, even if they do not feel able to endorse any one general approach to estimation problems. Now comes a startling confession. The 14 baseball players of Table 1 were not chosen at random! Player 1 was preselected on the basis of being the best hitter in the major leagues during several preceding seasons. When we first looked at the 1970 data he had had 45 at bats. The other 13 players were just those who happended to have the same number at that time. This selection procedure was calculated to embarrass the James- Stein rule, which could be expected to substantially misestimate Player l's true batting average. In Efron and Morris (1972) we discuss modification of the James-Stein rule designed to protect unusual individuals from the effects of simultaneous estimation, just what Dr Smith calls the outlier problem. For the case at hand, the modification might consist of following the Lindley rule as closely as possible subject to the constraint that no</page><page sequence="42">420 Discussion of the Paper by Professor Efron and Dr Morris [No. 3, estimate was to be more than one sample standard deviation, about 0-070, removed from the m.l.e. Thus Player 1 would be estimated by 0 330 = 0 400-0070 instead of 0 303, Player 2 by 0-308, Player 14 by 0-248 and the other 11 players by ' as given. This rule preserves most (about 90 per cent) of the reduction of sum of squares risk of Lindley's rule as compared to the m.l.e., while sharply limiting the maximum possible squared- error risk for any one player. We mention this in connection with Professors Barnard and Bartlett's "unusual student" example. In our papers we have considered not only the total sum of squared- errors loss function but also loss functions concentrating on subsets of the parameters, as in (2.3). This includes the case of wanting to estimate just one of the parameters, say 014 in the baseball example, but having information, on other cases of possible but not certain relevance. To answer partially our own question to Dr Nelder, our estimate of 014 would definitely be greater than 0178, even if the appropriate loss function was (814- 014)2 instead of (4i - 02)2. Exactly how much greater would depend on a priori feelings of how typical Player 14 was, as in the last half of Section 8, and how important it was to limit the maximum possible risk of the estimation procedure. One non-Bayesian justification for making such a correction is that xl4 is the minimum observation, and the mean of a minimum diminishes the minimum of means. Thus Ex14 &lt; 014 when xl, .... x13 are also considered (since E min (x,, x14) &lt; min (01, ..., 014) 014). The interpretation is that Player 14 is low for reasons involving both less skill and bad luck. To estimate his skill, his average must be adjusted upwards to remove the luck component. The observations xl, ..., x13 provide some basis for separating these skill and luck factors. We took Professor Barnard's 1962 question seriously and it is particularly gratifying that he should treat our answer in the same spirit. He is certainly correct in questioning the binomial model for the baseball data, which is suspect on a variety of grounds. How- ever, it can be shown that if the 8Q were actually binomial with n = 45 and probability parameters 0i as given in the bottom line of the table, then 3' would be expected to out- perform 8 by about the amount which actually occurred, so the example is not overly favourable. Professor Stone's estimator, with D known instead of estimated, is discussed briefly at (6.8)-(6.9) of Efron and Morris (1973). It can be expected to have reasonable but not outstanding risk properties compared say with the James-Stein plus rule of the same shrinking power. On the other hand, the derivation by cross-validation is quite intriguing. It is analogous to deriving x as the minimizer over m of I(xi - m)2 instead of as the minimum variance estimator of the population expectation. The parameteric parallel to cross-validation is empirical linear Bayes estimation. This is because the minimization of C amounts to choosing a good linear predictor for xij in terms of both directly and indirectly related data. As k goes to infinity with r held fixed, Stone's estimator goes to the correct linear Bayes form as described in Section 9 of Efron and Morris (1973). Perhaps the cross-validation method can be applied to our two-group problem by introducing another linear term, depending on the average response in the "other" group, into the definition of xi, and then preceding as shown. Professor Stein's new risk formula (1) is highly useful. Applied to "one-group" estimators of the form {1 - A(S)} xi, A(S) any function of S, it can be shown to enlarge substantially the class of estimators known to dominate the m.l.e. (that is it relaxes the Baranchik conditions described in Section 3 of Efron and Morris, 1973). However, the enlargement is obtained by working directly with (1) and going far outside the class of superharmonic estimators. The superharmonic condition is sufficient but not necessary. In the case above it seems definitely too strong a restriction. This is to say that we do not find the suggested form (4) the obvious right thing to do. However, disagreeing with Professor Stein on this subject is as hopeful a prospect as spotting Bobby Fischer two pawns, so we can only eagerly await his further contributions to the area which he created.</page><page sequence="43">19731 Discussion of the Paper by Professor Efron and Dr Morris 421 REFERENCES IN THE DISCUSSION BIRNBAUM, A. (1969). Concepts of statistical evidence. In Philosophy, Science and Method (S. Morgenbesser, P. Suppes and N. White, eds), pp. 112-143. New York: St Martin's Press. BROWN, L. D. (1966). On the admissibility of invariant estimators of one or more location parameters. Ann. Math. Statist., 37, 1087-1136. - (1971). Non-local asymptotic optimality of appropriate likelihood ratio tests. Ann. Math. Statist., 42, 1206-1240. COPAS, J. B. (1972). Empirical Bayes methods and the repeated use of a standard. Biometrika, 59, 349-360. LEONARD, T. (1972). Bayesian methods for binomial data. Biometrika, 59, 581-589. LINDLEY, D. V. (1971). The estimation of many parameters. In Foundations of Statistical Inference (V. P. Godambe and D. A. Sprott, eds), pp. 435-455. Toronto: Holt, Rinehart &amp; Winston. MOSTELLER, F. and TUKEY, J. W. (1968). Data analysis, including statistics. In The Handbook of Social Psychology (G. Lindsey and E. Aronson, eds), Vol. 2. Reading, Mass.: Addison- Wesley. SMITH, A. F. M. (1973a). A general Bayesian linear model. J. R. Statist. Soc. B, 35, 67-75. - (1973b). Bayes estimates in one-way and two-way models. Biometrika, 60, 319-329. ZIDEK, J. V. (1969). A presentation of Bayes invariant procedures in terms of Haar measure. Ann. Inst. Statist. Maths., 21, 291-308.</page></plain_text>