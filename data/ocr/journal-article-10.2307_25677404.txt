<plain_text><page sequence="1">SISIS ABSTRACT Today, in the midst of economic crisis, senior executives at US automakers and influential industry analysts frequently reflect on the progression that safety testing has taken from the crude trials done on the road, to controlled laboratory experiments, and to today's complex math-based simulation models. They use stories of this seemingly linear and natural sequence to justify further investment in simulation technologies. The analysis presented in this paper shows that change in the structures of automakers' organizations co-evolved with regulations specifying who was at fault in vehicle impacts, how vehicles should be built to withstand the force of an impact, and how testing should be done to assure that vehicles met those requirements. Changes in the regulatory environment were bolstered by new theories about crash test dynamics and changing technologies with which to test those theories. Thus, as new technological and regulatory innovations co-evolved with innovations in organizational structuring, ideas about how to best conduct crash tests shifted and catalyzed new cycles of technological, regulatory, and organizational innovation. However, this co-evolutionary story tells us that the move from road to lab to math was not natural or linear as today's managerial rhetoric would have us believe. Rather, the logic of math-based simulation was the result of technological, regulatory and organizational changes that created an industry-wide ideology that supported the move toward math while making it appear natural within the shifting structure of the industry. Keywords automotive engineering, co-evolution, innovation, organizational change, technological change From Road to Lab to Math: The Co-evolution of Technological, Regulatory, and Organizational Innovations for Automotive Crash Testing Paul M. Leonardi The year 2009 was a rough one for the global auto industry. Two of the world's largest automakers - General Motors and Chrysler - declared bankruptcy, Japanese automakers posted some of their largest financial losses in history, and overall consumer demand for vehicles fell to levels not seen since before World War II.1 Not surprisingly, automakers are actively looking for ways to cut costs and return to profitability. While the expenses Social Studies of Science 40/2 (April 2010) 243-274 ?The Author(s), 2010. Reprints and permissions: http://www.sagepub.co.uk/journalsPermissions.nav / www.sagepublications.com ISSN 0306-3127 DOI: 10.1177/0306312709346079</page><page sequence="2">244 Social Studies of Science 40/2 passed on to consumers from executive compensation and employee healthcare commitments have garnered much attention in the popular media, industry insiders suggest that design and engineering are among the most expensive costs in the production of vehicles. In fact. Hill (2007) estimates that nearly 30% of the cost of a consumer vehicle can be traced to engineering and testing efforts. Today, in the midst of severe financial crisis, leaders of the global auto industry, and of US automakers in particular, are placing a tremendous amount of faith in the ability of simulation tools to reduce engineering costs by revolutionizing the way automotive engineering is done. Such appeals to the potential of simulation technologies2 to dramatically reduce costs are the culmination of a decade-long campaign among automakers that the Society of Automotive Engineers (SAE) calls road to lab to math.3 As Gale (2005: 78) suggests, 'road to lab to math' reflects an industry-wide belief that engineering analysis is evolving away from expensive road and laboratory tests toward cost efficient math-based simulations. For example, an executive at one US automaker observed: Road to lab to math is basically the idea that you want to be as advanced on the evolutionary scale of engineering as possible. Math is the next log ical step in the process over testing on the road and in the lab. Math is much more cost effective because you don't have to build pre-production vehicles and then waste them. We've got to get out in front of the technol ogy so it doesn't leave us behind. We have to live and breathe math. When we do that, we can pass the savings on to the consumer.4 Commentators suggest that the pressure exerted by the current economic crisis, combined with an already popular belief in the effectiveness of math analyses for reducing engineering costs, will encourage US automakers to strengthen their commitments to purchasing and using simulation tech nologies. Indeed, despite the economic downturn, companies that produce math-based simulation and other automation technologies for the auto industry reported record profits in 2008.5 Of all automotive development functions that rely on math-based sim ulation technologies to reduce cost, none does so more than crashworthi ness engineering. Structural crashworthiness engineering represents the belief that the best chance a person has of surviving a crash with little or no injury is for the vehicle to absorb the energy of a collision so that energy is not transferred to him or her.6Today's crashworthiness engineers run both physical hardware tests (for example, crash pre-production vehicles into barriers) and math-based computer simulations (virtual tests of vehicles crashing into barriers). Through the late 1990s, math-based simulations were used in a decidedly reactive fashion; they verified the results of phys ical crash tests already conducted. But, because the cost of a pre-produc tion vehicle crash test is normally more than $650,000 and more than 20 crashes are usually necessary to produce a production vehicle, managers have been very interested in having math-based simulations supplant as much physical testing as possible. Since the early 2000s, most automakers</page><page sequence="3">Leonardi: From Road to Lab to Math 245 have invested large sums of money to move math-based simulation to a more proactive role in crashworthiness analysis. This paper traces crash testing practices at US automakers during the 20th century as they moved crash testing from the open road into con trolled laboratory conditions and, eventually, to simulations conducted in computer-enabled math-based environments.7 It would be easy to read the history of crash testing as one of natural evolution - newer testing tech nologies subsuming the old in a linear fashion - in which each successive step brought dramatic costs savings to automakers. To be sure, today's auto industry leaders often tell such a story. They say that technologies evolved by following a natural logic, and they use this rhetoric to suggest that math based analyses could save that industry and bolster that economy. As another senior executive commented, the 'linearity of change is persuasive' in its ability to assuage fears associated with corporate performance and to marshal resources to buy more math-based technologies: If you look at the history of crash testing in the industry, it's been moving, for a long time, toward math. If I need to make a business case for spending money on math tools it's easy to do if I can point out that this is where the technology wants to go - it's where it's going and we can't stop it. So people get that, you know, so pointing out the linearity of change is persuasive. So we do that and we say we either invest the money in the tools now, which will help us keep our costs down for hardware, or we wait until we don't have a choice and at that point we've wasted money.8 Such technologically deterministic rhetoric is hardly surprising. Research shows that managers often draw discursively on accounts of linear techno logical change to encourage practices that coincide with their ideologies (Choi &amp; Mody, 2009; Leonardi &amp; Jackson, 2009). Although such deterministic rhetoric might be persuasive, it is likely to be inaccurate. Studies conducted within the frameworks of the social con struction of technology (Pinch &amp; Bijker, 1984; Bijker, 1995), actor-network theory (Latour, 1987; Law, 1992), and systems theory (Hughes, 1987, 1994) convincingly show why such deterministic notions of technological change are problematic. Studies within these traditions suggest that technologies rarely follow a process of linear innovation. Instead, they are subject to changes at many stages, which are brought about by social and political inter ests in their development (Mackay et al., 2000; Law, 2002; Kline, 2003; Pinch, 2003). As Bijker and Law (1992: 8) note, all three variants of con structivism agree that technological change cannot be 'explained in terms of an unfolding internal logic', but rather that 'technology, the social world, and the course of history should all be treated as rather messy contingencies'. In addition to focusing on the negotiations and disputes occurring between groups that are deemed to be relevant to a technology's develop ment, and the ways that heterogeneous networks of people, technologies, facts, along with other human an non-human actors are structured to advance some goals over others, researchers interested in the development of technologies have recently turned to the role of regulation in shaping</page><page sequence="4">246 Social Studies of Science 40/2 technological change (Guston, 2001; Kleinman &amp; Kinchy, 2003; Laird, 2003; Mirowski &amp;Van Horn, 2005). The collective findings of this research suggest that government regulation co-evolves with technological innova tions. New technologies provide opportunities for social change. As social change occurs, citizen activism and government response often result in regulation that constrains which groups can be relevant to new technolo gies and shapes the kinds of networks that innovators see as necessary to advance their agendas. As such, regulation is not only shaped by but also shapes the direction of technological change (Miller, 2001). Crashworthiness is one of the most heavily regulated areas in automotive engineering. All automakers must certify that their vehicles meet minimum requirements outlined by the US Federal Motor Vehicle Safety Standards (FMVSS).The National Highway Traffic Safety Administration (NHTSA), the regulatory body that sets FMVSS requirements, along with the Insurance Institute for Highway Safety (IIHS), a US-based non-profit organization dedicated to reducing vehicular deaths, greatly influence regulations that affect how automobiles are built. To understand how crash testing moved from road, to lab, to math, the testing technologies themselves and the regu latory bodies that circumscribe them must be viewed as key actants. Such an explanation also requires a focus on changes in the organiza tional structure at the automakers. As we shall see, new technologies for test ing could not create changes in regulatory practice if the organizational structures adopted by automakers were not configured in such a way that automakers could actually conceive of using those technologies. As the analy sis presented below shows, a more robust account of why crash testing moved from the road, to the lab and eventually to math is achieved when techno logical, regulatory and organizational innovations are taken into account. The analysis shows that change in the structures of automakers' organizations co evolved with regulations specifying who was at fault in vehicle impacts, how vehicles should be built to withstand the force of an impact, and how testing should be done to assure that vehicles met those requirements. Changes in the regulatory environment were bolstered by new theories about crash test dynamics and changing technologies with which to test those theories. Thus, as new technological and regulatory innovations co-evolved with innovations in organizational structure, ideas about how best to conduct crash tests shifted and catalyzed new cycles of technological, regulatory, and organiza tional innovation. However, this co-evolutionary story tells us that the move from road to lab to math was not natural or linear as today's managerial rhet oric would have us believe. Rather, the logic of math-based simulation was the result of technological, regulatory and organizational changes that created an industry-wide ideology that supported the move toward math while mak ing it appear natural within the shifting structure of the industry. On the Road Smith (1994) argues that in the late 1800s and early 1900s, Americans exhibited an unbridled faith in the promise of technological progress.</page><page sequence="5">Leonardi: From Road to Lab to Math 247 New technologies were seen as the solution to a broad range of social and political problems because they were often linked explicitly to scientific progress, which since the Enlightenment promised positive change rooted in the natural description of the world. Laird's (1996) analysis of early automobile advertising shows a consistent pattern of appeals to ideals of progress, status, sport, and leisure. Thus, not only did Americans associate early automobiles with technological prowess, automakers actively worked to construct a popular conception of automobiles as powerful, seductive, and 'must-have' objects for the average consumer (Clarke, 2007).9 Automobile historians have consistently suggested that from its intro duction the automobile seemed superior to other forms of transportation on utilitarian grounds because it was cleaner, safer, more reliable, and more economical than the prevailing forms of transportation such as the horse (Rae, 1965; Flink, 1970, 1975). In addition to such pragmatic advantages, Flink (1972: 455) suggests that the automobile promised to bring about grand social changes that fit with the ethos of a population that was becom ing increasingly individualistic and migrant. Certainly, the high cost of early automobiles was prohibitive to most consumers, ensuring that only the wealthiest of citizens could afford such a purchase. The automobile's cost helped it to garner an early reputation as a commodity for those with money and to cement an increasing status divide between the rich and poor.10 In addition, a lack of infrastructure for driving, servicing, and main taining automobiles hampered their use for serious transportation. Instead, as Rae (1965) notes, most early automobiles were used for sport and recre ation rather than for utilitarian purposes. The automobile's association with science, technology, and popular American ideals of the early 20th century, together with its pragmatic utility over conventional forms of transportation, meant that Americans were bullish about its prospects. In the eyes of many early commentators, the automobile could do no wrong. In fact, if there were problems associated with the pro duction or maintenance of new automobiles, those problems were typically suspected to be the result of some human rather than mechanical error. In the case of an automobile accident, then, to blame the vehicle as a root cause was to make an accusation against popular convention and to label oneself a Luddite.11 Very quickly, injurious automobile accidents became associated with high speeds and 'reckless' driving. Citing a number of court decisions, which ruled that 'when driven at the proper rate of speed [automobiles] were no more dangerous than vehicles drawn by horses', Brown (1908: 225-26), a lawyer writing for the Yale Law Journal, suggested that: The speed of which [automobiles] are capable intoxicates and bewilders the senses, and deadens them to the dangers which surround the machine, and by a sudden mishap may turn it in the twinkling of an eye into a terrible engine of destruction. Thus, although the design and robustness of the turn of the century auto mobile left much to be desired, legal critics normally placed blame for crashes on the drivers themselves:</page><page sequence="6">248 Social Studies of Science 40/2 A motor vehicle is not a machine of danger when controlled by an intelligent prudent driver. ... It is evident therefore, that it is the manner of driving the machine, and that alone, which threatens public safety. (Huddy, 1905: 86) The growing tendency to find fault with the driver in the case of an accident was pronounced perhaps most strongly in a discussion about automobile safety appearing in the periodical Horseless Age in 1908. The article claimed that 'the well built automobile is inherently a very safe vehicle' and predicted that 'when the speed craze dies out ... accidents will become so rare as to stamp the automobile as the very safest of road vehicles'.12 Given the legal precedent that placed the fault for automobile accidents on drivers themselves, there were few attempts by either the government or the automakers to produce safer vehicles in the first two decades of the 20th century. Instead, the dominant trajectory of design was aimed at increasing the reliability, speed, and styling of new automobiles. By 1912, automobile ownership proliferated. The cost of the automobile dropped substantially with the introduction of mass production into most firms, and automobiles were used increasingly for transportation rather than leisure activities.13 In 1913, the National Safety Council was founded to educate Americans on the causes of accidental injury and death and began to collect statistics on fatality rates in the US associated with various types of public activities. The National Safety Council conducted large-scale surveys of police and insur ance data on motor vehicle accidents in 1917 and 1922 and concluded that the number of deaths associated with automobile accidents was growing at an 'alarming rate' (reported in MacLennan, 1988). In 1924 the National Safety Council, in conjunction with the US Chamber of Commerce peti tioned the auto industry to fund the first of two national conferences on street and highway safety. Herbert Hoover - then secretary of commerce - presided over the meetings at which participants recommended that the government work in conjunction with the auto industry and consumers to produce a 'balanced traffic safety program'. The result of these conferences reinforced the popular view that highways and vehicles were built about as well as could be expected under existing technology, and that traffic accidents were there fore traceable to willful, careless, irresponsible or incompetent drivers.14 The growing trend of placing the blame for automobile accidents on drivers allowed fledgling automobile companies to virtually ignore the prob lem of automotive safety.15 Lack of attention to safety was so entrenched in the early industry that the SAE did not begin to publicly discuss the issue of safety until a publication appeared in its conference proceedings in 1921.16 In a paper entitled 'Automobile Insurance', A.R. Small, a vice president in the insurance industry, wrote about the growing number of liability claims filed by individuals involved in automobile accidents. Small urged automakers to consider whether they could find ways to reduce the possibility of driver caused accidents, and thus of claims insurance companies would have to pay, through better design. Small (1921: 507) suggested: When driving at night in Chicago I sometimes think that if I had a different windshield I could see better. I have a sedan car with a blind spot when</page><page sequence="7">Leonardi'. From Road to Lab to Math 249 turning a corner. I do not know whether others have had the sensation of turning a left-hand corner and finding that they were pivoting on a car they did not see. Possibly the question of blind spots, from die driver's viewpoint, and whether to use a straight or slanting windshield are items that designers should take into consideration. Despite Small's urging, the discussions that followed the presentation of his paper at SAE conferences in New York and Cleveland showed no interest in his assertion that vehicles could be designed to help drivers avoid accidents. Accompanied by the recommendations of the 1924 and 1926 National Safety Council conferences to focus on educating the driver, the idea of designing features of the automobile to compensate for driver limitations lay fallow for nearly 10 years.17 Although automakers and the federal government gradually became interested in the safety problem, they continued to maintain a focus on deficiencies of the driver as the primary cause of accidents.18 In a joint effort to determine which features might best help drivers to avoid acci dents, the automotive and insurance industries worked together to begin to diagnose and analyze the common causes of accidents. In 1936, J.M. Orr, general manager of the Equitable Automotive Insurance Company, pre sented one of the first standard accident report forms that moved beyond describing what damage occurred in an accident to document the circum stances that caused the accident to occur in the first place. The form (see Fig. 1), which was pilot tested by claims adjusters in Equitable's Pittsburgh offices, included along the left column a variety of questions that prompted the adjuster to uncover the conditions that led to the accident. This infor mation could then be analyzed and used by automotive manufactures to isolate those particular features of vehicle design that would be most capa ble of overcoming driver shortcomings. Such a form did not document in great detail the extent of the damage caused, or how the vehicle faired in the accident. Other than for some rudimentary markings made by an adjuster on the right-hand column of the form to indicate the origin of impact, information about the vehicle's performance was virtually lost. Data obtained during the pilot testing of the form suggested that automo bile manufactures should consider improving the vehicle's ability to assist the driver in seven different categories: visibility, stability, steering, comfort, ventilation, lighting, and braking. Moving into the mid 1930s, an ethos of individualism in early 20th century America, a number of court decisions that placed the culpability of accidents on the drivers of vehicles, the creation of national associations that began tracking statistics on vehicle accidents, and a fledgling insurance industry that was losing money by paying out a large number of liability claims all worked together to begin to create a popular logic that said: (1) when a vehicle crashes into an object damage occurs; and (2) drivers are responsible for vehicle accidents. The auto industry attempted to respond to these concerns by initiating several organizational changes. These changes can be illustrated by following strategies adopted by General Motors during this period.</page><page sequence="8">250 Social Studies of Science 40/2 FIGURE 1 Front and reverse sides of accident report form ""rr ? ? --11 &gt;?*&gt;~ &gt;~ ..--.---^r.? j ?mm must www ?un or hum?rrr bamac* ^-/:z:"-:;:...:--3r^:-zr..;r: #Qi f| Q ,.- -,,-,. ?.-. ,.. -.?,-. . utrr not nam txx&gt; n*t an mcct tat OUKM. VM1ATM? ?T! ?"i' ?*. ^'ZI^ ^^IZL'Za^''-jf"nn~^--" ?* ?? f *? ? ??t ?r c?irt?ct o? ont* ca* ==' ? - ,?-1 f? Source: Orr (1936: 319). Under the guidance of founder and president William C. Durant, GM amassed a portfolio of more than eight independent automobile companies through the first two decades of the 1900s. When Pierre S. du Pont succeeded Durant as president in 1920 he immediately set to the task of reorganizing the company to more effectively control the divisional firms in GM's portfolio. On 3 January 1921 du Pont succeeded in obtaining board approval for a plan drafted by Alfred P. Sloan, known as the 'Organizing Study'. Sloan (1963) advocated a decentralized organizational structure. Following this plan, GM established a centralized 'operations staff' that would oversee product devel opment processes common among the 14 divisions.19 In 1924, the operations staff opened a proving grounds facility in Milford, MI, about 40 miles (65 km) north-west of Detroit. At the proving grounds, they road-tested vehicles developed in each of the divisions. By centralizing the common testing of vehicles developed in decentralized divisions, GM was able to capitalize on economies of scale, and for the first time invest in testing facil ities that could help to improve vehicle performance in each of the divisions. The goal of early proving ground testing was to recreate the circumstances in which drivers found themselves on American roads.20 This meant that, for the first time, vehicles did not have to be tested on city streets. Controlled road testing and comparisons could be made on how fast a vehicle could stop, how stable it was through turns, and how much visibility it permitted the driver.</page><page sequence="9">Leonardi: From Road to Lab to Math 251 FIGURE 2 Brake test at GM's Milford Proving Grounds Source: General Motors Media Archives, General Motors Corp. Thus, proving ground technology in the 1930s was limited to cement courses and close observations and measurement, sometimes with the aid of still pho tos, to determine vehicle performance.21 To construct the types of tests at the proving grounds that would simulate real-world driving conditions (such as the braking test conducted in a 'water alley' at GM's Milford Proving Grounds depicted in Fig. 2) the automobile companies used newly available data from insurance forms to determine what sorts of conditions led to the accidents customers most fre quently experienced. Harry Barr, who worked as an automotive engineer on GM's operations staff and later went on to become SAE president, recalled his early days conducting safety testing at the proving grounds that were aimed at mimicking normal driving conditions, 'I can still remember in the early 1930s when we would drive a car onto a spiral ramp to make it roll over, or we would push it over the side of a hill to see how well its com ponents held together' (Barr, 1970: 823). Because the automakers had adopted a multidivisional organizational structure with centralized operations, they now had proving grounds that were large enough to perform vehicle tests. Thus, following the recommenda tions of insurance and government agencies to develop and test features that would help drivers avoid accidents was now a feasible solution, which it was not 10 years earlier when Small first suggested such measures to the SAE. Similarly, both insurance and government agencies knew that the automakers now possessed these capabilities for testing, and were thus emboldened in their recommendations that the automakers should do so.22 Proving ground testing in the 1930s only involved components that would aid in crash avoid ance: they had not yet begun to test ways in which the structure of the vehi cle might be able to protect the occupants in the event of a collision. There is evidence that by the mid 1930s the idea that vehicles could be designed to absorb the energy produced in a crash event had begun to perco late within the automotive industry. In 1936, Frank Fageol who was then pres ident ofTwin Coach Co., a builder of motorbus coaches located in Kent Ohio, published a paper in the SAE Transactions entitled 'Safety in Body Design</page><page sequence="10">252 Social Studies of Science 40/2 through Chassisless Construction.' In the paper, Fageol suggested that for the past 30 years, automobile manufactures had followed the tradition of horse and-buggy type construction, which separated the body from the frame of the vehicle, and that they could learn a great deal from observing the structures used by boat and bridge builders that integrated the two structures. Fageol advocated what he called a 'chassisless design' - what automakers today refer to as 'unibody construction' - which would help distribute the load of the vehicle throughout the entire vehicle structure instead of just the chas sis.23 Fageol suggested that if the driver of a vehicle could not avoid a crash, 'probably the next in importance is the ability ... to cushion the impact for the occupants'. In order to do so, Fageol (1936: 482) articulated a plan to let the structure of the vehicle absorb the kinetic energy produced in the crash so that it would not be transferred to the occupant: Impact resistance is dependent on the local deflection of the body members. When two quite rigid members collide, the net result is a violent rebound of both, the rebound distance being inversely proportionate to their respective weights. There is a certain amount of kinetic energy stored in each which must be dissipated according to physical laws. It is desirable to make the vehicle shock absorbing to the greatest possible extent. To understand why automakers began to focus on crash avoidance at almost the complete exclusion of a focus on the robustness of a vehicle structure during an impact requires consideration of another organizational structuring arrangement prevalent in the automotive industry in the 1920s and 1930s. In the early years of the industry, automakers typically produced the power-train and chassis of a vehicle and then contracted with a coach builder to craft the vehicle's sheet-metal body. This division of labor was a direct remnant of the structure of the horse-drawn carriage industry, in which the chassis on which a carriage sat (since it did not include any sus pension or power-train components) was relatively unimportant compared to its carriage. In fact, many of the early coachbuilders such as the Fisher brothers who founded the famous Fisher Body Co., migrated to the auto motive industry from the horse-drawn carriage industry. To integrate com ponents made at separate times by two separate companies, the dominant design for automobiles in the first half of the 20th century was character ized by body-on-frame structure, where a coach (the body) was simply bolted to chassis (the frame) and then sent to the dealer showroom. Although coachbuilders such as Fageol did have the growing awareness that their designs could potentially absorb the energy produced in a crash, automakers did not listen. If the automakers were to follow Fageol's plan to build unibody vehicles, they would have to adopt one of two strategies. Either they would have to cede their chassis building activities to the coach builders, and lose considerable power in the industry, or they would have to begin their own coach building operations. GM was one of the first companies to attempt one of these plans. In 1926 GM completed a take-over of Fisher Body and sought to incorporate the newly acquired company as one it its divisions (for discussion, see Chandler &amp;</page><page sequence="11">Leonardi: From Road to Lab to Math 253 Salsbury, 1971). However, Fisher Body was so powerful that GM President Alfred P. Sloan gave the Fisher brothers almost complete autonomy; thus, even though Fisher was technically a GM division, it effectively ran as a sepa rate company. The result of this troubled integration was that GM, the one automaker that did have the capability to integrate coach building and chassis construction into a coherent safety package, did not do so for nearly 20 more years. Thus, in 1936 arguments such as the one made by Fageol to focus efforts not only on crash avoidance, but also on crash management, largely fell on deaf ears. Instead, automakers used the new technologies of road testing at their proving grounds to develop safety features that would help drivers to avoid accidents. The interaction of these technological, regulatory, and organizational innovations helped create and reinforce an ideology that drivers were to blame for accidents, so that the best automakers could do was develop components to help the accident-prone avoid them. Certainly, the creation of this ideology also influenced the continued conduct of actions that gave rise to it in the first place. In other words, litigation that found fault for the accident with the driver, together with organizational structures that dis couraged comprehensive, costly testing, encouraged more development of technologies suited to helping drivers avoid accidents. However, the post World War II infusion from the aeronautics industry into the automotive world of scientific research and technology served as a catalyst for new forces that brought about a novel understanding of the accident problem and ways to deal with it. In the Lab In 1917, an American cadet named Hugh De Haven was the sole survivor of a military aircraft accident. After he was released from the hospital, De Haven inspected the wreckage and found that only his cockpit had remained substantially intact. De Haven concluded that the intactness of the cockpit's structure was the answer to his survival.24 He reasoned that the human body was capable of withstanding tremendous forces inflicted by impacts, and that in order to mitigate these forces, humans needed a 'crashworthy' structure around them (De Haven, 1964 [1952]).To explain this phenomenon scientifically, De Haven drew on the concept of'deceler ation'. The term is roughly synonymous with 'negative acceleration'. To illustrate how the crashworthiness of a structure could affect its decelera tion, De Haven collected evidence from newspapers of people who had sur vived falls from great heights or suicide attempts. By exploring the details surrounding several of these cases, De Haven noticed a common element: free-falling humans often escaped serious injury when their deceleration was checked by striking some light frangible structure such as a sheet metal duct. He also noted that their impact was in a supine or prone position and that the object was 'soft' enough to bring them to a stop through a relatively long distance. In 1942 De Haven published a description of eight of the cases he had examined over a 15-year period and elucidated four major points relevant for the design of crashworthiness structures. He wrote:</page><page sequence="12">254 Social Studies of Science 40/2 Obviously, if the body could tolerate pressure within only narrow limits, few improvements would be worth consideration, since the force and resulting pressure of a severe crash are at best formidable. Evidence on the other hand, that the body can tolerate the force of an extreme crash - with out injury - would indicate that (1) extreme force within limits can be harmless to the body; (2) structural environment is the dominant cause of injury; (3) mechanical structure, at present responsible for recurring injury, can be altered to eliminate or greatly modify many causes and results of mechanical injury, and (4) the greater the evidence of body tol erance of force and pressure the wider the possibility for considering engi neering improvements. (De Haven, 1942: 586-87) De Haven's observations were finally given heed in 1942, the same year of his important publication, by Dr Eugene F. Dubois at the Cornell University Medical College in New York. DuBois took interest in De Haven's findings and helped him obtain the support of the Civil Aeronautics Administration and a small grant from the National Research Council to conduct research into the causes of injuries in aircraft accidents. He examined crashed air crafts and concluded that passengers died immediately in cockpits that were seriously damaged and that when cockpits were only slightly damaged people often died weeks later because they had collided forcefully with objects inside the plane. The results of these studies attracted the attention of the Bureaus of Medicine and Surgery of the US Navy and the Surgeon General's Office of the US Air Force, both of which were interested in the possibilities of using crash-injury research as a possible means for reducing casualties in the military. In the late 1940s De Haven was put in contact with Indiana state police man Sergeant Elmer Paul, who had analyzed a number of accidents occur ring on Indiana roadways in the late 1940s and early 1950s. In his analysis, Paul noted that most accident situations involved the impact of the vehicle with whatever it hit, what he called the first collision, followed instanta neously by the impact of the occupant with the inside of the vehicle, what he called the second collision. Paul was convinced that it was this second collision that caused injury and death in an automobile accident and he per suaded the Indiana authorities to establish the first systematic investigation of injury occurrence in automobiles wrecked on the state's highways In 1954, researchers at UCLA began conducting some of the first con trolled and highly instrumented crash tests (Severy et al., 1959).The UCLA study was notable not only because it represented one of the first attempts to produce and record a vehicle's behavior in a crash, by controlling vari ables such as speed, angle of impact, and decelerative force in a laboratory like setting, but also because it relied on an emerging theory of crash energy dynamics. Severy and colleagues laid out a detailed understanding of crash dynamics for their readers. A moving vehicle has kinetic energy, or energy in motion. The more the vehicle weighs and the faster it travels the more kinetic energy it has. Due to the conservation of energy law, the kinetic energy present in a moving vehicle must go somewhere when the vehicle stops. During a normal non-collision stop most of the kinetic energy is absorbed in the braking system through the production of heat. But when a</page><page sequence="13">Leonardi: From Road to Lab to Math 255 vehicle crashes into another object, the force of the collision suddenly changes the direction of the energy, causing acceleration, or in the case of an automobile crash, deceleration. Deceleration occurs as energy is suddenly dissipated not by heating the brakes but by crushing the vehicle and, in the process, exerting tremendous force on the vehicle's occupants. In applying this emerging understanding of crash energy dynamics to controlled crash tests, the UCLA researchers set out to understand the effects of accelerative forces generated in the vehicle impact. To do so, they employed small electromechanical devices called 'accelerometers'. An accelerometer contains microscopic crystal structures that are stressed by accelerative forces and generate an electrical current that can be read by a digital or analogue device.25 The UCLA researchers placed accelerometers throughout the test vehicle and data were transmitted via a cable to an instrument-recording car that trailed behind the collision vehicles (as shown in Fig. 3). The accelero meters captured data that could not be observed directly about the forces generated during the crash event. Due in large part to the work of De Haven and the introduction of techniques for testing crash energy dynamics by the UCLA group, automakers began, in the early 1950s, to seriously con sider how the structure of the vehicle itself could be designed to absorb energy in a crash event. By this point, thirty years had passed since automakers first began acquiring coach building firms. The result of an inte grated body and frame manufacturing system meant, finally, that an increasing number new cars built in the mid to late 1950s had unibody structures. Unibody construction became so popular during this time period, in fact, that Chrysler announced that by 1960 all of its cars except for the Chrysler Imperial would feature a unibody construction. This important organizational change meant that the idea of using the entire vehicle as an energy absorbing structure was feasible in a way that it was not in the early 1930s. By the early 1950s automakers were thinking seriously about using the structure of the vehicle to absorb kinetic energy. However, the automakers confronted a design paradox that had to be solved to use the structure of the vehicle in this way. On the one hand it seemed that the best way to dissipate energy was to crush the structure of the vehicle as much as possible. On the other hand, the structure couldn't crush too much and impede upon the occupant compartment, thereby crushing the vehicle's passengers. The result is an apparent paradox where crash forces and accelerations must be low enough to preserve a tolerable acceleration environment for the restraint system being used (the seat belt is not ripped from its moorings), and high enough to protect the physical environment of the crash victims (the passen ger compartment of the vehicle does not crush in on them).26 This paradox made it difficult to think about designing vehicle structures in a way that could protect the occupant from the force of the collision while simulta neously maintaining the integrity of the occupant compartment. On 28 August 1952, Bela Barenyi, a Hungarian-born engineer who worked for Daimler-Benz was granted patent number DE-854157 from the German patent office for an automobile structure that solved this paradox. Barenyi's ingenious design split the structure of the vehicle into three parts:</page><page sequence="14">256 Social Studies of Science 40/2 FIGURE 3 Vehicle control and photographic systems for UCLA test sheave 1 /-power-assist vehicle J &gt;r **ii3B? camera a / instrument recording and-"^ / safety car no. i IBj I ^-crash car no . i rcamera b 1 ^ /-impact center rcamera e / / ? camera c A / / camera 0 \ ,y&gt;^^U*K&lt; lighting unit ??-Jt?-??-^j^?~-?-o? /^\^ L \ ^camera l camera /TTV camera u. ft k / \ \ mounted on 25* -ttMCHCFFsa/ \Wrato?r \ \- tow -cable release O ^1400' monorail crash car no. 2-^^L camera | X instrument recording and y safety car no 2 Source: Severy et al. (1959: 239). two deformable areas known as 'crumple zones' (one in the front and the other in the vehicle's rear), and a rigid passenger compartment in the middle called the 'transition zone' or the 'safety cage'. The crumple zone was a structural feature designed to compress during a collision to absorb energy from the impact. Crumple zones work by lengthening the time a vehicle takes to come to a stop, thereby reducing the magnitude of the forces and deceleration felt by the occupants, since they are spread over a longer time. The crumple zone concept solved the paradoxical need for a structure that was simultaneously rigid and deformable by placing weak spots in strategic locations. The strategic design created two mechanisms by which the energy from an impact could be managed. First in deforming the metal work of the car, energy from the impact was purposefully converted into heat, which reduced the amount of energy left to damage the passenger area. Second, since the collapse was controlled, energy from the impact could be directed away from the passenger area.</page><page sequence="15">Leonardi: From Road to Lab to Math 257 A logic of passive safety developed from the combination of innovations in a theory of crash energy management carried over largely from the aero space industry, laboratory testing to assure that occupants were not injured by structures within the cabin of the vehicle, a crumple zone structure that allowed crash forces to be dissipated throughout the vehicle in a controlled manner, and sophisticated testing equipment such as accelerometers that could be used to measure such forces. While the safety establishment still felt that it was important to help drivers avoid crashes, and built many active safety features, such as more responsive braking and steering systems to help them do so, it also began to realize that the vehicle itself could help the occupants survive a crash independent of any effort on the part of the driver. Although national safety associations and the automakers were beginning to seriously consider the role that the vehicle itself played in the occupants' chances of surviving an automobile accident, much evidence suggests that the American mentality toward the automobile was changing from its previously Utopian stance. Flink (1972: 468) observes that during this period: Americans began to have critical second thoughts about the automobile industry and its product. John Keats caught the new mood well in his witty diatribe against the postwar American automobile culture, The Insolent Chariots. Keats said, 'The American's marriage to the American automo bile is at an end, and it is only a matter of minutes to the final pistol shot, although who pulls the trigger has yet to be determined.' In the mid 1950s and early 1960s, the emerging consensus was that driv ers were certainly at fault for many accidents, but that the automakers too had a key responsibility to make sure that occupants were safe when acci dents did occur. This popular opinion was reinforced by a number of con temporary court decisions made on the apportionment of damages in lawsuits about automobile accidents. In an influential paper published in the Harvard Law Review in 1956, Howard Katz cited a number of recent court cases concerning automobile accidents, which decided in favor of the driver and against the automobile companies. Katz also drew on the research from Cornell's Crash Injury Research Center to argue that automakers not only had an obligation to help drivers avoid the first colli sion, but that they also had the obligation to protect the occupant from the second collision as well: [F]aced until recently with an apparent predisposition of the courts against holding them liable for negligent design and by a general public apathy toward occupant safety, the manufacturers have in this aspect of design been in a 'rut worn deep by fifty years of unimaginative thinking'. Automobile manufacturers have been negligent in failing to design and market reasonably safe automobiles. This failure has created a vast area of unnecessary hazard to human life and is a substantial contributing cause of the high injury and fatality rate in motorcar accidents. Tort liability arises from the creation of unreasonable risks. Nothing in law or logic insulates manufactures from liability for deficiencies in design any more than for defects in construction. (Katz, 1956: 864)</page><page sequence="16">258 Social Studies of Science 40/2 Katz argued that a precedent was emerging to find legal culpability with automobile makers who did not work to abate the effects of the second collision. Perhaps in response to this emerging precedent, throughout the 1950s and 1960s, the courts increased the frequency with which they found automakers liable for damages resulting from the second collision.27 Convinced that the auto industry now had the scientific, and techno logical resources to detect and correct design flaws resulting in vehicular deaths, but had taken little initiative to apply these resources to the design of their consumer vehicles, safety advocates conducted a series of congres sional hearings. These were organized by US Senator Abraham Ribicoff in 1965 and 1966, in an attempt to enact federal regulation requiring automakers to produce safer vehicles.28 At the same time, Ralph Nader (1966) published his influential book, Unsafe at Any Speed, which provided a scathing critique of the automobile industry's approach toward the acci dent problem. Nader contended that despite having knowledge about crash energy management and an understanding of the sorts of designs and fea tures that would best protect occupants during a collision, the auto indus try had purposefully focused attention away from designing safe vehicles and toward styling and other matters that would bring them higher profits. Nader further suggested that the auto makers had deliberately ignored evidence showing the potential dangers of the second collision and that they had consistently worked to fix blame on drivers for accidents, rather than to focus on unsafe vehicles. Fearing regulation that could, for the first time in history, potentially halt their ability to sell vehicles, the automakers began to fight back. In a panel discussion about the Senate hearings, at the 1965 meeting of the Stapp Car Crash Conference, Kenneth Stonnex, a safety engineering direc tor at GM's proving grounds, argued that market competition would be sufficient to drive innovation in safety features: It is hard to think of any useful outcome of a requirement for Federal model approval. Every model is tested exhaustively by its designers and manufac turers, long before it goes into production, and the sharp competition in the industry precludes, on a practical basis, that any manufacturer produce a car deficient in safety performance. Federal model approval would be a formality without added effectiveness that I can see. (Stonnex, 1965: 427) Those who did not work for the automakers disagreed with Stonnex. At the same Stapp conference, panelist Donald Huelke (1965: 431) argued in favor of Federal regulation, suggesting that 'the apathetic American motoring public will now need to accept these safety features that are built into the car whether they like to or not'. By 1966, the Ribicoff hearings had succeeded in capturing the attention of the American motoring public and of the Federal government. On 9 September 1966, President Lyndon Johnson signed the National Traffic and Motor Vehicle Safety act and the Highway Safety Act into law. These acts created the National Traffic Safety Agency, the National Highway Safety Agency, and the National Highway Safety Bureau as well as the first</page><page sequence="17">Leonardi: From Road to Lab to Math 259 mandatory federal safety standards for motor vehicles. Implementation authority for the safety standards was assigned to the Department of Commerce and, shortly thereafter, to the National Highway Safety Bureau (NHSB) within the newly formed Department of Transportation. The NHSB was headed by Dr William Haddon and had a legislative mandate under Title 49 of the United States Code, Chapter 301, Motor Vehicle Safety, to issue federal motor vehicle safety standards and regulations to which manufacturers of motor vehicle and equipment items had to conform and certify compliance. As a columnist for Atlantic Monthly observed just after the passage of the act, the Federal government 'could demand only a single change in windshield wipers, or they could ban convertibles and small cars' (Drew, 1966: 240). Pressed by regulation and an increasingly negative public image, automakers initiated several important organizational changes in the late 1950s and early 1960s. In 1954 Chrysler opened a new and advanced prov ing ground facility in Chelsea MI and in 1956 Ford opened its Michigan proving ground facility in Romeo, MI, in part, to begin serious safety test ing efforts. In the late 1950s, Chrysler, Ford, and GM formally established the positions of 'safety engineer' in their operations staff. This organiza tional change meant that, for first time, major automobile manufactures' safety tests were fully conducted and analyzed not by parts technicians, but by engineers who worked to integrate the emerging theories of crash energy management into vehicle testing.29 This important organizational change meant that the automakers now had dedicated staff who were charged with generating data on vehicle impacts and could better inform design param eters within the various divisions. With the establishment of the 'safety engineer' position, engineers at the proving grounds looked to more controlled environments than the replicated road tests they had previously conducted in order to determine the impact of vehicle loads. Many of the first tests conducted by the new safety engineers used the basic design pioneered by researchers at UCLA. However, fully operational vehicles were needed to conduct that kind of test, and the cost to perform such tests repeatedly on upwards of 20 different vehicles proved quite prohibitive. In the fall of 1961, safety engineers at GM discovered a new technology (used by the military to shock test aircraft parts) for testing deceleration called a pneumatic accelerator or, more popularly, an 'impact sled' (Cichowski 1964). This new device, developed in cooperation with what was then Consolidated Vacuum Corp., was the first sled that was capa ble of simulating actual collision acceleration waveforms on complete cars. In a test on an impact sled, the vehicle and occupants were at rest prior to the impact - there was no motion of the passengers relative to the vehicle. A force that duplicates the waveform of an actual impact was applied to the vehicle, accelerating it in the same reaction as the applied force. The passengers momentarily remained in place and just as in an actual head on-impact, develop motion relative to the vehicle. The organizational changes at US automakers in the late 1960s, which had produced a new class of safety engineers and new testing technologies such as</page><page sequence="18">260 Social Studies of Science 40/2 impact sled, made it possible for the automakers to test decelerative forces without completely destroying vehicles. Together with increasing public awareness of the crash problem, litigation against the automakers for not effectively working to prevent the 'second collision', and, perhaps most importantly, an emerging science of crash energy management, these changes combined to initiate a monumental change in regulatory policy in the mid 1960s. The first standard issued by NHSB made seat belts mandatory on all passenger vehicles sold in the US. This seat belt standard (FMVSS 209) became effective on 1 March 1967 and demanded compliance for all vehicles manufactured on and after 1 January 1968. FMVSS 209, as well as all subsequent safety standards, was written in terms of minimum safety performance requirements for motor vehicles or items of motor vehicle equipment. Following the 1966 act, the US automakers quickly increased safety-testing efforts. In 1968, General Motors officially inau gurated two new testing facilities at its Milford Proving Grounds, known as the Safety Research and Development Laboratory and the Vehicle Dynamics Test Area. The SAE also began establishing its own standard practices for safety, publishing, in 1969, specifications for the first stan dard 50th percentile male anthropomorphic test dummy (ATD) as SAE recommended practice J963 (Starkey et al., 1969). Similar efforts by other automakers and professional associations signaled that the auto motive industry had fully absorbed the responsibility to take seriously the effects of the second collision. The interaction of technological, regulatory, and organizational inno vations since the 1940s helped to bring crash testing practices off of the road and into the laboratory, where detailed scientific tests, as opposed to basic, often anecdotal, observations about how vehicles faired in a collision, could be executed. As technologies for controlled testing co-evolved with organizational innovations that made those tests cost effective and feasible, a new ideology arose in the arena of vehicle safety, suggesting that: (1) driv ers could not help but be involved in accidents; and (2) by not developing vehicles that could protect them when accidents did occur, the automakers were largely responsible for an increased motor vehicle fatality rate in the US. Consequently, trial lawyers and advocacy groups began to pressure the federal government to regulate automotive design. These groups were ulti mately successful in securing regulation because the move into the labora tory made it possible for the government to replicate and verify crash impact data. Toward Math In the final months of 1969, William Haddon left the National Highway Safety Bureau to become president of the Insurance Institute for Highway Safety (IIHS), a post he occupied until his death in 1985. Under Haddon's leadership, the IIHS began to collect statistics on crash injuries and fatalities in the US and began to conduct tests of their own to determine how to</page><page sequence="19">Leonardi: From Road to Lab to Math 261 make vehicles more crashworthy. Following Haddon's departure, and with the passage of the Highway Safety Act of 1970, the National Highway Safety Bureau, the National Traffic Safety Agency, and the National Highway Safety Agency were consolidated into one agency: The National Highway Traffic Safety Administration. The NHTSA quickly began estab lishing standards in the form of FMVSS regulations NHTSA did not initially test vehicles to assure that they met the per formance requirements dictated by the FMVSS regulations. Instead, its strategy was to force the automakers to comply through self-certification and testing. Thus, NHTSA effectively transferred the burden of testing and implementation of safe designs from the agency to the automakers them selves. NHTSA administrators argued that it was reasonable to expect automakers to bear this burden for at least two reasons. First, theories of crash energy management were becoming well understood in the industry, and most automakers were aware of the sorts of physical alterations they had to make to the vehicle in order to achieve effective energy distribution throughout the structure. In 1972, Robert Carter, who just 1 year before had become the acting Associate Administrator for Motor Vehicle Programs at NHTSA, published a paper admonishing automakers for having not yet provided better protection for vehicle accidents occurring at highway speeds. He pointed out that the basic theories of crash energy management could easily be applied to build more crashworthy structures. Carter urged automakers that the best energy distribution was theoretically achieved in a vehicle through a square wave acceleration pulse. To achieve such gains in accelerative force reduction, he advocated a number of popular design solutions: To preserve a tolerable psychical environment for the crash victim in crashes of this severity the car must either be made longer to increase the available crush distance, or the structure must be modified to generate higher accelerations early in the crash pulse to more efficiently use the available crush space. (Carter, 1972: 1479) Essentially, Carter's proposed solution was to increase the length of the crush zones on the vehicle and increase the vehicle's mass so that there was more space for the vehicle to crush and absorb the energy of the collision. Carter's proposal, based on evidence gained through laboratory testing, was at the leading edge of a growing trend: Government regulators felt there was now a sufficient body of crash test knowledge with which they could make recommendations about vehicle design. NHTSA also justified its decision to pass the burden of testing on to the car companies because, as we have seen, throughout the late 1950s and 1960s the automakers had developed sophisticated and accurate con trolled testing facilities in their laboratories to measure deceleration and energy distribution directly. NHTSA believed that with such advanced testing techniques the automakers could easily measure the performance of their vehicles against the criteria established in the FMVSS regulations to self-certify.</page><page sequence="20">262 Social Studies of Science 40/2 Due to the increasing complexity of designing vehicles that not only had to be safer than before, but now had to be so with fewer of the features normally known to enhance safety (that is, more mass and larger crumple zones), automakers were forced to find a different strategy to allow them to handle the iterative nature of crashworthiness design. According to researchers in the automakers' research and development (R&amp;D) labs in the early 1970s, the direction that provided the most promise for dealing with these added constraints was computer simulation.30 Computer simu lations of vehicle impacts would allow engineers to test the crashworthiness of their designs in a mathematical environment, as well as to iterate a sim ulation until it achieved a reliable level of crash safety performance. They could perform such iterations without using expensive physical prototypes. As early as the mid 1960s, automotive engineers began to develop simple lumped-parameter models to test vehicle dynamics. A lumped-parameter model simply describes the functional relationships between system inputs and system outputs, without explaining the particular functioning of each parameter in the system. Although these early models were relatively novel, they were not very accurate since they relied on a substantial amount of mathematical approximation from experimental data. Because laboratory testing had only recently advanced to the point where safety engineers had an understanding of the how energy was distributed in an impact, and had only recently developed testing technologies to capture these data, there were still many questions about the sequencing of events occurring during a collisions, which had to be answered before more accurate models could be built. A potential solution to the shortcomings of lumped-parameter models began to surface in the aerospace industry in the early 1970s. This technique known as finite element analysis (FEA) was used in the aerospace industry to determine the spatial distribution of phenomena, such as stress, heat, dis placement, vibration, acoustic waves, and magnetic fields on airplane wings and fuselages. Rather than treat an entire object or collection of objects as black boxes, as lumped-parameters models did, the finite element method decomposed an object into a large (though finite) number of much smaller objects known as 'elements'. The elements are considered to be connected at defined nodes (corner and mid-segment points), and the entire connected system composes a defined structure called a 'mesh' (see Fig. 4 for an illus tration). This mesh is programmed to contain the material and structural properties that define how the structure will react to certain loading condi tions. These material and structural properties are normally related in a stiff ness matrix, which defines the deformation patterns of the materials used to build automobiles (such as steel, plastic, and foam) in relation to the shape (normally called the 'geometry') of the structure those materials constitute. This entire system of equations is represented mathematically as a very large matrix, taking the form of complex differential equations or integral expres sions, with the unknowns (in the case of crashworthiness analysis, usually displacements) solved at the nodes. Because the finite element method cal culates parameters at each node, it has a very high level of accuracy com pared to lumped-parameter models.</page><page sequence="21">Leonardi: From Road to Lab to Math 263 FIGURE 4 Relationship between nodes, elements, and mesh in finite element analysis -^^y. Mesh I ^ I-^V^Nocle ;^ Element 11?"?v?T'^v \t-* Due to their computational intensity, finite element models required an incredible amount of computing power compared to lumped-parameter models. As early observers in the auto industry commented, the application of finite element analysis to automotive design only became possible with the advent of fast time-sharing computers.31 The earliest computers used in the auto industry had to be fast enough to run the finite element code, which would solve the differential equations in the models. Time sharing computers such as the Teletype Model 33 and the Houston Instruments DP-1 were among the earliest models used in the auto industry for finite element analysis.32 These computers made possible early finite element (FE) code, which was developed in the mid 1960s in the aerospace industry with funding from government contracts. Until the mid 1970s, most automakers experimented with finite element models for crashworthiness analysis in their R&amp;D labs, but did not regularly use them in routine engineering work.33 Most early FE solvers rendered the output in two dimensions, which made it difficult for engineers to visually determine whether the model behaved in a similar way as the test vehicle, and hence whether it had a high degree of correlation. However, the advan tages of the FE models over lumped-parameter models for production engineering work began to become clear to the automakers by the mid 1970s when a number of reports published in the SAE Transactions began to discuss the favorable results of experiments performed on FE models at the R&amp;D labs of GM, Ford, and Chrysler (see, for example, Peterson, 1971; Davis, 1974; Wotiz, 1974). Timed almost precisely with these optimistic reports on the potential benefits of FE techniques for automotive analysis, the Lawrence Livermore National Laboratories released a new finite element code called DYNA3D, which, when processed on a Control Data CDC-7600 supercomputer, allowed the user to render the finite element code in three dimensions and to compute results for dynamic analyses. This pair of innovations was eagerly welcomed by safety engineers who could now run a full set of analyses simulating the physical crash tests conducted in the safety laboratories at the proving grounds. These innovations in computing power and code made finite element models much more</page><page sequence="22">264 Social Studies of Science 40/2 tractable for normal engineering use and provided the potential to reduce the number of expensive crash tests the automakers had to deploy to meet NHTSA certification. Thus, these innovations in finite element code and computer processing capabilities effectively brought this new technology out of the R&amp;D labs and into the production environment. The cost in late 1970s of purchasing and installing supercomputers on which the finite element code could be solved, and implementing individ ual workstation terminals on which safety engineers could pre and post process their finite element models, proved a major concern for automakers. Although several supercomputers were needed to solve the finite element code, a larger cost came from implementing individual terminals for engi neers. To take advantage of the analytic capabilities provided by finite ele ment methods, the automakers had to figure out a way to organize analytic practices so that FEA could decrease the need for crash tests without incur ring large structural costs in computing and engineering resources. One option was to employ supercomputers in safety research labs at the proving grounds and instruct safety engineers in theories of finite element analysis. While this approach would reduce the number of terminals the automak ers had to purchase, it had two disadvantages. First, there were not many safety engineers at the proving grounds. To pull these engineers away from the physical tests, which were still necessary for government certification, would mean the automakers would have to hire more safety engineers to fill their shoes. Second, in an era before computer networking was common place, installing supercomputers at proving grounds that were distant from engineering centers (such as GM's engineering center in Warren, MI, and Ford's engineering center in Dearborn, MI) meant that the supercomput ers could not be used for other engineering applications. In essence, the automakers would be buying very expensive and soon to be out of date supercomputers for a very small number of engineers. A second option was to train the design engineers (DEs), who originally designed the vehicle's parts, on how to use finite element methods. In this way, DEs could simultaneously design a part or an assemblage of parts, test their crashworthiness, and immediately iterate to a new design if necessary. This approach had several drawbacks as well. First, although computer aided design (CAD) technologies were coming into common use at the automakers in the late 1970s and early 1980s, many of the DEs still drafted parts by hand and did not have much experience working with computers. Using FEA would require DEs not only to learn how to use complex computer software, but it also would require the automakers to purchase terminals for each individual engineer. In addition, using FE technologies to analyze crashworthiness of a vehicle would require DEs to understand the physics behind crash tests and theories of crash energy management. Although many DEs were trained in mechanical engineering, few had back grounds in structural dynamics or physics, and still others had technical training but no formal engineering background. Finally, DEs were respon sible for the design of specific vehicle parts or assemblies, but rarely ever saw the design for an entire vehicle. Performing crashworthiness analysis on</page><page sequence="23">Leonardi: From Road to Lab to Math 265 a particular part would not provide a clear picture of the crashworthiness of the entire vehicle, so in order for DEs to do crash analysis, their roles would have to be restructured in such a way that they would work on designing larger portions of the vehicle. This alteration in role structure would defy conventional logics of product development performance in the automotive industry (Clark et al., 1987). A third option was to create a new organization of engineers that would be responsible for building and analyzing simulations to predict a vehicle's performance in a number of areas. These engineers would work closely with design engineers at the automakers' engineering centers and with test engineers at the proving grounds, serving as an interface between these two worlds. These engineers would receive architectural drawings of the parts from design engineers, convert these CAD drawings into FE models, inte grate the parts from various design engineers to build a complete vehicle model (normally consisting of more than 30,000 parts) and then run prospective tests that would attempt to predict the ways that energy would be distributed in the vehicle during a crash. Because their primary responsi bility was to build and analyze FE models, engineers' interactions with the test engineers at the proving grounds would most likely be limited to encounters occurring only a few times a month. By locating the new organ ization of engineers at the automakers' engineering centers, then, supercom puters could be implemented that other engineering functions could also make use of. In addition, by limiting model building and analysis activities to a smaller group of engineers, fewer terminals would need to be purchased. In the late 1970s, automakers were evaluating these three options for structuring the engineering organizations to begin to leverage the capabili ties of FEA for crashworthiness analysis (Carl &amp; Hamaan, 1974). In an issue of the SAE Transactions, engineers at Ford published recommenda tions for structuring organizations to take advantages of the capabilities of FE methods. The authors advocated the creation of computer-aided engineering (CAE) department that would build and analyze models constructed by DEs in the various automotive divisions: The critical question is how to organize a large company to use most effec tively the technical skills available. ... In many cases [a centralized] approach is to be preferred from the standpoint of effective utilization of manpower, minimal training, and the development of technical profi ciency. It also offers the financial means to support a terminal-based com puter system, computer graphics, and related software development. When properly managed, a centralized activity offers an efficient use of human and financial resources. (Carl &amp; Hamaan, 1974: 16-18) By the early 1980s, the US automakers had established CAE centers, much like the ones recommended by Carl and Hamaan, in which engineers performed computational analysis on a number of different performance criteria for vehicle dynamics including, crashworthiness analysis, noise and vibration analysis, ride and handling analysis, aerodynamics and thermal analysis, and heating ventilation, and air-conditioning analysis. These new</page><page sequence="24">266 Social Studies of Science 40/2 performance engineers (PEs) worked within a particular specialty and normally held bachelors or advanced degrees in the engineering sub discipline in which they worked. The decision to structure PEs who ran simulations of vehicle crashes on FE models into one organization, combined with the personal computing revolution in the 1980s, produced dramatic increases in the power of FE models to help predict the crashworthiness of a vehicle and reduce the num ber of physical tests automakers had to conduct. The increased fidelity of math models allowed PEs to conduct more iterations for each crash scenario and, in so doing, optimize on a particular design rather than merely meet the FMVSS requirement. Thomke (1998) suggested that in the field of crash worthiness engineering since the 1990s, simulations from FE models have consistently led to an increase in problem-solving cycles while simultane ously reducing the cost of design iterations. Thomke also found that this increased capacity to iterate through diverse experiments with novel possi bilities enabled crashworthiness engineers to learn more effectively than with conventional physical prototypes and thus to design better vehicles. By the turn of the millennium, the use of math-based simulations for crashworthiness analysis had become well entrenched at US automakers. Leonardi (2007b) documents that in 2000, crashworthiness engineers at one major US automakers submitted slightly over 30,000 simulations to be solved by DYNA software alone and that the average FE model contained more than 300,000 elements. With the ability of simulation technologies to aid in more proactive vehicle design, automakers no longer considered designing vehicles simply to meet FMVSS requirements; instead, they con sistently set objectives for vehicle programs with the goal of obtaining a spe cific rating from the consumer testing agencies (for example, a 5-star versus a 4-star rating from NHTSA).34 This meant that those tests administered by consumer agencies began to have even more important effects on the structural design of vehicles. To be sure, most automakers considered the various tests conducted in the markets in which they sold their automobiles as drivers of vehicle design. Since the mid 1970s, the regulatory, technological, and organizational innovations described above continued to co-evolve, making the move to math-based engineering analysis seem inevitable. Automakers were not only bullish on the prospects of math-based simulations improving their ability to design and build safer vehicles - they were also confident that simulations would help to alleviate a variety of structural (testing) and personnel (tech nician) costs that were making it difficult for firms to compete in the world market. GM, Ford, and Chrysler - and most automakers around the world, for that matter - had all given their engineering divisions direct orders to increase math-based tests for vehicle dynamics, including, and most impor tantly, crashworthiness analysis, and to reduce the number of physical tests necessary to validate vehicles (Becker et al., 2005). Thus, automakers are, today, betting big money on the ability of math-based tools such as finite element analysis to bring about dramatic changes in engineering knowledge and engineering practice in their product development divisions.</page><page sequence="25">Leonardi: From Road to Lab to Math 267 Conclusion Today, in the midst of economic crisis, senior executives at US automakers and influential industry analysts frequently reflect on the progression that safety testing has taken from the crude trials done on the road, to controlled laboratory experiments, and to today's complex math-based simulation models, and they use stories of this seemingly linear and natural sequence of change to justify further investment in simulation technologies. As the managers quoted in the opening of this paper implicitly suggest, pitching the use of simulation technologies as a sure and quick route to regain financial solvency works precisely because people are reassured by visions of techno logical determinism. As Leonardi (2008: 979) argues, discursive accounts that are tied to notions of technological determinism can be effectively utilized to rationalize behavior, especially in uncertain times, because they provide 'cognitive relief about an uncertain future'. Beneath this deterministic rhetoric, however, lies a more complicated history. Practices of crash testing did indeed move from road to lab to math during the 20th century, but not because of any technological imperative or natural logic. Nor did they move in discrete steps. While the locus of test ing shifted from road to lab to math, crash testing is still done today using some amount of road testing along with laboratory verification (Leonardi &amp; Bailey, 2008). This paper has shown that the technologies used to test vehicle crashes co-evolved with legal action and a shifting US regulatory environment. It was not until testing methods were developed, which were accurate, affordable and replicable, that successful litigation was brought against automakers. Lawyers pointed towards laboratory testing proce dures to suggest, in the words of Nader (1966), that the automakers 'could have done better' to build safer vehicles. The increasing precedent to find fault with automakers encouraged regulation. With regulation came stricter guidelines that encouraged greater quantities of testing, and more advanced procedures. These findings sit nicely with existing constructivist approaches to technological change that highlight the mutually constitutive nature of technology and regulation. The foregoing analysis also suggests, however, that a third type of inno vation was necessary to bring about these mutual changes in crash testing and regulation: structural change in the automakers' own organizations. For example, during the first few decades of the 20th century, automakers were structured as decentralized organizations. Decentralized units did not share common resources nor did they have design authority over the whole vehicle. Thus early responses to auto accidents brought local changes to the vehicle that would help drivers to avoid crashing. It was not until the automakers moved to a more centralized structure that they established common methods for proving vehicle designs and testing facilities that could make recommendations for changes in the vehicle structure. These organizational changes, coupled with an emerging science of crash energy management set the stage for successful litigation against the automakers (indeed, it was actually possible for them to know what designs were better),</page><page sequence="26">268 Social Studies of Science 40/2 which then begot regulation. As regulation intensified, demanding more fine-grained testing, automakers responded by again making alterations to organizational structure, establishing new roles and responsibilities, which encouraged the implementation and use of new mathematical testing tech nologies. Thus, the changes, over time, in strategies toward crash testing were made possible by technological, regulatory, and organizational inno vations, which all evolved in response to each other. The findings of this study indicate that the move to math was by no means predetermined or the exclusive result of technological innovation. Rather, the move was slowly wrought by co-evolutionary changes in the macro environments in which crash testing occurred. One distinctive feature of this analysis is that it moves the role of organizational structure from the back ground of technological development and places it on equal footing with socio-cultural and regulatory innovations. For many years, STS researchers have acknowledged that organizations, the laboratory in particular, are important backdrops for the production of scientific knowledge and techno logical artifacts (Latour &amp; Woolgar, 1979; Mukerji, 1998; Knorr-Cetina, 1999).Yet despite this recognition, shifts in organizational structure have not always factored prominently into explanations of technological change. Klein and Kleinman (2002) argue that constructivist studies of techno logical change typically adopt an 'agency-centered' perspective. In other words, the agency of individual actors and, by association, their ability to profoundly and often autonomously affect the course of a technology's development is figured prominently in constructivist accounts of technolog ical change. Klein and Kleinman, along with others (Vaughan, 1999; Orlikowski &amp; Barley, 2001; Kranakis, 2004), have suggested that, a pro nounced focus on the agency of individual actors to shape the developmen tal path of a new technology is to be expected from early constructivist studies. Social construction, actor-network and systems perspectives, in particular, arose in response to popular and appealing deterministic notions of technological change in academia and in society at large (Williams &amp; Edge, 1996). In leveling their critiques against deterministic thinking, contributors often emphasized human agents' interest, autonomy, and capability as antidotes for technologically deterministic thinking, which claimed that humans could exert little force on a technological trajectory in the face of an artifact's intrinsic logic. In recent years, as constructivist sensibilities have taken hold, scholars have begun to move away from an agency-centered focus to consider the role that government and policy play in technological change (Brown, 2001; Parthasarathy, 2004; Wetmore, 2004). The history of US crash testing suggests that including changes in organizational structuring into construc tivist accounts may be needed to accurately explain technological change, especially in situations where technologies are developed, implemented, and used in formal organizational settings. Researchers who labor at the boundary of STS and organizational studies are beginning to combine insights from both realms to paint more accurate pictures of technological change that marry constructivist notions of negotiation, translation, and</page><page sequence="27">Leonardi: From Road to Lab to Math 269 heterogeneous network building with the reality that organizational structures both enable and constrain people's working practices (Leonardi, 2007a; Orlikowski, 2007; Woolgar et al., 2009). As this analysis demon strates, the move to math was enabled when automakers shifted their organizational structures in response to, and in advance of, technological and regulatory changes, thus enabling a co-evolutionary path that brought the industry from road to lab to math. Notes I would like to thank Diane Bailey and JoAnne Yates for helpful comments on earlier versions of this paper. 1. More details on the slowdown in auto sales in 2008 can be found on the website for the Alliance of Automobile Manufacturers: &lt;www.autoalliance.org/&gt; (accessed 16 July 2009). 2. I use the term 'math-based simulation technologies' to refer to tools that enable users to perform mathematical computations on digitized drawings. Within engineering organizations, another common term for these technologies are computer-aided engineering (CAE) tools. 3. The SAE is an independent professional association established in the US in 1905 to promote the use of standards in the fledgling automobile industry. Today, the SAE sets most auto-industry standards for the testing, measuring, and designing of automobiles and their components. 4. Interview conducted by author on 23 August 2006. 5. For more details on this figure see Miel (2008). 6. Today, crashworthiness engineering is typically divided into two specialties: structural analysis and occupant analysis. Those who conduct structural analysis examine the movement of energy through a vehicle's structure and do not typically run analyses with safety features like airbags or seat restraints. The latter fall within the domain of occupant analyses. In this paper I focus on structural crashworthiness analysis. For more details on occupant analysis and the innovations used by engineers performing this work see Wetmore (2003, 2008). 7. For a complete list of sources used to construct this case history, see Leonardi (2007b). These published and archival data were complemented by more than 50 interviews with managers, at a major US automaker. Interviews were used to understand the history and current practices of crashworthiness engineering and to triangulate and expand hypotheses emerging from a review of the archival material. 8. Interview conducted by author on 11 November 2006. 9. The emergence of the automobile in the US at the turn of the 20th century was not without opposition. Norton (2007) shows how jaywalkers fought with motorists over rights to be on urban roads and Kline &amp; Pinch (1996) discuss how rural farmers often sabotaged dirt paths used by motorists to speed through the country. Despite such resistance, historians generally suggest that although cars were somewhat of an annoyance to many, the general ethos that surrounded them was one of optimism and intrigue (Rae, 1965; Flink, 1970; Laird, 1996). 10. See for discussion, Borg (1999). 11. For more discussion on this point see Volti (1996). 12. 'The Safety of the Automobile' (28 October 1908), Horseless Age 22: 580. 13. For a detailed discussion of this transition see Clarke (2007). 14. For detailed accounts of the outcomes of these joint hearings, see Hasbrook (1956) and Nader (1966). 15. Wetmore (2003) finds some evidence suggesting that several early automakers were conscious of the fact that their vehicles were causing injury and death to passengers and pedestrians. However, during the first two decades of the 20th century, it appears that automakers, collectively, did little to explicitly design their vehicles to promote safety.</page><page sequence="28">270 Social Studies of Science 40/2 16. The Society for Automotive Engineers (SAE) was originally founded in 1905 as the Society for Automobile Engineering. Today, the journal Transactions of the Society of Automotive Engineers is called simply SAE Transactions, and remains one of the most important and influential journals on automotive engineering. 17. 1926 was also a notable year because it was the first year in which the automobile industry experienced saturation of the market. By early 1927, replacement demand for automobiles accounted for more new car sales than initial and multiple car sales combined. With more than 26.7 million motor vehicles registered, total production of automobiles in the US in 1927 was more than 5.3 million units, an output that would not again be equaled until 1949 (Flink, 1972). Some argue that this saturation of the market for automobiles was one of the major factors contributing to the Great Depression of the 1930s. For one such argument see Chandler (1964). 18. For examples of such thinking see the following reports, see Huelke (1968), Kemper (1968), and King (1951). 19. Certainly, GM was the exemplar of decentralized structure. Evidence suggests that Chrysler also adopted a heavily decentralized structure during this period, and that Ford, while maintaining a relatively unified public face, also operated in a decentralized manner, giving tremendous autonomy to its Lincoln (established in 1922) and Mercury (established in 1938) brands (Hyde, 2003; Cahill, 1992). 20. Interview with Crashworthiness Manager, April 2006. This comment made in the interview piqued my interest to explore the 'safety history center' at the research site, which was nothing more than a small archive with memos from the company's previous crashworthiness managers (from the 1920s through the 1980s) some of which indicated management's intention of moving testing in this direction. 21. For a detailed historical account of proving ground technology in the 1930s see Cichowski (1964). 22. For more information on the insurance company's stance toward accident testing see Nerlove &amp; Graham (1928). 23. Unibody (sometimes referred to as unit-body) is a construction technique that uses the external skin of an object to support some or most of the load on the structure. Unibody construction is achieved by integrating the chassis with the body of the vehicle into a single unit. 24. For a good discussion of De Haven's reactions to his crash see Hasbrook (1956). 25. The ability of crystals and certain ceramic materials to generate a voltage in response to applied mechanical stress is known as the piezoelectric effect. 26. Informants consistently referred to this paradox as one of the main challenges of engineering design. 27. For a review of detailed review of such cases see Dillard &amp; Hart (1955), Nader &amp; Page (1967) and Prosser (1950). 28. For a detailed discussion of the Ribicoff hearings see Wetmore (2004). 29. Two informants still employed by the automaker were engineers at this time and recalled fondly the formal establishment of the 'safety engineer' role. 30. Several informants who worked at the automaker during this period recalled the struggles of convincing management that computer simulations could provide accurate results of crash tests and might eventually replace physical prototypes. 31. Authors such as Peterson (1971) and Wotiz (1974) discuss in more detail the computational advantages of finite elements methods for use in analyzing automobile structures in the 1970s. 32. For a more detailed description of the early technical infrastructures developed to support finite element applications in the auto industry see Carl and Hamaan (1974). 33. Informants indicated that, at the particular automaker I studied, it was not until 1978 that engineers first began experimenting with crude FE models on production vehicle design. 34. Management at the automaker interviewed were explicit, when establishing a specification for a vehicle program, about the number of starts the vehicle should earn.</page><page sequence="29">Leonardi: From Road to Lab to Math 271 Typically the automaker set requirements that exceeded those basic guidelines outlined by NHTSA and aimed to achieve high ratings in IIHS tests. References Barr, Harry F. (1970) 'Automotive Safety in Review', SAE Transactions 79: 823-38. Becker, Markus C, Pasquale Salvatore &amp; Francesco Zirpoli (2005) 'The Impact of Virtual Simulation Tools on Problem-Solving and New Product Development Organization', Research Policy 34: 1305-21. Bijker, Wiebe E. (1995) Of Bicycles, Bakelites, and Bulbs: Toward a Theory of Sociotechnical Change (Cambridge, MA: The MIT Press). Bijker, Wiebe E. &amp; John Law (1992) 'Shaping Technology/Building Society: General Introduction', in WE. Bijker &amp; J. Law (eds), Shaping Technology I Building Society: Studies in Sociotechnical Change (Cambridge, MA: MIT Press): 1-14. Borg, Kevin (1999) 'The "Chauffeur Problem" in the Early Auto Era: Structuration Theory and the Users of Technology', Technology and Culture 40(4): 797-832. Brown, H.B. (1908) 'The Status of the Automobile', Yale Law Journal 17(4) (February): 223-31. Brown, Mark B. (2001) 'The Civic Shaping of Technology: California's Electric Vehicle Program', Science, Technology, &amp; Human Values 26(1): 56-81. Cahill, Marie (1992) A History of Ford Motor Company (New York: Smithmark Publishers). Carl, E.J. &amp; WC. Hamaan (1974) 'How Finite Element Methods are Introduced in Large and Small Organizations', SAE Transactions 83: 10-22. Carter, R.L (1972) 'Passive Protection at 50 mph', SAE Transactions 81: 1474-505. Chandler, A.D. (1964) Giant Enterprise: Ford, General Motors and The Automobile Industry (New York: Harcourt, Brace &amp; World). Chandler, A.D. &amp; S.S. Salsbury (1971) Pierre S. du Pont and the Making of the Modern Corporation (NewYork: Harper &amp; Row). Choi, Hyungsub &amp; Cyrus CM. Mody (2009) 'The Long History of Molecular Electronics: Microelectronics Origins of Nanotechnology ', Social Studies of Science 39(1): 11-50. Cichowski, WG. (1964) 'A New Laboratory Device for Passanger Car Safety Studies', SAE Transactions 72: 363-74. Clark, K.B., B. Chew &amp;T. Fujimoto (1987) 'Product Development in the World Auto Industry', Brookings Paper on Economic Activity 3: 729-81. Clarke, Sally H. (2007) Trust and Power: Consumers, the Modern Corporation, and the Making of the United States Automobile Market (Cambridge: Cambridge University Press). Davis, R.L. (1974) 'How Finite Element Methods Improve the Design Cycle', SAE Transactions 83: 1-3. De Haven, Hugh (1942) 'Mechanical Analysis of Survival in Falls from Heights of Fifty to One Hundred and Fifty Feet', War Medicine 2: 586-96. De Haven, Hugh (1964 [1952]) 'Accident Survival - Airplane and Passenger Automobile', in William H., E.A. Suchman &amp; D. Klein (eds), Accident Research: Methods and Approaches (NewYork: Harper &amp; Row): 562-68. Dillard, H.C. &amp; H. Hart (1955) 'Product Liability: Directions for Use and the Duty to Warn', Virginia Law Review 41(2): 145-82. Drew, E.B. (1966) 'The Politics of Auto Safety', Atlantic Monthly (October): 95-102. Fageol, Frank R. (1936) 'Safety in Body Design Through Chassisless Construction', SAE Transactions 31: 480-83. Flink, James J. (1970) America Adopts the Automobile, 1895-1910 (Cambridge, MA: MIT Press). Flink, James J. (1972) 'Three Stages of American Automobile Consciousness', American Quarterly 24(4) (October): 451-73. Flink, James J. (1975) The Car Culture (Cambridge, MA: MIT Press). Gale, Nigel (2005) 'Road-to-lab-to-math: A New Path to Improved Product', in Automotive Engineering International (May): 78-79.</page><page sequence="30">272 Social Studies of Science 40/2 Guston, David H. (2001) 'Boundary Organizations in Environmental Policy and Science: An Introduction', Science, Technology, &amp; Human Values 26(4): 399-408. Hasbrook, Howard A. (1956) 'The Historical Development of the Crash-impact Engineering Point of View', Clinical Orthopedics 8: 268-74. Hill, K. (2007) Contribution of the Motor Vehicle Supplier Sector to the Economies of the United States and its 50 States (Research Triangle Park, NC: The Motor &amp; Equipment Manufacturers Association). Huddy, Xenophon P. (1905) 'The Motor Car's Status', Yale Law Journal 15(2) (December): 83-86. Huelke, D.F. (1965) 'Federal and State Standards for Automotive Safety: Panel Discussion', The Ninth Stapp Car Crash Conference: 430-32. Huelke, Donald F. (1968) 'Automobile Accidents. Where We've Been, Where We Are, What Needs to be Done', Journal of Risk and Insurance 35(1) (March): 61-66. Hughes, Thomas P. (1987) 'The Evolution of Large Technological Systems', in WE. Bijker, T. P. Hughes &amp;T. J. Pinch (eds), The Social Construction of Technological Systems: New Directions in the Sociology and History of Technology (Cambridge, MA: MIT Press): 51-82. Hughes, Thomas P. (1994) 'Technological Momentum', in M.R. Smith &amp; L. Marx (eds), Does Technology Drive History? The Dilemma of Technological Determinism (Cambridge, MA: MIT Press): 101-13. Hyde, Charles K. (2003) Riding the Roller Coaster: A History of the Chrysler Corporation (Detroit, IL: Wayne State University Press). Katz, Harold A. (1956) 'liability of Automobile Manufacturers for Unsafe Design of Passenger Cars', Harvard Law Review 69(5) (March): 863-73. Kemper, James S. Jr (1968) 'The Highway Safety Legislation: Its Implications for Insurance', Journal of Risk and Insurance 35(1) (March): 67-73. King, R.P. (1951) 'Highway Safety - Through the Eyes of a Motor Vehicle Administrator', SAE Transactions 5: 463-87. Klein, Hans K. &amp; Daniel Lee Kleinman (2002) 'The Social Construction of Technology: Structural Considerations', Science, Technology, &amp; Human Values 27(1): 28-52. Kleinman, Daniel Lee &amp; Abby J. Kinchy (2003) 'Boundaries in Science Policy Making: Bovine Growth Hormone in the European Union', Sociological Quarterly 44(4): 577-95. Kline, Ronald (2003) 'Resisting Consumer Technology in Rural America: The Telephone and Electrification', in N. Oudshoorn &amp; T. Pinch (eds), How Users Matter: The Co construction of Users and Technology (Cambridge, MA: MIT Press): 51-66. Kline, Ronald &amp; Trevor J. Pinch (1996) 'Users as Agents of Technological Change: The Social Construction of the Automobile in the Rural United States', Technology and Culture 37(4): 763-95. Knorr-Cetina, Karin (1999) Epistemic Cultures: How the Sciences Make Knowledge (Cambridge, MA: Harvard University Press). Kranakis, Eda (2004) 'Fixing the Blame: Organizational Culture and the Quebec Bridge Collapse', Technology and Culture 45(3): 487-518. Laird, Frank N. (2003) 'Constructing the Future: Advocating Energy Technologies in the Cold War', Technology and Culture 44(1): 27-49. Laird, Pamela Walker (1996) '"The Car Without a Single Weakness": Early Automobile Advertising', Technology and Culture 37(4): 796-812. Latour, Bruno (1987) Science in Action: How to Follow Scientists and Engineers through Society (Cambridge, MA: Harvard University Press). Latour, Bruno &amp; Steve Woolgar (1979) Laboratory Life: The Social Construction of Scientific Facts (Beverly Hills, CA: Sage Publications). Law, John (1992) 'Notes on the Theory of the Actor-network: Ordering, Strategy, and Heterogeneity', Systems Practice 5(4): 379-93. Law, John (2002) 'Objects and Spaces', Theory, Culture &amp; Society 19(5): 91-105. Leonardi, Paul M. (2007a) 'Activating the Informational Capabilities of Information Technology for Organizational Change', Organization Science 18(5): 813-31.</page><page sequence="31">Leonardi: From Road to Lab to Math 273 Leonardi, Paul M. (2007b) Organizing Technology: Toward a Theory of Socio-material Imbrication. PhD Dissertation, Stanford University. Leonardi, Paul M. (2008) 'Indeterminacy and the Discourse of Inevitability in International Technology Management', Academy of Management Review 33(4): 975-84. Leonardi, Paul M. &amp; Diane E. Bailey (2008) 'Transformational Technologies and the Creation of New Work Practices: Making Implicit Knowledge Explicit in Task-Based Offshoring', MIS Quarterly 32(2): 411-36. Leonardi, Paul M. &amp; Michele H. Jackson (2009) 'Technological Grounding: Enrolling Technology as a Discursive Resource to Justify Cultural Change in Organizations', Science, Technology, &amp; Human Values 34(3): 393-418. Mackay, Hugh, Chris Carne, Paul Beynon-Davies &amp; DougTudhope (2000) 'Reconfiguring the User: Using Rapid Application Development', Social Studies of Science 30(5): 737-57. MacLennan, Carol A. (1988) 'From Accident to Crash: The Auto Industry and the Politics of Injury', Medical Anthropology Quarterly 2(3): 233-50. Miel, Rhoda (2008) 'Auto Suppliers Seek Edge with Automation', Plastics News (August): 23. Miller, Clark (2001) 'Hybrid Management: Boundary Organizations, Science Policy, and Environmental Governance in the Climate Regime', Science, Technology, &amp; Human Values 26(4): 478-500. Mirowski, Philip &amp; Robert Van Horn (2005) 'The Contract Research Organization and the Commercialization of Scientific Research', Social Studies of Science 35(4): 503-48. Mukerji, Chandra (1998) 'The Collective Construction of Scientific Genius', inY. Engstrom &amp; D. Middleton (eds), Cognition and Communication at Work (Cambridge: Cambridge University Press): 257-78. Nader, Ralph (1966) Unsafe at Any Speed: The Designed-in Dangers of the American Automobile (NewYork: Pocket Books). Nader, Ralph &amp; J.A. Page (1967) 'Automobile Design and the Judicial Process', California Law Review 55 (3): 645-77. Nerlove, S.H. &amp;W.J. Graham (1928) 'The Trend of Personal Automobile Accidents', Journal of Business of the University of Chicago 1(2) (April): 174-201. Norton, Peter D. (2007) 'Street Rivals: Jaywalking and the Invention of the Motor Age Street', Technology and Culture 48(2): 331-59. Orlikowski, Wanda J. (2007) 'Sociomaterial Practices: Exploring Technology at Work', Organization Studies 28(9): 1435-48. Orlikowski, Wanda J. &amp; Stephen R. Barley (2001) 'Technology and Institutions: What Information Systems Research and Organization Studies Can Learn from Each Other', MIS Quarterly 25: 145-65. Orr, J.M. (1936) 'Safety in Motor-vehicle Operation and Maintenance', SAE Transactions 31: 313-23. Parthasarathy, Shobita (2004) 'Regulating Risk: Defining Genetic Privacy in the United States and Britain', Science, Technology, &amp; Human Values 29(3): 332-52. Peterson, Willy (1971) 'Application of Finite Element Method to Predict Static Response of Automotive Body Structures', SAE Transactions 80: 1056-72. Pinch, Trevor J. (2003) 'Giving Birth to New Users: How the Minimoog Was Sold to Rock and Roll', in N. Oudshoorn &amp;T. Pinch (eds), How Users Matter: The Co-construction of Users and Technology (Cambridge, MA: MIT Press): 247-70. Pinch, Trevor J. &amp;Wiebe E. Bijker (1984) 'The Social Construction of Facts and Artifacts: Or How the Sociology of Science and the Sociology of Technology Might Benefit Each Other', Social Studies of Science 14: 399-441. Prosser, W.L. (1950) 'Proximate Cause in California', California Law Review 38(3): 369-425. Rae, John B. (1965) The American Automobile: A Brief History (Chicago, IL: University of Chicago Press). Severy, D.M, J.H. Mathewson &amp; A.W. Siegel (1959) 'Automobile Head-On Collisions', SAE Transactions 67: 238-62.</page><page sequence="32">274 Social Studies of Science 40/2 Sloan, Alfred P. (1963) My Years With General Motors (New York: Doubleday). Small, A.R. (1921) 'Automobile Insurance', SAE Transactions 16: 500-20. Smith, Merritt R. (1994) 'Technological Determinism in American Culture', in M.R. Smith &amp; L. Marx (eds), Does Technology Drive History? The Dilemma of Technological Determinism (Cambridge:The MIT Press): 1-35. Starkey, J.A, J.W. Young, W.D. Horn, W.J. Sobkow, S.W Alderson, WG. Cichowski, M.T. Krag &amp; J.H. Auerbach (1969) 'The First Standard Automotive Crash Dummy', SAE Transactions 78: 935-48. Stonnex, K.A. (1965) 'Panel Discussion: Federal and State Standards for Automotive Safety Design', Proceedings of the Ninth Stapp Car Crash Conference: 425-28. Thomke, Stefan H. (1998) 'Simulation, Learning and R&amp;D Performance: Evidence from Automotive Development', Research Policy 27: 55-74. Vaughan, Diane (1999) 'The Role of the Organization in the Production ofTechno-scientific Knowledge', Social Studies of Science 29(6): 913-43. Volti, Rudi (1996) 'A Century of Automobility', Technology and Culture 37(4): 663-85. Wetmore, Jameson M. (2003) Systems of Restraint: Redistributing Responsibilities for Automobile Safety in the United States since the 1960s, PhD Dissertation, Cornell University. Wetmore, Jameson M. (2004) 'Redefining Risks and Redistributing Responsibilities: Building Networks to Increase Automobile Safety', Science, Technology, &amp; Human Values 29(3): 377-405. Wetmore, Jameson M. (2008) 'Engineering with Uncertainty: Monitoring Air Bag Performance', Science and Engineering Ethics 14(2): 201-18. Williams, Robin &amp; David Edge (1996) 'The Social Shaping of Technology', Research Policy 25: 865-99. Woolgar, Steve, Catelijne Coopmans &amp; Daniel Neyland (2009) 'Does STS Mean Business?', Organization 16(1): 5-30. Wotiz, A. (1974) 'Finite Element Model Data Checkout with Interactive Graphics', SAE Transactions 83: 1523-31. Paul M. Leonardi is Assistant Professor of Communication Studies and Industrial Engineering and Management Sciences at Northwestern University, where he holds the Breed Junior Chair in Design. His research examines how organizations co-evolve with the technologies they develop and implement. His recent publications on these topics include: 'Crossing the Implementation Line: The Mutual Constitution of Technology and Organizing Across Development and Use Activities' Communication Theory, 2009; 'Why Do People Reject New Technologies and Stymie Organizational Changes of which They Are in Favor? Exploring Misalignments Between Social Interactions and Materiality', Human Communication Research, 2009; and 'Activating the Informational Capabilities of Information Technology for Organizational Change', Organization Science, 2007. Address: Department of Communication Studies, Northwestern University, 2240 Campus Drive, Evanston, IL 60201, USA; fax: +1 847 467 1035; email: leonardi@northwestern.edu</page></plain_text>