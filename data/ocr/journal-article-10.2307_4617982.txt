<plain_text><page sequence="1">Gil Weinberg and Scott Driscoll Georgia Institute of Technology 840 McMillan St. Atlanta, Georgia 30322 USA {gilw, scott.driscoll}@gatech.edu music.gatech.edu/mtg/ Toward Robotic Musicianship We present the development of a robotic percus- sionist named Haile that is designed to demonstrate musicianship. We define robotic musicianship in this context as a combination of musical, percep- tual, and interaction skills with the capacity to pro- duce rich acoustic responses in a physical and visual manner. Haile listens to live human players, ana- lyzes perceptual aspects of their playing in real time, and uses the product of this analysis to play along in a collaborative and improvisatory manner. It is designed to combine the benefits of computa- tional power, perceptual modeling, and algorithmic music with the richness, visual interactivity, and expression of acoustic playing. We believe that com- bining machine listening, improvisational algo- rithms, and mechanical operations with human creativity and expression can lead to novel musical experiences and outcome. Haile can therefore serve as a test bed for novel forms of musical human- machine interaction, bringing perceptual aspects of computer music into the physical world both visu- ally and acoustically. This article presents our goals for the project and the approaches we took in design, mechanics, per- ception, and interaction to address these goals. Af- ter an overview of related work in musical robotics, machine musicianship, and music percep- tion, we describe Haile's design, the development of two robotic arms that can strike different loca- tions on a drum with controllable volume levels, applications developed for low- and high-level per- ceptual listening and improvisation, and two inter- active compositions for humans and a robotic percussionist that use Haile's capabilities. We con- clude with a description of a user study that was conducted in an effort to evaluate Haile's percep- tual, mechanical, and interaction functionalities. The results of the study showed significant correla- tion between humans' and Haile's rhythmic percep- tion as well as strong user satisfaction from Haile's perceptual and mechanical capabilities. The study also indicated areas for improvement, such as the need for better timbre and loudness control as well as more advanced and responsive interaction schemes. Goals and Motivation Most computer-supported interactive music sys- tems are hampered by their inanimate nature, which does not provide players and audiences with physical and visual cues that are essential for creat- ing expressive musical interactions. For example, motion size often corresponds to loudness, and ges- ture location often relates to pitch. These cues pro- vide visual feedback and help players anticipate and coordinate their playing. They also create a more engaging experience for the audience by providing a visual connection to the sound. Computer-supported interactive music systems are also limited by the electronic reproduction and amplification of sound through speakers, which cannot fully capture the richness of acoustic sound. Our approach for addressing these limitations is to employ a mechanical apparatus that converts dig- ital musical instructions into physically generated acoustic sound. We believe that musical robots can bring together the unique capabilities of computa- tional power and the expression and richness of acoustic sounds created through physical and visual interaction. A musical robot can combine algorith- mic analysis and response capabilities that are not humanly possible with rich sound and visual ges- tures that cannot be reproduced by loudspeakers. We hope that such novel human-machine interac- tion can lead to new musical experiences and new music that cannot be conceived by traditional means. The engaging power of musical robots can also be useful as an educational tool, introducing learners not only to music but also to the mathe- Computer Music Journal, 30:4, pp. 28-45, Winter 2006 ? 2006 Massachusetts Institute of Technology. 28 Computer Music Journal</page><page sequence="2">matics, physics, and technology behind it in an in- teractive, hands-on manner. Current research directions in musical robotics focus mostly on sound production and rarely ad- dress perceptual aspects of musicianship, such as listening, analysis, improvisation, or group interac- tion. Such automated devices can be classified in one of two ways: robotic musical instruments, which are mechanical constructions that can be played by live musicians or triggered by pre- recorded sequences (Singer, Larke, and Bianciardi 2003; Jorda 2002; Dannenberg et al. 2005); or an- thropomorphic musical robots-hominoid robots that attempt to imitate the action of human musi- cians (Takanishi and Maeda 1998; Sony 2003; Toy- ota 2004). Only a few attempts have been made to develop perceptual robots that are controlled by neural networks or other autonomous methods (Baginsky 2004). The work described in this article addresses our preliminary research goal in this area: the develop- ment of a robotic percussionist that can demon- strate perceptual, physical, and social aspects of musicianship. The application we developed is aimed at allowing the robot to analyze live rhyth- mic input in real time and to react in an expressive manner through responsive and engaging acoustic drumming. We conclude by presenting a number of possible future research directions that expand the concept of robotic musicianship to pitch-based in- struments, addressing melodic and harmonic per- ceptual and interaction models. Related Work We have identified a number of research fields that relate to our attempt to achieve robotic musician- ship. These fields are musical robotics, which fo- cuses on the construction of automated mechanical sound generators; machine musicianship, which centers on computer models of music theory, per- ception, and interaction (Rowe 2004); and rhythmic perceptual modeling, which can be seen as a subset field of machine musicianship that bears particular relevance to our initial focus on percussion. Musical Robotics Early work on musical robotics primarily addressed mechanical keyboard instruments such as the Pi- anista (1863) by French inventor Jean Louis Nestor Fourneaux. (See Kapur 2005 for a comprehensive historical review of musical robots.) In recent years, the field has received significant commercial, artis- tic, and academic interest, expanding to anthropo- morphic designs (Rosheim 1994) as well as other robotic musical instruments, including chordo- phones, aerophones, membranophones, and idio- phones. Several approaches have been recently explored for robotic stringed instruments. Guitar- Bot (Singer, Larke, and Bianciardi 2003), for ex- ample, is a mechanical guitar with four strings, each equipped with a sliding bridge, picking mechanism, and damper solenoid. Both the sliding and picking mechanisms are operated with DC servomotors un- der closed-loop feedback control, and the entire sys- tem is controlled via MIDI signals. Jordi's Electric Guitar Robot (Jorda 2002), on the other hand, has six strings that can be plucked by twelve picks, driven by an electro-valve hammer-finger. Current approaches for mechanical guitars, however, are not designed to explore the full range of sonic variety through string techniques such as bouncing, bow- ing, strumming, scratching, or rubbing. Other attempts have been made to develop ex- pressive wind instrument robots. Takanishi's An- thropomorphic Flutist Robot (Takanishi and Maeda 1998), for example, uses a complex mechanical imi- tation of human organs in an effort to accurately reproduce human flute playing. The elaborate appa- ratus includes robotic lungs, lips, fingers, and tongue. The robot, called WF-4RII, has also been used for educational purposes, teaching beginners the basics of flute playing (Solis et al. 2004). Other examples for aerophone robotic instruments are Toyota's Robotic Trumpeter (2004) and the Autosax (Rae 2005), which are programmed to follow deter- ministic rules. More varied work has been done on robotic per- cussionists, both for idiophone and membranophone instruments. The ModBots (Singer et al. 2004), for example, are miniature modular instruments de- Weinberg and Driscoll 29</page><page sequence="3">signed to affix to virtually any structure. Each Mod- Bot consists of only one electromechanical actuator (a rotary motor or a linear solenoid) that responds to varying degrees of supply voltage regulated by a mi- crocontroller. A more elaborated mechanism by Singer is used in the TibetBots (Singer et al. 2004), which consist of six robotic arms that strike three Tibetan singing bowls. Here, an effort was made to capture a wider timbral variety by using two robotic arms, controlled by solenoids, for each bowl to pro- duce a richer set of sounds. Another approach for broadening timbre and pitch versatility is employed by the Thelxiepeia (Baginsky 2004). The instrument consists of a me- chanical drumstick and a motorized mechanism to rotate the drum circumference that can lead to the production of a range of pitches. Researchers such as Pongas, Billard, and Schaal (2006) have also studied lower-level control aspects such as joint synchro- nization and phase locking with rhythmic robots. Other robotic instruments and anthropomorphic robots that influence our work were developed by Trimpin (2000), Crick, Munz, and Scassellati (2006), Dorsenn (2006), and Brooks et al. (2004). Machine Musicianship In his book Machine Musicianship, Robert Rowe (2004) describes systems that demonstrate musi- cianship as those that analyze, perform, and com- pose music with computers based on theoretical foundations in fields such as music theory, com- puter music, cognition, artificial intelligence, and human-computer interaction. Scholars from a variety of fields have explored different approaches for the design of such analysis-performance- composition musical systems. One of the earliest research directions in this area was the score fol- lower, in which the computer tracks a live soloist and synchronizes accompaniment MIDI data (Dan- nenberg 1984; Vercoe 1984)-and more recently, au- dio data (Orio, Lemouton, and Schwarz 2003)-to musical input. The classic score-following approach focuses on matching predetermined musical events to real- time input. A more improvisatory approach is taker by systems such as Robert Rowe's Cypher (1992) or George Lewis's Voyager (2000). Here, the software analyzes musical input and responds by controlling and manipulating a variety of parameters such as melody, harmony, rhythm, timbre, and orchestra- tion. David Cope (1996) has taken a non-real-time approach to machine musicianship in his system for analyzing composers' styles based on MIDI rendi- tions of their compositions. Mr. Cope's algorithm learns the style of a given composer by modeling as- pects such as expectation, memory, and musical in- tent. It can then generate new compositions with stylistic similarities to the originals. Francois Pa- chet's Continuator (2002), on the other hand, takes a real-time approach in learning the improvisation style of musicians as they play polyphonic MIDI in- struments. Continuator can then continue the im- provisation by performing in the style of the analyzed performer. Other research in the field of machine musician- ship that informed our work addresses areas such as theoretical modeling of improvisation and musical interaction (Pressing 1994; Johnson-Laird 2002) and experimentations with real-time human-robotic interaction such as Mari Kimura's composition GuitarBotana for the Guitarbot (Singer, Larke, and Bianciardi 2003). Rhythmic Perceptual Modeling Computational modeling of rhythm perception can be seen as a subset of the field of machine musician- ship that is particularity relevant to the rhythmic phase of our work. Research in this area addresses computational approaches for modeling both low- and high-level rhythmic percepts. Lower-level cog- nitive rhythmic modeling addresses percepts from onset detection to tempo and beat detection using audio sources (e.g., Puckette, Apel, and Zicarelli 1998; Scheirer 1998; Foote and Uchihashi 2001) as well as MIDI (e.g., Winkler 2001). Higher-level rhythmic percepts include more subjective concepts such as rhythmic stability, similarity, and tension. Similarity comparison modeling typically focuses 30 Computer Music Journal</page><page sequence="4">on how well two rhythms overlap. For instance, Paulus and Klapuri (2002) correlate low-level fea- tures of the audio signal, Tanguiane (1993) counts the number of coincident onsets, and Coyle and Schmulevich (1998) correlate a sequence of note du- ration ratios. Desain and Honing (2002), on the other hand, use a computational model for rhyth- mic stability as a potential measure of similarity and predictability that is based on the relationship between pairs of adjacent interval durations. In our research, we attempted to implement some of these models in an effort to extract cognitive meaning from real-time live drumming, and to allow Haile to respond to the acoustic input based on its cogni- tive understanding of low- and high-level rhythmic concepts. Challenges As part of our effort to construct a robotic percus- sionist that can demonstrate musicianship, we identified a number of challenges in design, me- chanics, perception, and interaction. Our main chal- lenge in designing Haile's physical body was to create a mechanical device that encourages humans to collaborate with a machine in an expressive and intuitive manner. The robot's shape, construction materials, and the manner in which technology is embedded in the physical form had to support intu- itive and engaging musical interaction. Mechani- cally, our main challenge was to develop a dexterous robotic apparatus that could accurately translate perceptually based performance algorithms into a rich acoustic and visual performance. Ultimately, we aimed to control actions that explore the rich sonic variety of percussive musical instruments through multiple playing techniques and sound- production methods. In perception, our challenge was to implement models for low-level percepts such as note onset and pitch as well as high-level musical percepts such as rhythmic stability and similarity, allowing the robot to obtain a meaning- ful representation of the music to which it listens. Based on this perceptual analysis, an additional challenge was to develop responsive improvisation algorithms, generating musical responses that intu- itively relate to human input. In interaction design, our goal was to develop per- formance algorithms that would enable the robot to collaborate with human players in a meaningful and inspiring manner, using transformative and genera- tive methods both sequentially and synchronously. Later in this article, we present future educational challenges in developing constructionist musical activities for Haile that allow novices to interact with the robot while learning music, mathematics, acoustics, engineering, and programming. Implementation To encourage familiar and expressive interactions with human players, Haile's design is anthropomor- phic, employing two percussive arms that can move to different locations along the drum's radius and strike with varying velocities. We now consider our implementation of this robot, including aspects of its physical design, mechanics, perception, and interactivity. Physical Design Haile is designed to play a Native American pow- wow drum, a unique multi-player instrument that supports the collaborative nature of the project. To match the natural aesthetics of the Native Ameri- can powwow ritual, we chose to construct the robot from wood. A design made by Clint Cope was used as a basis for Haile's appearance. The wooden parts were made on a CNC wood-cutting machine and constructed from several layers of plywood that were glued together. Metal joints were designed to allow shoulder and elbow movement as well as leg adjustability for different drum heights. While at- tempting to create a warm and organic look for the robot, it was also important that the technology would not be completely hidden, so that players and learners would be able to see and understand the robot's operation. We therefore left the mechanical apparatuses uncovered and embedded a number of Weinberg and Driscoll 31</page><page sequence="5">Figure 1. Haile's anthropo- morphic design. Figure 2. The right arm slider mechanisms. Figure 3. The right arm striking mechanism. 5~ I ;% :;??*? ~:Bhl ~p??: ', i $' t ' IPi~i t ,, ?.?. ? ~ ~9dl:.~p 8~~ r : : i _: ': a:;e -?a, 'rC:? s?,,, i ;:? ~j~a~:? ??~ i~-? 'c ???t; f -r ,:, I, *:i 5) j "r: -?s ,- 3 B~IPI LEDs on Haile's body that provide an additional rep- resentation of the mechanical actions (see Figure 1). Mechanics Haile controls two robotic arms; the right arm is designed to play fast notes, and the left arm is de- signed to produce larger and more visible motions that produce louder sounds in comparison to the right arm. Both arms can adjust the sound of strikes in two manners: Different pitches are achieved by striking the drumhead in different locations along its radius, and volume is adjusted by hitting with orto 000a 00 ;0 potentiometer motor Figure 2 solenoid' sling mechanism ,, Figure 3 varying velocities. To move to different positions over the drumhead, each arm employs a linear slidc a belt, a pulley system, and a potentiometer that provides feedback (see Figure 2). Unlike robotic drumming systems that allow hits at only a few dis crete locations (Jorda 2002; Rae 2005), Haile's arms can strike anywhere on a line between the center an the rim of the drum, moving the 10 inches (about 2: cm) between these two points in about 250 msec. The right arm's striking mechanism is loosely based on a piano hammer action and consists of a solenoid-driven device and a return spring (see Fig- ure 3). The arm strikes at a maximum speed of 15 Hz, faster than the left arm's maximum speed of 11 Hz. (For comparison, the human winner of the 200z International Fastest Drummer award played 1,180 32 Computer Music Journal</page><page sequence="6">Figure 4. Haile's right arm design. tor~ ide I . Sile no *1 Striker1 ~ 4~ beats per minute-less than 10 Hz per arm-as re- ported on drummagazine.com.) However, the right arm cannot generate a wide dynamic range or pro- vide easily noticeable visual cues, which limits Haile's expression and interaction potential. The left arm was designed to address these shortcomings using larger visual movements and a more powerful and sophisticated hitting mechanism. Whereas the striking component of the right arm is about the size of a finger and can move only 2.5 inches (about 6.4 cm) vertically, the entire left forearm takes part in the striking motion and can move up and down 8 inches (about 20 cm). A linear motor and an encoder located at the elbow are used to provide sufficient force and control for the larger mass and motions. Additional images showing the mechanical con- struction of the arms are provided in Figures 4 and 5. In an effort to provide an easy-to-program envi- ronment for Haile, we decided to use Max/MSP, an intuitive graphical programming environment that can make the project accessible to composers, per- formers, and students. Our first one-armed proto- type incorporated the USB-based Teleo System from makingthings.com as the main interface between Max/MSP and Haile's sensors and motors. Low- level control of the solenoid-based right arm's posi- tion was computed within Max/MSP, which required a continuous feed of position updates to the computer. This consumed much of the commu- Figure 5. The linear motor and encoder provide closed-loop position and velocity over the left arm height while the gear mo- tor and potentiometer con- trol distance from the center of the drum. wlinear motor potentiometerj / .- tgearmotor - encode nication bandwidth as well as processor time on the main computer. The current two-arm mechanism uses multiple onboard microprocessors for local low-level control as well as Ethernet communication with the main computer. The new system, therefore, facilitates much faster and more sophisticated control (with its 2-msec control loop) and requires only low- bandwidth communication with the operating com- puter. Each arm is locally controlled by an 18F452 PIC microprocessor, both of which receive RS232 communications from a Modtronix Ethernet board (SBC68EC). The Ethernet board receives 3-byte pack- ets from the computer: a control byte and two data bytes. The protocol uses an address bit in the control byte to send the information onto the appropriate arm processor. The two data bytes typically contain position and velocity set points for each hit, but they can also be used to update the control parameters. Weinberg and Driscoll 33</page><page sequence="7">Figure 6. Magnitude plots from 60-Hz, 300-Hz, and 5- kHz frequency bands over several low- and high- pitched hits showing the relatively slow decay of the low-pitched hits. (A 2,048-point FFT and a 512- point hop size were used.) Two onboard PIC microprocessors are responsible for controlling the arms' sliding and hitting mecha- nisms, ensuring that the impacts occur at the de- sired positions and velocities. To allow enough time for the arms to move to the correct locations and execute the strokes, a 300-msec delay line is imple- mented between signal reception and impact. It has been shown that rhythmic errors of only 5 msec are detectable by average listeners (Coren and Ward 1984); therefore, it was important to ensure that this delay remained accurate and constant regard- less of different hit velocities, allowing us to easily compensate for it in the higher-level interaction application. Both arms store incoming hit commands in a first-in-first-out queue, moving toward the location of a new note immediately after each hit. Owing to its short vertical hitting range, the solenoid-driven right arm has a fairly consistent stroke time for both soft and loud hits. We therefore implemented the 300-msec delay as a constant for this arm. The left arm, on the other hand, undergoes much larger movements, which require complex feedback con- trol to ensure that impacts occur at the correct time regardless of hits velocity. While waiting for incom- ing notes, the left arm remains about one inch (about 2.5 cm) above the surface of the drum. When a new note is received, the arm is raised to a height proportional to the loudness of the hit. After a delay determined by the desired velocity and elevation, the arm starts descending toward the drumhead un- der velocity control. After impact, the arm returns back to its standby position above the drumhead. Extremely fast notes employ a slightly different control mode that makes use of the bounce of the arm in preparation for the next hit. The left arm, therefore, controls a wide dynamic range and pro- vides performers and viewers with anticipatory and real-time visual cues, enhancing expression and en- riching the interaction representation. Perception As a test bed for musical human-robot interaction, we developed a number of independent perceptual modules for Haile that can be embedded in a variety -20 60 Hz -40 300 Hz =80 _aa -100 5kHz -120 -140 100(1.16sec) 120(1.39sec) 140(S.63sec) 160(1.86sec) 180(2.09sec) 200(2.32sec) FFT Slice number (time in seconds) of combinations in compositions and other interac- tion schemes. Each module addresses one percep- tual aspect-from hit onset, amplitude, and pitch detection, through beat and density analysis, to rhythmic stability and similarity perception. We base our low-level modules for hit onset and ampli- tude detection on the Max/MSP bonk- object (Puckette, Apel, and Zicarelli 1998), and we adjust its output to the unique character of the powwow drum. The bonk- object provides effective onset attack detection, but its frequency band output is insufficient for accurate pitch detection of the powwow's low and long-reverberating sounds. Be- cause bonk- is hard-coded with a 256-point analysis window, the lowest frequency it can analyze is 172Hz-too high for the powwow drum, which has a natural fundamental frequency of about 60 Hz. Moreover, pitch detection is complicated when high-frequency hits are masked by the long decay of the previous low-pitched strikes. To address these issues, we wrote a Max/MSP ex- ternal object that uses 2,048-point analysis win- dows to determine the magnitude and derivative of lower-frequency bins. By taking into account the spectral changes in addition to magnitudes, we can better determine whether energy in a particular fre- quency band came from the current hit or from pre- vious ones (see Figure 6). Other relatively low-level perceptual modeling 34 Computer Music Journal</page><page sequence="8">Figure 7. Cognitive ex- pectancy of neighboring time intervals: (left) the expectancy of intervals A and 1-A is higher at integer ratios, 1:1 being the high- est; (right) bias toward ex- pecting intervals of 600 msec (after Desain and Honing 2002). Eb(A, 1,A) Eb(0.3,B) 0.9 0.9 0.8 0.8 0.7 0.7 0.6 I 0.6 I o 05 S t I 0 .50.5 x 0.4 0.4 0.3 0.3 0.1 - 0.1 0 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 A B modules provide beat detection, using Tristan Je- han's beat- Max/MSP object based on Scheirer (1998), and density detection, where we examine the number of note onsets per unit time to repre- sent the density of the rhythmic structure. We also implement a number of higher-level rhythmic anal- ysis modules for percepts such as rhythmic stabil- ity, based on Desain and Honing's computational model (2002), and similarity using the model of Tanguiane (1993). The stability model is based on the relationship between pairs of adjacent note du- rations that are rated according to their perceptual expectancy. This depends on three main criteria: Perfect-integer relationships are favored, ratios have inherent expectancies (i.e., 1:2 is favored to 1:3, and 3:1 is favored to 1:3), and durations of 600 msec are preferred. The expectancy function may be com- puted as E,(A,B) = (round(r)-r) x 2(r-floor(r)-0.5fx round(r)ddr where A and B are the durations of the two neigh- boring intervals, r = max (AIB,BIA) represents the near-integer relationship between note durations, p controls the shape of the peaks, and d is negative and affects the decay rate as the ratios increases. This function is symmetric around r = 1 when the total duration is fixed (see Figure 7, left). Generally, the expectancy function favors small near-integer ratios and becomes asymmetric when the total du- ration varies, exhibiting the bias toward the 600- msec interval (see Figure 7, right). Our similarity rating is derived from Tanguiane's binary representation, where two rhythms are first quantized and then given a score based on the num- ber of note-onset overlaps and near-overlaps. To support real-time interaction with human players, we developed two Max/MSP externals that analyze and generate rhythms based on these stability and similarity models. These externals have recently been embedded in a live interaction module that reads measure-length rhythmic phrases and modi- fies them based on desired stability and similarity parameters. Both parameters vary between 0 and 1 and are used together to select an appropriate rhythm from a database of pre-analyzed rhythms. A stability rating of 1 indicates the most stable rhythm in the database, and 0 corresponds to the least stable rhythm. The similarity parameter deter- mines the relative contribution of similarity and stability (for detail, see Weinberg, Driscoll, and Thatcher 2006). Interaction Design The main challenge in designing the interaction with Haile was to implement our perceptual mod- Weinberg and Driscoll 35</page><page sequence="9">Figure 8. Model of sequen- tial decentralized interac- tion. Musical actions are taken in succession with- out synchronous input from other participants and with no central system to coordinate the interac- tion. Action Action Action Time ules in a manner that would lead to an inspiring human-machine collaboration. The approach we took to address this challenge is based on our theory of interdependent group interaction in intercon- nected musical networks (Weinberg et al. 2005). At the core of this theory is a categorization of collabo- rative musical interactions in networks of artificial and live players based on sequential and synchro- nous operations with centralized and decentralized control schemes. For example, in sequential decen- tralized interactions, players create their musical materials with no influence from a central system or other players and can then interact with the algo- rithmic response in a sequential manner (see Figure 8). In a synchronous centralized network topology, on the other hand, players modify and manipulate their peers' music in real-time, interacting through a computerized hub that performs analysis and gen- erative functions (see Figure 9). More sophisticated schemes of interaction can be designed by combin- ing centralized, decentralized, synchronous, and se- quential interactions in different directions, and by embedding weighted gates of influence between par- ticipants (see Figure 10). Based on these ideas, we developed six different interaction modes for Haile: Imitation, Stochastic Transformation, Perceptual Transformation, Beat Detection, Simple Accompaniment, and Perceptual Accompaniment. These interaction modes use dif- ferent perceptual modules and can be embedded in different combinations in interactive compositions and educational activities. In the first mode, Imita- tion, Haile merely repeats what it hears based on its low-level onset, pitch, and amplitude perception modules. Players can play a rhythm, and after a couple of seconds of inactivity, Haile imitates it in a sequential call-and-response manner. Haile uses Figure 9. Model of synchro- nous centralized interac- tion. Human and machine players take musical ac- tions simultaneously and interact through a comput- erized hub that interprets and analyzes the input data. Action Action Hub Action Action one of the arms to play lower pitches close to the drumhead center and the other arm to play higher pitches close to the rim. In the second mode, Stochastic Transformation, Haile improvises in a call-and-response manner based on players' input. Here, the robot stochasti- cally divides, multiplies, or skips certain beats in the input rhythm, creating variations of users' rhythmic motifs while keeping their original feel. Different transformation coefficients can be adjusted manu- ally or automated to control the level of similarity between their motifs and Haile's responses. In the Perceptual Transformation mode, Haile an- alyzes the stability level of users' rhythms and re- sponds by choosing and playing other rhythms that have similar levels of stability to the original input. In this mode Haile automatically responds after a specified phrase length. Imitation, Stochastic Trans- formation, and Perceptual Transformation are all sequential interaction modes that form decentral- ized call-and-response routines between human players and the robot. Beat Detection and Simple Accompaniment modes, on the other hand, allow synchronous interaction where humans play simul- taneously with Haile. In Beat Detection mode, Haile uses the Max/MSP object beat- to track the tempo and beat of the in- put rhythm. Although beat- can be effective for pre-recorded audio, in a live setting human players naturally adjust to the robot's tempo, which leads to an unsteady input tempo that is difficult for beat- to follow. Haile therefore uses beat- to listen for a short period (5-10 sec) and then locks the tempo be- fore joining in. 36 Computer Music Journal</page><page sequence="10">Figure 10. A combination of centralized, decentral- ized, synchronous, and se- quential musical actions in an asymmetric topology with weighted gates of in- fluence (after Weinberg 2005). 10 1 0 Action 33 Action50 I 0 Ac - O .. .. .... 50 Action SHub A simpler yet effective synchronous interaction mode is Simple Accompaniment, where Haile plays pre-recorded MIDI files so that players can interact with it by entering their own rhythms or by modify- ing elements such as drumhead pressure to modu- late and transform Haile's timbres in real time. This synchronous centralized mode allows com- posers to feature their structured compositions in a manner that is not susceptible to algorithmic trans- formation or significant user input. The Simple Ac- companiment mode is also useful for sections of synchronized unisons where human players and Haile play together. Perhaps the most advanced mode of interaction is the Perceptual Accompaniment mode, which combines synchronous, sequential, centralized, and decentralized operations. Here, Haile plays simulta- neously with human players while listening to and analyzing their input. It then creates local call-and- response interactions with different players based on its perceptual analysis. In this mode, we employ the amplitude and density perceptual modules de- scribed previously. While Haile plays short looped sequences (captured during the Imitation and Sto- chastic Transformation modes), it also listens to and analyzes the amplitude and density curves of human playing. It then modifies its looped sequence based on the amplitude and density coefficients of the human players. When the rhythmic input from the human players is dense, Haile plays sparsely, providing only the strong beats and allowing hu- mans to perform denser solos. When humans play sparsely, on the other hand, Haile then improvises using dense rhythms that are based on stochastic and perceptual transformations. Haile also responds in direct relationship to the amplitude of the human players so that the louder humans play, the stronger Haile plays to accommodate the human dynamics, and vice versa. Compositions The authors wrote two compositions for the sys- tem, each using a different set of perceptual and in- teraction modules. The first composition, titled Pow, premiered at the Eyedrum Gallery in Atlanta as part of the Listening Machines concert in January 2005. The second piece, titled Jam'aa, was commis- sioned by the Hamaabada Performing Art Center and premiered in Jerusalem, Israel, in March 2006. Video clips of these compositions are available on- line at www.cc.gatech.edu/-gilwein/Haile.htm. [Editor's note: Also see the DVD accompanying this issue.] Pow The composition Pow, written for one or two human players and a one-armed robotic percussionist play- ing a powwow drum, served as test case for some of Haile's early mechanical, perceptual, and interac- tion modules (see Figure 11). The piece begins with call-and-response routines, featuring Haile's onset, amplitude, and pitch detection in Imitation mode. It develops into an improvisatory section where Haile invokes the Stochastic Improvisation mode. Here analyzed pitch, amplitude, and rhythmic data are used to generate simple stochastic manipula- tions such as hit division and pitch averaging. A structured section then follows, using the Simple Accompaniment mode, where human players inter- act with Haile in a synchronous manner, taking turns as soloists based on a prerecorded 7/4-time MIDI file. The piece ends with a short showcase of Haile's mechanical abilities, featuring a fast trill that slides back and forth along the drum's radius. Jam'aa The composition Jam'aa ("gathering" in Arabic) builds on the unique communal nature of the Weinberg and Driscoll 37</page><page sequence="11">Figure 11. The composi- tion Pow for the first one- arm prototype of Haile as performed in concert at the Eyedrum, Atlanta, Georgia. \j. Middle-Eastern ipercussio)n ensemble, attcmpting to enrich its improvisational nature, call-and-respJonsc routines, and virtuosic solos with algorithmic trans- formation and human-robotic interactions (see Fig- ure 12). In Jam'aa, we added to the sonic variety of the piece by incorporating two robotic arms and by including other percussive instruments such as dar- bukas (goblet-shaped hand drums), djembes, and tambourines. Here, Haile listens to audio input via directional microphones installed inside two dar- buka drums played by humans. It responds by play- ing a powwow drum using two arms while other human players join the drum circle, supporting the beat without interacting directly with the robot. In some sections of the piece, the left arm merely provides the beat, whereas in other sections it par- ticipates in the algorithmic interaction. Jam'aa uses interaction modes that were not included in I'hm such as Perccptual Transformation, in which Hailc responds based on its cognition of rhythmic stability and similarity, and Perceptual Accompani- ment, in which Haile modifies its drumming in real time based on the perceived density and loud- ness of human playing. We also developed a new re- sponse algorithm for Jam'aa called Morphing, where Haile combines elements from two or more of the motifs played by humans based on a number of integration functions. A more detailed descrip- tion of the piece can be found in Weinberg, Driscoll, and Thatcher (2006). User Study To evaluate our approaches in design, mechanics, perception, and interaction, we conducted a user 38 Computer Music Journal</page><page sequence="12">Figure 12. The composition Jam'aa for two darbuka players and robotic percus- sionist as performed in concert at Hamaabada Art Center, Jerusalem, Israel. 4 .* ,t i-. ?I~"~i?:~~' a -?-? ~?, 'P:"~;~ i., i ?,- 1? i- ?~ r.; f i~?? ~?-~ ? ?~r~ rr; - rr r- 5? ' z t ? i C L ;&lt;: B u ~iL~, I'' 'L: t I c C?::,e . ?.I? ar, 5?.- ?" s": i~aazairolb~a:i ,Y; H'(. *~TS*~L~UI: S`' ~4-~C `?~~c.' ?:r'~~'Y . .? aa 3rr?, -.~ ? ?m:L'?.? ,. ,. ('. ~. 3 _i S.?~t~~pp study where subjects were asked to interact with Haile, to participate iiin a perceptual cxperimlent, and to complete a questionnaire regarding their experi- ence. The 14 undergraduate students who partici- pated in the study were enrolled in the percussion ensemble class at the Georgia Institute of Technol- ogy in Spring 2006 and had at least eight years of ex- perience each in playing percussion instruments. This level of experience was required to support the musical interaction with Haile as well as to support a meaningful discussion about subjects' experiences. Each subject spent about 20 minutes experiment- ing with four different interaction modes: Imitation, Stochastic Transformation, Perceptual Accompani- ment, and Perceptual Transformation. Subjects were then asked to compare their notion of rhyth- mic stability with Haile's algorithmic implementa- tion. As part of the perceptual experiment on stabilityv subiects were asked to improvise a one- measure rhythmic phrase while Haile provided a 4/4-time beat at 90 beats per minute. Subjects were then randomly presented with three transforma- tions of their phrase: a less stable version, a version with similar stability, and a more stable version. The transformed measures were generated by our Max/MSP stability external using stability ratings of 0.1, 0.5, and 0.9 for less, similar, and more stabil- ity, respectively. The original phrase and the three transforma- tions were played by Haile's right arm while its left arm provided a metronomic beat. All phrases were played twice to familiarize subjects with the materials. Students were then asked to indicate which phrase, in their opinion, was less stable, sim- ilarly stable, or more stable in comparison to the original input. Stability was explained as represent- Weinberg and Driscoll 39</page><page sequence="13">ing the "predictability of" or "ease of tapping one's foot along with" a particular rhythm. [Editor's note: For a video clip from the experiment, see www.coa.gatech.edu/-gil/HaileUserStudy.mov or the DVD accompanying this issue.] The goal of this experiment was not to reach a definite, well- controlled conclusion regarding the rhythmic sta- bility model we used, but rather to obtain a preliminary notion about the correlation between our algorithmic implementation and a number of human subjects' perceptions in an interactive setting. The next section of the user study involved a written survey where subjects were asked to answer questions describing their impression of Haile's physical design, mechanical operation, the different perceptual and interaction modes, as well as a num- ber of general questions about human-robot interac- tion and "robotic musicianship." The survey included 39 questions such as "What aspects of the design and mechanical operation make Haile com- pelling to play with?" "What design aspects are problematic and require improvements?" "What musical aspects were captured by Haile in a satis- factory manner?" "What aspects were not captured well?" "Did Haile's response make musical sense?" "Did the responses encourage you to play differ- ently than usual and in what ways?" "Did the inter- action with Haile encourage you to come up with new musical ideas?" and "Do you think that new musical experiences, and new music, can evolve from musical human-robot interaction?" Results Most subjects addressed Haile's physical design in positive terms, using descriptors such as "unique," "artistic," "stylized," "organic," and "functional." Other opinions included "the design offered a feel- ing of comfort," "the design was pleasing and invit- ing," and "if Haile was not anthropomorphic, it would not have been as encouraging to play with." When asked about caveats in the design, several subjects mentioned "too many visible electronics" and "exposed cabling" and suggested that future de- signs should be "less cluttered." Another critique was that "the design did not appear to be versatile for use with other varieties of drums." Regarding Haile's mechanical operation, subjects provided positive comments regarding the steadi- ness and accuracy of the left hand and the speed and "smoothness" of the right hand. The main mechan- ical caveats mentioned were Haile's limited timbre and volume control as well as the lack of larger and more visual movements. Only one respondent com- plained about the mechanical noise Haile produces. In the perceptual rhythmic stability study, half of the respondents (7 out of 14) correctly identified the three transformations. (By comparison, a random re- sponse would choose 2.3 out of 14 correctly, on av- erage.) The majority of confusions were between similar and more-stable transformations and be- tween similar and less-stable transformations. Only three responses out of the total 42 decisions confused a more-stable version for a less-stable version, implying that larger differences in algorith- mic stability ratings made differentiation easier. Only one subject labeled all three generated rhythms incorrectly. Subjects' responses to the four interaction modes were varied. In Imitation Mode, respondents men- tioned Haile's "accuracy" and "good timing and speed" as positive traits and its lack of volume con- trol as a caveat. Responses to the question "How well did Haile imitate your playing?" ranged from "pretty well" to "amazingly well." Some differ- ences between the interaction modes became appar- ent. For example, in Stochastic Transformation Mode (STM), about 85% of the subjects provided a clear positive response to the question "Was Haile responsive to your playing?" Only about 40% gave such a clear positive response to this question in Perceptual Accompaniment Mode (PAM). Respon- dents refer to the delay between user input and ro- botic response in PAM as the main cause for the "less responsive feel." To the question "Did Haile's responses encourage you to play differently than usual?" 50% of the subjects provided a positive re- sponse in STM whereas only 30% gave a positive re- sponse to this question in PAM. When asked to describe how different than usual their playing was in STM, subjects focused on two contradicting motivations. Some mentioned that 40 Computer Music Journal</page><page sequence="14">they played simpler rhythms than usual so Haile could transform them easily and in an identifiable manner. Others made an effort to play complex rhythms to challenge and test Haile's abilities. These behaviors were less apparent in PAM. Al- though only 40% (across all interaction modes) pro- vided a positive answer to the question "Did Haile's responses encourage you to come up with new mu- sical ideas?", more than 90% of participants an- swered positively to the question "Do you think that new musical experiences and new music can evolve from human-machine musical interac- tions? ", strengthening their answers with terms such as "definitely," "certainly," and "without question. " Discussion Based on the experiment and survey, we feel that our preliminary attempt at robotic musicianship provided promising results. The most encouraging survey outcome, in our opinion, was that subjects felt that the human-machine collaboration estab- lished with Haile did on occasion lead to novel mu- sical experiences and new musical ideas that would not have been conceived by other means. It is clear, though, that further work in mechanics, perception, and interaction design is required to create a robot that can truly demonstrate "musicianship." Nearly all subjects addressed Haile's design in positive terms, strengthening our assumption that the wood and the organic look function well in a drum-circle context. Our decision to complement the organic look with exposed electronics was criticized by some subjects, although we feel that this hybrid de- sign conveys the robotic functionalities and reflects the electroacoustic nature of the project. Mechanically, most subjects were impressed with the speed and smoothness of Haile's operation. Only one subject complained about the noise pro- duced by the robot, which suggests that most play- ers were able to either mask the noise or to accept it as an inherent and acceptable aspect of human- robot interaction. Several subjects, however, indi- cated that Haile's right arm, which was responsible for playing back the transformations, did not pro- vide satisfactory visual cues and could not produce adequate variety of loudness and timbre. The con- trol mechanism that we later developed for the left arm was designed to address this problem, and it can now provide a wide dynamic range and large vi- sual cues. We therefore plan to use the left arm as the playing device in future user studies. We also in- tend to improve its control in an effort to increase timbral variety through various techniques such as applying damping briefly after hits. The user study and survey also provided encour- aging results in regard to Haile's perceptual and in- teraction modules. The high percentage of positive responses regarding Imitation Mode indicates that our low-level onset and pitch-detection algorithms were effective. In general, a large majority of the re- spondents indicated that Haile was responsive to their playing. Perceptual Accompaniment Mode (PAM), however, was an exception to this rule, as subjects felt Haile was not responding to their ac- tions with acceptable timing. PAM was also unique in the high percentage of subjects who reported that they did not play differently in comparison to play- ing with humans. We explain these results by the synchronous accompaniment nature of PAM, which is more familiar to most percussion students. Most subjects, on the other hand, felt compelled to play differently than usual in sequential call-and- response modes such as Stochastic Transformation Mode (STM). Here, subjects changed their usual drumming behaviors either by simplifying their rhythms to better follow Haile's responses or by playing complex rhythms in an effort to challenge the robot's perceptual and mechanical abilities. We believe that these behaviors were caused by the novelty effect as players attempted to explore Haile's physical and cognitive boundaries. We as- sume that subjects may develop more complex in- teraction behaviors if given longer play times. Given the high level of variance in the notion of rhythmic stability in human perception, we feel that our rhythm stability experiment performed better than expected. Some problems in our method may have also hindered the results. For example, misalignment of subject drumming with the metronome during recording led to misaligned transformations, which may have been unjustifi- Weinberg and Driscoll 41</page><page sequence="15">ably perceived as unstable. Also, because the trans- formed rhythms were generated based on subjects' input, the relative difference between the output stabilities in some cases became minimal and diffi- cult to identify. For example, when a subject's origi- nal phrase was extremely stable, the algorithm would not be able to produce an identifiably "more stable" phrase. Asking subjects to play a unified mid-stability rhythm as input could have solved this problem, although we were specifically inter- ested in evaluating Haile's perception in a live im- provisatory context. As indicated earlier, the most encouraging results were that 40% of subjects stated that the interac- tion with Haile encouraged them to come up with new musical ideas, and more than 90% claimed that they believe that new musical experiences and new music can evolve from such human-machine interaction. This may indicate that although the po- tential for creating novel musical experiences be- tween humans and robots was not fully realized in our current implementation, the experience led a large majority of the subjects to feel optimistic about the prospect of achieving such novel musical experiences in the future. Future Work We identify several directions for immediate and long-term future work. Mechanically, we intend to further investigate the workings of hand drumming in an effort to improve Haile's timbral control. Drum timbre is highly dependent on multiple fac- tors such as hand shape, contact area, contact loca- tion, contact duration, and pressure on the skin (Taylor 2004). A wide sonic variation can be pro- duced by playing a hand drum using different stroke motions, contact areas (e.g., fingers vs. palm), leav- ing the hand momentarily on the skin to briefly dampen it, stretching the skin with the other hand, etc. Most mechanical instruments (player pianos, drums, mallets, etc.) produce only quick bounces off the surface and avoid "human finesse" during the hit. But it is this finesse that makes a human player's expression and intonation interesting and colorful. We believe that current technology cannot support the creation of a robot with such dexterity that would compare with a human's expressivity and virtuosity, but we do believe that significant ad- vancements are possible. We would also like to expand on what is hu- manly possible by experimenting with alternative striker shapes, materials, and mechanisms that do not reflect traditional percussion instruments. In light of our plan to continue improving Haile's me- chanical stroke variety and timbre control, we also plan to explore better approaches for the perception of timbre and stroke variety. To this end, we are ex- amining a number of neural-network and machine- learning approaches (e.g., Chordia 2004; Tindale et al. 2005). In the longer term, we also hope to expand the promise of robotic musicianship to other instru- ments, such as pitched membranophones, idio- phones, chordophones, and aerophones. This direction would call for further research into per- ceptual modeling of pitch and tonality, allowing Haile to listen to and respond to pitch-based mono- phonic (and ultimately polyphonic) musical instru- ments. Some of the percepts that we have started to investigate in that regard are local attractions, melodic similarity (Hewlett and Selfridge-Field 1998), and melodic complexity (Narmour 1992). We are also considering implementing models for melodic attraction (Lerdahl and Jackendoff 1983), melodic tension (Narmour 1990), and contour direc- tionality (Trehub, Bull, and Thorpe 1984). The choice of percepts and modeling schemes will be in- tegral to the definition of future interaction and re- sponse algorithms. We plan to continue to evaluate our current interaction design with in-depth user studies and to adapt future interaction modes to Haile's new capabilities. In particular, we are inter- ested in designing new interaction schemes that would take advantage of Haile's ability to listen to and interact with multiple players in a group. In addition to expanding our research in mechan- ics, perception, and interaction design, we also plan to investigate the use of Haile for educational pur- poses. Our educational pedagogy is informed by the theory of constructionism, which demonstrates 42 Computer Music Journal</page><page sequence="16">how learning is most effective when students con- struct personally meaningful technological artifacts (Papert 1980). The theory emphasizes the unique ability of computers to provide personal and config- urable learning experiences to a wide variety of learners. Recent research elaborated on Papert's ideas, showing how interaction with digital physi- cal objects enhances children's and adults' learning (Resnick et al. 1996). In the field of music, however, little has been done to develop physical construc- tionist systems that can provide a compelling inter- disciplinary education, not only in music, but also in mathematics, the sciences, and computer pro- gramming. For our educational work with Haile, therefore, we hope to capitalize on the beneficial ef- fect of music education on learning in domains of knowledge beyond that of music (Schillinger 1976; Bamberger 2000; Rauscher, Shaw, and Ky 1993). We plan to build on our previous work in this area (Weinberg, Lackner, and Jay 2000) by developing a constructionist educational application for Haile that would allow learners to translate abstract mu- sical ideas into symbolic representations and physi- cal gestures. The mathematical and scientific aspects of the project will be guided and motivated by learners' drive to rhythmically compose acoustic sounds, creating personal interactive musical com- positions. The educational environment will allow learners to develop their intuitions by emphasizing shared structures in music and mathematics such as hierarchies, periodicity, units, ratio-proportion, symmetries, and patterns. Students will also be able to experiment with creating perceptual social be- haviors by programming rule-based responses in an effort to make Haile an expressive, responsive, and intriguing playing companion. Acknowledgments We would like to thank Clint Cope, Eric Fontaine, Mitch Parry, and Travis Thatcher for their contribu- tion to the project. We also thank the Music Depart- ment, the College of Architecture, the Advanced Wood Product Laboratory, and the GVU center at the Georgia Institute of Technology for their support. References Baginsky, N. A. 2004. "The Three Sirens: A Self-Learning Robotic Rock Band." Available online at www .the-three-sirens.info (accessed May 1, 2006). Bamberger, J. 2000. "Music, Math, and Science: Towards an Integrated Curriculum." Journal for Learning Through Music Spring:32-35. Brooks, A.G., et al. 2004. "Robot's Play: Interactive Games with Sociable Machines." Computer Entertain- ment 2(3):10-20. Chordia, P. 2004. Automatic Tabla Transcription. Ph.D. Thesis, Stanford University. Cope, D. 1996. Experiments in Music Intelligence. Madi- son, Wisconsin: A-R Editions. Coren, S., and L. M. Ward. 1984. Sensation and Percep- tion. San Diego, California: Harcourt. Coyle, E. J., and I. A. Shmulevich. 1998. "System for Ma- chine Recognition of Music Patterns." Paper presented at the 1998 IEEE International Conference on Acous- tics, Speech, and Signal Processing, Seattle, 12-15 May. Crick, C., M. Munz, and B. Scassellati, 2006. "Robotic Drumming: Synchronization in Social Tasks." Pre- sented at the 15th IEEE International Symposium on Robot and Human Interactive Communication. Uni- versity of Hertfordshire, Hatfield, United Kingdom, 6-8 September 2006. Dannenberg, R. 1984. "An Online Algorithm for Real-Time Accompaniment." Proceedings of the 1984 International Computer Music Conference. San Francisco, California: International Computer Music Association, pp. 193-198. Dannenberg, R. B., B. Brown, G. Zeglin, and R. Lupish. 2005. "McBlare: A Robotic Bagpipe Player." Proceed- ings of the 2005 International Conference on New In- terface for Musical Expression. Vancouver, Canada: University of British Columbia, pp. 80-84. Desain, P., and H. J. Honing. 2002. "Rhythmic Stability as Explanation of Category Size." Paper presented at the 2002 International Conference on Music Perception and Cognition, University of New South Wales, Syd- ney, July 17-21. Dorsenn, M. V. 2006. "The Cell." Available online at www.cell.org.au (accessed May 2006). Foote, J., and S. Uchihashi. 2001. "The Beat Spectrum: A New Approach to Rhythmic Analysis." Paper pre- sented at the 2001 International Conference on Multi- media and Expo, Tokyo, 22-25 August. Hewlett, B., and E. Selfridge-Field. 1998. Melodic Similar- ity : Concepts, Procedures, and Applications. Cam- bridge, Massachusetts: MIT Press. Weinberg and Driscoll 43</page><page sequence="17">Johnson-Laird, P. N. 2002. "How Jazz Musicians Impro- vise." Music Perception 19(3):415-442. Jorda, S. 2002. "Afasia: The Ultimate Homeric One-Man Multimedia Band." Paper presented at the 2002 Inter- national Conference on New Interface for Musical Ex- pression, Dublin, 24-26 May. Kapur, A. 2005. "A History of Robotic Musical Instru- ments." Proceedings of the 2005 International Com- puter Music Conference. San Francisco, California: International Computer Music Association, pp. 21-28. Lerdahl, F., and R. Jackendoff. 1983. A Generative Theory of Tonal Music. Cambridge, Massachusetts: MIT Press. Lewis, G. 2000. "Too Many Notes: Computers, Complex- ity, and Culture in Voyager." Leonardo Music Journal 10:33-39. Narmour, E. 1990. The Analysis and Cognition of Basic Melodic Structures. Chicago: University of Chicago Press. Narmour, E. 1992. The Analysis and Cognition of Melodic Complexity: The Implication-Realization Model. Chicago: University of Chicago Press. Orio, N., S. Lemouton, and D. Schwarz. 2003. "Score Fol- lowing: State of the Art and New Developments." Pro- ceedings of the 2003 International Conference on New Interfaces for Musical Expression. Montreal: McGill University, pp. 36-41. Pachet, F. 2002. "The Continuator: Musical Interaction with Style." Proceedings of the 2002 International Computer Music Conference. San Francisco, Califor- nia: International Computer Music Association, pp. 211-218. Papert, S. 1980. Mindstorm. New York: Basic Books. Paulus, J., and A. Klapuri. 2002. "Measuring the Similar- ity of Rhythmic Patterns." Paper presented at the 2002 International Conference on Music Information Re- trieval, Paris, France, 13-17 October. Pongas, D., A. Billard, and S. Schaal. 2006. "Rapid Syn- chronization and Accurate Phase-Locking of Rhythmic Motor Primitives." Proceedings of the 2005 Interna- tional Conference on Intelligent Robots and Systems (IR OS 2005). New York: Institute for Electrical and Electronics Engineers, pp. 2911-2916. Pressing, J. 1994. Compositions for Improvisers: An Aus- tralian Perspective. Melbourne: La Trobe University Press. Puckette, M., T. Apel, and D. D. Zicarelli. 1998. "Real- Time Audio Analysis Tools for Pd and MSP." Proceed- ings of the 1998 International Computer Music Conference. San Francisco, California: International Computer Music Association, pp. 109-112. Rae, G. W. 2005. Robotic Instruments. Available online logosfoundation.org/instrum gwr/automatons.html (accessed May 1, 2005). Rauscher, F. H., G. L. Shaw, and K. N. Ky. 1993. "Music and Spatial Task Performance." Nature 365:611. Resnick, M., F. Martin, R. Sargent, and B. Silverman. 1996. "Programmable Bricks: Toys to Think With." IBM Systems Journal 35:3-4. Rosheim, M. 1994. Robot Evolution: The Development Anthrobotics. New York: Wiley. Rowe, R. 1992. Interactive Music Systems: Machine Lis tening and Composing. Cambridge, Massachusetts: MIT Press. Rowe, R. 2004. Machine Musicianship. Cambridge, Mas sachusetts: MIT Press. Scheirer, E. 1998. "Tempo and Beat Analysis of Acoustic Musical Signals." Journal of the Acoustical Society of America 103(1):588-601. Schillinger, J. 1976. The Mathematical Basis of the Arts. New York: Da Capo Press. Singer, E., J. Feddersen, C. Redmon, and B. Bowen. 2004. "LEMUR's Musical Robots." Proceedings of the 2004 International Conference on New Interfaces for Musi cal Expression. Hamamatsu, Japan: Shizuoka Univer- sity of Art and Culture, pp. 181-184. Singer, E., K. Larke, and D. Bianciardi. 2003. "LEMUR GuitarBot: MIDI Robotic String Instrument." Proceec ings of the 2003 International Conference on New In- terfaces for Musical Expression. Montreal: McGill University, pp. 188-191. Solis, J., et al. 2004. "The Anthropomorphic Flutist Robot WF-4 Teaching Flute Playing to Beginner Stu- dents." Proceedings of the 2004 IEEE International Conference on Robotics &amp; Automation. New York: Institute for Electrical and Electronics Engineers, pp. 146-151. Sony. 2003. "QRIO Conductor Robot." Available online www.sony.net/SonyInfo/QRIO/works/20040325e.htrr (accessed May 1, 2006) Takanishi, A., and M. Maeda. 1998. "Development of Ar thropomorphic Flutist Robot WF-3RIV." Proceedings the 1998 International Computer Music Conference. San Francisco, California: International Computer Mt sic Association, pp. 328-331. Tanguiane, A. 1993. Artificial Perception and Music Recognition. New York: Springer. Taylor, M. 2004. Remembering How to Drum: Djembe Technique. DVD. Blue 7 Media B0001GNB86. Tindale, A. R., A. Kapur, W. A. Schloss, and G. Tzane- takis. 2005. "Indirect Acquisition of Percussion Ges- 44 Computer Music Journal</page><page sequence="18">tures Using Timbre Recognition." Paper presented at the 2005 Colloque Interdisciplinaire de Musicologie, Montreal, 10 March. Toyota. 2004. "Trumpet Robot." Available online at www.toyota.co.jp/en/special/robot (accessed May 2006). Trehub, S. E., D. Bull, and L. A. Thorpe. 1984. "Infants' Perception of Melodies: The Role of Melodic Contour." Child Development 55(1):821-830. Trimpin. 2000. SoundSculptures: Five Examples. Munich: MGM MediaGruppe Munchen. Vercoe, B. 1984. "The Synthetic Performer in the Context of Live Performance." Proceedings of the 1984 Interna- tional Computer Music Conference. San Francisco, California: International Computer Music Association, pp. 199-200. Weinberg, G. 2005. "Interconnected Musical Networks: Toward a Theoretical Framework." Computer Music Journal 29(2):23-39. Weinberg, G., S. Driscoll, and M. Parry. 2005. "Musical Interactions with a Perceptual Robotic Percussionist." Proceedings of the 2005 of IEEE International Work- shop on Robot and Human Interactive Communica- tion. New York: Institute for Electrical and Electronics Engineers, pp. 456-461. Weinberg, G., S. Driscoll, and T. Thatcher. 2006. "Jam'aa for Haile." Proceedings of the 2006 International Con- ference and Exhibition on Computer Graphics and In- teractive Techniques (SIGGRPAH 2006). New York: Association for Computing Machinery, p. 124. Weinberg, G., T. Lackner, and J. Jay. 2000. "The Musical Fireflies: Learning About Mathematical Patterns in Music Through Expression and Play." Proceedings of the 2000 Colloquium on Musical Informatics. L'Aquila, Italy: Instituto Gramma, pp. 146-149. Winkler, T. 2001. Composing Interactive Music: Tech- niques and Ideas Using Max. Cambridge, Massachu- setts: MIT Press. Weinberg and Driscoll 45</page></plain_text>