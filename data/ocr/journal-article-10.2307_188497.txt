<plain_text><page sequence="1">OBJECTIVITY IN EXPERIMENTAL INQUIRY: BREAKING DATA-TECHNIQUE CIRCLES* SYLVIA CULPtt Department of Philosophy Western Michigan University I respond to H. M. Collins's claim (1985, 1990, 1993) that experimental inquiry cannot be objective because the only criterium experimentalists have for deter- mining whether a technique is "working" is the production of "correct" (i.e., the expected) data. Collins claims that the "experimenters' regress," the name he gives to this data-technique circle, cannot be broken using the resources of experiment alone. I argue that the data-technique circle, can be broken even though any interpretation of the raw data produced by techniques is theory-dependent. How- ever, it is possible to break this circle by eliminating dependence on even those theoretical presuppositions that are shared by an entire scientific community through the use of multiple independently theory-dependent techniques to pro- duce robust bodies of data. Moreover, I argue, that it is the production of robust bodies of data that convinces experimentalists of the objectivity of their data in- terpretations. 1. Introduction. Harry Collins (1985, 1990, 1993) has challenged the stan- dard view of experiments as providing an objective way for testing hy- potheses. The standard view is based on the belief that objectivity is pos- sible because experimenter bias and subjective expectations can be eliminated either by the use of particular experimental designs (controlled or blinded designs) or by the replication of experiments (replications by other experimenters with different personal or shared biases). Collins ar- gues, however, that experiments are rarely repeated by other experiment- ers and that, even in those cases where an experiment is repeated, ". . . it can never be clear whether a second experiment has been done sufficiently well to count as a check on the results of a first" (1985, 2). Experiments are not replicable, according to Collins, because there is a "circle, which . . [he] calls the 'experimenters' regress' [sic]" (ibid., 84), *Received January 1994; Revised September 1994 tSend reprint requests to the author, Department of Philosophy, Western Michigan Uni- versity, Kalamazoo, MI 49008 :I would like to thank Philip Kitcher, Gerald Doppelt, Arthur Falk, Quentin Smith, and an anonymous referee for their insightful criticism and helpful suggestions. Earlier versions of this paper were presented at the University of California, San Diego, University of Illinios, Urbana-Champaign, University of California, Irvine, Rice University and Western Michigan University. This work was supported by a New Faculty Research Support Program Grant and by funds from the Faculty Research and Creative Activities Support Fund, Western Michigan University. Philosophy of Science, 62 (1995) pp. 430-450 Copyright ? 1995 by the Philosophy of Science Association. 438</page><page sequence="2">OBJECTIVITY IN EXPERIMENTAL INQUIRY between experimental technique and the data produced by it. Experimen- talists cannot determine if a technique is "working" other than by the production of "correct", i.e., the expected, data. As an example, Collins describes Joseph Weber's experiments aimed at detecting gravitational ra- diation and argues that: we will have no idea whether we can do it [build a gravity wave de- tector] until we try to see if we obtain the correct outcome. But ... what the correct outcome is depends upon whether there are gravity waves hitting the Earth in detectable fluxes. To find this out we must build a good gravity wave detector and have a look. But we won't know if we have built a good detector until we have tried it and ob- tained the correct outcome! (ibid., 83-84) Moreover, according to Collins, experiments are not replicable because the experimenters' regress cannot be broken using the resources of exper- iment alone. In order to break the experimenters' regress, experimentalists must resort to the use of " 'nonscientific tactics' . . . because the resources of experiment alone are insufficient" (ibid., 143). Thus, Collins concludes that experiments cannot provide an objective way for testing hypotheses since experimental "replicability can and should act as a demarcation cri- terion for objective knowledge" (ibid., 19). Collins's position, therefore, like the positions of other social constructivists, is in the skeptical tradition about the force of experiment that extends back to Kuhn and Feyerabend. I will argue that Collins is right in identifying what he calls the 'exper- imenters' regress' and what I'll call a data-technique circle. Experimental techniques produce what scientists call "raw data"', such as splodges or digital readouts. These raw data must be interpreted before they can be- come what scientists call "data" or "facts". I also will argue, however, that data-technique circles can be broken and that the resources of exper- iment alone are sufficient. 2. Data Interpretations Are Theory-Dependent. In any interpretation of raw data, large chunks of theory must be taken for granted. Ask a scientist to justify a particular interpretation of the raw data produced by a tech- nique and she probably would respond by describing how the technique works: Because the technique works this way, I can justify the following interpretation. . ., Here I did a chemical reaction that works like this ..., So these splodges mean .... In describing how this technique works, how- ever, she must presuppose a substantial amount of theory. How exactly does this theory-dependence threaten the objectivity of raw data interpre- tations? A scientist will fail in making objective interpretations of her raw 'Philosophers such as Hanson (1958), Kuhn (1970) and Feyerabend (1975) argue that there is no such thing as raw data since data only counts as data within the context of a theory. 439</page><page sequence="3">SYLVIA CULP data to the extent that her interpretations are biased by dependence on idiosyncratic presuppositions. It is obvious that her interpretation will be biased if it depends on a false theoretical presupposition; but it is not so obvious that it could be biased even if it depends only on true theoretical presuppositions. In conducting an experiment, however, a scientist may not be aware of all of the theories involved in a raw data interpretation. For instance, she may believe that her raw data is the result of a certain chemical reaction when, in fact, it is the result of a side reaction. Thus, even if she uses a true theoretical presupposition about the chemical re- action to interpret her raw data, her interpretation could be biased by her false presupposition about which theories to employ in her interpretation. In their history of modern scientific objectivity, Daston and Galison (1992) argue that in the mid-19th century, scientists embraced the use of machines as a way of eliminating dependence on idiosyncratic presuppo- sitions. Scientists adopted mechanical reproduction as a way of "hemming in their own temptation to impose systems, aesthetic norms, hypotheses, language, even anthropomorphic elements on pictorial representation" (ibid., 103). Similarly, Ackermann (1985) argues that scientific instruments rescue objectivity in that "the relevant facts [produced by instruments] will not be the impressions of individual scientists of data ... but publicly negotiated provisional fixed points for scientific argument about the sig- nificance of theory" (ibid., 34). The use of machines or instruments, however, can eliminate dependence on only some of the idiosyncratic presuppositions that influence a raw data interpretation. An instrument may produce the same splodges over and over so that, for instance, everyone agrees that one splodge is higher than another, and yet there could be disagreement about how to interpret these splodges. Longino (1990) argues that objectivity can be achieved by intersubjec- tive criticism aimed at blocking the influence of individual subjective pref- erences. Because objectivity depends on acts of intersubjective criticism, it depends on the social character of scientific inquiry. Scientific communities rather than individuals are objective, and scientific communities can be objective only to the extent that they permit criticism. According to Longino, community standards, including substantive principles, episte- mic values, and social values, are necessary conditions for attaining critical discourse and can be used for assessing the objectivity of scientific com- munities. However, if intersubjective agreement depends only on community stan- dards, even when dependence on idiosyncratic presuppositions is elimi- nated, dependence on presuppositions shared by the entire community might not be. Any raw data interpretation dependent on a false theoretical presupposition, even one shared by an entire community, will be biased. 440</page><page sequence="4">OBJECTIVITY IN EXPERIMENTAL INQUIRY Social constructivists explain consensus within scientific communities by appealing to the existence of shared theoretical presuppositions, and they deny the possibility of objectivity by arguing that dependence on these shared theoretical presuppositions, even if they are false, cannot be elim- inated. Thus, in order to refute the social constructivist denial of objec- tivity, a way must be found in the breaking of data-technique circles to eliminate dependence on shared theoretical presuppositions as well as de- pendence on idiosyncratic presuppositions. I will argue that it is possible to break data-technique circles by elimi- nating dependence on at least some and possibly all shared theoretical presuppositions. This dependence can be eliminated by using a number of techniques, each of which is theory-dependent in a different way, to pro- duce a robust body of data. If a scientist fails to make an objective raw data interpretation to the extent that her biases enter into her interpreta- tion, then, depending on the extent to which dependence on idiosyncratic or shared biases has been eliminated, some raw data interpretations will be more objective than others. Thus, objectivity comes in degrees; so that, one interpretation, X, of some raw data is more objective than another interpretation, Y, of the same raw data if the data produced by the X interpretation are comparable with a more robust body of data than the data produced by the Y interpretation. My argument builds upon Wimsatt's (1974, 1981, 1991) use of the con- cept of robustness for a kind of multiple determination that can be used "to 'triangulate' on the existence and character of a common phenome- non, object, or result" (1981, 125). Hacking (1983), Franklin (1986, 1990) and Jardine (1986, 1991) have also argued that experimentalists use, and are warranted in using, multiple techniques to determine the reliability of observations made with scientific instruments. I will begin by considering a real instance of experimentalists dealing with a data-technique circle; for, I will argue, not only is it possible to eliminate dependence on shared theoretical presuppositions by producing a robust body of data (a body of data produced by a number of different techniques), but it is the production of robust bodies of data that convinces scientists of the objectivity of raw data interpretations. 3. DNA Sequencing.2 Before the chemical sequencing technique was de- veloped during the mid-1970s in Walter Gilbert's lab at Harvard several other techniques were being used to obtain information about the se- quence of nucleotides on segments of DNA. None of these techniques, however, produced raw data that could be interpreted unambiguously. In Gilbert's lab, a sequence for a 24 basepair (bp) long segment of DNA 2This section is based on two interviews (May 10,1989 and March 6, 1991) with J. Gregor Sutcliffe, on his doctoral thesis (Sutcliffe 1978a) and on published articles cited in these notes. 441</page><page sequence="5">SYLVIA CULP from the lac operator of E. coli had been arrived at by laborious proce- dures that left gaps at both ends and did not account for all of the raw data (Gilbert and Maxam 1973). In the early 1970s, an alternative technique called the plus and minus technique had been developed in Fred Sanger's lab at the MRC in Cam- bridge. Sanger's lab had used this technique to sequence the 5375 base genome of the bacteriophage 0X174. This technique, however, also pro- duced ambiguous raw sequence data. The published sequence of 0X174 relied heavily on data obtained by RNA and protein sequencing tech- niques as well as on data obtained by other DNA sequencing techniques. The authors of the paper reporting on this sequence estimated that there would be about 30 errors in their sequence due to insufficiencies in the plus and minus technique and lack of confirming data produced by other techniques. The authors noted that "as with other methods of sequencing nucleic acids, the plus and minus technique used by itself cannot be re- garded as a completely reliable system and occasionally errors may oc- cur. .. we are not certain that there is any scientific justification for estab- lishing every detail. . . " (Sanger et. al. 1977, 690). The new DNA sequencing technique, called the chemical technique, that was developed by Maxam and Gilbert in the mid-1970s is now in wide- spread use. It is commonly performed by labeling both strands of a double-stranded DNA segment on one of their ends (usually their 5' ends) and then separating the strands so that each can be sequenced individually. The solution containing the strand of DNA that is to be sequenced is divided into four aliquots so that each aliquot can be treated with different chemical reagents that selectively modify one or two bases. For instance, one aliquot is treated with dimethylsulfate so that the DNA strands will contain modified guanine bases. The treatment is limited so that a single molecule of DNA will have only one of its guanine bases modified. In the same way, the second aliquot is treated with acid so that both guanine and adenine bases are modified. The third aliquot is treated with hydrazine so that both thymine and cytosine bases are modified and the fourth ali- quot is treated with hydrazine and salt so that only cytosine bases are modified. After these modification reactions are performed, each of the aliquots is treated with piperidine at 90?C so that the DNA strands are broken at the site of a base modification. After these reactions are completed each aliquot contains a nested set of DNA fragments, all of which share a common end-the end with the radioactive label. Finally the aliquots are loaded onto a polyacrylamide gel and submitted to gel electrophoresis. Polyacrylamide gels can resolve single strands of DNA that differ in length by a single nucleotide. After the electrophoresis is complete, X-ray film is exposed by laying it on the gel. The pattern of radioactive bands seen on 442</page><page sequence="6">OBJECTIVITY IN EXPERIMENTAL INQUIRY the X-ray film is the raw data that is interpreted by "reading" the sequence ladder (see Figure 1). In their first report on this new sequencing technique, Maxam's and Gilbert's only comment on the reliability of the new technique was that the sequence for each strand of the double-stranded DNA fragment was "consistent with and confirms that of the other" (Maxam and Gilbert 1977, 561). What they meant was that their sequence was self-confirming because, when the sequence for each strand was lined up in the opposite orientation with the sequence for the other strand, the adenines could be paired with the thymines and the guanines could be paired with the cy- tosines. For their sequence to be self-confirming, however, a particular structure of DNA had to be presupposed. The structure that they presupposed-that DNA has two strands of nucleotides running in opposite orientations with the nucleotides on each strand being base-paired with the nucleotides on the other strand such that G's are base-paired with C's and A's are base-paired with T's-had first been proposed by Watson and Crick and then tested by numerous experiments. Moreover, because this structure was consistent with and could be used to explain a considerable body of experimental data from classical genetics, this structure had the "over-whelming biological merits" of a self-complementary DNA molecule that, according to Watson, had been immediately recognized by himself, Crick, their colleagues and com- petitors (Watson 1968). Unfortunately, even if this structure is assumed to be correct, the se- quence from each strand cannot corroborate the sequence of the other. For instance, if a "mistake" is made in data interpretation so that bases that are really A's are reported as G's, then the sequence reported for one strand would not be consistent with the sequence reported for the other. But, if at the same time this "mistake" is made in data interpretation along with the further "mistake" of reporting bases that are really C's as T's, then, even though the sequence of one strand would be consistent with the sequence of the other, the reported sequence would claim that there are G's where there are really A's and T's where there are really C's. For this reason at least one member of Gilbert's lab doubted the reli- ability of the chemical sequencing technique. J. Gregor Sutcliffe, a grad- uate student, had many heated arguments with Gilbert about the new technique. Finally, Gilbert suggested that Sutcliffe test the chemical se- quencing technique for himself. The test they devised was for Sutcliffe to sequence a segment of DNA that encoded a protein with an already known amino acid sequence. In February 1977, when they began considering this project, there were few proteins for which amino acid sequence data had been or could be obtained and for which DNA containing the structural gene could be 443</page><page sequence="7">SYLVIA CULP SANGER A C G T *0 ?I'*OW 440 woooWY Wm* -A~ -100"14"W la69 MAXAM-GI LBERT G A+ G T +C c e-r G &lt;-* 't _ * t ,* +""&lt;-o G &lt;^ ^^_. A **i T C - -,. AG ,* *::, , '*' C i,t i ^?, I * ? &lt; A C ? A ** + &gt;+ G J* F A C G * * .-*-* C T G- '.' ':' A C A A -' .. &lt;-G-" ' " ' ? ' G T T T C T T .......0.._f ...'. . .h Figure 1. The Same Segment of DNA Sequenced by both the Sanger (Chain-termination Technique) and the Maxam-Gilbert (Chemical Technique). These photographs of two X-Ray films were provided by J. Gregor Sutcliffe. They show sequence ladders for the same segment of DNA, the interpretation of the raw data is written between the two panels with the bands corresponding to guanine bases indicated by arrows. A = adenine; C = cytosine; G = guanine; T = thymine. 444 .. - 1 ...- - 1. -1. I--.. : 3iB i V gl iliil :i i.` lli il?t? 9?i</page><page sequence="8">OBJECTIVITY IN EXPERIMENTAL INQUIRY isolated in large enough amounts for performing the chemical sequencing technique. However, a friend of Gilbert's at Harvard, Jeremy Knowles, was studying one such group of proteins, the penicillinases (enzymes that inactivate penicillin and cephalosporin antibiotics). Knowles had wanted an amino acid sequence of a penicillinase for his work on chemical catal- ysis, so he had been in contact with Richard Ambler, who was working in Edinburgh on the amino acid sequence of a penicillinase whose gene is contained on the R6K plasmid found in E. coli bacteria. Gilbert's lab had obtained the pBR322 plasmid from Herbert Boyer. Boyer's group had constructed this plasmid by combining segments of DNA from various other plasmids. One of these segments of DNA con- tained a penicillinase gene, called amp-r, that was related to the penicillin- ase gene contained on the R6K plasmid. The function of the amp-r gene on pBR322 could be checked by transforming E. coli cells with pBR322 plasmid DNA and growing the cells on nutrient agar plates containing ampicillin. Only the bacteria cells containing the amp-r gene could grow on plates containing ampicillin because the penicillinase encoded by the amp-r gene inactivated the ampicillin. Gilbert and Knowles suggested to Sutcliffe that, as a dissertation pro- ject, he could learn the chemical sequencing technique and then use it to sequence the amp-r penicillinase gene. Knowles obtained Ambler's partial amino acid sequence data for the penicillinase protein. The protein se- quencing techniques used by Ambler were based on procedures first de- veloped by Sanger 30 years before for his sequencing of the insulin protein. By the mid-1970's, although the procedures for sequencing a protein were considered routine, sequencing a protein the size of penicillinase (263 amino acids) was still considered a Herculean task.3 Knowles kept Ambler's amino acid sequence data sealed in his office while Sutcliffe learned the chemical sequencing technique and used it to sequence the penicillinase gene. Sutcliffe began learning to sequence by the chemical technique in March 1977. He began with an estimate that the amp-r gene would be about 700bp long, by far the longest sequence attempted by anyone in Gilbert's lab. As Sutcliffe worked on sequencing the amp-r gene he found that the raw data produced by the chemical sequencing technique (a series of bands on an autoradiogram; for an ex- ample see Figure 1) were much less ambiguous than the raw data that had been produced by the previously available DNA sequencing techniques. For instance, when Sutcliffe asked several members of the lab to indepen- dently interpret the raw data, he found that the series of bands on the autoradiograms were so sharply defined and clearly separated that every- one assigned the same base (A, G, C or T) to the bands on the films. He 3The procedures Ambler used are described in a textbook by Light (1974) written for undergraduate chemistry majors. 445</page><page sequence="9">SYLVIA CULP also found that even though he did not sequence the two strands of DNA at the same time, the DNA sequences ended up being complementary. But more importantly, Sutcliffe had been preparing a restriction map of the pBR322 plasmid DNA at the same time he was sequencing. As he assembled his sequence data, Sutcliffe found that his sequence contained the restriction enzyme sites predicted by his restriction map. Restriction enzymes, enzymes that cut double-stranded DNA at specific sequences of four to eight nucleotides called restriction sites, had been isolated for the first time in 1970. By 1977, a number of different restriction enzymes had been isolated so that restriction maps could be made by digesting a seg- ment of DNA with combinations of restriction enzymes having different sequence specificities. These restriction maps, which showed the location of restriction sites on a segment of DNA, reflected the arrangement of specific nucleotide sequences on that segment. Restriction maps, however, can only indicate that a restriction site is within a range of 10 to 20 nu- cleotides on a particular DNA fragment. Because these maps cannot de- termine the location of a restriction site to the nearest nucleotide, they cannot be used to predict the exact location of a restriction site. Sutcliffe, therefore, could not use them to predict the exact location of a single restriction site in his DNA sequences but he did use them to predict the order of several different restriction sites within the segment of DNA he was sequencing. Now, if the technique used for determining the sequence specificities of restriction enzymes had been the chemical sequencing technique, then find- ing restriction sites would not have been a test of the chemical sequencing technique. Long before the chemical sequencing technique was developed, however, a battery of other techniques had been used to determine the DNA sequence of the restriction sites for the restriction enzymes Sutcliffe was using to make his restriction maps (Old, Murray and Roizes 1975). Sutcliffe pushed on with the sequencing of the amp-r gene and the pBR322 plasmid until September 1977. In September he finally presented his DNA sequence of the amp-r gene to Gilbert. Gilbert read some of the raw data from the films and concurred with Sutcliffe's interpretation. On a Sunday afternoon in late September, Sutcliffe and Gilbert drove over to Knowles's house for tea. For the first time, the DNA sequence for the amp-r gene and the amino acid sequence of its penicillinase protein prod- uct were compared. Ambler had sent Knowles a partial amino acid se- quence that contained unordered, non-overlapping peptides. They began by reading the amino acid sequence and checking the DNA sequence against it. But soon they were using the DNA sequence to order the pep- tides and to correct a few errors in the amino acid sequence. Only one difference between the two sequences remained when they had finished. They concluded that their interpretation of the raw data produced by the 446</page><page sequence="10">OBJECTIVITY IN EXPERIMENTAL INQUIRY chemical sequencing technique was a reliable report on the sequence of nucleotides in segments of DNA (Sutcliffe 1978b; Ambler and Scott 1978). At the same time that Sutcliffe was intentionally testing the chemical technique for DNA sequencing, scientists in other laboratories were trying to learn this new technique. Their aim was to use this technique for solving problems that had arisen in the course of their research programs. As a result, they did not set out to intentionally subject this technique to critical tests. Instead, they simply wanted to get the technique to work in their laboratory. The criteria they used for counting the technique as working were production of unambiguous raw data, i.e., sequence ladders with discrete, unsmeared bands, and production of DNA sequence data that matched the DNA sequence of pBR322 worked out by Sutcliffe. In some cases, however, there was de facto testing of DNA sequencing techniques. In December 1977 Fred Sanger published yet another new technique for sequencing DNA. This technique, called the chain-termination technique, creates a sequence ladder by using DNA polymerase in an enzymatic re- action to synthesize a new strand of DNA from a single-stranded DNA template. An inhibitor that terminates the DNA synthesis reaction at a specific nucleotide is added to each reaction. For example, by adding the terminator ddCTP in limiting amounts to a DNA synthesis reaction, a nested set of single stranded DNA molecules can be created that all begin at the same place and end in cytosine. Four synthesis reactions are run, each ending in either G, A, T, or C; a radiolabeled tracer is added to each reaction so that the newly synthesized DNA strands can be visualized. The newly synthesized DNA strands are separated into a sequence ladder by running the reaction products on a polyacrylamide gel. The sequence lad- ders are visualized by placing this gel on X-ray film (Sanger, Nicklen and Coulson 1977). The chemical and the chain-termination techniques produce raw data, i.e., splodges on X-ray films, that can be compared directly (see the two sequence ladders in Figure 1). Moreover, these two techniques draw upon different theoretical presuppositions for the interpretation of the raw data sequence ladders. The chemical technique relies on specific chemical mod- ification reactions to generate four different sets of nested DNA molecules. The chain-termination technique relies on the addition of specific termi- nator nucleotides to DNA synthesis reactions to generate the four different sets of nested DNA molecules. There is no well-known report in the scientific literature about a critical comparison of the data produced by these two techniques. However, in many laboratories both techniques are used or have been used, sometimes to sequence the same segment of DNA (see Figure 1). No news probably means good news; for a report would have been warranted only if two different nucleotide sequences had been produced when both the chemical 447</page><page sequence="11">SYLVIA CULP and chain-termination techniques were used to sequence the same segment of DNA. There is still a possibility that both techniques are producing nucleotide sequences that by coincidence just happen to be wrong in the same way. But is this the improbable coincidence that should concern us? After all, it would seem to be an improbable coincidence that these two techniques can end up producing comparable raw data (see the two sequence ladders in Figure 1) and that even though different theoretical presuppositions are drawn upon for the interpretation of the raw data produced by each tech- nique, both can end up producing the same data (see the interpretation of the sequence ladders in Figure 1). It would also seem to be an improbable coincidence that the chemical sequencing technique can produce data that correspond with data produced by restriction enzyme mapping techniques and that corresponding data can be produced when two different biolog- ical macromolecules (DNA and protein) are sequenced by techniques that rely on different chemical reactions and different theoretical presupposi- tions for interpreting raw data. In the next two sections, I will argue that from these improbable coincidences we can infer that objective interpre- tations of raw data are possible and I will develop three criteria for ob- jectivity that are based on data robustness. 4. A Conjunctive Common Cause Argument. When comparable data can be produced by a number of techniques and the raw data interpretations for these techniques do not draw on the same theoretical presuppositions, this remarkable agreement in the data (interpreted raw data) would seem to be an improbable coincidence unless the raw data interpretations have been constrained by something other than shared theoretical presupposi- tions. Let us consider the statistical relations among the comparable data sets. What is the probability that the two sets could have been obtained by chance? The two sets of comparable data, i.e., two sets of raw data whose interpretations produce comparable data, could be in a direct causal rel- evance relation if there were a causal process connecting them, and if that causal process was responsible for the transmission of causal influence from one to the other, for instance, if one of the sets of data simply had been copied from the other. But, if one of the sets of comparable data has not been copied from the other and if this apparent coincidence is too improbable to be attributed to chance, then they are likely to be in an indirect causal relevance relation with their production connected to a common cause by causal processes. Making the inference that two sets of data (produced by two different techniques that do not share the theoretical presuppositions used in the raw data interpretation) about the property, P, of object x have been con- 448</page><page sequence="12">OBJECTIVITY IN EXPERIMENTAL INQUIRY strained by x having P draws upon the principle of the common cause.4 This principle appeals to a statistical structure called a conjunctive fork where the causal processes are physically independent of each other, i.e., they do not physically intersect, but they come from common background conditions. Salmon (1984) has cited Perrin's argument for the reality of atoms and molecules as an example of a conjunctive common cause ar- gument.5 In Les Atomes (1913), Perrin stressed the fact that Avogadro's number (the number of molecules in a mole of a pure chemical compound) had been ascertained thirteen distinct ways. "It seems clear that Perrin would not have been satisfied with the determination of Avogadro's num- ber by any single method, no matter how carefully applied and no matter how precise the results. It is the 'remarkable agreement' among the results of many diverse methods that supports his conclusion about the reality of molecules" (Salmon 1984, 219). For if there were no common cause (atoms, ions and molecules), "these different experiments designed to as- certain Avogadro's number would be genuinely independent experiments, and the striking numerical agreement in their results would constitute an utterly astonishing coincidence" (ibid., 220). To characterize the structure of common causes, Reichenbach (1956) introduced the notion of a conjunctive fork that is defined in terms of the following four conditions: (1) P(A.B\C) = P(A\C) x P(B\C) (2) P(A.B\C) = P(A\C) x P(B\C) (3) P(A\C) &gt; P(A\C) (4) P(B\C) &gt; P(B\C) These four conditions entail:6 (5) P(A.B) &gt; P(A) x P(B) These formulas can be applied to the situation of two techniques pro- ducing comparable data. A is the event of getting the data by technique I, B is the event of getting the data by technique II, C is the state of the entities being measured were they to have the particular causal properties that explain the lack of independence between A and B, and C is the state of the entities being measured were they not to have these causal prop- erties. Formulas (1) and (2) would hold if, given a state of entities being meas- ured, C or C, A and B occur independently, e.g., the event of getting data 4Salmon 1984 and Reichenbach 1956 'Salmon appeals to conjunctive fork causal processes for arguing in support of the reality of theoretical entities. My concern is not with the reality of these kinds of entities, but with the reliability of reports about their properties. 6See Reichenbach (1956) for the proof. Salmon also works out this proof in a footnote on p.160. 449</page><page sequence="13">SYLVIA CULP by technique I is physically separate and distinct from the event of getting data by technique II. There is a problem, of course, with assigning values to probabilities in (2), but this problem can be gotten around by counter- factually assigning values depending on an alternative "catchall" hypoth- esis that the state of the entities is such that they do not have the causal properties that explain the lack of independence between A and B. The claim that inequalities (3) and (4) would hold depends on whether the chances of obtaining the A results or the B results when the entities do have the causal properties that explain the lack of independence between A and B is greater than the chances of obtaining them fortuitously when the entities do not have these causal properties. For any one technique, it is conceivable that the same results will be obtained whether C or C holds, P(A\C) = P(A\C), but it is unlikely that this will be the case for every technique used. With this understanding of conjunctive common cause arguments, we can begin to spell out the conditions for a robustness criterion. To use a robustness criterion, a set of techniques must be developed such that it would be an improbable coincidence for all of them to produce compa- rable data. In other words, it must be possible to develop a set of tech- niques that meets the following two conditions: (1) if there is a common cause, i.e., the sets of data (produced by the different techniques) about the property, P, of object x, have been constrained by x having P, then all of the techniques in the set must produce comparable data, and (2) if there is no common cause, then at least one technique in this set must produce data that are not comparable with the data produced by the other tech- niques. 5. Theory-Dependence. For a set of techniques to meet the conditions out- lined above, the techniques must not all use the same theoretical presup- positions in making raw data interpretations. The number of theoretical presuppositions that are not shared by members of this set of techniques will determine just how improbable the coincidence is for all of them to produce comparable data. A technique is a series of manipulations aimed at getting raw data. As we saw above, interpretations of raw data depend on theories about the processes being used to produce the raw data. For the interpretations of raw data produced by two different techniques to be independently theory- dependent, therefore, the two sets of theories used in the interpretations of raw data must not be the same. Even if most of the theories contained in the two sets are different, however, both sets will always share one theory, the theory-of-the-object, x, whose property, P, is being "described" or "measured" by the two techniques. Clearly, it would not be such an improbable coincidence for two techniques to produce comparable data 450</page><page sequence="14">OBJECTIVITY IN EXPERIMENTAL INQUIRY if their raw data interpretations depended on the same theory-of-the- object. At this point it is helpful to adopt a distinction that Kosso (1989) draws between sub-theories of the theory-of-the-object (Tx). He identifies Txl as the sub-theory containing those propositions belonging to Tx that can be confirmed by the observation that x has P and Tx2 as the sub-theory con- taining those propositions belonging to Tx that are not independent7 of the collection of theories {Ti} used in supporting the claim that x has P. For any two techniques reporting on the same property, P, of an object, the two sets of theories used to interpret their raw data will share those propositions belonging to Tx that can be confirmed by the observation that x has P, i.e., the Txl sub-theory identified by Kosso. On the other hand, a part of the theory-of-x, Tx2, may differ for each technique de- pending on how the production of raw data draws upon Tx. Thus, if Tx2 differs for the two raw data interpretations, then each do not depend on Tx in the same way, i.e., different parts of Tx are being used in each of the raw data interpretations. Moreover, the relation between Txl and Tx2 and the relation between Tx2 and {Ti} will determine how dependent a raw data interpretation is on Tx.8 In other words, these two relations determine whether one raw data interpretation is more dependent on Tx than another. When Tx2 and Txl are disjoint, a raw data interpretation will be more independent if Tx2 is a proper subset of {Ti} than if Tx2 = {Ti} . A raw data interpretation will be more dependent on Tx when Tx2 and Txl are not disjoint than when they are. Here again, the relation between Tx2 and {Ti} marks gra- dations on this scale. A raw data interpretation is most dependent on Tx when Txl = Tx2 and {Tx2} = {Ti} so that support for the raw data interpretation that x has P is based completely on the entire theory about x having P. To illustrate, let us look back at the two techniques for sequencing DNA (see Table 1 for a summary of the following discussion). Both the chemical technique and the chain-termination technique can confirm the part of the theory-of-DNA, Txl, that includes subtheories about the properties of: * having nucleotides with 4 different bases, guanine, adenine, thymine or cytosine * having these nucleotides in a linear order * having a complementary sequence of the bases on the two inter- twined chains 7Kosso considers two theories to be independent "if the truth or falsity of one is isolated from the truth or falsity of the other" (ibid., 44). 8Here I am drawing upon what Kosso (1989) calls his scale of degree of independence of an account. 451</page><page sequence="15">SYLVIA CULP TABLE 1. THEORY-DEPENDENCE OF THE CHEMICAL AND CHAIN-TERMINATION TECHNIQUES FOR DNA SEQUENCING Chemical Technique Chain-termination Technique (Maxam-Gilbert) (Sanger) Txl: Subtheory of the theory-of-DNA that can be confirmed by the data produced by these techniques DNA has nucleotides with 4 different bases, guanine, adenine, thymine or cytosine. DNA has bases that are arranged in a linear order. DNA has a complementary sequence of bases on two intertwined chains. DNA has bases on the complementary strands such that adenine usually pairs with thymine and guanine with cytosine. DNA has bases that are arranged in a highly irregular but not random order. DNA has bases that sometimes are arranged in an order that has "coding" significance. {Ti}: Theories about: * polynucleotide kinase reaction * chemical reactions for modifying bases * chemical reactions for DNA strand scis- sion * gel electrophoresis * autoradiography Tx2: Subtheory of the theory-of-DNA that not independent of {Ti} DNA has nucleotides with 4 different ba- ses: guanine, adenine, thymine or cytosine. DNA has bases that are arranged in a lin- ear order. {Ti}: Theories about: * enzymatic test tube reactions using pol I to make cDNA from ssDNA templates * gel electrophoresis * autoradiography Tx2: subtheory of the theory-of-DNA that not independent of {Ti} DNA has nucleotides with 4 different ba- ses, guanine, adenine, thymine or cytosine. DNA has bases that are arranged in a lin- ear order. DNA has a complementary sequence of bases on two intertwined chains. * having a complementary sequence of the bases where adenine usu- ally pairs with thymine and guanine with cytosine. * having the bases arranged in a highly irregular order that is not randomly generated and for some segments of DNA has "coding" significance. The {Ti} for the chemical technique contains theories about the polynu- cleotide kinase reaction for labeling the 5' end of DNA segments, theories about the chemical reactions for modifying purine and pyrimidine bases and for DNA strand scission at the modified base, and theories about gel electrophoresis and autoradiography. On the other hand, the {Ti} for the chain-termination technique contains theories about enzymatic test tube reactions using the polymerase I enzyme from E. coli to make a comple- mentary strand of DNA from a single-stranded template as well as theories about gel electrophoresis and autoradiography. For the chemical sequencing technique, the theory-of-DNA dictates 452</page><page sequence="16">OBJECTIVITY IN EXPERIMENTAL INQUIRY which chemical modification reactions are performed. The four reactions presuppose that the only bases in DNA are adenine, guanine, thymine and cytosine. If there are other bases-modified versions of adenine, guanine, thymine or cytosine-they may or may not be detected depending on how they react with the chemical reagents used for the base modifications. In this case, therefore, the interpretation of the splodges on the autoradi- ogram depends on part of the theory-of-DNA. Tx2 contains the part of the theory-of DNA that includes subtheories about the properties of having nucleotides with 4 different bases-guanine, adenine, thymine or cytosine-and having these nucleotides in a linear order. How dependent on the theory-of-DNA is the chemical sequencing tech- nique? Part, but not all, of Txl intersects with Tx2. Theoretical claims about the sequence of a segment of DNA-such as the property of having an order of the bases on the two intertwined chains that is complementary with adenine usually pairing with thymine and guanine with cytosine, the property of having an order of bases that is highly irregular and yet not random, and the property of sometimes having an order of bases that has "coding" significance-are independent of the theories used to justify the interpretation of the splodges. Moreover, although the theories in Tx2 act in support of the interpretation, they do so only in cooperation with other theories, i.e., Tx2 is in {Ti} but not {Tx2} = {Ti}. The chain termination technique is more dependent on the theory-of- DNA than the chemical technique. Interpreting the splodges on the au- toradiograms produced by the chain-termination technique presupposes theories about DNA replication that are dependent on the theory-of- DNA, in particular, on the portion of the theory about the sequence of the bases on the two intertwined chains being complementary. For the chain termination technique, more of Txl intersects with Tx2 than for the chemical technique. However, theoretical claims about the sequence of a segment of DNA-such as the property of having a complementary base pairing on the two chains where adenine pairs with thymine and guanine with cytosine and the property of having an order of the purine and py- rimidine bases along a chain that is highly irregular, yet not random, and the property of sometimes having an order that has "coding" signifi- cance-are still independent of the theories used to justify the interpreta- tion of the splodges. How independently theory-dependent are the interpretations of the splodges on the autoradiograms produced by these two techniques? The steps they use to manipulate DNA draw upon some different theories. The theories which support the chemical reactions used for end-labeling DNA segments, modifying bases, and breaking DNA strands differ from the theories which support the enzymatic reactions used in synthesizing a strand of DNA with the polymerase I enzyme from a single stranded tem- 453</page><page sequence="17">SYLVIA CULP plate. Both of these techniques do draw on theories about chemical re- actions, but the modifying reactions and the synthesizing reactions draw on different portions of chemical theory. On the other hand, they do share theories about the mobility of DNA in acrylamide gels and the detection of radiolabels by autoradiography. Both use the theory-of-DNA in pre- supposing that the bases are adenine, guanine, thymidine and cytosine. However, when the chemical technique draws upon this presupposition, it is to justify the use of particular modifying chemical reactions; while when the chain-termination technique draws upon this presupposition, it is to justify adding particular trinucleotides to an enzymatic reaction for DNA synthesis. Clearly, in order to satisfy a robustness criterion, other techniques, which draw upon different theories than the ones used by the chemical and chain-termination techniques have to be added to the set containing these two techniques. 6. Objectivity and Robustness. In the last two sections I argued for a ro- bustness criterion that is at least implicitly employed by scientists seeking objective raw data interpretations. This criterion is one that can only be satisfied over time and by degrees. Three factors play a role in determining the degree of robustness for a body of data: 1) the size of the set of tech- niques producing comparable data; 2) for the set of techniques, the degree to which theories (including Tx2 of the theory-of-the-object) used in one raw data interpretation differ from the theories used in the other raw data interpretations; and 3) for each technique in the set, the degree to which its raw data interpretation is dependent on the theory-of-the-object, e.g., Tx2 and Txl being disjoint rather than Txl = Tx2 and {Tx2} = {Ti}. The degree to which the robustness criterion can be fulfilled at any time depends on the techniques that have been or can be developed. Unfortu- nately, an experimentalist may wish to develop several techniques for re- porting on a particular property and simply not be able to develop more than one technique that is overly dependent on the theory-of-the-object. The objectivity of any interpretation of the raw data produced by this technique will be significantly limited until another technique can be de- veloped-one that uses a different set of theories for raw data interpre- tation or is less dependent on the theory-of-the-object. Two possible criticisms of a robustness criterion are either that it is too conservative or that it is not conservative enough. Let us say that a newly developed technique produces data that is not comparable with the data produced by several well entrenched techniques. The data produced by the set of older techniques would always be more robust than the data produced by a single new technique. The robustness criterion would be too conservative if the data produced by the new technique were dismissed because they are not compatable with the more robust data produced by 454</page><page sequence="18">OBJECTIVITY IN EXPERIMENTAL INQUIRY the set of well entrenched techniques.9 On the other hand, the robustness criterion would not be conservative enough if all of the data produced by the set of older techniques along with the data produced by the newly developed technique were simply discarded because a technique has been found that does not produce comparable data. This problem can be resolved, however, by evaluating the raw data interpretations for all of the techniques. First, the techniques can be eval- uated to determine whether the raw data interpretation for the newly de- veloped technique is more or less dependent on the theory-of-the-object than the interpretations for the other techniques in the set. And second, the theories that play a role in each of the raw data interpretations can be examined to determine whether a theory that was shared by all the other techniques is not shared with the new technique. If the raw data interpre- tation for the new technique is less dependent on the theory-of-the-object than the interpretations for the other techniques or if it does not use a theory that all the other interpretations use, then there would be good reasons for throwing out the data produced by a set of older techniques in favor of the data produced by a new technique. Thus, the robustness criterion need not be too conservative. Even so, until these new data can be produced by yet another technique, we would have to be concerned with the objectivity of any claims about that property. On the other hand, even if the raw data interpretation for the new tech- nique does not use the same theories that all the other interpretations use, if it were the case that the raw data interpretation for the new technique is more dependent on the theory-of-the-object than the interpretations for the other techniques (e.g., more of Txl intersects with Tx2, or {Tx2} = {Ti} rather than {Tx2} being a proper subset of {Ti}), then there would be good reasons for keeping the data produced by the set of techniques and discarding the data produced by the new technique. Thus, the ro- bustness criterion can be conservative enough. Moreover, in some cases, there will be another resource for resolving this problem. Sometimes we may be able to show that, although a new technique produces data for reporting on one property of an object that are not comparable with the data produced by other techniques reporting on this property, the new technique also produces data reporting on other properties of the object that are comparable with the data produced by the other techniques. For instance, if yet another technique for DNA se- quencing were developed, it might happen that when its raw data is inter- preted the data will indicate that some human DNA contains nucleotides 9Moreover, if the robustness of the data is preserved by jettisoning the data produced by the newly developed technique, then one could argue that sets of techniques producing robust data have been constructed by simply not including any technique that happens to produce discordant data. 455</page><page sequence="19">SYLVIA CULP with modified guanine bases. Now, this technique could be used to se- quence the same segment of DNA that had been sequenced by the chem- ical and chain-termination techniques. If the data on the adenine, thymine, cytosine and some guanine bases was comparable for these three tech- niques, then there would be good reasons for throwing out the data on guanine bases produced by the older techniques in favor of the new data. As discussed above, we also would have good reasons for throwing out the old data, if we could show that neither the chemical technique nor the chain termination technique identifies the modified guanine bases because their raw data interpretations use theories which assume that guanine ba- ses are not modified. Another possible criticism of a robustness criterion recently has been launched by Nicolas Rasmussen. In "Facts, Artifacts, and Mesosomes: Practicing Epistemology with the Electron Microscope" (1993), Rasmus- sen presents the bacterial mesosome as a test case for robustness. The bacterial mesosome is a bag-shaped membranous structure first observed by electron microscopists. Rasmussen claims that robustness as a criterion for the reality or artifactuality of experimental results fails this test, be- cause, after 15 years of interpreting electron micrographs of specimens prepared by a variety of techniques as supporting evidence for the exis- tence of mesosomes, electron microscopists ultimately concluded that mes- osomes were an artifact produced by these preparation techniques. He also claims that the "issue of the mesosome resembles that of high flux gravity waves as described by Harry Collins, especially in the obviousness of 'experimenter's regress' or logical circularity entailed in the evaluation of experiments according to their results. .. ." (ibid., 256). In "Defending Robustness: the Bacterial Mesosome as a Test Case" (1994), I have responded to Rasmussen by arguing that a more complete reading of the research literature on the mesosome shows that ultimately the more robust body of data did not support the mesosome. Mesosomes were not consistently observed when electron microscopists attempted to observe them by varying conditions with already established sample prep- aration techniques and by using a variety of newly developed techniques. Thus, electron microscopists used, and were warranted in using, robust- ness as a criterion for the artifactuality of mesosomes. For the purposes of this paper, therefore, I will respond only to Rasmussen's claim that: independent theory of methods and instruments is not in practice de- pended on by biological electron microscopists to assure reliability of observations, or to decide reliably between conflicting observations ... prior findings and independent methods are indeed brought into agreement with novel ones to a great extent, but not by means of simply calibrating the latter against the former; rather, a complex two- 456</page><page sequence="20">OBJECTIVITY IN EXPERIMENTAL INQUIRY way negotiation may result in changes to the understanding of evi- dence on both sides of the calibration. The validity of evidence concerning a new entity is often undecidable by each of several in- dependent methods, and by comparison of new data with precedent (ibid., 231-232). As I argued above, experimentalists who are using a robustness criterion and who obtain data using a new technique that are not comparable with the data produced by an older technique should not simply calibrate the latter against the former. Thus, Rasmussen's observation that "electron microscope observations are indeed reconciled as much as possible with light microscope observations, but this is an interdependent, two-way pro- cess: sometimes consilience is attained by attributing false but rationaliz- able appearances to the light microscope" (ibid., 235) is not inconsistent with the claim that electron microscopists were using a robustness criterion for evaluating their data. Moreover, as I have argued in this paper, they were warranted in using robustness as a criterion for evaluating their in- terpretations of electron micrographs, because it is by the production of robust bodies of data that the "experimenters' regress" can be broken. REFERENCES Ackermann, R. J. (1985), Data, Instruments, and Theory. Princeton: Princeton University Press. Ambler, R. P. and Scott, G. K. (1978), "Partial Amino Acid Sequence of Penicillinase Coded by Escherichia coli Plasmid R6K", Proceedings of the National Academy of Sci- ences, USA 75: 3732-3736. Collins, H. M. (1985), Changing Order. London: SAGE Publications. . (1990), Artificial Experts. Cambridge, MA: MIT Press. Collins, H. M. and Pinch, T. (1993), The Golem. Cambridge: Cambridge University Press. Costerton, J. W. (1979), "The Role of Electron Microscopy in the Elucidation of Bacterial Structure and Function", Annual Review of Microbiology 33: 459-480. Culp, S. C. (1994), "Defending Robustness: the Bacterial Mesosome as a Test Case", in D. Hull, M. Forbes and R.M. Burian (eds), PSA-1994, 1. East Lansing: The Philosophy of Science Association, pp. 46-57. Daston, L. and Galison, P. (1992), "The Image of Objectivity", Representations 40: 81-128. Feyerabend, P. (1975), Against Method. London: Verso. Franklin, A. (1986), The Neglect of Experiment. Cambridge: Cambridge University Press. . (1990), Experiment Right or Wrong. Cambridge: Cambridge University Press. Gilbert, W. and Maxam, A. (1973), "The Nucleotide Sequence of the lac Operator", Pro- ceedings of the National Academy of Sciences, USA 70: 3581-3584. Jardine, N. (1986), The Fortunes of Inquiry. Oxford: Clarendon Press. . (1991), The Scenes of Inquiry. Oxford: Clarendon Press. Hacking, I. (1983), Representing and Intervening. Cambridge: Cambridge University Press. Hanson, N. R. (1958), Patterns of Discovery. Cambridge: Cambridge University Press. Kosso, P. (1989), Observability and Observation in Physical Science. Dordrecht: Kluwer Ac- ademic Publishers. Kuhn, T. (1970), The Structure of Scientific Revolutions. 2nd ed. Chicago: University of Chicago Press. Light, A. (1974), Proteins: Structure and Function. Englewood Cliffs: Prentice-Hall, Inc. Longino, H. (1990), Science as Social Knowledge. Princeton: Princeton University Press. 457</page><page sequence="21">458 SYLVIA CULP Maxam, A. and Gilbert, W. (1977), "A New Method for Sequencing DNA", Proceedings of the National Academy of Sciences, USA 74: 560-564. Old, R.; Murray, K.; and Roizes, G. (1975), "Recognition Sequence of Restriction Endo- nuclease III from Hemophilus influenzae", Journal of Molecular Biology 92: 331-339. Perrin, J. ([1913] 1923), Atoms. Translated by D.L. Hammick. Originally published as Les Atomes (Paris: Alcan). New York: Van Nostrand. Rasmussen, N. (1993), "Facts, Artifacts, and Mesosomes: Practicing Epistemology with the Electron Microscope", Studies in the. History of the Philosophy of Science. 24: 227-265. Reichenbach, H. (1956), The Direction of Time. Berkeley: University of California Press. Salmon, W. (1984), Scientific Explanation and the Causal Structure of the World. Princeton: Princeton University Press. Sanger, F.; Nicklen, S.; and Coulson, A. R. (1977), "DNA Sequencing with Chain- terminating Inhibitors", Proceedings of the National Academy of Sciences, USA 74: 5463-5467. Sanger, F; Air, G. M.; Barrell, B. G.; Brown, N. L.; Coulson, A. R.; Fiddes, J. C.; Hutchison III, C. A.; Slocombe, P. M.; and Smith, M. (1977), "Nucleotide Sequence of Bacteri- ophage 0X174 DNA", Nature 265: 687-695. Salton, M. R. J. and Owen, P. (1976), "Bacterial Membrane Structure", Annual Review of Microbiology 30: 451-482. Sutcliffe, J. G. (1978a), "Nucleotide Sequence of pBR322", Diss. Harvard University. . (1978b), "Nucleotide Sequence of the Ampicillin Resistance Gene of Escherichia coli Plasmid pBR322", Proceedings of the National Academy of Sciences, USA 75: 3737- 3741. Watson, J. D. (1968), The Double Helix. New York: Atheneum. Wimsatt, W.C. (1974), "Complexity and Organization", in K.F. Schaffner and R.S. Cohen (eds.), PSA-1972 (Boston Studies in the Philosophy of Science, vol. 20). Dordrecht: Reidel, pp. 67-86. . (1981), "Robustness, Reliability, and Overdetermination", in M. B. Brewer and B. E. Collins (eds.), Scientific Inquiry and the Social Sciences. San Francisco: Jossey- Bass Publishers, pp. 124-163. . (1991), "Taming the Dimensions Visualizations in Science", in M. Forbes, L. Wes- sels, and A. Fine (eds.), PSA-1990, 2. East Lansing: The Philosophy of Science Asso- ciation, pp. 111-135.</page></plain_text>