<plain_text><page sequence="1">2005-01-2689 New Behavioral Paradigms for Virtual Human Models Norman I. Badler, Jan Allbeck and Seung-Joo Lee University of Pennsylvania Richard J. Rabbitz, Timothy T. Broderick and Kevin M. Mulkern Lockheed Martin Copyright © 2005 SAE International ABSTRACT The earliest Digital Human Modeling systems were non- interactive analysis packages with crude graphics. Next generation systems added interactivity and articulated kinematic human models. The newest systems use real- time computer graphics, deformable figures, motion controllers, and user interfaces. Our long-term goal is to free the user as much as possible from interactive human model manipulation through direct understanding and execution of task instructions. We present a next generation DHM testbed that includes a scriptable interface, real-time collision-avoidance reach, empirical joint motion models, a versatile locomotion engine, motion capture and synthetic motion blends and combinations, and a smooth skinned scalable human model. INTRODUCTION Digital Human Modeling systems have been continuously improving since the earliest computer programs for evaluating reach, fit, and accommodation. As DHM software tools evolved, they became more interactive, more flexible, more graphically sophisticated, and more programmable. Differentiation across application domains and analysis needs further clarified DHM toolsets. User communities grew and industry and governments came to value the toolsets' utility. Proprietary corporate systems were often replaced with and supported by third party software products. In this environment, further DHM development has been shaped by technological factors. The third party vendors have considerable incentive to integrate human modeling into computer-based vehicle and factory design, analysis, and workflow, while human modeling feature extensions are often less well motivated, either because of small user communities or perhaps the specificity and difficulty in developing some of the requisite tools. In this environment, research labs play a significant role: efficient development of novel technologies is still pursued in research labs such as those at the University of Michigan, University of Iowa, and University of Pennsylvania. Economic factors also contribute to the direction of DHM developments. The human factors engineer or designer is the ultimate user of a DHM tool, and the designer's time, cost, and work efficiency should be an important factor in considering the utility of an "ideal" DHM tool. Our view of the software landscape is that a number of high level goals would serve to advance DHM systems to the next level of applicability, usability and performance: • TASK SPECIFICATION: The DHM tool should be controllable through an interface that allows the efficient generation of required tasks. The designer should have a "virtual subject" available, and should be able to easily and compactly express tasks that it should perform. The task taxonomy of Ianni [1] is a good start for a task vocabulary, and a Parameterized Action Representation [2] has been designed to mediate between language instructions and action execution. It is essential that basic human actions (such as reach, look-at, walk, carry, put, position, and push) be easily expressed with respect to the agent and the workplace environment. • EXPRESSIVE POWER: The DHM tool should hide from the user details. of human behavior that one ought to expect of a real person. The primarily weakness in a simple DHM tool is the reliance on the operator to interactively and manually manipulate the human figure. While this might be useful in difficult access situations, it is unnecessary if the human model has powerful models of confined space reach and locomotive navigation. Ability to easily handle (grasp, carry, push, position, release) external objects is essential to maintenance and operational tasks. 723</page><page sequence="2">. MODULAR ARCHITECTURE: The analysis toolset should be easy to extend with new features or specific case analyses. A static toolset will frustrate change and adaptation to new requirements or situations. Programmability or a "plug-in" architecture that allows for easy extensibility (and encourages software development) is highly desirable. The designer need not be a master programmer, but the toolset should encourage extension, exploration and customization. • PROGRAM ACCESS: The DHM system should be controllable by an external task modeling and simulation system such as Brahms [3], IMPRINT [4, 5], or pmfSERV [6]. Not every task analysis needs to be done interactively. Complex workflows, team activities, extensive accommodation analyses, and psychophysical factors might be more readily simulated in existing task simulation tools. It should be possible to get animated graphical outputs from these tools showing the work performed in the 3D workplace models, using digital human models with appropriate clothing, size, and skill, and autonomously performing the required tasks either through the procedural actions (noted above) or motion captured or scripted actions stored in an action database. While no ideal DHM system meeting these goals currently exists, we have been engaged in pursuing them for several years. In the discussion that follows we describe the Human Model Testbed that we are building to push the field of DHM much closer to these goals. HUMAN MODEL TESTBED SYSTEM We have been building a next generation digital Human Model Testbed (HMT). Our system is based on the Jack Toolkit from UGS and consists of several smart motion generators with high-level interfaces suitable for creating a parameterized scripting interface. This scripting interface provides a mode for instructing digital humans that is close to the way people instruct each other. In the next few sections, we will describe the motion generators constructed for HMT, including motion capture, collision-free constrained space arm reach, and locomotion, followed by a description of our script interface and future work. MOTION CAPTURE The motion capture module for HMT consists of two parts; a real-time retargeting system and a motion capture playback and blending system. Figure 1: Real-time motion capture retargeting. http:llhms.upenn.edulLiveActorlvcp-demo-web.mov The real-time retargeting system was created for a Virtual Checkpoint (VCP) demonstration for an exhibit at l/ITSEC 2003. The demonstration included an avatar soldier running a military checkpoint, stopping cars, checking drivers' identification, and taking drivers into custody (Figure 1). Interaction with the virtual checkpoint was based on real-time recognition of six gestures. During the exhibition, two actors were motion captured in Ascension Technology's ReActor2 [7], an active optical motion capture system, and successfully controlled the checkpoint simulation. The real-time retargeting system takes 3D marker positions from the live subject and returns joint angles which are then used to animate an avatar. The system assumes a humanoid avatar skeleton and consistent motion capture marker positions, but makes no assumptions about the size of the segments for the avatar or the human motion capture subject. The system is initialized by using a standard T-pose to determine the initial position of the markers and map them to positions on the avatar. The ReActor2 system tags the marker data from the 3D positional markers with information to indicate relative marker location (e.g., marker 1 is on the left toe, marker 2 is on the left ankle, etc.). From these tags and the initial positions of the markers in the T-pose, segment lengths and joint centers can be roughly calculated for the motion capture subject. Corresponding information for the avatar can be directly obtained from the Jack Toolkit. From this data offset information can be stored. Using this offset data, the current position of the markers, and simple geometric transforms, joint angle data is generated in real-time (30 frames/second) as it is streamed from the ReActor2 to the HMT through a socket connection. While real-time motion capture has its place in some situations, other applications such as animations for simulations are better suited to playing back stored motion capture data in context (Figure 2). 724</page><page sequence="3">Figure 2: Motion capture playback. http:llhms.upenn.edulLMC0lbulkhead2b.avi In the HMT, motion capture files are stored in Biovision hierarchical data (BVH) format and are specific to the human models' skeletal structures and sizes. The BVH files are generated by applying motion capture data from the ReActor2 to a skeleton in Alias/Kaydara's MOCAP software. The BVH files are then parsed by the HMT and the stored joint angle data is applied to the human figures. In order to smoothly transition from one set of motion capture data to another, we have implemented a standard motion blending technique [8]. Blended actions can be saved as BVH files and blended with other captured actions to create long series of continuous actions. Playback requires only specifying the digital human, the action, and a start time. While motion capture data provides the most natural movements, it is difficult to apply in context-sensitive object interactions. While some work has been done to adapt motion capture data to context-sensitive object interactions [9], it is often necessary to use procedural animations when interacting with complex, object rich environments. COLLISION-FREE REACH Often inverse kinematics (IK) methods are used to control a virtual human figure reach to a given position in space. For high degree of freedom chains, there are no fast non-iterative IK methods. Workplace obstacles present additional constraints. Robotics configuration- space approaches offer complete but expensive exponential searches which often result in unnatural looking motions. Human figure animations often claim "naturalness" but rely on vague or intuitive definitions of the term. Arm reach using inverse kinematics has been well studied, but real-time techniques appear to lack either human movement qualities or environment sensitivity. We have created a real-time, collision-free IK solution for an articulated human figure. The use of graphics hardware accelerates collision detection. Naturalness and efficiency arise from constraining available solutions by simple strength (comfort) functions and empirical multi-joint dependencies extracted from motion captured data [10]. Movement paths are discovered through spatial search guided by a partition of the workplace. Our demonstration system shows the efficacy of this approach and its suitability for ergonomics analysis or VR avatar applications (Figure 3). Our collision-free procedural reach assumes that the digital human is within reach of a specified reach goal. If the goal is out of reach failure is returned. Given a starting human configuration (posture), an end-effector, and a goal, a real-time, collision-free reach will be animated provided a collision-free path is possible. If a collision-free path is not possible, failure will be returned. The interface to the reach module includes only the specification of the selected digital human, which hand, and a goal position. More details on our collision-free reach can be found in [11] and [12]. Figure 3: Naturai, collision-free reach. http:llhms.upenn.edulReachlGuidedReach2.avi LOCOMOTION Locomotion is one of the most common actions in virtual environments, yet it remains a difficult action to simulate well. The human eye is well conditioned to the movements of walking and therefore unnatural movements are quickly recognized. A great variety of locomotion research has been done over the past two decades [13], however, our focus for the HMT has been on the usability of the locomotion module and its application in different environments and scenarios. Our aim has been to create a locomotion module that would allow naïve users to instruct digital humans to walk in a complex environment while performing actions such as sitting, standing, pushing,, and carrying (Figure 4). The base locomotion module consists of two components; a footstep generator and walk generator. Given key points along a path and a list of floor objects, the footstep generator determines the 2D position and orientation of all of the foot prints along the path. B- splines are used to create a smooth curve through the key points. Then foot prints are laid out along the spline in accordance with a user set (or default) stride length, as well as width and environmental conditions. Figure 5 illustrates a gap in the floor terrain that requires an 725</page><page sequence="4">alteration of the step length which is automatically detected and calculated by the locomotion module. Figure 4: Locomotion on uneven, multi-level, and dynamic terrain. http:llhms. upenn. edulLMCOImedia.html The footstep generator only determines 2D coordinates for the footprints. The heights of footprints are determined in the walk generator to enable walking on dynamically changing terrain, such as the deck of a ship. Figure 5: Non-uniform step length. http:llhms.upenn.edulbigMovieslunevenWalk.mov The walk generator includes an environment checker that dynamically determines the height and slope of the floor terrain. From this information a foot and ankle configuration is determined and used as a goal position and orientation to be fed into our analytic inverse kinematics software (I KAN [14]). Checking foot height and configuration during walk generation ensures accurate contact with the terrain. This method provides natural walk on uneven, multi-level, and even dynamically moving floors. We have also built carrying and pushing components onto the HMT locomotion module. Carrying can be a complex action. Which hand is the object being carried in or both? Where should the object be grasped for carrying? Where is the object carried in relation to the torso? How heavy is the object? The carrying routine implemented for HMT includes parameterization that addresses all of these questions. The hand or hands grasping the object are specified as a parameter. The definition of the object to be carried includes grasp sites that are used to determine the position of the hands on the object being carried. There are also sites in the upper, middle, and lower torso segments of the digital human. These sites are used to specify the carrying position of the object (e.g., 10cm directly in front of the middle torso). This position along with a weight parameter influences the upper body movement and posture of the human simulating the weight of the object being carried. Pushing an object such as a handcart or dolly creates another set of difficulties. When someone pushes a dolly, the dolly and person do not normally follow the same path. The person's hands have to remain on the cart handles. The incline of the dolly needs to be specified and the dolly must follow the terrain. In the HMT, the dolly path is constructed from the foot steps of the digital human pushing the dolly. A radius between the dolly and the pusher is maintained. As the digital human walks, the position and orientation of the dolly are updated to maintain a radius between them and to follow the path. Similar to carried objects, the dolly handles are tagged with grasp sites. The hands of the dolly pusher are constrained to the handles of the dolly using IKAN with these grasp sites as goals. The incline of the dolly is specified as a parameter, but could be calculated as a function of the terrain slope and cargo weight. The dolly follows the slope of the terrain by using a number of sample points on each tire. This number can be user-specified, but by default six points on each tire are used. The global distance between these points and the ground surface are checked at every frame. The height and orientation of the dolly are updated using this distance information in order to keep the dolly constrained to the ground surface. The script interface to our locomotion module requires only the name of the digital human, the action name (i.e., walk), a path file- name, a list of floor objects, walk speed, and a start time. Carrying also requires the type of carry (which hand or hands) and the object to be carried. Push requires the name of the object being pushed. ADDITIONAL FEATURES There are other key features in the HMT that enhance ergonomics analyses. These features are necessary but not innovative technologies. First are cameras and view ports. In the HMT we can aim the camera to any object, but we can also attach the camera to any digital human and open a separate view port that renders its eye view. This can be used to assess object visibility. The HMT also includes a graphical representation of view cones to aid in this assessment. Another useful technology comes from the Jack Toolkit. Constraints or attachments are very convenient for placing a digital human in a moving vehicle or having it carry a box. Constraints define the relationship between 726</page><page sequence="5">two objects and then maintain that relationship while the constraint is on. The relationship can be based on position or orientation or both and may include a limited number of dimensions. Over the years, people's expectations of the appearance of digital humans have increased. They see the characters in video games and movies like Final Fantasy and expect even digital humans for ergonomics analysis to look that good and run in real-time. Advances in hardware technology are now making real-time smooth skinned digital humans possible. Deformation techniques are well established. The difficulty faced by many in this community is getting a well designed deformable figure from modeling software, such as Maya or 3D Studio Max, into their DHM application. Exporting the mesh, skeleton, and deformation weights and importing them into another application can be a tedious process. Doing so in a generalized pipeline between the modeling software and ergonomics software is often beyond the ability or available level of effort allocated for creating better digital human appearance. The SANTOS model [15] bypasses this step by creating a deformable model, but then the flexibility of modern modeling packages such as Maya is not available for appearance or clothing alternatives. Figure 6 shows one of our first deformable digital humans in the HMT. This deformable head and face model was created in Maya and translated into OpenGL by Catherine Pelachaud's research group [16]. We then adapted it to the HMT. Figure 6: A deformable face in the HMT. SCRIPTING The goal of the HMT is to be able to instruct digital humans in a manner as close as possible to the instruction of real humans. In previous research, we explored using natural language to instruct or program digital humans [17]. We found, however, that ambiguities in natural language currently limit its use to applications constructed for specialized domains. General solutions in natural language processing are not viable yet. Therefore the goal of the scripting interface for the HMT was to create a simple interface with a straightforward syntax and to eliminate, for even a naïve user, the need for any information not readily available. Figure 7 shows a sample script. While our goal of having completely readable/understandable scripts has not yet been met, we are making progress toward that goal. John sitFromWalk J:'hmtJan'hmt'data'JohnOfficePath.in cube office_chair2 chair_seat 90 10 Todd walkFromSit J:'hmtJan'hmt'data'ToddPath.in cube chair chair_seat 100 25 Todd reach cubel.cube.leftGrasp L 275 Todd reach cubel.cube.rightGrasp R 275 Todd carry2Hands cubel.cube J:'hmtJan'hmt'data'ToddCarryPath.in 90 J:'hmtJan'hmt'data'officeFloors.dat 290 cameraO attach Todd noOrient 250 1 Kimberly reach cube9.cube.rightGrasp R 730 Kimberly reach cube9.cube.rightGrasp L 730 Kimberly walkll J:'hmtJan'hmt'data'KimberlyPath.in multibox 80 595 Jacob push J:'hmtJan'hmt'data'JacobDollyPath.in dolly uneven 35 1000 cameraO attach Jacob noOrient 425 1 100 Jill carry2Hands cubei 5.cube J:'hmtJan'hmt'data'JillCarryPath.in 60 J:'hmtJan'hmt'data'outFloors.dat 1 600 cameraO attach Jill noOrient 450 1650 Figure 7; Sample script. Extra line returns added for clarity. http:llhms.upenn.edulbigMovieslofficeWalking.avi The basic syntax for all actions is the same: &lt;actor&gt; &lt;action&gt; &lt;parameters&gt; &lt;start time&gt;. The actor is the digital human performing the action. Currently we have not implemented synchronized team actions although we expect to approach that requirement through the PaT-Net concept used in earlier Jack applications [18], The indicated action determines how many parameters the script parser expects. Parameters include the names of objects that participate in the actions, path files that contain path key points, walking speed, left or right hand specification, and offsets. The start time is specified as a frame number from the start of the script. After the environment, geometry for the digital humans, and objects have been loaded in the HMT, a script can be loaded and executed. FUTURE WORK Currently the number and ordering of parameters in the scripts are predetermined. Our next step is to change 727</page><page sequence="6">the syntax of the scripts to a tagged XML form that would permit optional parameters and varied ordering. We are also in the process of removing the need for floor object specification in the scripts. Currently floor objects are explicitly specified so that they can be accessed by the environment checker in the locomotion module. We plan to tag floor object geometry in the modeling process and register them automatically when the geometry is loaded. In an effort to make the scripts even more human readable and enhance our motion generators, we plan to further develop smart objects in the HMT. Smart objects are objects labeled or tagged with information about how actions associated with objects should be performed [19]. Grasp sites associated with boxes to be carried and handles on the dolly are a form of smart object information. Ideally we would like an automated process to appropriately label all modeled objects with this information, but this would require a considerable effort in object recognition. For now we plan to create a GUI system that will enable a modeler to include this information, possibly based on existing product data management structures. Having objects tagged with grasp sites, named parts, fronts and backs, and other common information will aid in the creation of general grasping routines as well as routines for picking up and putting down objects. These actions are highly dependent on the object shape and size as well as the environmental conditions (is the object on the floor or in a draw or cabinet?) and digital human anthropometry. We also plan to take a parallel step away from our current script interface toward a GUI based interface. We would allow a user to select actors and objects from a database represented in a GUI and drag and drop them into a GUI script. Instead of specifying explicit timing information, action timing will be specified relative to other actions (e.g. After loosening the bolt, remove the drive). This step will remove the numeric terms from the script and link actions with other events or their terminations. CONCLUSION The HMT system is an attempt to organize known basic human task actions into an easily scripted and automatically executed animation. The most basic human action of reaching without collisions and with respect for empirical joint motions has informed our real- time reach algorithm. The essential need to locomote through a complex multi-level and dynamically moving space while carrying or pushing an object informs our locomotion module. An attention model for head and eye movements is being redesigned for the HMT environment [20], The scripting interface is a major step toward constrained language-based commands for the human figures. As HMT grows on top of the Jack Toolkit, we expect future deployment and adoption of HMT to be a natural next step forward for the DHM user community. ACKNOWLEDGMENTS This research is partially supported by Lockheed-Martin Corporation, NASA NRA NAG9-1279 and NRA 03- OBPR-01 -0000-01 47, the National Science Foundation IIS-0200983, and the Office of Naval Research (VI RTE) N000 1404 10259. REFERENCES 1 . Ianni, J. A specification for human action representation, in Digitai Human Modeling for Design and Engineering. 1999. The Hague, The Netherlands: Society for Automotive Engineers (SAE). 2. Badler, N., et al., A parameterized action representation for virtual human agents, in Embodied Conversational Agents, J.S. J. Cassell, S. Prévost, and E. Churchill, Editor. 2000, MIT Press: Cambridge, MA. p. 256-284. 3. Clancy, W., et al., Brahms: Simulating practice for work systems design. International Journal on Human-Computer Studies, 1998. 49: p. 831- 865. 4 . Micro Analysis &amp; Design: IMPRINT. http://www.maad.com/index.pl/crew station des ian tool. 5. Design, M.A., IMPRINT. http://www.maad.com/index.pl/crew station des ian tool. 6. Silverman, B., et al. Toward a human behavior modeling anthology for developing synthetic agents, in 10th Conf. On Computer Generated Forces and Behavioral Representation. 2001 : SISO. 7. Ascension Technologies, inc.: ReActor2. http://www.ascension- tech.com/products/reactor.php. 8. Kovar, L., M. Gleicher, and F. Pighin. Motion Graphs, in ACM SIGGRAPH. 2002. San Antonio, TX: ACM Press. 9. Bindiganavale, R., Building Parameterized Action Representations from Observation, in Department of Computer and Information Science. 2000, University of Pennsylvania: Philadelphia, PA. 10. Delleman, N.J. Postural Behavior in Static Reaching Sidewards, in Conference and Exhibition "Digitai Human Modeling for Design and Engineering". 2003. Montreal: SAE International. 1 1 . Liu, Y. and N.I. Badler. Real-Time Reach Planning for Animated Characters Using Hardware Acceleration, in Computer Animation and Social Agents. 2003. New Brunswick, NJ: IEEE Computer Society. 12. Zhao, L., Y. Liu, and N.I. Badler. Applying Empirical Data on Upper Torso Movement to Real-time Collision-free Reach Tasks, in SAE 728</page><page sequence="7">Digital Human Modeling for Design and Engineering Symposium. 2005. Iowa City, Iowa. 13. Multon, F., et al., Computer Animation of Human Walking: a Survey. Journal of Vizualization and Computer Animation, 1999. 10: p. 39-54. 14. Tolani, D., A. Goswami, and N. Badler, Real- time inverse Kinematics Techniques for Anthropomorphic Limbs. Graphical Models, 2000. 62(5): p. 353-388. 1 5. University ofiowa: SANTOS. http://www.dioital- humans.org/main.htm. 16. Pelachaud, C., et al. Embodied Contextual Agentin Information Delivering Application, in First International Joint Conference on Autonomous Agents and Multi-agent Systems. 2002. Bologna, Italy: ACM Press. 17. Bindiganavale, R., et al. Dynamically Altering Agent Behaviors Using Naturai Language Instructions, in Autonomous Agents. 2000. 18. Badler, N.I., M. Palmer, and R. Bindiganavale, Animation Control for Real-time Virtual Humans. Communications of the ACM, 1999. 42(8): p. 64- 73. 19. Kallmann, M., et al. Planning Collision-Free Reaching Motions for Interactive Object Manipulation and Grasping, in Eurographics. 2003. Granada, Spain. 20. Chopra-Khullar, S. and N.I. Badler, Where to look? Automating attending behaviors of virtual human characters. Autonomous Agents and Multi-agent Systems, 2001. 4(1/2): p. 9-23. 729</page></plain_text>