<plain_text><page sequence="1">Can a Computer Evaluate? John R. Ginther, University of Chicago Can a computer evaluate? In order to answer unambiguously it seems necessary to clarify the question first, to define the con- struct "evaluation." The meanings for the term are related to a variety of views of the nature of reality, and I alert the reader to the possibility that the theoretical construct which is developed and called "evaluation" here will, when used to guide the collec- tion of evidence, bias the conclusions. However, the reader must also be aware that procedures stemming from other, perhaps more restrictive, definitions may likewise result in biased conclusions. The educational world already seems afloat in the latter. We define evaluation as the garnering and consideration of evi- dence that something is good or satisfactory or valuable. Typically, in educational institutions, the evidence is displayed by a student who has learned to do some particular thing, or is improving along some specifiable path. Evaluation, as defined, has two commanding characteristics: 1. Evaluation can be thought of as a developmental process proceeding from awareness and definition, through the develop- ment of scales of measurement. The scales impose various degrees of rigor (e.g., nominal, ordinal, interval, and ratio scales), but the process of achieving such scales of measurement always entails definition. Definition, in general, is concerned with the systematic order of the conceptual schemes of science, and with the nature of the relations between different entities. Measurement has a more limited func- tion, that of establishing metrical order among different manifesta- tions of particular properties and of making scientific events amen- able to mathematical descriptions.' We can think of a continuum of mental processes, a taxonomy of objectives, a classification of socioeconomic conditions, or a catalog of personality structures as attempts at definition. In at- tempting to garner evidence based on these definitions, we typi- cally find that clearer differentiations and tighter specification of criteria are desirable. As sharper insights into the variables are 602 School Review</page><page sequence="2">John R. Ginther developed, scales become useful, and portions of the evaluative effort become classifiable as measurement. Anyone who has seri- ously considered the gathering of evidence about student learning will recognize the definition-to-measurement movement. He will also recognize that often no true measurement is possible. Thus, the first characteristic of evaluation is movement between defini- tion and measurement. 2. A set of categories which stems from the field of psychologi- cal testing constitutes the basis for the second characteristic of evaluation. Psychological testing is sometimes divided into three general areas labeled measurement, evaluation, and assessment. Shifts across these three categories are remarkable in terms of (a) expansion of the kinds of evidence used, (b) loosening of the speci- ficity of the stimulus, (c) decrease in the amount of structure re- quired in the student response, (d) increase in use of normative criteria for judging, and (e) increase in the complexity of both the task presented to the student and the corresponding systems used to analyze and coordinate the data obtained. Thus, the characteristics of evaluation include both the defini- tion-to-measurement process and the measurement-to-assessment shift. Perhaps we can profit from keeping these two characteristics of evaluation before us as we look for evidence that a machine can evaluate. Evidence Clearly, very few persons (if any) would claim that a machine can evaluate. Neither would one expect to find any large number of persons who thought that a machine could recognize a pattern or had intelligence. In all these cases, it is assumed that a human intervenes to provide structure for both hardware and software. Perhaps with this understanding in mind, the reader will permit lapses into anthropomorphism. I do not attempt here to present a comprehensive survey of efforts to evaluate by machine. The armed services and NASA have conducted many projects of this nature which are not cited here; I merely mention the many studies reviewed by Karl Zinn which have phases relevant to our topic. For example, the report on August 1971 603</page><page sequence="3">Comment "5Conditioning Accurate Prosody" is laden with evaluations made by machine: A system designed especially for training or research on prosodic characteristics of speech (Lane, 1966) was programmed to compare an aural response from the student with a recorded model. The ex- tent and direction of discrepancy in pitch, loudness, or tempo are displayed for the student. Performance history is used in automatic decisions about the selection of the next dimension for attention and progression to the next exercise. The computer is also used . . . to allow tolerance for error to be adjusted by the learning strategy.2 Similarly, I can only acknowledge the development of instruc- tional games designed for use on computers-games which allow evaluation of students on a range of criteria for which, without the computer, it would be impossible to garner evidence.3 Some studies contribute primarily to the psychological testing shift characteristic of evaluation. In such studies investigators ex- tend the amount or kind of information usable in evaluation. Our first look at the machine evaluating examines such extensions. Imagine that teachers in a school were to write a 200-300- word report on each of their students twice a year. Over the years, such reports accumulate; at specified times the teachers are re- quired to summarize these reports on particular students, thus pro- viding a kind of evaluation of students. The reports contain "teachers' opinions about the aptitude, achievement, abilities, values, attitudes, peer associations, discipline problems, and even health concerns of every student."4 This was the task undertaken by the Lesley-Ellis Project, using the General Inquirer computer program and the Third General Inquirer Dictionary. The results were validated by human judges who sorted the machine-retrieved sentences into categories, as the machine program had done.5 The levels of agreement between humans and the machine were as high as levels of agreement among humans in similar sorting tasks (e.g., in the development and validation of items for psychological tests). Certainly McManus's system simplifies the problem of reducing to useful form the relatively loosely structured verbal evaluations of students. The work suggests additional studies-for instance, of the usefulness of such reduced information-but it also indicates the feasibility of using extended verbal forms in the periodic eval- uation of students. 604 School Review</page><page sequence="4">John R. Ginther The results from the Lesley-Ellis Project permit the tentative assertion that some evaluative efforts are fundamentally a sorting or categorization of information with subsequent judgments based on simple, linear combinations of the characteristics on which the sorts were made. With this assertion in mind, it is apparent that archaeologists have already done much of the preliminary work necessary for computers to evaluate their data." Furthermore, greatly extended sets of information can be stored with some newly found hope of using them: "With a computerized data file, any feature that is recorded can be used as a basis for searching the file, and one can retrieve data in a form that groups together all specimens which share any one of the enormous number of differ- ent logically possible combinations of features."'7 Another author illuminates the consequences of this potential and indicates the importance for archaeology of having investiga- tors turn from questions of when and where to questions of how and why: "It is also becoming increasingly clear to archaeologists that how one manipulates his data depends on the problem one is trying to solve; and furthermore, that as archaeologists we have not tackled all the problems we could or indeed should. Part of the reason for this lies in the past, when our ability to manipulate data was not very great. We explored what we could and left complicated and time-consuming tasks aside.8 From archaeology, then, we see another movement toward ex- tending the data base for evaluation, both because the computer can make a variety of sortings and weightings rapidly, and because the broadened range of subsequent speculations is desirable as a characteristic of the field of study. A complex computerized system for grading chemistry tests, updating test information, and even weighting test scores to de- termine final grades demonstrates several facets of evaluation.9 The computer-centered system used extended types of test items and provided for as many as nine answer choices. Test results- including statistical information on both progress and precise standing in the class-were available within twenty-four hours. Detailed diagnostic tables were provided each lecturer as well. Another chemistry teacher has engaged the computer to develop an interesting twist to the use of Port-a-Punch answer cards, which permits the student to create answers instead of choosing August 1971 605</page><page sequence="5">Comment among alternatives offered. "The statement, 'The simplest formula of a compound . . . is C(1) H(2) 0(3) N(5) C1(6) would be com- pleted 6 4 2 4 1 1 for the compound C6 H4 02 NC1. In this case the 4 in the fourth column directs the computer to accept numbers in the other columns as simple integers."10 Yet another team of investigators has overcome the constraining problem of appraising a set of answers to test questions, when many of the answers are dependent on earlier ones. Altenburg, King, and Campbell developed a computer program for grading of sequential answers. They were able to grade answers to prob- lems for which previous answers are required as data. Even though there was a prior error, this did not necessarily result in subse- quent errors when the later calculations were correct."1 Again, the computational rather than the linguistic potential of the computer is at the root of an innovation. One can hope that the importance of innovations like this in extending both the kinds of stimuli (test items) and responses (answers) will spur others to attempt similar extensions based on the language-processing capacity of the machine. Computer Naturals In addition to increasing the quality of the evaluation, the introduction of new techniques into the process of evaluation can lead to more efficiency in the gathering of information, reducing the time required for testing. We have been working on two projects which are "naturals" for the computer. One of these in- volves sequential item testing, by now a familiar concept to stu- dents of testing and measurement. Richard Weiland, my research assistant at the time, developed an experimental computer pro- gram to implement sequential item testing. The program selected the next question in a test sequence from a group of questions with specified values for the item parameters validity and diffi- culty; the choice of the next item depended on the student's answer to the previous item, as well as the characteristics of his earlier performance on this set of tasks or items. This work should enable us to automate to a very great extent the sorting of students for differentiated levels of instruction over short intervals. The computerized sequencing should enable a student to "sort him- 606 School Review</page><page sequence="6">John R. Ginther self" in a minute or two by responding to a half-dozen or so test questions. The difficulty in extending this work lies in developing the test questions and gathering the data which will yield usable parameter values. The second project involves Reading Attenuated Text, a title given to the procedure of oral reading of text material which has been systematically altered by dropping letters from the right end of words.12 The technique promises a quick and valid estimate of a student's knowledge. Like the sequential item technique, Read- ing Attenuated Text is cumbersome to administer. Both tech- niques are simplified, however, when the speed of the computer is introduced to control the selection and display of test material and-in the case of Reading Attenuated Text-to create successive- ly more difficult (i.e., more severely attenuated) texts. In the case of sequential item testing, one might say that the computer eval- uates, for the teacher need only look at the student's final classifi- cation to estimate closely his achievement. In the attenuated-text procedure, the computer plays a smaller role in evaluation, yet it is indispensable to the nearly instantaneous creation of text material. Perhaps we have examined enough studies to indicate that at- tempts are underway to provide extended forms and types of evi- dence for use in evaluation, and to increase the use of normative criteria in reaching judgments. Let us now seek evidence of the definition-to-measurement, or the process characteristic of eval- uation. Definition-to- Measurement Project Essay Grade of the University of Connecticut has a five- year history of investigations which can be summarized in part as follows. In dealing with "criteria of overall quality" of an essay, the "correlations between judgments of 138 essays done by five 'judges,' four of them human and one of them a computer," were quite similar. "From a practical point of view, the five judges are indistinguishable from one another."13 At this juncture, the project seems to have turned in the di- rection of developing more precise definitions for traits to be used in place of "criteria of overall quality." Efforts were made to eval- August 1971 607</page><page sequence="7">Comment uate "five traits believed important in essays."14 The data reported indicate that "the multiple-regression coefficients found in pre- dicting the pooled human ratings with 30 independent proxes found in the essays by the computer program" ranged from .62 to .72. After shrinking the coefficients "to eliminate capitalization on chance from the number of predictor variables," and then cor- recting "for the unreliability of the human group," the multiple R's ranged from .64 to .78 for the five traits.15 Page is continuing his investigation. In some of the more recent work, he says, "the newest programs apparently do better than the individual, expert English teacher." The definition-to-measurement process seems evident. Some essay examinations are used to identify students with spe- cific, remediable writing weaknesses. One might wish to identify only the poorest performers and provide instruction for them rather than to scale essays in some more rigorous fashion. An analysis of the products of such an examination at the graduate school level has shown that computerized procedures can lead to a sorting of essays which matches exactly the sorting based on scores assigned by university faculty who routinely read and grade such essays: "Essays by graduate students were scored by two, some- times three university professors. The resulting scores permitted the essays to be sorted into three groups: the very best, the very poorest, and all the rest. By use of the General Inquirer program it was found that the best and poorest essays could be sorted accu- rately from a mixed batch of essays of all three groups."16 Initially, the computer program identified the characteristics which differentiated a batch of good and poor essays, as deter- mined by the human readers. Next, a different mixed batch of essays was evaluated by the program, using the characteristics de- rived from the first phase of the investigation as criteria for sort- ing. This time there were good and poor essays, as well as a re- maining group which were apparently neither good nor poor. Examination of the scores assigned to the papers by human read- ers revealed that the highest score had been assigned to the essay in the computer's good category, the lowest scores had been as- signed to those in the poor category, and that the remaining essays had all received scores in between. The computer output suggested the specific criteria which dif- 608 School Review</page><page sequence="8">John R. Ginther ferentiated the essays, in addition to some criteria which were used but found not critical. Such information may be suggestive to readers who have not previously given careful consideration to the criteria being invoked when grading essays. Thus a dual function of the computerized procedures in evaluation is evident: (a) they do some of the evaluation, and (b) they suggest criteria which may lead to discussion and further clarification (definition) of the cri- teria to be used. Perhaps the most advanced use of the computer in evaluating student work is embodied in Entwisle and Huggins's report from Johns Hopkins."7 The need was clear; students had to gain experi- ence in the design of electrical circuitry for complicated jobs. The nub of the problem was to evaluate designs and provide feedback for students. The initial but different difficulty was that students took so long to create the designs that few could be completed during the course. The two investigators devised a computer pro- gram, JOBSHOP, which, via simulation, enables students to design creatively at the console rather than at the workbench or drawing board. The program also permits thoroughly detailed evaluation of the designs. In the nine-week session, fifteen different and so- phisticated designs were built, modified, and tested. Private Language Argument Surely the work of machines in analyzing language used by teachers to evaluate and describe the products and actions of students is thought-provoking, even astounding. So too is the com- puters' grading of essay material. However, one could take the position that the most important evaluative information lies beyond public language, somewhere in the realm of Cartesian "inner occurrences," which only an individual can know, and thus, become or provide unique meanings for the words he uses. Admittedly, this is extreme Cartesianism but it suggests the stance of some who condemn attempts at objective, cross-validated eval- uation. Wittgenstein's substantial counter-argument suggests that the extreme Cartesian view leads to a private language which is not a genuine language; that we are without criteria for validating it, or for answering the question: Was the inner occurrence the same both times? The resulting "private language argument" suggests August 1971 609</page><page sequence="9">Comment ways of bringing inner feelings under tighter surveillance, and thus perhaps learning to use sensitively a greater range of vocabu- lary when referring to or reporting them. The computer work on language analysis to date suggests that such an enriched vocabu- lary input might still be analyzed by machine and be useful to the teacher in evaluating pupil progress. Further, increased sensi- tivity to assigned meaning and increased precision of language usage might enhance the validity coefficients of machine analysis by increasing the validity of the criterion (i.e., agreement of human analysts). Descriptive Language Hare's ideas about the difference between descriptive and eval- uative language are a useful guide to one's own past evaluation efforts.s8 More often than not teachers discover that they have been saying "She is an 'A' student," or "He is a good student," and that they are unable immediately to specify what criteria are involved. Such statements Hare would label as evaluative. A specifiable par- ticular, such as "she uses the widest range of sentence types of any student in the class," would be descriptive. Unfortunately, from an evaluator's point of view, Hare's definition of evaluative lan- guage makes it the less acceptable and the less useful of the two kinds of language. Nevertheless, some attempt to move from the typically used evaluative language to the more precise descriptive language may help us improve on past performance in evaluating students. The work at the Lesley-Ellis School suggests that the machine may even help us make sense of some of the evaluative language we have used in the past. This is certainly the case with the Bhushan-Ginther study, where the faculty members reading the papers felt there was no agreement on the criteria used in scoring the free response essay examinations, and that, further, the exami- nation served no useful function. The results we achieved clearly suggest that there very probably was a higher degree of consensus about the criteria used than was apparent to the essay readers. Further, we suggested several ways in which the results of such examinations could be used in the guidance of students. This potential of the computer comes close to being magical. The work 610 School Review</page><page sequence="10">John R. Ginther at Lesley-Ellis is not quite so magical since it comes closer to being a straightforward, although highly sophisticated, retrieval system. In this latter case, the actual statements and comments previously made by teachers are retrieved. In the study at Chicago, essays which had been categorized were examined by computer; then an analysis system was used to suggest criteria which human readers might have been using unconsciously, criteria masked by evalu- ative language. New Directions The potential impact of evaluation by machine has been sug- gested by the work in archaeology, by the various analyses of essay material, and by the JOBSHOP prototype. In the first case, some significant shift in the boundaries of knowledge in archaeology may be in the offing. In the second case, a definite improvement in the definition of criteria used, and even some evidence relative to the private language argument may result from the computer work in grading essay material. The work in electrical engineering actually made course objectives realistic for the first time. Certain improvements in the efficiency of testing are suggested by other projects. Another significant contribution to evaluation stands in the wings. It is the development of computer-assisted procedures for determining human characteristics relevant to instruction (e.g., individual strategies for solving problems). Herbert Thelen of the University of Chicago once devised a simple teaching machine which consisted of a board holding two sheets of tinfoil wired to a battery so that contact between the sheets closed a circuit which lighted a bulb. Between the sheets of foil was a punched answer sheet. Atop the foil was a worksheet. Touching the worksheet at appropriate places would light the bulb, for the top foil would be depressed through a punched hole, etc. One exercise involved a typed page of text, complete except for commas. The student's task was to place commas correctly. Without providing detail, it can be said that Thelen discovered a limited number of patterns students used in approaching this problem. Some would place commas randomly; some would develop a working hypothesis about comma placement and proceed to test it; some would draw August 1971 611</page><page sequence="11">Comment on knowledge they possessed to accurately place a few commas. The exact number and description of the patterns are not impor- tant here. The point is that such situations can readily be de- veloped for some types of computer terminals, and the patterns recorded and identified automatically. Thus it seems reasonable to anticipate such a use of computers in the extended evaluation of the characteristics of learners. Conclusion The history of efforts to evaluate by machine reflects the funda- mental characteristics of evaluation: (1) to define criteria for evi- dence and strive for increasingly rigorous scales of measurement as our understanding develops; and (2) to extend the number and kinds of evidence used, loosen the specificity of both stimuli and responses, and increase the use of normative criteria in judging results. The success of some efforts and the apparent lack of clear failure in others suggest the promise of the computer as a tool in evaluation. This use of the machine may yield more satisfaction more quickly than has computer-assisted instruction. The nature of some of the studies reported here leads easily to the conclusion that indeed the machine can evaluate. The more important con- clusion is that the effort to evaluate by machine may rejuvenate debate and research on the private language argument, and arouse further concern for the use of descriptive language, activities which have already suggested significant improvements in evaluation as defined here. 612 School Review</page><page sequence="12">John R. Ginther 1. Peter Caws, "Definition and Measurement in Physics," in Measure- ment: Definitions and Theories, ed. C. West Churchmand and Philburn Ratoosh (New York: John Wiley 8c Sons, 1959), pp. 3-4. 2. Karl L. Zinn, "Computer Technology for Teaching and Research on Instruction," Review of Educational Research 37, no. 5 (December 1967): 619. 3. See, e.g., E. M. Babb and L. M. Eisgruber, Management Games for Teaching and Research (Chicago: Educational Methods, Inc., 1966). 4. John McManus, "Report on the Lesley-Ellis Project" (paper presented at the AERA pre-session on "The Computer and Natural Language," Los Angeles, February, 1969). 5. Ibid., p. 3. 6. See "Section II: Computers in Anthropology and Archaeology," in Computers in Humanistic Research, ed. Edmund A. Bowles (Englewood Cliffs, N.J.: Prentice-Hall, Inc., 1967). 7. George L. Cogwill, "Computers and Prehistoric Archaeology," ibid., p. 49. 8. Dee F. Green, "A Modern Innovation in Archaeology," ibid., p. 37. 9. J. Adlin Mann, Jr., Harry Zeitlin, and Allan B. Delfino, "A Computer Centered Chemistry Records and Grading System," Journal of Chemical Edu- cation 44, no. 11 (November 1967): 673-77. 10. Norman A. Frigerio, "Notes on Computer Grading of Examinations," ibid., no. 7 (July 1967), pp. 413-14. 11. John F. Altenburg, Lowell A. King, and Carolyn Campbell, "Com- puter Grading of General Chemistry Laboratory Reports," ibid., 45, no. 9 (September 1968): 615-16. 12. John R. Ginther, "Introduction to the Technique" (copyrighted lec- ture, University of Chicago, May, 1968). 13. Ellis B. Page provided a useful set of references in his paper, "Statis- tical and Linguistic Strategies in the Computer Grading of Essays" (paper presented at the AERA pre-session on "The Computers and Natural Lan- guage," Los Angeles, February 1969, p. 5). 14. Ibid., p. 5. 15. Ibid., p. 6. 16. Vidya Bhushan and John R. Ginther, "Discriminating between a Good and a Poor Essay," Behavioral Science 13, no. 5 (September 1968): 417. 17. Doris R. Entwisle and W. H. Huggins, "Simulated Environments in Higher Education," School Review 75, no. 4 (Winter 1967): 378-91. 18. R. M. Hare, The Language of Morals (New York: Oxford University Press, 1964), esp. chaps. 6, 7, 8. August 1971 613</page></plain_text>