<plain_text><page sequence="1">RECHERCHES EMPIRIQUES EMPIRICAL STUDIES IMPLEMENTATION DE MÉCANISMES D'ANTICIPATION VISUO-MOTRICE EN TÉLÉOPÉRATION par Y. Rybarczyk*' **, D. Mestre**, P. Hoppenot* et E. Colle* SUMMARY IMPLEMENTATION OF VISUO-MOTOR ANTICIPATORY SCHEMES IN TEILOPERATION A major issue in teleoperation is to try to reduce the gap that exists between the functioning of the human operator and that of the machine. The bionic approach, which consists of giving human properties to robots, is effectively a means of reducing the natural differences between the modes of functioning of these two entities, biological and mechanical. In the experimental study presented here, we implemented the visuo-motor anticipatory behaviour observed in locomoting humans using a teleoperated mobile robot. The ecological validity of such an implementation was empirically tested using a slalom route task, with independent experimental groups : the first one piloting a robot equipped with a camera that visually anticipated movement and the other one piloting a robot with a fixed camera. This « human-like » behaviour was also implemented using two different coupling modes. In the first mode, the orientation of the camera (on board the robot) was dependant on the direction of displacement of the robot, the operator controlling the latter (base mode). In the second mode, the direction of displacement was defined by the orientation of the camera, which was manipulated by the operator (camera mode). Experimental results show : 1 I that the operator is more efficient and seems much more at ease when a visuo-motor anticipatory scheme is implemented on the teleoperated device and 2 I that this effect is more pronounced in the camera mode. These results suggest that there is a case for introducing sensori-motor anticipatory « human-like » behaviour into artificial partially-automated teleoperated systems. Key words : Teleoperation, Bionic, Human-Machine Interfacing, Visuo-motor co- ordinations. * Université d'Évry, Laboratoire des systèmes complexes, fre cnrs 2494, 40, rue du Pel- voux, 91020 Évry Cedex. E-mail : yrybarc@iup.univ-evry.fr. ** Université de la Méditerranée, Institut de neurosciences physiologiques et cognitives, fre cnrs 2109, 13402 Marseille, Cedex 20. E-mail : mestre@lnf.cnrs-mrs.fr. Le Travail Humain, tome 67, n° 3/2004, 209-233</page><page sequence="2">210 Y. Rybarczyk, D. Mestre, P. Hoppenot et E. Colle I. INTRODUCTION La source des difficultés que rencontre un opérateur dans le cadre d'une Coopération homme-machine (chm) réside dans l'écart fonctionnel existant entre l'homme et la machine. En effet, les mécanismes de con- trôle sensori-moteurs, qui d'ordinaire sont naturellement automatisés chez l'individu, nécessitent maintenant, par exemple, un enclenchement simultané de boutons, avec un nombre de combinaisons possibles très important (Gray &amp; Wilson, 1988). Ainsi, les capacités du système robo- tique sont loin de permettre une analogie de mouvement avec les modes opératoires humains « naturels ». Les utilisateurs sont, en particulier, sur- pris par le retard et l'écart entre leur geste et les mouvements du robot qui engendre des trajectoires selon ses propres règles de fonctionnement (Ver- cher, Gauthier, Bertrand, &amp; Magenes, 1989). Jusqu'à présent, les moyens techniques mis en place pour réduire cet écart fonctionnel entre l'homme et la machine n'ont permis qu'une inté- gration relativement limitée des propriétés fonctionnelles de l'opérateur humain (Rabardel, 1993). C'est pourquoi l'étude qui suit vise à observer une approche « bionique » (discipline née de la fusion entre la biologie et la technologie informatique) de la coopération homme-machine, afin de réduire plus efficacement la différence de nature entre le mode de fonc- tionnement de la machine et de l'homme. Cette démarche consiste à rechercher chez les êtres vivants des modèles fonctionnels du couplage sensori-moteur, en vue de réalisations techniques mieux adaptées au con- trôle d'engins robotisés (télémanipulation, téléopération). En effet, la nature a cet avantage sur l'ingénieur d'avoir mis des millions d'années pour élaborer des mécanismes biologiques, dont seuls les mieux adaptés ont été conservés au cours de la sélection naturelle. Dans le cas d'une machine semi-automatique, où il existe un souci d'assimilation (au sens de Piaget, 1967) de l'engin par l'opérateur humain, le modèle animal naturellement choisi sera bien évidemment l'homme. Ainsi, on voit se développer l'idée d'une amélioration du système méca- nique d'un robot par implementation de stratégies adaptatives propres aux animaux supérieurs, et en particulier l'homme, lorsqu'ils doivent faire face à des variations inattendues dans l'environnement. Par exemple, à partir de l'enregistrement de la modulation de l'impédance humaine en réponse aux modifications de la tâche, on élabore des robots présentant des préréflexes mimant l'activité musculaire de l'homme, rendant l'engin beaucoup mieux adapté à l'utilisateur humain (Kazerooni, 1989). En condition de téléopération, un opérateur humain n'agit qu'indirec- tement sur l'environnement physique, par l'intermédiaire d'une machine, et ne perçoit également qu'indirectement les résultats de son action, sous forme d'une image vidéo. Cette situation où l'entité qui ordonne le mou- vement n'est pas la même que celle qui l'accomplit, va s'accompagner d'un appauvrissement sensoriel préjudiciable à l'accomplissement de l'action. En effet, en situation « naturelle », le guidage du mouvement s'effectue à partir d'une intégration des informations sensorielles affé-</page><page sequence="3">Anticipation visuo-motnce en téléopération 211 rentes des diverses modalités sensorielles dont dispose l'organisme. En particulier, il existe un dialogue continu entre le système visuel et le sys- tème proprioceptif. Il en résulte un gain d'efficacité qui se caractérise par une meilleure précision et une réduction de la variabilité spatiale et tem- porelle de la réponse motrice. L'absence ou la mauvaise restitution de certains types d'informations sensorielles vont contraindre l'opérateur à privilégier certaines modalités sensorielles sur d'autres. Ainsi, de nombreuses études ont démontré qu'en condition de téléopération, la modalité visuelle est surexploitée par rap- port à une situation naturelle (Terré, 1990; Mestre &amp; Péruch, 1995). C'est encore ce que viennent confirmer les travaux sur la restitution du sens du toucher, démontrant que la discrimination de textures ou de la résistance des surfaces s'avère plus efficace sous forme d'informations visuelles que kinesthésiques ou somatosensorielles (Grimbergen, 1997). Le fait que la vision soit prioritaire sur tous les autres sens, signifie que les limitations technologiques dans cette modalité auront de lourdes conséquences sur la performance de l'opérateur. Or, quels que soient les progrès techniques mis au point pour augmenter le réalisme d'une scène visuelle, ceux-ci placent toujours l'observateur dans une situation d'appauvrissement visuel. D'une manière générale, on observe une baisse de la performance à cause de la réduction des indices permettant en situa- tion « naturelle » la perception de la structure de l'espace d'action (Mas- simo &amp; Sheridan, 1989). Plus précisément, des travaux ont montré qu'il y a une détérioration systématique de la performance, par rapport à une situation « naturelle », lorsqu'on utilise un écran vidéo (Smith &amp; Smith, 1990). Cela est notam- ment dû à des difficultés d'évaluation des distances et de la profondeur, distordues par l'interface et la caméra vidéo. Viennent s'ajouter à cela une dégradation d'indices monoculaires tels que la taille, la luminance ou l'accommodation, ainsi que la perte d'indices binoculaires comme la dis- parité et la parallaxe binoculaire (Reinhardt-Rutland, 1996). Il faut aussi noter que ces contraintes ne se limitent pas à la qualité de l'image elle-même mais concernent aussi le caractère dynamique de sa retransmission. En effet, une telle condition de vision indirecte entraîne une absence des indices de mouvement que sont la parallaxe de mouve- ment ainsi que les mouvements de l'observateur qui ne génèrent pas de flux optique (Cornilleau-Péres &amp; Gielsen, 1996), dont on connaît le rôle majeur dans le contrôle du déplacement (Gibson, 1979 ; Warren, Mestre, Blackwell, &amp; Morris, 1991). De plus, cette déficience sera amplifiée par la réduction de la taille du champ optique dont on sait, déjà en situation « naturelle », les conséquences négatives sur la précision spatiale (Coello &amp; Grealy, 1997). Ainsi, les limitations visuelles engendrées par la situation de téléopération peuvent se décomposer suivant la qualité des paramètres temporels et spatiaux du retour vidéo. D'un point de vue temporel, la commande d'un engin téléopéré néces- site d'exercer sur lui un contrôle quasi continu, si on ne peut s'appuyer sur des actions automatisées (par exemple, l'évitement d'obstacle). Pour cela, un tel système doit être capable de transmettre rapidement une grande quantité d'images depuis l'engin en question jusqu'à la station de contrôle ;</page><page sequence="4">212 Y. Rybarczyk, D. Mestre, P. Hoppenot et E. Colle lorsque cette fréquence de transmission de l'image diminue, la capacité de contrôle de l'opérateur diminue également (Vercher &amp; Gauthier, 1992). Malheureusement les limitations technologiques sont telles qu'il existe plusieurs sources de délais temporels. Elles sont liées au renouvellement des images vidéo, à la transmission des commandes de l'opérateur vers les organes effecteurs de la machine et, en retour, des informations réponses vers l'opérateur (Held &amp; Durlach, 1993). Le problème est d'autant plus complexe qu'il existe des interactions entre les délais liés au système de téléopération et les délais physiologiques des boucles de contrôle propres à chaque système sensori-moteur impliqué dans le contrôle des mouve- ments de l'œil, de la tête ou de la main. Enfin, des travaux ont montré qu'un délai compris entre 50 et 100 ms (entre l'action de l'opérateur et le retour visuel de cette action) semble être une valeur maximale pour permettre le contrôle visuel du mouvement d'une cible présentée sur un terminal vidéo (Lui, Tharp, French, Lai, &amp; Stark, 1993). De plus, il apparaît nécessaire d'avoir une fréquence de rafraîchissement uniforme et élevée pour assurer la perception directe du mouvement visuel sur un terminal de visualisation. Sur le plan spatial, la téléopération pose le problème de la limitation du champ visuel exploitable par l'opérateur et, en particulier, du manque de vision périphérique (Hightower, Spain, &amp; Bowles, 1987 ; Miller &amp; McGo- vern, 1988 ; Padmos &amp; Van Εφ, 1996). En condition « naturelle », la taille du champ visuel humain est d'environ 180°. De nombreuses études ont testé l'effet de la taille de ce champ sur diverses tâches telles que la capture d'un objet en mouvement ou le contrôle de la locomotion (Leibowitz, 1986). Des résultats convergents montrent que la réduction du champ visuel périphérique produit une « compression » des tailles perçues des objets et de leur distance à l'observateur (Hagen, Jones, &amp; Reed, 1978). Inversement, lorsqu'un sujet bénéficie d'un élargissement de son champ visuel fonctionnel, on observe une amélioration de la performance de pilo- tage d'un mobile en environnement virtuel (Péruch &amp; Mestre, 1999). On pourrait donc penser que plus on se rapproche de la taille d'un champ visuel « naturel », meilleure va être la performance de l'opérateur, notam- ment dans des tâches de contrôle de la trajectoire de véhicules. Cependant, le problème semble complexifié par le fait que, lorsque la taille du champ visuel est supérieure à 120°, les sensations de vection (sensation de déplacement de soi visuellement induite) s'accompagnent parfois de troubles neurovégétatifs (Pausch, Créa, &amp; Conway, 1993). Les malaises peuvent être attribués à des délais et discordances visuo- vestibulaires liés aux limitations temporelles décrites aux paragraphes pré- cédents. L'hypothèse est aussi avancée que de grands champs de vision produisent ces malaises en même temps qu'ils renforcent le réalisme de la visualisation (So, 1994). Il semble donc qu'une adéquation doive être recherchée entre le besoin d'un champ de vision suffisamment large pour permettre une bonne appréhension de l'environnement et les problèmes posés par un champ de vision trop large et trop « immersif » qui semble de nature à perturber l'observateur. En définitive, ces études tirées de la psychologie expérimentale, per- mettent de définir un champ d'investigation des comportements humains</page><page sequence="5">Anticipation visuo-motrice en téléopération 213 utiles à la conception d'une coopération homme-machine propre à la téléopération. D'une manière générale, l'ensemble de ces travaux met en évidence que la contrainte sensori-motrice majeure à laquelle est soumise un téléopérateur, provient des limitations visuo-temporelles et visuo- spatiales des feed-back qui lui sont renvoyés par le système. Or un indi- vidu présente, toutes proportions gardées, les mêmes limitations physiolo- giques d'ordre spatio-temporel que celles soulignées au niveau du système mécanique. De ce fait, il élabore des stratégies sensori-motrices afin de les compenser. C'est pour cette raison que, selon notre démarche bionique, ce sont ces stratégies adaptatives « naturelles » que nous allons tenter de décrire et d'implémenter dans le cadre du téléguidage d'un engin, avec la vision comme modalité sensorielle prioritairement impliquée. Lorsqu'un individu est impliqué dans la réalisation d'une tâche pré- cise, il parvient rapidement à anticiper sur les actions à venir, ce qui lui permet d'augmenter son efficacité. Cette faculté est le fait que le cerveau s'est construit au cours de l'évolution de manière à anticiper systémati- quement sur les actions futures. Se déplacer pour échapper à un prédateur ou chasser une proie, nécessite d'émettre des hypothèses sur le monde afin de deviner les intentions d'autrui. Ainsi, il ne s'agit pas de simples réflexes, de réponses passives à des stimuli sensoriels, mais au contraire, le contrôle de l'action nécessite que le cerveau soit un prédicteur qui simule les actions de l'autre comme de soi-même. Par exemple, lors d'un mouve- ment de capture d'une balle, les enregistrements neurophysiologiques chez l'homme prouvent que le cerveau n'attend jamais que le sens du tou- cher soit activé pour produire un début de réponse. Dans cette situation, le cerveau va produire une contraction des muscles 300 ms avant que l'objet ne touche la main (Lacquaniti &amp; Maioli, 1987). Pour revenir au caractère prédateur, l'apparition de la vision fovéale chez ces bêtes s'est accompagnée de la migration des yeux d'une position latérale à une position frontale, ce qui aurait permis, en particulier, la mesure de la distance des obstacles en profondeur, l'apparition de la poursuite oculaire visuellement guidée... Mais ces avantages de la vision frontale ont eu pour inconvénient que ces animaux (dont font partie l'homme) ont alors perdu la vision panoramique (propre aux proies) et qu'il leur fut nécessaire de reconstruire l'environnement visuel à partir d'une succession de vues « locales ». L'orientation du regard n'a donc plus été utilisée uniquement pour orienter le corps, mais pour explorer le monde visuel. Ainsi, c'est par ses propriétés d'optimisation de l'explo- ration de l'espace que le comportement adaptatif d'anticipation visuelle a été sélectionné par l'évolution. Cette stratégie d'anticipation visuelle va concerner tout aussi bien des mouvements de préhension que de locomotion. En ce qui concerne la préhension, les travaux menés par Land, Mennie et Rusted (1999) sur les activités quotidiennes laissent apparaître que, lors de la manipulation d'objets, le regard est déplacé vers le prochain objet à saisir, une seconde environ avant que l'utilisation du premier ne soit terminée. Ainsi, chaque étape des actions de tous les jours, y compris les plus inconscientes et automatiques, est systématiquement supervisée par notre regard. Celui-ci ayant tendance à anticiper les activités motrices, comme si les mouve-</page><page sequence="6">214 Κ Rybarczyk, D. Mestre, P. Hoppenot et E. Colle ments oculaires représentaient un signal prédictif sur les actions à venir. Ces résultats sous-tendent l'existence d'un « script » mental qui guide la réalisation de nos actions. Ces dernières sont alors définies par un but glo- bal qui se subdivise en plusieurs sous-buts, dont le premier est représenté par les fixations oculaires prédictives. De même, lors du contrôle du déplacement locomoteur, l'axe du regard va anticiper systématiquement sur la trajectoire réalisée. Il a été mis en évidence, que lors de son déplacement, l'individu ne conserve pas son axe céphalique rigoureusement aligné avec le reste du corps. En effet, il apparaît que pour des trajectoires courbes, l'orientation de la tête du sujet est déviée dans la direction de la marche, vers la concavité de la tra- jectoire. Plus précisément, la direction de la tête, ou plutôt du regard, gui- derait le déplacement en anticipant systématiquement les changements de direction de la locomotion d'un intervalle d'environ 200 ms (Grasso, Gla- sauer, Takei, &amp; Berthoz, 1996). Cet intervalle d'anticipation est en fait dépendant du degré de courbure de la trajectoire. Il peut atteindre une valeur d'une seconde pour un virage à angle droit. Une stratégie de type « je vais là où je regarde » et non pas « je regarde là où je vais », semble sous-tendre ce guidage de la trajectoire de déplacement (Land, 1998). Il en va de même pour le contournement d'un repère. Les enregistrements des mouvements du regard et du corps montrent que le regard pointe en direction du repère bien avant que l'individu ne se trouve à son niveau, le réalignement de la tête dans la direction de la marche ne s'effectuant qu'après son franchissement (Grasso, Prévost, Ivanenko, &amp; Berthoz, 1998). Cela suggère que l'orientation du regard est contrôlée pas à pas selon un mécanisme prédictif de la nouvelle direction à emprunter (Patla, Prentice, Robinson, &amp; Neufeld, 1991). De plus, il est important de souli- gner que ces deux études de Grasso (1996 et 1998) mettent en évidence le même pattern dans la dynamique d'orientation de la tête, que l'individu ait à réaliser une trajectoire courbe (1996) ou un contournement de repère (1998). Ce qui montre que ces deux tâches semblent relativement similaires quant aux mécanismes de contrôle sensori-moteur qu'elles met- tent en jeu. De telles observations ont aussi été recueillies pour la conduite auto- mobile. Dans ces conditions, on constate un déplacement de l'axe du regard du conducteur une à deux secondes avant d'atteindre la convexité de la courbe. De plus, on remarque, malgré le décalage temporel, une déviation angulaire des roues identique à celle du regard (Land &amp; Lee, 1994). Par cette stratégie, l'automobiliste cherche à utiliser les propriétés optiques particulières de la tangente au virage pour guider son déplace- ment. En effet, dans cette partie de la courbe, il y a une annulation dans la direction du flux optique qui reste à une même position dans le champ visuel pour une courbure de virage constante, rendant ce point particuliè- rement pertinent pour stabiliser la trajectoire du véhicule (Raviv &amp; Her- man, 1993). En ce sens, une tâche de suivi de trajectoire s'apparente à un cas particulier de contournement de repère, puisque dans les deux situa- tions, l'individu va chercher à se baser sur ces repères physiques stables de l'environnement pour contrôler son déplacement (Land, 1998). Des étu- des psychophysiques montrent que cette stratégie de fixation correspond</page><page sequence="7">Anticipation visuo-motrice en téléopération 215 aussi à une optimisation de la prise d'information utile pour le contrôle de la trajectoire (Mestre, 2001). Ce comportement d'anticipation visuelle sur le déplacement apparaît donc comme une formidable stratégie que la nature a mise au point pour pallier les limitations spatio-temporelles inhérentes au système visuo- moteur des animaux supérieurs. Aussi, comme il a déjà été souligné pré- cédemment, les systèmes télérobotiques présentant des limitations identi- ques, nous avons cherché à implémenter sur le robot mobile de téléopéra- tion ce même type de comportement. Le résultat escompté était une amélioration de la rapidité et de la régularité des trajectoires du robot, à l'instar de la locomotion humaine appuyée par les propriétés prédictives du cerveau. Pour cela, une analogie a été effectuée entre la direction du regard humain et la caméra mobile qui équipe le robot. Au vu de l'architecture fonctionnelle de notre système (sur laquelle on reviendra plus en détail dans la section suivante), il s'offre deux possibilités d'implémenter une anticipation visuelle sur le déplacement : soit a) par automatisation du mouvement anticipatoire de la caméra en fonction des commandes de navigation que l'opérateur envoie au robot soit, inversement b) par auto- matisation de la navigation du robot à partir des commandes que l'opérateur envoie à la caméra. Par évaluation expérimentale de ces modes de commande, cette étude va chercher à identifier l'interfaçage offrant la meilleure compatibilité homme-machine pour une tâche de conduite à distance d'un engin mobile. En effet, la question centrale est de savoir, à partir des deux sens de couplage caméra-base mobile, si l'opérateur doit commander la caméra qui contrôle la base mobile, ou bien si l'opérateur doit commander la base mobile qui contrôle la caméra. Enfin, l'ensemble de ces résultats sera discuté au regard des systèmes de coopération homme-machine privilégiant une approche non automatisée. Les hypothèses de notre travail expérimental étaient donc les suivan- tes. Premièrement, une situation dans laquelle la caméra est mobile et pointe vers la trajectoire future de l'engin téléopéré devrait conduire à de meilleures performances (en termes de contrôle de la trajectoire) qu'une situation dans laquelle la caméra est fixe et pointe toujours dans l'axe de l'engin. Deuxièmement, par analogie avec les travaux évoqués ci-dessus, une situation dans laquelle la caméra « anticipe » temporellement les changements d'orientation de l'engin devrait aussi conduire à une meil- leure performance. C'est donc dans la situation où l'opérateur contrôle la caméra, qui elle-même commande la « motricité » de l'engin, que le con- trôle de la trajectoire devrait être optimalisé. II. MATÉRIEL EXPÉRIMENTAL : LE SYSTÈME TÉLÉROBOTIQUE ARPH ARPH (Assistance robotique pour personnes handicapées) est un sys- tème télérobotique d'assistance aux personnes handicapées. Il vise à pal- lier, au moins partiellement, des incapacités de déplacement et de saisie</page><page sequence="8">216 F. Rybarczyk, D. Mestre, P. Hoppenot et E. Colle pour une personne handicapée physique. Il est constitué d'une plate- forme mobile portant un bras manipulateur. La plate-forme, d'une dimension de 0,4 m de large, est mue par deux roues motrices indépen- dantes, une roue folle à l'avant stabilisant le véhicule. Les moteurs choisis, largement utilisés pour les fauteuils roulants électriques dans un souci d'homogénéisation avec le matériel existant, sont inclus dans le sys- tème DX. Il s'agit d'un réseau sur lequel l'ensemble des équipements est branché de telle sorte qu'ils soient commandables par une seule interface, classiquement un joystick. Un point d'entrée sur ce réseau existe par l'intermédiaire de la liaison parallèle d'un PC : le dxkey. Le robot est également équipé d'une caméra orientable en site et en azimut par l'intermédiaire d'une liaison série. Son champ optique hori- zontal est de 50° et son champ vertical de 38°. Ce capteur permet de retourner à l'opérateur une image de l'environnement dans lequel le robot évolue, sur un terminal de visualisation de 31 sur 23 cm. L'ensemble du système, moteurs et capteur, est piloté par un PC embarqué sur le robot. Ce dernier est relié à un PC de commande par l'intermédiaire d'une liaison tcp/ip HF. Une architecture client/serveurs structure la partie informa- tique. Les serveurs sont écrits en langage C, le client en Java. Dans la pré- sente expérimentation, deux modes de coopération homme-machine ont été développés : le modèle d'anticipation sur le déplacement par com- mande de la plate-forme mobile contre le modèle par commande de la caméra. À noter que pour ces deux modèles, la valeur des angles est tou- jours exprimée en fonction d'un référentiel relatif au robot (et non absolu sur l'environnement). II . 1 . Modèle « plate-forme » Dans cette situation, l'opérateur ne contrôle qu'indirectement l'orien- tation de la caméra, en fonction de la commande de navigation directe qu'il envoie au robot. Ainsi, la personne n'aura qu'une maîtrise relative sur la direction de son regard, celui-ci ne s'orientant que de manière réflexe suivant la commande locomotrice. À partir de l'analogie réalisée entre l'œil humain et la caméra mobile du robot, le mouvement de cette dernière a été automatisé de manière à ce qu'elle s'oriente toujours en direction du point de tangente à la trajectoire interne de déplacement, c'est-à-dire à l'endroit même où les travaux neuroscientifiques montrent que l'information visuelle est la plus pertinente pour guider la locomotion. C'est ce qui est illustré par la figure 1 qui représente une vue de dessus schématique de la plate-forme mobile se déplaçant suivant le « modèle plate-forme ». Dans notre architecture robotique, l'angle de navigation du robot est défini par le ratio entre la vitesse linéaire et la vitesse angulaire que l'opérateur envoie aux moteurs du véhicule. Ainsi, le système informa- tique peut calculer en ligne le rayon de courbure (r) de la trajectoire du robot, à partir du rapport de la vitesse linéaire (v) sur la vitesse angulaire (w) de déplacement. Par la suite, l'angle de direction de la caméra (à) est obtenu à partir de ce rayon de courbure (r) et en utilisant les propriétés</page><page sequence="9">Anticipation visuo-motrice en téléopération 217 axe du robot axe de la caméra ^^y^ " trajectoire s^ /^** - " du robot / / r^- point de I / m** V """ tangente 4' ► r &lt; ► L a = arccos(l-((L/2)/r)) Fig. 1 . - Principe de la modélisation « plate-forme ». L'angle de rotation de la caméra est calculé à partir du rayon de courbure (r) de la trajectoire du robot, par utilisation des règles trigonométriques. Ici, cos a = (r - (L/2))/r, où IV2 correspond à la demi-largeur du robot. Le rayon (r) est obtenu par le rapport de la vitesse de translation sur la vitesse de rotation du robot. Principle of« platform » modelling. The camera's rotation angle is computed by the curve radius (r) of the robot's trajectory, using mgonometry. Here, cos a = (r(LI2))lr, where the width of the robot equals L. The radius (r) is obtained by dividing the translation speed by the rotation speed of the robot. trigonométriques. Cet angle (α) correspond donc à l'angle compris entre Taxe de l'avant du véhicule et l'axe de direction de la caméra. Ici, cos a = (r- (L/2))/r, où 172 correspond à la demi-largeur du robot (dans le cas de notre plate-forme L/2 = 0,2 m), d'où la modi- fication automatique d'orientation de la caméra suivant l'équation : a = arc cos (1 - ((L/2)/r)). II. 2. Modèle «caméra» Cette seconde modélisation de l'anticipation visuelle est construite suivant un raisonnement inverse au modèle précédent. Ici, c'est l'axe d'orientation de la caméra qui va déterminer la trajectoire du robot. L'opérateur va contrôler directement les mouvements de caméra, alors que l'angle de navigation du robot ne sera inféré qu'à partir de la direc-</page><page sequence="10">218 y. Rybarczyk, D. Mestre, P. Hoppenot et E. Colle tion de celle-ci. Ainsi, à l'inverse du modèle « plate-forme », la vision est maintenant activement contrôlée par l'homme et le déplacement du robot est automatisé. En pratique, le modèle est inspiré du comporte- ment d'anticipation visuelle qui consiste à fixer un repère et à le conser- ver dans son champ visuel, afin de décrire une trajectoire idéale autour de lui (Land &amp; Lee, 1994 ; Murray, Reid, &amp; Davidson, 1997). Cette situation, tout en contraignant l'opérateur à conserver le repère de navi- gation dans le champ de la caméra, apporte à l'individu un contrôle visuel continuel sur le repère le plus proche du robot, limitant ainsi les risques de collisions. La figure 2 schématise, vue d'en haut, la représen- tation géométrique de ce contournement de repère par le robot. axe de la axe *™u du directiond«la caméra axe *™u du trajectoire du robot robot * I /D(t) 'z{xy S(t) = a(t)-arc sin (R/D(t)) Fig. 2. - Principe de la modélisation « caméra ». L'angle de navigation du robot (S) est défini par la différence entre l'angle (à), compris entre la direction de la caméra celle de l'avant du robot, et de l'angle (#), compris entre la direction de la caméra et un axe imaginaire tangent à l'orbite de sécurité (R). Cet angle (z) est calculé par règles trigonométriques telles que sin z(t) = R/D(r). D, la distance entre le robot et le repère de navigation, est obtenue par la vitesse linéaire (v) du véhicule rapportée au taux de variation d'angle de la caméra {à), tel que D = (v/(da/dt)) sin a. Principle of « camera » modelling. The robot's navigation angle (S) is defined by the difference between the angle (a), included between the camera's direction and the heading of the robot, and the angle (z), included between the camera's direction and an imaginary axis tangent to the orbit of safety (R). This angle ζ is calculated by using trigonometry in a way that sin z(t) = RID(t). D, the distance between the robot and the landmark, is obtained by the ratio of the rate of change of the camera angle to speed, such as D = (vl(daldt)) sin a.</page><page sequence="11">Anticipation visuo-motrice en téléopération 219 Selon ce modèle, si l'opérateur pointe la caméra toujours en direction du repère de navigation, le véhicule doit le contourner en respectant une orbite de sécurité R. L'angle de navigation du robot (S) est défini par la différence entre l'angle (&lt;z), compris entre l'axe de la caméra et l'axe avant du robot, et l'angle (2), compris entre l'axe de la caméra et un axe imagi- naire tangent à l'orbite de sécurité. Cet angle ζ est obtenu par relations trigonométriques de telle manière que sin z(i) = R/D(r), où D correspond à la distance entre le robot et le repère. C'est précisément au niveau de la détermination de la valeur D que se traduit le niveau de coopération entre l'homme et la machine. C'est l'humain qui détermine D à travers son pilotage de la caméra en azimut. En effet, D est obtenu par la vitesse linéaire (v) du véhicule rapportée au taux de variation d'angle de la caméra (a), tel que D = (ν/ (da/ dt)) sin a. Au final, la trajectoire de la base mobile est recalculée à chaque itération selon la formule suivante : S(î) = a(t) - arc sin (R/D(r)). Les premiers résultats en simulation montrent qu'un véhicule équipé d'une telle équation, semblerait parfaitement capable d'enrouler un repère tout en restant à équidistance de lui. Il reste maintenant à tester la validité de cette modélisation en condition de téléopération, compte tenu du bruit lié à l'interaction avec une entité humaine devant rediriger la caméra au fur et à mesure de la progression du robot. En effet, à la diffé- rence d'un mode totalement automatisé où l'on peut implémenter des algorithmes de vision artificielle permettant à la caméra de fixer précisé- ment le repère de navigation lors du déplacement du robot, l'opérateur ne satisfera que grossièrement cette condition. En revanche, deux pro- priétés liées à un mode de contrôle semi-automatique pourront concourir à compenser, dans une certaine mesure, ce défaut de précision. Premiè- rement, un opérateur humain n'a pas la nécessité absolue de cibler par- faitement le repère qu'il doit contourner. Au contraire, ses capacités d'adaptation sont telles qu'elles lui permettent d'ajuster successivement les rotations de caméra. Deuxièmement, via une propriété davantage matérielle, par le fait que la largeur du champ optique horizontal de la caméra (50°) permet à l'opérateur une certaine marge d'erreur dans le ciblage du repère, sans que celui-ci ne sorte du champ visuel de la caméra. C'est d'ailleurs, grâce à la largeur du champ de caméra et à la dynamique lente de rotation, qu'il n'a pas été nécessaire d'implémenter l'équivalent d'un réflexe vestibulo-oculaire dans l'algorithme de pilotage (c'est-à-dire un mouvement compensatoire de caméra opposé au mouve- ment de rotation de la base). L'opérateur peut par exemple se contenter de visualiser le repère de navigation sur les extrémités de son terminal de visualisation, la rotation de la base n'engendrant ainsi qu'un déplacement du repère de cette position latérale à une position plus centrale sur l'écran. Ensuite, l'opérateur n'a plus qu'à laisser dériver l'image du repère, par déplacement de la base, puis refixer, par commande de la caméra, ce dernier en périphérie de l'écran quand celui-ci en sort et ainsi de suite.</page><page sequence="12">220 F. Rybarczyky D. Mestre, P. Hoppenot et E. Colle III. PROTOCOLE EXPÉRIMENTAL ULI. Sujets Une population de 21 étudiants ou membres du laboratoire âgés entre 25 et 35 ans a participé à l'ensemble des quatre sessions expérimen- tales. Trois groupes indépendants de sept sujets ont effectué chacun l'une des deux conditions expérimentales. Ils possédaient tous une vision nor- male ou utilisaient des verres correcteurs leur assurant une bonne vision. Tous étaient naïfs quant au but de l'expérimentation. L'ensemble de l'expérience durait une heure environ pour chaque sujet. 111. 2. Conditions II y a avait trois conditions expérimentales différentes de vision. Une condition de vision fixe, pour laquelle la caméra restait immobilisée droit devant dans le sens de marche du robot et deux conditions de vision mobile d'anticipation, l'une où la caméra était mobilisée selon le « modèle plate-forme » et l'autre où les mouvements de caméra étaient commandés selon le « modèle caméra ». 111. 3. Procédure Sur l'ensemble de l'expérience, tous les sujets étaient placés en situa- tion de téléopération, c'est-à-dire qu'ils n'avaient qu'une vision indirecte, via un terminal de visualisation, de l'environnement où évoluait le robot. Dans un premier temps, les sujets effectuaient une période d'entraî- nement afin de se familiariser à la situation de téléopération et au mode de commande du robot. Suivant la condition expérimentale, cette première phase leur permettait également d'apprécier les avantages et les limites de leur angle de vue. Les conditions d'entraînements imposées aux sujets étaient identiques pour chaque groupe, et consistaient à faire « slalomer » le robot entre des balises. L'entraînement était interrompu lorsque le sujet (a) se sentait à l'aise (b) était capable de combiner translation et rotation de véhicule pour réaliser des trajectoires curvilinéaires, et (c) qu'il limitait son nombre de collisions avec les repères. L'ensemble des sujets est rapidement parvenu à remplir ces critères d'apprentissage. Cette phase d'entraînement a duré en moyenne trente minutes par sujet. Dans un deuxième temps, les sujets étaient confrontés à la phase de test. Leur tâche consistait à faire exécuter au robot un parcours de « sla- lom » entre quatre balises. Celles-ci étaient organisées de telle manière que le robot devait effectuer plusieurs types de virages compris entre 20 et 180°. Les dimensions générales du parcours s'étendaient sur une super- ficie de 4,50 m de long sur 3,90 m de large. L'espace entre chaque balise était quant à lui de 0,9 m. Le trajet était effectué une fois dans un sens et</page><page sequence="13">Anticipation visuo-motrice en téléopération 221 une fois dans le sens opposé d'une session à l'autre, afin d'éviter que l'opérateur ne développe trop facilement une stratégie de navigation sté- réotypée. La consigne donnée au sujet était de réaliser le parcours le plus rapidement possible tout en évitant les collisions. Sur chacune des ses- sions, la performance a été évaluée à partir du temps d'exécution du tra- jet, du nombre d'arrêts, du nombre de collisions avec les repères et d'un indice permettant d'estimer la régularité des trajectoires. L'analyse statis- tique utilisée pour le traitement des résultats est une anova. IV. ANALYSE DES RÉSULTATS IV. 1. Temps D'exécution du parcours Tout d'abord, on observe une diminution significative du temps néces- saire pour effectuer le parcours au cours des sessions [F(3;54) = 5,57 ; ρ &lt; .002]. Cet effet de session montre que les opérateurs améliorent natu- rellement leur performance au cours du temps, en particulier au niveau des premières sessions de test (fig. 3). Mais surtout, les sujets s'avèrent significativement plus rapides en condition de vision mobile (modèle caméra et modèle plate-forme) qu'en condition de vision fixe [F(2;18) = 7,28 ; ρ &lt; .005]. En effet, le temps d'exécution du parcours est beaucoup plus court lorsque l'opérateur peut Fig. 3. - Temps moyen d'exécution du parcours pour les trois conditions de contrôle du robot au cours des quatre sessions expérimentales. Ce temps est significativement plus court lorsque la caméra anticipe sur le déplacement que lorsque qu'elle reste immobilisée dans Taxe de l'avant du véhicule (p &lt; .005). The average travel time for the three conditions of control modes, over the course of the four experimental sessions. The time taken with an anncipatory camera is less than that taken with a fixed camera (p &lt; .005).</page><page sequence="14">222 Y. Rybarczyky D. Mestre, P. Hoppenot et £. Colle bénéficier d'un angle de caméra qui anticipe sur le déplacement (175 s, pour le modèle plate-forme ; 154 s, pour le modèle caméra) que lorsque celle-ci reste dans une position fixe (254 s). À remarquer que cette différence significative se maintient au cours des sessions [F(l;12) = 6,03 ; ρ &lt; .03, à la 4e session], ce qui tendrait à prouver que l'apprentissage à lui seul ne parviendrait pas à combler l'infériorité de la vision fixe par rapport à la vision mobile. En revanche, il n'y a aucune différence significative entre les deux conditions de vision mobile [F(l;12) = 0,28 ; NS'. IV. 2. Nombre d'arrêts À la différence du paramètre précédent, malgré la présence d'une ten- dance générale de diminution de la quantité d'arrêts au cours des ses- sions, en particulier entre la première et la deuxième session (fig. 4), celle- ci n'apparaît pas comme étant significative [F(3;54) = 2,27 ; NS'. Des arguments en termes de surentraînement des sujets avant la phase de test peuvent expliquer cette absence d'effet de session sur le nombre d'arrêts. En effet, une des conditions pour valider la phase d'apprentissage étant que l'opérateur soit capable de combiner translation et rotation, pour réa- liser des trajectoires curvilinéaires, a fortement réduit sa propension à stopper le véhicule. En ce qui concerne la comparaison principale qui oppose les visions mobiles à la vision fixe, on retrouve une différence largement significative en faveur de la condition de vision mobile [F(2;18) = 20,78 ; ρ &lt; .00001]. Fig. 4. - Nombre moyen d'arrêts pour les trois conditions de mode de contrôle au cours des quatre sessions expérimentales. Il y a significativement moins d'arrêts dans les deux conditions anticipatoires que dans la condition vision fixe (p &lt; .00001). The average number of stops for the three conditions of control modes, over the course of the four experimental sessions. There are fewer stops in the two anticipatory conditions than in the fixed condition (p&lt;. 00001).</page><page sequence="15">Anticipation visuo-motnce en téléopération 223 Ainsi, lorsque la caméra anticipe visuellement sur les courbes à venir, l'opérateur parvient d'autant plus facilement à faire effectuer au robot des trajectoires curvilignes, ce qui a pour effet de diminuer drastiquement le nombre d'arrêts du véhicule (le nombre moyen d'arrêts est de : 18,7, pour la condition fixe ; 2,5, pour la condition « plate-forme » ; 0,4, pour la condition « caméra »). Il est surtout important de remarquer, une fois de plus, que cette supé- riorité des visions mobiles au niveau de la régularité temporelle du dépla- cement résiste de manière significative jusqu'à la dernière session [F(1;12) = 26,78 ; ρ &lt; .0002], ce qui était l'idée de primauté de l'effet de condition sur l'effet d'entraînement. Il faut noter aussi la nette tendance du modèle « plate-forme » à produire davantage d'arrêts que le modèle « caméra » (nombre d'arrêts quasi nul), même si cette dernière ne mène pas à une différence significative [F(l;12) = 3,29 ; ρ &lt; .10]. IV. 3. Nombre de collisions Ce paramètre, présentant certains résultats différents par rapport aux analyses précédentes, vient apporter quelques nuances. Premièrement, on ne constate également pas d'effet de session [F(3;54) = 0,63 ; NS], Cela s'explique par le fait que la condition de vision fixe et la condition « plate- forme » décrivent des patterns de performance opposés au cours des ses- sions (fig. 5), ce qui aboutit à un effet d'interaction significatif entre ces deux conditions et les sessions [F(3;36) = 3,15 ; ρ &lt; .05]. Fig. 5. - Nombre moyen de collisions pour les trois conditions de contrôle du robot, au cours des quatre sessions expérimentales. Seule la condition d'anticipation visuelle suivant le modèle « caméra » assure un nombre de collisions significativement moins grand en comparaison avec le mode vision fixe (p &lt; .04). The average number of collisions for the three conditions of control modes, over the course of the four experimental sessions. Only the visual anticipation of the camera model gives a number of collisions that is significantly lower in comparison with the fixed mode (p &lt; .04).</page><page sequence="16">224 Y. Rybarczyk, D. Mestre, P. Hoppenot et E. Colle Ainsi, le nombre de collisions se réduit avec les sessions, surtout entre la première et la deuxième, pour la condition caméra fixe. Ce résultat indique que l'opérateur utilise des indices appris dans l'environnement pour en inférer la position relative des repères par rapport au robot. Au contraire, il y a une légère tendance [F(3;39)=l,l ; NS] à une augmenta- tion du nombre de ces collisions au fil des sessions, surtout entre les deux premières [F(l;13) = 3,06 ; p&lt;.10], pour les conditions de vision mobile. Ce phénomène est la possible traduction de la relation inverse existant entre la vitesse et la précision du déplacement. Deuxièmement, le nombre de collisions étant plus faible en condition de caméra mobile (0,5, pour le modèle plate-forme ; 0,4, pour le modèle caméra) qu'en caméra fixe (1,3 contacts), on observe encore une différence significative sur l'ensemble des trois conditions de vision [F(2;18) = 4,10 ; ρ &lt; .03]. Cependant, il est important de noter que con- trairement aux autres paramètres, le nombre de collisions avec les repères n'est pas significativement différent entre la condition fixe et la condition « plate-forme » [F(l;12) = 4,32 ; NS]. Cela signifie que les indices appris dans l'environnement par le sujet lui suffisent à réaliser une performance d'évitement d'obstacles aussi bonne que lorsque l'opérateur bénéficie d'un balayage de caméra suivant un modèle « plate-forme ». En fait, seul le modèle « caméra » produit signi- ficativement moins de collisions que la condition fixe [F(l;12) = 5,52 ; ρ &lt; .04]. De plus, cette différence significative persiste encore à la troi- sième session expérimentale [F(l;12) = 9,82 ; ρ &lt; .01], montrant une plus grande régularité dans l'amélioration de la performance globale sui- vant un modèle de vision mobile « caméra ». IV. 4. Lissage des trajectoires Lorsqu'on observe les trajectoires décrites par le robot, on constate que leur caractère segmenté correspond au fait que les phases de change- ment de direction s'opèrent quand le véhicule est à l'arrêt. Inversement, leur caractère lissé correspond donc à une diminution de ces phases où la vitesse de rotation est élevée alors que la vitesse linéaire est nulle. À ce sujet, on a pu remarquer que les opérateurs placés en condition de vision mobile, ayant un nombre d'arrêts réduit, semblent effectuer des trajec- toires plus lissées que ceux du groupe de la vision fixe. Une manière de quantifier ce lissage est alors de calculer le rayon de courbure instantané de chaque trajectoire, et d'évaluer la distribution fré- quentielle de ce rayon au cours de chacune des trajectoires (Péruch &amp; Mestre, 1999). Concrètement, le rayon de courbure (r) est calculé comme le rapport de la vitesse linéaire instantanée (v) sur la vitesse de rotation instantanée (w), selon la formule suivante : , N v(m I sec) r(m) , N = ^ - . w( radians I sec) Par la suite, le rayon de courbure est converti en logarithme décimal. Ainsi, si le robot a une vitesse linéaire faible et une vitesse de rotation</page><page sequence="17">Anticipation visuo-motnce en téléopération 225 élevée, le rayon de courbure sera très petit (&lt; 1), d'autant plus petit que la vitesse de rotation sera importante. La valeur logarithmique de r sera alors négative. Inversement, si le véhicule avance et tourne en même temps (trajectoire curvilinéaire), le rayon de courbure sera très grand (s&gt; 1) et son logarithme supérieur ou égal à 0. Une trajectoire dans laquelle le sujet s'arrête pour prendre les virages donne lieu à une distribution bimodale des rayons de courbure, avec un pic de distribution centré sur des valeurs négatives du logarithme du rayon et un autre pic centré sur des valeurs positives ou nulles de ce loga- rithme. Au contraire, une trajectoire essentiellement curvilinéaire corres- pond à une distribution unimodale centrée sur une valeur supérieure ou égale à 0 du logarithme de rayon de courbure. Pour chaque trajectoire, la distribution du logarithme de rayon de courbure a été calculée et répartie en 15 catégories. Ces catégories corres- pondent à des valeurs comprises entre - 4 et - 3,5, - 3,5 et - 3, - 3 et - 2,5, ..., 2,5 et 5, c'est-à-dire suivant une échelle d'intervalle constante permettant d'analyser les résultats à partir d'un test d'ANOVA. Enfin, les Fig. 6. - Distribution moyenne des (logarithmes des) rayons de courbure, exprimée en pour- centage du nombre total d'occurrences, en fonction des sessions expérimentales, pour l'ensemble des trois conditions expérimentales. Le plus grand pic (centré sur une valeur de 0) correspond aux grandes courbes de trajectoire du robot, alors que le plus petit pic (centré des valeurs de - 2,5 et - 2) correspond à des virages effectués avec une très petite vitesse linéaire. Average distribution of (logarithms of) curve radius ; expressed as a percentage of the total number of occurrences y following the sessions, and for all the three experimental conditions. The higher peak (centred on a value ofO) corresponds to the big curves of robot trajectory, whereas the low peak (centred on values of -2,5 and - 2) corresponds to bends taken using a very low linear speed.</page><page sequence="18">226 y. Rybarczyk, D. Mestre, P. Hoppenot et E. Colle distributions ont été normalisées, en exprimant les occurrences de rayon de courbure dans chaque catégorie comme un pourcentage du nombre total d'occurrences pour chaque trajectoire. Les premiers résultats montrent un effet significatif du facteur caté- gorie [F(14;252) = 215,63 ; ρ &lt; .00001], indiquant que la distribution des rayons de courbure n'est pas monotone. On observe, en effet, deux pics principaux (fig. 6). Un grand pic centré sur des valeurs du logarithme du rayon de courbure égales à 0, correspondant à des valeurs de rayons de courbure aux alentours de 1 m. Ce pic correspond à des trajectoires curvi- linéaires, dont le rayon de courbure est lié aux caractéristiques de l'espace expérimental ainsi qu'aux propriétés dynamiques du véhicule utilisé. Un deuxième pic de distribution se situe dans les valeurs négatives (catégories - 2,5 et 2, sur les figures). Il correspond à des situations dans lesquelles le sujet exécute une rotation avec une vitesse linéaire faible, voire nulle, afin de faire prendre les virages au robot. On note également une légère montée pour des valeurs de logarithme très élevées (catégories 1 et 1,5), pour lesquelles il ne s'agit plus de courbes mais de lignes droites, à l'échelle du véhicule. C'est pour cette raison que ces dernières données n'ont pas été soumises à plus d'analyses. Deuxièmement, il y a un effet significatif de session [F(3;54) = 340,60 ; ρ &lt; .00001], ainsi que d'interaction entre le facteur catégorie et le facteur session [F(42;756) = 1,91 ; ρ &lt; .0006]. Au cours des sessions, les catégories - 2,5 et - 2 passent de 3 % à 1 % d'occurrences, pendant que la catégorie 0 passe de 35 % à 39 % d'occurrences. Ces deux premiers effets massifs signifient qu'au fil de l'apprentissage (de la session 1 à la session 4, voir fig. 6), le sujet tourne de moins en moins sur place (le pic de distribution des catégories - 2,5 et - 2 s'effondre) alors que la curvilinéarité de la trajectoire s'accentue (le pic de distribution de la catégorie 0 s'élève). On peut donc en déduire que l'apprentissage de la tâche conduit à un lissage des trajectoires. Troisièmement, on observe également un effet d'interaction significa- tif entre la condition de vision et le facteur catégorie [F(28;252) = 21,28 ; p&lt; .00001]. En effet, la figure 7 montre que le pourcentage d'occur- rences des petits et grands rayons de courbure n'est pas le même selon la condition visuelle. Le grand pic, correspondant aux trajectoires curvili- néaires, est significativement plus élevé en visions mobiles qu'en vision fixe [F(2;18) = 28,61 ; ρ &lt; .00001]. Inversement, le petit pic, correspon- dant aux rotations sur place, est significativement plus bas en visions mobiles qu'en vision fixe [F(2;18) = 13,35 ; ρ &lt; .0003]. De plus, il est à noter que ces différences significatives entre les conditions sont préser- vées jusqu'à l'ultime session, aussi bien pour les trajectoires curvili- néaires [F(2;18) = 14,30 ; ρ &lt; .0002] que pour les rotations sur place [F(2;18) = 16,98; p&lt;.0001]. Cela signifie que l'anticipation visuelle apportée par une vision mobile assure une plus grande fluidité dans l'enchaînement des virages, que même un apprentissage en vision fixe ne peut égaler. Enfin, lorsqu'on compare les deux modèles de vision mobile entre eux, on constate également un effet d'interaction entre le facteur catégorie et la condition de vision [F(14;168) = 19,88 ; p&lt;. 00001]. En effet, on</page><page sequence="19">Anticipation visuo-motrice en téléopération 227 remarque toujours sur la figure 7, que la répartition entre les grands et les petits rayons de courbure n'est pas la même suivant le modèle « plate- forme » ou le modèle « caméra ». Ainsi, le grand pic est significativement plus élevé en condition « caméra » qu'en condition « plate-forme » [F(l;12) = 21,10 ; ρ &lt; .0006], cette différence se maintenant signifi- cative jusqu'à l'avant-dernière session expérimentale [F(l;12) = 36,88 ; ρ &lt; .0001]. Alors que les rotations sur place sont significativement plus nombreuses en condition « plate-forme » que « caméra » [F(l;12) = 5,63 ; /&gt;&lt;.O4]. Cette ultime différence entre les deux conditions de vision Fig. 7. - Distribution moyenne des (logarithmes des) rayons de courbure, exprimée en pour- centage du nombre, total d'occurrences, en fonction des trois conditions expérimentales de vision. Le pourcentage d'occurrence des petits rayons de courbure est significativement plus bas (p &lt; .04) et celui des grands rayons de courbure significativement plus haut (p &lt; .0006) dans le mode de contrôle suivant le modèle « caméra », en comparaison avec les deux autres modes de contrôle du robot. Average distribution of (logarithms of) curve radius, expressed as a percentage of the total number of occurrences, following the three experimental conditions of vision. The percentage of occurrences of the small curve radius is significantly lower (p &lt; . 04) and that of the big curve radius significantly higher (p &lt; . 0006) in the « camera » mode of control, as compared with the two other modes of control of the robot.</page><page sequence="20">228 Y. Rybarczyk, Ζλ Mestre, P. Hoppenot et E. Colle mobile, associée à celle du paramètre du nombre de collisions, tend à montrer qu'un contrôle de l'anticipation visuelle sur le déplacement sui- vant un interfaçage par commande directe de la caméra assure une meil- leure compatibilité homme-machine. V. DISCUSSION Cette expérience a visé à étudier les conséquences de l'implémentation du comportement humain d'anticipation visuelle, suivant deux modèles de couplage vision-locomotion, sur la performance de navigation. Pour cela, deux conditions de vision mobile (le modèle « plate-forme » contre le modèle « caméra ») ont été comparées à une condition de vision fixe au cours de quatre sessions expérimentales successives. Le modèle « plate- forme » consistait à contrôler le déplacement du robot par commande directe de la plate-forme mobile, l'orientation de la caméra étant asservie à l'angle de navigation de cette plate-forme à partir d'un algorithme de contrôle. À l'inverse, dans le modèle « caméra », la commande du robot était assurée par un contrôle direct de la caméra et c'est la trajectoire de la base mobile qui était asservie à la direction de la caméra. Pour l'ensemble des conditions, une tendance générale a été observée quant à l'amé- lioration de la performance au fil des sessions (principalement entre les deux premières sessions), même si celle-ci ne s'est avérée statiquement significative qu'au niveau du temps d'exécution du parcours et du lissage des trajectoires. Ce premier résultat témoigne donc d'une intégration rela- tivement rapide, de la part du sujet, des propriétés dynamiques du véhi- cule téléopéré. Le résultat majeur de cette expérience est de révéler une différence significative, sur la totalité des paramètres analysés (temps de parcours, nombre d'arrêts, nombre de collisions et lissage des trajectoires), entre les conditions de vision. Ainsi, un téléopérateur qui bénéficie d'une caméra mobile va voir fortement diminuer son temps de déplacement, ses phases d'immobilisation, ses contacts avec l'environnement expérimental et va présenter des trajectoires plus régulières (plus lissées). Les raisons de cet avantage proviennent de la possibilité d'anticipation visuelle sur l'angle de navigation, couplée à une augmentation du champ visuel fonctionnel, confirmant de ce fait l'importance de la vision périphérique (Johansson &amp; Börjesson, 1989 ; Warren et al, 1991). En effet, on a pu enregistrer que les opérateurs placés dans les deux conditions de vision mobile exploitent largement les avantages que leur offre cette situation, en allant chercher des angles visuels bien plus conséquents (supérieurs à l 'hémi-champ de la caméra à partir de 2 m de rayon de courbure, pour le modèle « plate- forme », et de 0,85 m, pour le modèle « caméra ») que ce que leur fournit une simple vision statique (ici, seulement 25°, si l'on considère que le centre d'attention de l'opérateur se situe plutôt dans l'hémi-champ de l'écran du côté duquel il fait virer le robot). Du reste, on peut noter que l'amélioration de la performance au cours des sessions est légèrement plus marquée dans la condition vision fixe.</page><page sequence="21">Anticipation visuo-motrice en téléopération 229 Cet effet, plus particulièrement présent sur le paramètre du nombre de collisions, peut s'expliquer par le fait que, dans cette condition, les sujets acquièrent une plus grande représentation d'indices environnementaux pour compenser leur absence de perception des repères de navigation, cela au prix d'une élévation de leur charge de travail mental (Péruch, Ver- cher, &amp; Gauthier, 1996). Aussi, cette acquisition ne parvient néanmoins pas à égaler l'avantage naturel apporté par le comportement anticipatoire. Ce fait confirme les raisons pour lesquelles cette stratégie a été mise en place et conservée au cours de l'évolution (Berthoz, 1997), afin d'optimiser les mouvements de préhension (Land et aly 1999), de loco- motion (Grasso et al, 1996) ou de conduite automobile (Land &amp; Lee, 1994). Cette persistance, au fil des sessions, du bénéfice d'une vision mobile sur une vision fixe va à rencontre des résultats de Péruch et Mestre (1999), pour lesquels la supériorité de la vision mobile n'apparaissait qu'aux premières sessions. Aussi, leur condition expérimentale se diffé- renciait, entre autres aspects méthodologiques, de la nôtre par le fait que le sujet se trouvait dans un mode de contrôle du véhicule entièrement non automatisé, autant au niveau de la commande locomotrice que de la commande visuelle. Ainsi, la limitation de performance causée par l'élévation de la charge de travail mental d'une telle situation contribue à ce qu'une simple vision solidaire de sa base parvienne à des niveaux de performance comparable, grâce à l'acquisition d'indices appris au fil des sessions. Au contraire, dans la conception de notre système de coopération homme-machine, nous avons obéi à un principe de commande robotique, dit partagé. Dans ce modèle de coopération, les opérations de haut niveau cognitif sont réalisées par l'homme et celles de bas niveau cognitif par la machine, limitant ainsi les risques de surcharge mentale. Le réflexe d'anticipation visuelle sur le mouvement relevant de ce bas niveau, a pu être facilement implémentable sur le robot et efficacement utilisable dans le cadre d'un mode de commande semi-automatique. Cela dit, il semble qu'entre les deux modèles d'anticipation visuelle sur le mouvement, l'un paraisse encore plus efficace que l'autre. Pour le paramètre du nombre de collisions par exemple, seul le modèle où l'opérateur a un contrôle direct sur la vision (modèle « caméra ») fournit une performance significativement plus élevée que la condition de vision fixe. Cette différence entre les deux modes de vision mobile pourrait s'interpréter, en première approche, par le fait que le modèle « caméra » a davantage été conçu dans l'optique d'un contournement de repères que le modèle « plate-forme », ce dernier étant plus destiné à la réalisation de tra- jectoires courbes. Cependant, même si cet aspect technique a certaine- ment influencé ce résultat, il n'explique pour autant pas la totalité de la différence. En effet, la similarité entre le pattern de réponse visuo-moteur suite à la réalisation d'une trajectoire courbe (Grasso et al&gt; 1996) et celui suite à un contournement de repères (Grasso et al, 1998), montre que ces deux types d'actions ne sont pas si différentes. Ainsi, les deux modèles de vision mobile devant être aussi performants dans la tâche expérimentale, on peut en déduire que la supériorité du modèle « caméra » viendrait</page><page sequence="22">230 Y. Rybarczyk, D. Mestre, P. Hoppenot et E. Colle d'une plus grande compatibilité entre l'architecture temporelle du pro- gramme moteur de ce modèle et celle de l'homme. De même, bien que le pilotage d'un mobile équipé du modèle « plate- forme » soit significativement moins saccadé que celui d'un mobile à caméra fixe, de tels véhicules permettent un pilotage bien moins fluide que suivant un modèle « caméra ». Si, là encore, ce résultat est à moduler au vu de l'implémentation technique des deux modèles, et en particulier par le fait que le modèle « caméra » est conçu de telle sorte à limiter les petits rayons de courbure, ce phénomène n'explique pas la supériorité massive des grands rayons de courbure dans la condition « caméra » par rapport à la condition « plate-forme ». Cette dernière différence semble davantage s'interpréter par le fait que lorsque l'opérateur a un contrôle direct sur la direction de son axe visuel (modèle « caméra »), il bénéficie d'une meilleure anticipation sur la localisation du repère à venir, ce qui réduit sa nécessité à immobiliser le véhicule, tourner sur place pour explo- rer l'environnement et effectuer un trajet rectiligne jusqu'à un point stra- tégique du parcours. On comprend bien, en effet, que ce dernier pattern de déplacement va considérablement limiter la réalisation de grands rayons de courbure (catégorie 0 sur la figure 7), sa représentation la plus caricaturale étant la condition « fixe ». De ce fait, une interprétation neuroscientifique peut expliquer une partie de cette supériorité du couplage regard-locomotion dans le sens caméra vers plate-forme. En effet, on sait que lors de l'exécution du mou- vement en situation naturelle, la fovéalisation de la cible tout au long du déroulement de l'action entraîne une optimisation des ajustements moteurs se traduisant par une réduction de la variabilité spatiale des tra- jectoires (Prablanc, Pélisson, &amp; Goodale, 1986). Ainsi, la tête, portant les yeux, est utilisée comme une centrale inertielle de guidage qui est stabi- lisée dans l'espace et à partir de laquelle le mouvement du corps est coor- donné (Patla, Adkin, &amp; Ballard, 1999). Le fait que le reste du corps se contente de suivre la direction indiquée par l'orientation volontaire du regard est une explication sur la plus grande compatibilité homme- machine observée dans le modèle « caméra », les études ergonomiques montrant que les modes de commande semi-automatiques les plus effi- caces sont ceux où le contrôle de haut niveau est laissé à la volonté de l'opérateur humain (Endsley &amp; Kaber, 1999 ; Parasuraman, Masalonis, &amp; Hancock, 2000). En conclusion, l'ensemble de ce travail présente deux intérêts majeurs, l'un sur le plan de l'ingénierie, l'autre sur celui de l'ergonomie. Du point de vue de la conception technique, cette étude montre l'avantage d'entreprendre une approche bionique pour la réalisation d'un robot mobile téléopéré par une personne humaine. Ainsi, l'implémentation du comportement humain d'anticipation visuelle sur le mouvement a permis de réduire efficacement l'écart fonctionnel existant entre l'homme et la machine, dans le cadre précis du pilotage à distance d'un robot mobile. Aussi, ces résultats encourageants montrent qu'il serait intéressant d'appliquer cette même approche pour des tâches plus générales de navi- gation et d'exploration. Sur un plan plus ergonomique, on constate que plus Pinterfacage de contrôle du mobile est isomorphe au comportement</page><page sequence="23">Anticipation visuo-motrice en téléopération 231 humain, plus grande est la compatibilité homme-machine. En cela, nos résultats militent davantage en faveur d'une interprétation de l'orga- nisation temporelle de la commande motrice des mouvements depuis la tête jusqu'aux pieds (Patla et al.&gt; 1991), plutôt qu'en termes de com- mande simultanée sur l'ensemble du système nerveux, où seule la diffé- rence d'inertie entre les segments corporels permet un déplacement pré- coce de la tête (Biguer, Jeannerod, &amp; Prablanc, 1982). BIBLIOGRAPHIE Berthoz, A. (1997). Le sens du mouvement. Paris : Odile Jacob. Biguer, B., Jeannerod, M.3 &amp; Prablanc, C. (1982). The coordination of eye, head, and arm movements during reaching at a single visual target. Experimental Brain Research, 46, 301-304. Coello, Y., &amp; Grealy, M. A. (1997). Effect of size and frame of visual field on the accuracy of an aiming movement. Perception, 26, 287-300. Cornilleau-Péres, V., &amp; Gielen, C. C. A. M. (1996). Interactions between self- motion and depth perception in the processing of optic flow. Trends in Neuro- sciencesy 19, 196-202. Endsley, M. R., &amp; Kaber, D. B. (1999). Level of automation effects on perfor- mance, situation awareness and workload in dynamic control task. Ergonomics, 42, 462-492. Gibson, J. J. (1979). The Ecological Approach to Visual Perception. Boston, MA : Houghton Mifflin. Grasso, R., Glasauer, S., Takei, Y., &amp; Berthoz, A. (1996). The predictive brain : Anticipatory control of head direction for the steering of locomotion. NeuroRe- port, 7, 1170-1174. Grasso, R., Prévost, P., Ivanenko, Y. P., &amp; Berthoz, A. (1998). Eye-head coordi- nation for the steering of locomotion in humans : An anticipatory synergy. Neuroscience Letters, 253, 115-118. Gray Sue, V., &amp; Wilson, J. R. (1988). User Safety Requirements for Robot Safety, a Task Analysis Approach. Paper presented at the 10th Ergonomics International Association Symposium. Sydney, Australia, August. Grimbergen, K. A. (1997). Minimally invasive surgery : Human-machine aspects and engineering approaches. In T. B. Sheridan &amp; T. Van Lunteren (Eds.), Perspectives on the Human Controller. Essays in Honor of Henk, G. Stassen (pp. 223-231). Mahwah, NJ : Lawrence Erlbaum Associates, INC, Publishers. Hagen, Μ. Α., Jones, R. K., &amp; Reed, E. S. (1978). On a neglected variable in theories of pictorial perception : Truncation of visual field. Perception &amp; Psy- chophysics, 23, 326-330. Held, R., &amp; Durlach, N. (1993). Telepresence, time delay and adaptation. In S. R. Ellis, M. K. Kaiser, &amp; A. J. Grunwald (Eds.), Pictorial Communication in Virtual and Real Environments. New York : Taylor and Francis. Hightower, J. D., Spain, E. H.5 &amp; Bowles, R. W. (1987). Telepresence : A Hybrid Approach to High-Performance Robots. Paper presented at the Third International Conference on Advanced Robotics (ICAR'87). Seoul, Korea, May. Johansson, G., &amp; Börjesson, Ε. (1989). Toward a new theory of vision studies in wide-angle space perception. Ecological Psychology, L 301-331. Kazerooni, H. (1989). Theory and Experiments on Human-Robot Interaction via Transfer of Power and Information Signals. Paper presented at the IEEE Confe- rence on Robotas and Automation. Scottsdale, USA, December.</page><page sequence="24">232 Κ Rybarczyky D. Mestre, P. Hoppenot et E. Colle Lacquaniti, F., &amp; Maioli, C. (1987). Anticipatory and reflex coactivation of anta- gonist muscles in catching. Brain Research, 406, 373-378. Land, M. F. (1998). The visual control of steering. In L. R. Harris &amp; K. Jenkin (Eds.), Vision and Action (pp. 163-180). Cambridge, UK, Cambridge Univer- sity Press. Land, M. F., Mennie, N., &amp; Rusted, J. (1999). The roles of vision and eye move- ments in the control of activities of daily living. Perception, 28, 1311-1328. Land, M. F., &amp; Lee, D. N. (1994). Where we look when we steer ? Nature, 369, 742-744. Leibowitz, H. W. (1986). Recent advances in our understanding of peripheral vision and some implications. IEEE Transaction on Systems, Man and Cyberne- tics, 23, 183-193. Lui, Α., Tharp, G., French, L., Lai, S., &amp; Stark, L. (1993). Some of what one needs to know about head-mounted displays to improve teleoperator perfor- mance. IEEE Transaction on Robotics and Automation, 9, 638-648. Massimo, M., &amp; Sheridan, T. (1989). Variable force and visual feedback effects and teleoperator man/machine performance. Paper presented at the Nasa Conference on Space Telerobotics. Pasadena, USA, May. Mestre, D. (2001). Dynamic evaluation of the functional visual field in driving. Paper presented at the Conference on the Driving Assessment. Aspen, USA, August. Mestre, D., &amp; Péruch, P. (1995). Rapport final sur V experimentation « vision mobile » réalisée dans le cadre du contrat « Vision mobile et téléopération ». Marseille : Uni- versité d'Aix-Marseille II, Laboratoire de cognition et mouvement (ura CNRS 1166). Miller, D. P., &amp; McGovern, D. E. (1988). A laboratory-simulation approach to the evaluation of vision systems for teleoperated vehicles. Paper presented at the International Symposium on Teleoperation and Control San Francisco, CA, April. Murray, D. M., Reid, I. D., &amp; Davidson, A. J. (1997). Steering without represen- tation with the use of active fixation. Perception, 26, 1519-1528. Padmos, P., &amp; Van Erp, J. (1996). Driving with camera view. In A. G. Gale, I. D. Brown, C. M. Haslegrave, H. W. Kruysse, &amp; S. P. Taylor (Eds.), Vision in Vehicles /F(pp. 232-247). Amsterdam : North-Holland. Parasuraman, R., Masalonis, A. J., &amp; Hancock, P. A. (2000). Fuzzy signal detec- tion theory : Basic postulates and formulas for analysing human and machine nerformance. Human Factors. 42. 636-659. Patla, A. E., Adkin, Α., &amp; Ballard, T. (1999). Online steering : Coordination and control of body center of mass, head and body reorientation. Experimental Brain Research, 129, 629-634. Patla, A. E., Prentice, S. D., Robinson, C, &amp; Neufeld, J. (1991). Visual control of locomotion : Strategies for changing direction and for going over obstacles. Journal of Experimental Psychology: Human Perception and Performance, 17, 603-634. Pausch, R., Créa, T., &amp; Conway, M. (1992). A literature survey for virtual envi- ronments : Military flight simulator visual systems and simulator sickness. Pre- sence, 1, 344-363. Péruch, P., &amp; Mestre, D. (1999). Between desktop and head immersion : Func- tional visual field during vehicle control and navigation in virtual environ- ments. Presence, 8, 54-64. Péruch, P., Vercher, J. L, &amp; Gauthier G. M. (1996). Active and passive visual information in remote-controlled vehicles. In A. G. Gale, I. D. Brown, C. M. Haslegrave, H. W. Kruysse, &amp; S. P. Taylor (Eds.), Vision in Vehicles V (pp. 211-216). Amsterdam : Elsevier Science BV.</page><page sequence="25">Anticipation visuo-motrice en téléopération 233 Piaget, J. (1967). Biologie et Connaissance. Essai sur les relations entre les régulations organiques et les processus cognitifs. Paris : Gallimard. Prablanc, C, Pélisson, D., &amp; Goodale, M. A. (1986). Visual control of reaching movements without vision of the limb. I. Role of retinal feedback of target position in guiding the hand. Experimental Brain Research, 62, 293-302. Rabardel, P. (1993). Représentation dans des situations d'activités instrumentées. In A. Weill-Fassina, P. Rabardel, &amp; D. Dubois (Eds.), Représentation pour l'action (pp. 97-111). Toulouse : Octarès. Raviv, D., &amp; Herman, M. (1993). Visual servoing from 2-D image cues. In Y. Aloimonos (Ed.), Active Perception (pp. 191-226.). Hillsdale, NT : Erlbaum. Reinhardt-Rutland, A. H. (1996). Remote operation : A selective review of research into visual depth perception. The Journal of General Psychology, 123, 237-248. Smith, T., &amp; Smith, K. (1990). Human factors of workstation telepresence. In S. Griffin (Ed.), Third annual workshop on SOARy89 (pp. 235-250). Houston, USA : NASA Conference Publication. So, R. Y. (1994). An Investigation of the Effects of Lags on Motion Sickness with a Head-Coupled Visual Display. Paper presented at the United Kingdom Informal Group Meeting on Human Response to Vibration. Alverstroke, UK, September. Terré, C. (1990). Conduite à distance d'un robot mobile pour la Sécurité civile : approche ergonomique. Thèse, Université René-Descartes, Paris. Vercher, J. L., Gauthier, G. M., Bertrand J. C, &amp; Magenes, G. (1989). Bimanual Micro-Manipulator for Televideo-Operation of Anthropomorphic Robots. Paper presented at the IEEE SMC 89. Cambridge, MA, Mav. Vercher, J. L., &amp; Gauthier, G. M. (1992). Oculo-manual coordination control : Ocular and manual tracking of visual targets with delayed visual feedback of the hand motion. Experimental Brain Research, 90, 599-609. Warren, W. H., Mestre, D., Blackwell, A. W., &amp; Morris, M. W. (1991). Percep- tion of circular heading from optic flow. Journal of Experimental Psychology : Human Perception and Performance, 17, 28-43. RÉSUMÉ L'approche bionique, consistant à donner des propriétés des êtres vivants à un robot, est un moyen de diminuer efficacement l'écart existant entre le fonctionnement d'un opérateur humain et d'une machine. Dans cette étude, le comportement d'anticipation visuo-motrice observé chez l'homme lors du contrôle des déplacements a été implemente sur une base mobile téléopérée. Cette implementation a été effectuée suivant deux modes de couplage : orientation de caméra asservie à la direction du déplacement (modèle « plate-forme ») ou bien, direction du déplacement asservie à l'orientation de la caméra (modèle « caméra »). Les résultats mon- trent 1 I qu'un téléopérateur est plus performant lorsqu'il pilote un véhicule pourvu d'anticipation et 2 I que cet avantage est d'autant plus marqué suivant le modèle « caméra ». Cela confirme que la coopération homme-machine bénéficie de l'implantation sur l'engin télé- opéré de modes de couplages visuo-moteurs proches de ceux de l'opérateur. Mots-clés : Téléopération, Bionique, Interfaçage Homme-Machine, Coordinations visuo-motrices. Manuscrit reçu : octobre 2002. Accepté par R. Analberti après révision : avril 2003.</page></plain_text>