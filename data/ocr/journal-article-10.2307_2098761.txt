<plain_text><page sequence="1">J. Soc. INDusT. APPL. MATH. Vol. 11, No. 1, March, 1963 Printed in U.S.A. A GEOMETRIC CONVERGENCE THEOREM FOR THE PERCEPTRON* ALAN G. KONHEIMt 1. Introduction. Our objectives in this paper are to describe and analyze a variant of the four layer series-coupled Perceptron reported upon in [1] and to prove a geometric convergence theorem for this new model. The term Perceptron, introduced by the psychologist F. Rosenblatt [5-9], refers to a class of signal-adaptive networks composed of 'neuron-like' compo- nents. It was hoped that the Perceptron, proposed as an approximate model of the brain, would contribute to our understanding of the role that the brain structure plays in realizing brain functions. We shall not be concerned with whether or not these components are in fact accurate representations of actual neurons [4] nor with the question of similarity of performance with the brain. What is relevant, at least from our point of view, is that this family of devices exhibit interesting behavior. We shall nevertheless persist in using anthropomorphic terminology, for perhaps the same reason that probability theory still retains its colorful and suggestive language. Briefly (the complete details are given in the appendix) our variant of the Perceptron consists of four so-called layers of components.t The first layer of photocells (the input stage) is connected by means of 'random' connections to a layer of threshold devices (the first layer of association units). Each unit in this layer is in turn connected to every threshold unit in the next layer by means of links with varying amplification factors. The units in the third layer are then connected to the response units in the final layer or output stage. When a visual stimulus (signal) is presented to the Perceptron, signals are transmitted from the active units (photocells) in the first stage to the units in the second layer and thereafter to the units in the third and fourth layers. If the input signal to the kth response unit exceeds the input signals to the remaining response units, the network gives the kth response. The problem is to adjust the amplification factors in such a manner as to force the Perceptron to give a preassigned response to each stimulus in a given (finite) class of inputs. The principal contributions of this paper are (1) an analysis of a special class of four layer series-coupled Perceptrons and (2) the development of * Received by the editors January 29, 1962. t Thomas J. Watson Research Center, International Business Machines Corpora- tion, Yorktown Heights, New York. $ See Fig. 1. I</page><page sequence="2">2 ALAN G. KONHEIM Al All INPUT STAGE FIRST LAYER OF SECOND LAYER OF OUTPUT STAGE (RETINA) ASSOCIATION UNITS ASSOCIATION UNITS (RESPONSE UNITS) FIG. 1. Four layer series-coupled Perceptron. 'geometric' conditions which insure that, after a suitable training period, the device will give the desired response to each input stimulus. 2. The space of input signals. Denote by {Sl, S2,IISN} the space of input signals (or stimuli) to this network. Let '~ be the equiva- lence relation partitioning 8 into classes A1, A2, *. * , AM of equal response, i.e., the elements of Ak are precisely those stimuli which should produce, from the trained network, the kth response. For S X 0 we define X(S = - f if the rth association unit in AI 'is active for the stimulus S, -O otherwise, and A'(S) = {r:xr(S) = 1}. Next we introduce the intersection numbers { QiJ iNi which are defined by (1) Qi = E Xr(Si)Xr(Sj)D Qte is the fraction of association units in A' which are active for both of the stimuli Si and S, . THEOREM 1. Thhenumbers {iQJeji enjoy the following properties: (2) The _ mof (iQis, Q Dt), (l3e ) the function d defined on c X 2 by d(Si, Se) =pe is + stimuliw is a pseudometric on e; furthermore d(Sp, Se) = 0 if and only if A'(S.) *Consider the mapping e :- VNa = = (xl X2 X XNa) Xs = 0or1} de- fined by if(S) = (xt(S), a2(Ss), *a * unia(St)). Then d(S;, Sj) is equal to N' times the Hamming distance between x(Si) and X(Sj).</page><page sequence="3">CONVERGENCE FOR THE PERCEPTRON 3 (4) d(Si , Si) &lt;1, (5) The matrix (Qij) is positive semidejinite. Proof. Properties (1) and (2) are immediate consequences of the defi- nition of Qij. In proving that d is a pseudometric it evidently suffices to verify the triangle inequality. We begin this demonstration by writing A'(Si) = {A'(Si) n A'(S1) n A'(Sk)} U {A'(Si) n AI(Sj)f n A'(Sk) } U {A'(Si) n A'(S,) n AI(Sk)c} U {A'(Si) n AI(S,)cf n AI(Sk)c} as the union of four disjoint sets with c denoting complementation. Next let us introduce the notation Qi*lj*2k*3 4. {number of elements in AI(Si)*l n A1(Sj)*2 n AI(Sk)*3} with *1, *2 and *3 denoting the presence or absence of complementation. We thereby obtain the equations Qii = Qijk + Qiik' + Qijlk + Qiickcl Qj = Qijk + Qiljk + QiJk' + Qilc ,k (2) Qkk = Qik + Qicjk + Qijck + Qilick, Qii = Qijk + Qitk. Qik = Qik + .Qijck, Qik = Qiik + Qicjk From (2) we find d(Si, Si) + d(S,, Sk) - d(Si, Sk) = 2(Qicjkc + Qijck) _ 0 which establishes the triangle inequality. If d(Si, Sj) = 0 then from property (2) of Theorem 1 we must have Qii = Qjj = Qij which implies AI(Si) = AI(Si). Equation (2) further shows d(Si , Si) = Qijck + Qiickc + Qicik + Qicjkc and this quantity is bounded by unity. To prove the final property let us note that (Qij) = Na1(P5) (Pij) T where T denotes transpose and Dij = Xj(Si) 3. The operation of the network. The input signal to the sth association unit in layer A",, a,,,, due to the stimulus S being presented to the network at time t is (3) y8(S, t) = Ea U,8(t)Xr(S) Let 0. denote the threshold of the unit a"8I and set II t 1, if Ty,(S, t) &gt; 08, a.,out (St) 0= \0 otherwise. The input signal to the kth response unit, Rk is then given by Rk, in(S, t) = Z'l WA(t)aeout(S, t)-</page><page sequence="4">4 ALAN G. KONHEIM The response of the Perceptron is governed by the following rule. The Perceptron gives the response S E Ak if and only if Rk*,in(S, t) &gt; max Rk,in(S, t). (4) Ic The problem is to determine the amplification factors {u,8(t)} and {wsk(t)} in such a manner that the Perceptron classification (as specified in (4)) agrees with the desired classification as t -* oo. Let ft denote the response of the Perceptron at time t. We shall say that ft converges weakly to - pro- vided that limit, HO f'( Si) = Iimittoo Oft (Sj) whenever Si ' Si . We shall say thatft converges strongly to - if in addition to the above condition we have limiting ft(Sj) $ limitt 0Oft(Sj) whenever Si Y-" S, . Weak convergence requires that the network recognize that equivalent stimuli are equivalent and strong convergence requires in addition that it distinguish between nonequivalent stimuli. These amplification factors are chosen as a result of a training procedure. The training procedure consists of presenting to this network a sequence of signals from 8 and after each presentation making an appropriate change in the amplification factors. Stimuli are presented every At seconds starting from t = 0; we denote by S(t) the stimulus presented at time t. To describe the variation in the {Urs(t)} we introduce fictitious amplifi- cation factors { v8((t) } which satisfy Vrs(t) = 0, t &lt; At2 (5) Vr8(t + At) - Vr8(t) = -BAtVrs(t) + eAth(S(t - At), S(t))Xr(S(t - At))X8(S(t)) where ( 1 ) e and a are positive numbers and (2) h(S(t - At), S(t)) {1 S(t) ' oS(th-rAt)ws This rule means that in addition to a decrement in the amplification factor proportional to its present value, we increment at time t + At by eAt pro- vided that (i) ar' was active for the stimulus (S(t - At)) shown at time t - At, (ii) a8' was active for the stimulus (S(t)) shown at time t and (iii) S(t) and S(t - At) are regarded as r-' equivalent.</page><page sequence="5">CONVERGENCE FOR THE PERCEPTRON 5 In terms of the {Vra(t)} we then define 1 t/At~r~~) (6) Ur8(t) = + t/t (jAt) Finally the {Wsk(t)} vary as a result of what Rosenblatt terms a reinforce- ment procedure [3]. 4. Analysis. We begin our analysis by assuming that the training se- quence is the result of a stationary ergodic stochastic process (on the integers) with values in S. Let fil j2=Prob {S(t - At) = S, and S(t) = Sj2. Using (5) and (6) we obtain the following difference equation for 'Yr(Si , t) 'yr(Si I (n + i)At) - Yr(Si, nAt) = -(at + 4 2) 7r(Si I nAt) (7) + e~t Na Ngt Nag + C-At E E E h (S;1 X Sj2 )filj2 (n)xa(Sjl )Xs(Si)Xr(Sj2 ) 8-1 j1-1 j2=1 wherefjlj2(n) is the (empirical) frequency with which the pair of stimuli Sji followed by SJ2 occurs in the sequence S(O), S(At), S(2At), ... S(nAt). Making use of (1) we simplify (7) to Yr (Si , (n + i)At) - y,(Si , nAt) = - A(oZt + n + 2) Yr(Si I nAt) + eNaAtt I E Qikfkj(n) }Xr(Sj) j=l k Sk-S i We are interested in the behavior of yr(Si , nAt) as n - oo. This limiting behavior can be obtained from the following two lemmas. LEMMA 1.* Let x be a function on the nonnegative integers satisfying (Ax)(n) = -a(n)x(n) + A3(n), n = 0, 1, ... t x(O) = 0, where 1 &gt; a(n) a a&gt; 0 and limitn--w1r(n) =. Then limit x(n) = . * A proof of this lemma will be found in Appendix B. t A denotes the forward difference operator (,Ax) (n) = x(n + 1) - x(n).</page><page sequence="6">6 ALAN G. KONHEIM LEMMA 2.* If the training sequence is the result of a stationary ergodic stochastic process (on the integers) with values in 8, then fjk (n) -&gt;fjA as n -&gt; oo with probability one. We thus obtain our THEOREM 2. 'Yr(Si) = limit 7Y(Si , nAt) Nag = ANaE { E Qikfi;} Xr(SI) Sk6 j j with probability one. 5. Conditions for convergence. Let us set Al (Si) = {r:7y,(Si) &gt; Or) and note that a sufficient condition for weak convergence is (8) Ao~oIA(Si) = A"I(Sj) whenever Si S, . To prove this assertion we shall choose a reinforcement procedure for which (9) w8k(t) 1, all s, k, for all t sufficiently large. This could be effected, in Rosenblatt's termi- nology, by a single positive (unit) reinforcement of active connections to the layer of response units. It would then follow from (8) and (9) that Rk,in(Si X t) = Rk,in(Sj, t) whenever Si -. Si at least for all t sufficiently large and this is weak con- vergence to A. We can now prove the following. THEOREM 3. A sufficient condition for weak convergence is the existence of a real number 0 such that for each index i, 1 &lt; i &lt; N.,, we have (1) Na ~ Z Qikfk; &gt; 0 whenever S, Si and k (2) Na { Qikfkj} &lt; 0. Sjr~LS; 8k'~8j Proof. We shall show that these conditions yield AX-(Si) = U A-(Si) sj~ Si-xSi *The proof of Lemma 2 is given in Appendix B.</page><page sequence="7">CONVERGENCE FOR THE PERCEPTRON 7 which certainly implies (8) when Or = 0 for all r. If we let Aij = Na - E Qikfkj k Sk-Sj then N* l ( Si) = EA ijxr(Sj)). i=1 If r E U A'(Sj) (say r E A'(Sj,) with S0' Si) then Si-Si 'yr(Si) = Aijo + Z Aijxr(Sj) &gt; 0 ivi by hypothesis (1) and hence r E A--(Si). If on the other hand r ( U A'(Sj) then by hypothesis (2) j r(Si)= Z Aij~r(Si) ? j Aij &lt; 0 so that r f A--(Si). THEOREM 4. If in addition to the conditions of Theorem 3 we have (10) [ U A.(,)] ln [ U AI(S )] C U AI(S ) ,E Ak SEAj SrEAk for each pair of indices k and j with k $ j, then the convergence is strong. Proof. Let the reinforcement procedure be chosen so that (9) holds. Then Riin(Si t) = number of elements in {[ U AI(Sji n [ U AI(SY)]} SEAj ,E Ak whenever Si E Ak and t is sufficiently large (with probability one). Thus by (10) we will have Rk,in(Si) 0t &gt; Rj,in(Si ) t) for all t sufficiently large with probability one. This is strong convergence. 6. Geometric conditions for convergence. Define the average distance of the stimulus Si to the equivalence class Ar by d (Si; Ar) -- Ed(Si X Sh) Xr h Sh E A where X\r is equal to the number of stimuli in the class Ar. Condition a. Qii = Qj,( &gt;0) whenever Si ',' Si. Condition a'. Qii = Q(&gt;0) for all Si .</page><page sequence="8">8 ALAN G. KONHEIM Condition A3. d(Si, Ar) = d(Si, A,) whenever Si, -, Si. Condition y. d(Si, A,) &lt; d(Sj, A,) whenever Si E A, and Si f Ar. Condition S. For each equivalence class A, there exists at least one associ- ation unit a81 with the following properties: (1) a8' is active for some stimulus in A, and (2) a8' is not active for any stimulus in At for any t, t $ r. THEOREM 5. Sufficient conditions for weak convergence are a and 3. Proof. Consider the stochastic process with probabilities fjk= A all j and k* and suppose for convenience that e and a are chosen so that - Naf = 1. We shall prove that (11 ) EZQik Qhk X all j Bk~B j Bk~S j whenever Si -Sh . This will yield yr(Si) = yr(Sh) and hence if we choose OJr to be any real number distinct from {7r(S,)I}N8 we shall have weak con- vergence. Let Si -- Sh and note that Z d(S, Sk) = E d(S, Sk) k k SkE Ar SkE Ar or XrQii + E Qkk 2 E Qik = XrQhh + e Qkk- 2 E Qhk k k k k SkE Ar SkE Ar SkEAr SkE Ar which with condition a proves that (11) holds. THEOREM 6. Sufficient conditions for strong convergence are a', j3, y and 6. Proof. We shall show that for each equivalence class At there exists an association unit in A" which will respond for all t sufficiently large only to stimuli in Ar with probability one. Let a8' be the association unit in A' (for the class A,) whose existence is postulated in condition 6. We shall show that 78(Si) &gt; max 78(Sj) whenever Si E Ar. Thus if 08 is chosen so that 78(Si) &gt; 08&gt; max 78(Sj) ssi * This stochastic process can be described as follows: independent chance experi- ments are performed at times nLt (n = 0, 1, ***). Each experiment consists of select- ing a stimulus from 8 according to the uniform probability distribution on S. For such a process it is readily verified that fjk(n) -- fik as n -- X with probability one.</page><page sequence="9">CONVERGENCE FOR THE PERCEPTRON 9 we shall have the inclusion A" (Si) n A" (S1) c A (Si), Si -. Si which implies strong convergence (see the argument in Theorem 4). Now N8Jg 78 (SY) = E { E Qvk} IX(S,) pal k N =E I E Qyk)Nt,a too1 k SkE Ag where Nt,s = L X8(Sk) = number of stimuli in At which activate a8. k SkE Ag By hypothesis Nt, = 0, t $ r, N t,,s &gt; ? t=r, and therefore 78(SY) = Nr,8 QVkv SkE Ar Next if Si E Ar and S i Ar then d(SiX Ar) &lt; d(Sj, Ar) or XrQii + E (Qkk - 2Qik) &lt; XrQjj + Z (Qkk 2Qjk) k k SkE Ar SkEAr which will yield, with condition a', TOMS) &gt; '8( A). 7. Comparison with the Block-Knight-Rosenblatt model. We should make a few remarks here concerning the Block-Knight-Rosenblatt model [1]. The principal difference between their model and the one presented here lies in the interconnections between layers A' and A" and the rules for altering the values of these connections. In the model of [1] the amplifi- cation factor of the link joining a1l to aI' is of the form Wr.(t) + 05r8 where 5r8 is the Kronecker delta and 0 the common threshold of the association units in AII. The rule for modifying the amplification factors {wr8(t) } may be described as follows: Wr8(t + At) - Wr(t) = AtWrs(t) + eAtV</page><page sequence="10">10 ALAN G. KONHEIM where V 1 if ar' is active at time t and al, is active at time t - At, 0O otherwise. Thus the alteration of amplification factors depends upon the state of the association units in both layer A' and A",. This rule yields a system of non- linear difference equations with a stochastic driving force. In [1] the authors approximate these equations with a system of nonlinear differential equa- tions. The differential equations possess solutions with limits as t - oo and these solutions are taken to be the solutions of the system of difference equations. The transition from difference equations to differential equations is not justified. The solutions of the difference equations in fact exhibit an oscillatory behavior for large t and it can be shown that with positive prob- ability there will be 'large' oscillations at infinity. A detailed analysis of this will appear in a later communication. APPENDIX A Description of the Perceptron A.1. The first layer (layer of sensory units). The first layer is composed of N8 sensory units. These units may be considered to be photocells situ- ated in a two-dimensional rectangular array; the array is often referred to as the retina of the Perceptron. We shall speak of the retinal point (a, A) and the corresponding sensory unit sa with 1 ? a &lt; A, 1 ? A ? B and N8 = AB, A sensory unit assumes one of two states: an active state, during which it emits a signal (of unit strength), and a null state, during which it emits no signal. A.2. The input signals (Stimuli). A stimulus S is a configuration of the retina in which certain sensory units are in the active state. We let s8,,(S) denote the output of Sas upon presentation of S and write Saj(S) fi1 if Sass is in the active state for S, =o if 8a, 0 is in the null state for S. To each stimulus S we thereby associate an A X B matrix (Sa, 0(S)) of elements 0 and 1 describing the state of the retina. A.3. The first layer of association units (A'). The second layer contains Na association units; we denote by a8' the sth association unit in A". The input signal to a8' is the arithmetic sum of the output signals of certain sensory units in the first layer. If this signal exceeds the threshold of a8I this unit transmits in turn a signal (again of unit strength) to the units in the next layer. All units in Al have the same threshold 0'.</page><page sequence="11">CONVERGENCE FOR THE PERCEPTRON 11 A.4. The connections between the first two layers. In what follows we let X and Y denote fixed nonnegative integers. An (X, Y)-connection matrix is an A X B matrix (can3) which satisfies the following conditions: (1) ca,13 = 0, +1 or -1, (2) the number of cars = +1 is equal to X, (3) the number of cars = -1 is equal to Y. The set of all (X, Y)-connection matrices will be denoted by Cx,y ; Cx,y contains N8! X!Y!(N - X- Y) matrices. In establishing the connections from layer 1 to layer 2, Na matrices (8) = a(8,0 (1 &lt; s &lt; Na) from Axy are chosen. The quantity 6(') is equal to the amplification factor of the link joining the output of as to the input of a8,. These connections are referred to as excitatory (5(8) = +1), in- hibitory (8 = -1) or null (6(') = 0). We should like to point out that there are other connection schemes [3] which admit the possibility of multiple excitatory and/or inhibitory connections from a sensory unit. To choose the Na connection matrices independent chance experiments are per- formed; each experiment consists of selecting a matrix from ex,y according to a given probability distribution. In most cases this distribution is as- sumed to be the uniform distribution. A.5. The operation of AI. The input signal to a8' upon presentation of the stimulus S is a = ,in(S) =E aa(S) (aA) where the summation is over all retinal points. Define 0() i x &gt; 0' 0(x) {O otherwise. The output signal from a8' is then given by aout( S) = 0( I8,in(S)). Perhaps a few words on the significance of the interconnections layer 1 -*&gt; layer 2 are called for. Let the space of input signals be identified with a (finite) subset of the space Lo,,r(X) of bounded real valued measurable functions on a compact space X. Here the space X is a finite set of lattice points with the discrete topology and uniform measure. The space of bounded linear functionals L*,,(X) is then identified with the set of finite</page><page sequence="12">12 ALAN G. KONHEIM signed measures on X. The input signal to the association unit a8' due to the input signal Si is then of the form L8(Si) where L8 E L*,r(X). The effect of the connections layer 1 -? layer 2 is to mask the input signal with a set of (randomly chosen) functionals. The use of a threshold detector in con- junction with the association unit constitutes what is referred to in the engineering literature as 'feature analysis'. APPENDIX B Proof of Lemma 1. We consider the system x(n + 1) -x(n) = -a(n)x(n) + 3(n), n = 0,1, 2, ... x(O) = 0. Introduce the generating functions X(z) = Z: x(n)z', n=O 00 Y(z) = E (Oa(n) - )x(n)zn, n=Oz and 00 U(z) = j (:(n)- #)Zn n=O and obtain 1 X(z) -X(z) = -(aX(z) + Y(z)) + A(z) + U(z). z We will first establish that the sequence {x(n)} defined by (1) is bounded. Consider the following 'curves' in the x-n plane: xln) X2(:(), x = a 11 x3(n) = -x2(n) with I I = maxm I p3(m) 1. We have (2) x3(n) &lt; xl(n) &lt; x2(n). The following assertion is to be proved: If for some value of n, say n = no, we have (2) satisfied with n = no and with xi replaced by x, then (2) is satisfied for all n &gt; no with xi replaced by x. For x(no + 1) = x(no) a(no)) + i(no).</page><page sequence="13">CONVERGENCE FOR THE PERCEPTRON 13 If xi(no) &lt; x(no) &lt; x2(no) then x(no + 1) &lt; x(no) &lt; x2(no) = x2(no + 1), x(nO + 1) &gt; (1 - a(no)) + 3(no) = #(no) &gt; x3(no) = x3(no + 1), =a(no) a(no)= while if x3(no) &lt; x(no) &lt; xi(no) then x(no + 1) ? x(no) &gt; x3(no) = x3(no + 1), _ 1(no) 13(no) x(no + 1) ( a (no)) + 13(no) = - = X2(nO) = X2(nO + 1). =a(no) a(rno) Since x(O) = 0 it follows that Ix(n) I __ '1 Thus the generating functions are all analytic in f z I &lt; 1. We obtain X(Z) = zo3 __ KYW + zU(z) (1- z)(1-rz) 1-rz - rz with r = 1 - a and this in turn yields 1 n n-i n n-1i-- | 01 - = - (a - a (k))x(k)rr + Z (P(k) - P3)rni| 1 r k=c k=O Let e &gt; 0 be given and choose K such that I a(k) - a I &lt; e -13(k) - 1 &lt; r whenever k _ K. Then for n &gt; K + 1 we have the estimate 0 n N-i -- x(n) - &lt; r +z (a- a(k))x(k) + (i(k) -3) Irn-k a -k1- nf-1 + E I (a a (k))x(k) + ((3(k) - -)-k k=N &lt; p rn + r- {r-N [2a(0), a + 2 1 Bl] (l 1+ l) 1 as n -- oo or, since e is arbitrary, limit x(n) = -. n*oo Ea</page><page sequence="14">14 ALAN G. KONHEIM Proof of Lemma 2. Our stochastic process is of the form 2 = {x(t, a): w E Q} where t takes on nonnegative integer values and x(t, a) E S. Define the random variables Ij for j = 1, 2, , N t by I;(x~n co)) x(n, w,) Ij(x(n, w) ) = {O' otherwise and note 1 n-1 fik(n, c) = - E Ij(x(p, &amp;))Ik(X(p + 1, W)). As 9C is ergodic and stationary there exists a (unique) measure preserving ergodic transformation T such that x(n, a) = Tnx(O, co). It follows im- mediately that the stochastic process yJ with sample function y(n, a) = Ij(x(n, &amp;))Ik(x(n + 1, a)) can be written in the form y(n, w) = Tny(O, W) and hence [2, p. 465] limit y(0, @) + y(l, 1) + + y(n, ) -E(y(0, a,)) n-*oo n+ 1 where E denotes expectation. But E(y(O, a)) = Prob {y(0, a) = 1} =fOk REFERENCES [1] H. D. BLOCK, B. W. KNIGHT JR., AND F. ROSENBLATT, Analysis of a four layer series-coupled perceptron, Cornell University Cognitive Systems Research Program (1960). (To appear in Rev. Mod. Phys.) [2] J. L. DooB, Stochastic Processes, John Wiley and Sons, New York. [3] A. M. GEOFFRION, The origin and significance of perceptron theory, Industrial Engineering Seminar, Cornell University (1960). [4] L. D. HARMON, Neural analogs, Proc. I.R.E., 49, pp. 1316-17. [5] F. ROSENBLATT, The perceptron, a theory of statistic separability in cognitive systems, Cornell Aeronautical Laboratory (1958). [6] , Two theorems of statistical separability in the perceptron, Cornell Aero- nautical Laboratory (1958). [7] , Perceptron simulation experiments, Cornell Aeronautical Laboratory (1959). [8] , On the convergence of reinforcement procedures in simple perceptrons, Cornell Aeronautical Laboratory (1960). [9] , Principles of neurodynamics, Cornell Aeronautical Laboratory (1961).</page></plain_text>