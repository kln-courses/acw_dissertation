<plain_text><page sequence="1">A History of Numerical Weather Prediction in the United States Philip Duncan Thompson National Center for Atmospheric Research,1 Boulder, Colo. 80303 1. Introduction On such occasions it is customary, I believe, to offer one's apologies in advance. In the first place, I am not a historian or even much of a scholar. Certainly not a thorough scholar. ' The National Center for Atmospheric Research is sponsored by the National Science Foundation. © 1983 American Meteorological Society In fact, it has occurred to me that it was probably a mistake to ask one of the minor actors in the drama to comment on the quality and interpretation of the entire performance. As one of the cast, I must admit to certain prejudices as to what the important problems are and have been, which approaches have been most promising, and which have been the most significant scientific and technological developments in a young, rapidly growing, and rather specialized field. Accordingly, I should apologize for the somewhat preten tious billing for this talk. I can say in my own defense that the</page><page sequence="2">756 Vol. 64, No. 7, July 1983 choice was not mine. Our worthy chairman was forced to concoct a title on short notice and wanted to give me the widest latitude possible. I appreciate his motives, but it would be more accurate to describe this history as "A Highly Personal and Anecdotal Account of Numerical Weather Prediction over the Past Thirty Years." I will try to be objec tive, and not to fall off the fine line between undue modesty and undue immodesty. I cannot presume to tell "The Defini tive History" of numerical weather prediction. I cannot live up to the advertising in another respect. That is, one cannot maintain proper historical perspective of a succession of related and complementary scientific develop ments if he restricts his view to events that took place only in a single nation or in a very recent era, isolated from past ac complishments in the whole international scientific com munity. Science just doesn't work that way. Accordingly, I shall review briefly the ideas and work that led up to the flowering of numerical weather prediction in the United States, and will occasionally refer to related or parallel work in this field abroad. As a final note of apology, I hope I will be forgiven if I in advertently overlook some important contributions, or if I appear not to appreciate the full significance of some devel opment that I single out forattention. There is little of a long history that can be said in an hour. Well, so much for apologies. 2. Definition and early history of the problem To whittle the subject down to barely manageable propor tions, I am going to define numerical weather prediction as the process of solving the equations that govern the behavior of the atmosphere, starting with approximately known initial and boundary conditions. I will speak most specifically about purely numerical methods for solving those equations, but will not exclude analytic methods. However, I cannot begin to include all of dynamical meteorology. In one very real sense, the whole thing began with Isaac Newton. Even though the theoretical framework of fluid dy namics was not yet complete, Newton's second law of motion alone suggested to his contemporary Halley (and, 50 years later, to Hadley) a simple and plausible explanation for the existence of the persistent northeast trade winds. This was the first instance of the new mechanistic view as applied to the large-scale meteorological motions of the atmosphere. I will not elaborate on these works, since they have already been described in more general histories. There was, how ever, no predictive element in these early treatments, for the simple reasons that the thermodynamics was still missing and the phenomenon under discussion was very nearly sta tionary in time. The laws of fluid motion and the principle of mass conservation were well known, but the relation between the heating of a fluid and its thermodynamical state was not. But with the discoveries of Boyle, Charles, Count Rumford, Laplace, and Joule, and with von Helmholtz's( 1858) formu lation of the first law of thermodynamics in the mid-19th cen tury, the last major piece of the purely hydrodynamical puz zle was dropped into place. To put the situation as of 1858 in perspective, the set of equations that describe the behavior of a nonviscous fluid in adiabatic motion was then formally complete, in the sense that the number of state variables was exactly equal to the number of independent equations. It was not until 1904, however, that Vilhelm Bjerknes—in a remarkable manifesto and testament of deterministic faith— stated the central problem of numerical weather prediction. This was the first explicit, coherent recognition that the fu ture state of the atmosphere is, in principle, completely de termined by its detailed initial state and known boundary conditions, together with Newton's equations of motion, the Boyle-Charles-Dalton equation of state, the equation of mass continuity, and the thermodynamic energy equation. Bjerknes went further: he outlined an ambitious, but logical, program of observation, graphical analysis of meteorologi cal data, and graphical solution of the governing equations. He succeeded in persuading the Norwegians to support an expanded network of surface observation stations, founded the famous Bergen School of synoptic and dynamical mete orology, and ushered in the famous polar front theory of cy clone formation. Beyond providing a clear goal and a sound physical approach to dynamical weather prediction, V. Bjerknes instilled his ideas in the minds of his students and their students in Bergen and Oslo, three of whom were later to write important chapters in the development of numerical weather prediction in the United States. We shall refer to Rossby, Eliassen, and Fjörtoft later. It is perhaps unfortunate that V. Bjerknes was so strongly influenced by C. A. Bjerknes's (père) predilection toward dif ferential geometry and graphical methods. This was a serious limitation, simply because graphical operations could then be carried out only manually. In 1922, the Cambridge University Press published one of the strangest, but most imaginative, contributions to the whole literature of meteorology, written by a rather obscure, slightly eccentric, and unconventional Englishman by the name of Lewis Fry Richardson. Its title was Weather Predic tion by Numerical Process, and it outlined a rational method by which tomorrow's weather could be calculated from to day's meteorological observations. Solidly based on funda mental physical principles and the eternal mathematical veri ties, Richardson's method of prediction might have been expected to remove one of the greatest of nature's uncertain ties—the weather to plant, sow, harvest, hunt, fish, or sail by. But Richardson's book did not remove those uncertainties. It was a candid report of an admitted, but glorious, failure. In the limited correspondence and personal contact be tween V. Bjerknes and Richardson, there is ample evidence that the latter was either influenced by Bjerknes' view of the physical problem, or at least agreed with it. It is clear, how ever, that Richardson had his own ideas about the mathemat ical formulation of the problem. Both understood the neces sity of approximate methods in solving highly nonlinear equations, but, whereas Bjerknes was inclined toward graph ical methods, Richardson had an early appreciation of dis crete variable methods—notably the method of finite differ ences. The principal virtue was that discrete variable methods were reducible to simple arithmetical computations that could, in principle, be carried out by an automaton. Richardson (later a versatile and highly original statisti cian and economist) had a lively interest in the new method of finite differences and set out to apply them to the problem</page><page sequence="3">Bulletin American Meteorological Society of weather prediction. Over a period of months, between ambulance trips to the Front during the last stages of World War I, he completed a test calculation by hand and half finished a manuscript in which he described his method and results. Ironically enough, all his papers disappeared in the general confusion of war. They were eventually found under a coal heap in Belgium, returned to Richardson, and were later expanded into Weather Prediction by Numerical Process. Richardson's trailblazing book on weather prediction is an oddly quixotic effort. He describes his method in meticulous detail, but his computations predicted tendencies so large that the large-scale atmospheric disturbances would move at speeds comparable with those of sound waves. The latter conclusion was clearly at odds with observation. Near the end of his book, he describes a phantasmagorical vision of the "weather factory"—a simply enormous organi zation of specialized human computers, housed in a huge hall, directed by a conductor perched on a raised pulpit, and communicating by telegraph, flashing colored lights, and pneumatic tubes. He estimated that, even using the new fangled mechanical desk calculators, it would take about 64 000 human automata to predict weather as fast as it actu ally happens in nature. Richardson's preface ends with a rather wistful, but pro phetic statement: "Perhaps some day in the dim future it will be possible to advance the computations faster than the weather advances and at a cost less than the saving to man kind due to the information gained. But that is a dream." That "dim future" came 25 years later. 3. The renaissance of numerical weather prediction Aside from more fundamental difficulties, the most discour aging aspect of Richardson's proposed method was the sheer volume of calculation required. It was apparent that an abso lutely necessary ingredient in the success of any practical scheme of "numerical weather prediction" was a computing device that was capable of calculating a one-day forecast in less than 24 hours. Even Richardson probably underesti mated the "administrative overhead" in dealing with auto mata. In retrospect, we now see that a one-day prediction, based on a simplified version of the hydrodynamical equa tions, requires on the order of 109 numerical and logical op erations. The requirements for total data storage (memory) capacity and rate of data transfer from storage to processor are equally severe. What was clearly needed was a computing organism capable of performing something like 104 opera tions per second. During World War II and the years immediately follow ing, substantial progress was made in designing numerical processors in which the "switching" elements were not cogged wheels or electromechanical relay switches, but con sisted essentially of radio "tubes" (or electronic switches), which have virtually no mechanical inertia. By 1945, in fact, Eckart and Mauchly had designed and built processors with 2 Richardson, a Quaker and pacifist, did not shirk his duty to re lieve suffering. He drove an ambulance. 757 speeds of the order of one logical operation per millisecond. These developments brought the basic operating speed of the processor to within one order of magnitude of that required for the routine application of numerical methods to weather prediction. But there were still two basic deficiencies in the system. First, the data storage device—an acoustic delay line—was limited by the density of sound impulses that could be cycli cally regenerated and propagated through a mercury-filled tube. Second, and more important, the programming of the processor was completely external and "human-limited" in the sense that the entire sequence of instructions to the proc essor was written out in advance and conveyed to it instruction by-instruction through a manually wired plugboard and by setting many switches. The big breakthrough, however, was not dependent on sheer hardware development. It arose from von Neumann's realization that computing machines of this class must be "self-programming"—i.e., it should not be necessary to tell the computer what to do in complete detail. If, as a simple example, the same sequence of operations is to be performed on different sets of data, one may achieve a degree of self programming capability by storing the operands (numbers to be operated on) at enumerated "addresses" or locations in the machine's memory and, in addition, storing in its me mory the execution orders, which include the address of the operand. Thus, the basic execution cycle for one set of data (or operands) may be reused merely by concluding the cycle with a sequence of instructions to change the addresses of the operands that appear in the basic execution cycle. One then repeats the whole cycle. This is the essential logical basis of "stored-programming," which broke the human bottleneck of writing out the entire sequence of instructions in advance. This feature of the stored-program machine made it ideally suited to the demands of large-scale hydrodynamical calculation. Early in 1946, von Neumann singled out the problem of numerical weather prediction forspecial attention. Although he had a deep appreciation of its practical importance and intrinsic scientific interest, he also regarded it as the most complex, interactive, and highly nonlinear problem that had ever been conceived of—one that would challenge the capa bilities of the fastest computing devices for many years. 1946 was a year of ferment, for the formulation of the problem and the means of solving it were at last moving to ward each other, albeit not by design. (Appendix A is the ini tial proposal to establish the Meteorology Project.) Late in 1945,1 had been assigned to the so-called Diver gence Project at UCLA, the objective of which was to calcu late surface pressure tendencies by vertical integration of the hydrostatic and continuity equations, using direct observa tions of winds and pressure. I was a little frustrated, having become aware of one major difficulty and two serious limita tions on the applicability of the results to practical forecast ing. These were: l)At any particular level, the horizontal divergence of the wind field tends to consist of two individually large, but almost compensating, effects; namely, the confluence of streamlines and variations of wind speed along the streamlines. In fact, a simple error analysis indicated</page><page sequence="4">758 that the error due solely to roundoff errors in the re ported winds was fully as large as the true divergence. This, as it later turned out, is a fundamental source of difficulty and will be referred to later as the "divergence error." 2) One of the shortcomings of the "tendency equation" is that it involves several variables. Thus it is not self-con tained, nor is it easily combined with other variants of the hydrodynamical equations to form a complete set. 3) To determine the pressure tendency at any arbitrary level, one must determine the vertical mass flux through the bottom of the column, which requires that one know the vertical air speed at any level. The latter is not, of course, observed directly, nor was it imme diately obvious how it could be calculated indirectly. Working more or less in isolation, I was discouraged enough by my estimate of the pernicious "divergence error" that I virtually abandoned the original objectives of the Divergence Project and concentrated on two basic limita tions on the whole approach. I first derived a diagnostic rela tion between the vertical air speed and the contemporaneous fields of pressure and horizontal velocity. This gave me a complete and fairly manageable set of equations. What I did not know was that my "new" equation had been derived by Richardson at least 28 years earlier. Next. I set about devising a numerical method for solving the complete set of equations, essentially by Taylor-expand ing all of the variables around their initial values. This in volved a lot of substitution and resubstitution from several messy-looking equations. My mistake lay in seeking a single equation in a single unknown: it would have been easier if I had calculated the time evolution of each variable separately in a stepwise fashion. On later analysis, however, it emerged that the finite-difference form of my equation was equivalent to the finite-difference form of Richardson's equations. But I didn't know about Richardson then. This incident proves the value of reading. If I had read the book, I would have saved several months of pencil-chewing and head-scratching. In the spring of 1946. I voiced some of my concerns and aired my views to a brand-new Ph.D. at UCLA, a fellow by the name of Jule Charney, whose office was next to mine. He had just finished a brilliant thesis on the instability and struc ture of baroclinic waves, and I thought that if anybody could criticize my ideas, he could. He could and did. By and large, however, we agreed on the nature of the problem and its at tendant difficulties. Unfortunately for me, Charney left on a National Research Council Fellowship, first to visit Rossby's Institute of Meteorology at the University of Chicago, and then to work with Eliassen and Fjörtoft at the University of Oslo. Meanwhile I ground away at an old Monroe desk calcula tor. trying to figure out short-cuts and becoming increasingly depressed by the burden of hand calculation. Then one fine afternoon in the early autumn of 1946, Prof. Jörgen Holmboe called me in. said that he was aware of what I was trying to do, and handed me an article from the New York Times Mag azine. It was an interview with Prof. John von Neumann of the Institute for Advanced Study and Dr. Vladimir Zworykin of RCA, in which they announced their intention of develop ing a very high speed electronic computing machine and of Vol. 64, No. 7, July 1983 applying it to the prediction of natural weather and of calcu lating the effects of human intervention in the natural proc esses of the atmosphere. The implications of this develop ment did not fall on deaf ears. Indeed, it was downright heady stuff. To mix metaphors, the grass didn't grow long under my feet. Next day, I called my commander. Gen. Ben Holzman, and requested authorization to travel to Princeton to meet with von Neumann. Also, if that were granted, would he please arrange the meeting. I was a pretty brash young man, but I felt that the chances of a greenhorn first lieutenant making any headway with a giant of von Neumann's stature were virtually nil unless I had a patron. Gen. Holzman grumbled a bit, but agreed to it if I traveled as extra crew on a military aircraft that was headed East anyway. The following day the arrangements were clear, and I made my way to Princeton via B-29, bus, stagecoach, train, oxcart, and the PJ&amp;B.3 My séance with von Neumann is not very clear in my memory: it couldn't have lasted more than an hour, and I was slightly overawed. But I had my speech pretty well set and blurted it out. Mostly, I just said what I wanted to do and why, and how I proposed to go about it. We talked a while about the computing problems. After about half an hour he asked if I would like to join his Electronic Computer Project as a meteorologist working on problems of numerical weather prediction. That question took no pondering. Then he asked how my assignment should be arranged. I suggested that he call Gen. Holzman and request it. He called, talked for a few minutes, held the phone and said Gen. Holzman would like to speak with me. The conversation was very short and one-sided. It went something like, "Well, I guess you'd better go back and get your gear. Orders will follow." Unbeknownst to me at that Jme, there had been a now famous meeting of interested meteorologists at Princeton in August 1946, instigated primarily by that great diplomat and entrepreneur, Carl-Gustav Rossby. The list of participants included the cream of the U.S. dynamical meteorologists, well-placed officials in the U.S. weather services, and repre sentatives of potential funding agencies: John von Neumann, C.-G. Rossby, Harry Wexler, J. Jaw, B. Haurwitz, V. P. Starr, R. B. Montgomery, H. C. Willett, A. Cahn, Jr., H. Panofsky, G. A. Hunt, J. E. Miller, C. L. Pekeris, J. Namias, W. M. Elsasser. J. Charney. Lt. Cmdr. D. F. Rex, and R. El liot. To my knowledge, this was the first time that all of the essential ingredients of success in this venture were brought together: a well-formulated problem; the technological means of solving it; the people, money, and other support needed to solve it; and, finally, the institutional mechanisms around which the whole effort could be organized. In the latter re gard, Dr. Harry Wexler, Dr. Francis Reichelderfer, and Dr. Earl Droessler were particularly influential in gaining support. Largely as a result of this meeting, von Neumann had al ready assembled a formidable and almost intimidating array of talent by late fall of 1946. Resident at the Institute at the moment of my arrival there were: Gilbert Hunt, a wartime-trained meteorologist, but at bot tom a mathematician. He was then preoccupied with his doc tor's thesis, but looked into the problem of numerical 3 PJ&amp;B means Princeton Junction and Back</page><page sequence="5">Bulletin American Meteorological Society weather prediction to the extent of generalizing Jean Leray's proof that solutions of the Navier-Stokes equations actually exist. I received considerable help from him in finding limit ing values of the phase-speed of dispersive waves. Dr. Chaim Pekeris, who had studied at M.I.T. with Rossby in the pre-World War II days. His interests were primarily in the behavior of high-energy blast waves in nonhomogeneous media, and in tidal theory, either atmospheric, crustal, or oceanographic. Prof. Paul Queney, of the Sorbonne, whose research lay mainly in the linear theory of steady-state flow over long ridges. When I first arrived, I shared an office with Queney in a sort of dormer just above the eaves of Fuld Hall, the main building of the Institute for Advanced Study. Our scientific discourse was negligible. Between my schoolboy French and his faulty English, however, we managed to figure out his in come tax. There had been a fourth member of the group, Albert Cahn, who departed shortly before I came on the scene. Cahn. who had collaborated with Rossby on the problem of geostrophic adjustment, left a single, brief memorandum in which he simply restated the hydrodynamical equations, but with no prescription for solving them. So you can see that it was a pretty mixed bag. One by one, all of these stalwarts left: Hunt, to Cornell; Pekeris, to Israel; Queney, back to France. Moreover, von Neumann was deeply engaged in other affairs. Once again I was almost alone, except for the fairly regular exchanges of visits with Profs. Haurwitzand Hans Panofsky, then both at N. Y.U. Panofsky had definite ideas about numerical approx imations, and Haurwitz (from whose book I had learned most about dynamical meteorology) encouraged me to learn more about hydrodynamics. In spite of my isolation, this was not at all a sterile period. I had discovered Richardson's book and read it. His results confirmed my deep suspicions of the "divergence error" and laid bare a similar type of error, due to calculating the hori zontal acceleration of air as the small difference between the pressure-gradient and Coriolis forces per unit mass, both of which may individually contain sizable errors. I read system atically through Lamb's Hydrodynamics. From von Neu mann and Goldstine I learned something of numerical analy sis and was amazed to discover that the solutions of finite-difference equations may be nothing like the solution of the corresponding differential equation. In particular, the stability of numerical solutions does not depend on the abso lute magnitude of the finite increments of space and time, but on their ratio. Through reading and discussions at the Institute, I learned some lovely mathematical techniques, but that wasn't getting us much further toward the main problems. I was acutely aware that one small boy wasn't enough to grapple with them. In February of 1947, I wrote a rather long and rambling letter to Charney,4 who was still at Rossby's Institute at the University of Chicago, propounding a few questions. One of "The reply to which is appended (Appendix B). Charney's letter of 12 February 1947 contains the germinal idea that later led to his famous paper "On the Scale of Atmospheric Motions" (1948). 759 these, I recall, had to do with distinguishing sound, gravity, and planetary waves in the initial state. A related question was why don't the large-scale transient cyclones of the mid latitudes travel at the speed of sound or gravity waves. In his return letter, Charney's first (and not serious) argument was anthropomorphic: If cyclones traveled at the speed of sound, all humanity would have been blown off the face of the earth. Q.E.D.: Cyclones are not sound waves. His second answer was that it must have to do with the sta bility and dispersion properties of a rather gently forced system—e.g., one which is not continually disturbed by thermonuclear blasts. In his reply he proposed that he stop off at Princeton for a few days late in March (1947), when we could discuss these questions at greater length and leisure. This came to pass, and a very stimulating time it was. I think that Charney and von Neumann had met at least a year before, but they certainly met on this occasion. After several days, innumerable pitchers of beer, and many late hours, Charney went on to Oslo. By this time he had convinced him self that something had to be done to distinguish sound, grav ity, and Rossby waves. I had begun some self-educational studies of nonlinear Rossby waves and waves in nonhomogeneous media, but be came increasingly distressed about the practical problem of matching the meteorological effort to the development and eventual emergence of a stored-program machine. Since my days at Princeton were numbered, I expressed some concern about this, but I needn't have bothered. Von Neumann, be tween his myriad enterprises and distractions, was fully aware of the problem and had already considered how to deal with it. He asked my opinion of this fellow Charney as a long-term member of the group and the scientific leader of a small nucleus of people working on the physical and math ematical formulation of concrete numerical experiments. I assured him he could find no better combination of mathe matical savvy and physical insight, but also ventured to sug gest that Eliassen be invited for a protracted visit, since he and Charney had complementary experience and similar in terests, and also had been working well together. In any event, negotiations were opened, and both Charney and Eli assen were duly invited. That von Neumann's judgement was eminently sound was evidenced by the underground reports of Charney's work in late 1947 and its subsequent publication in Geofysiska Publi kasjoner in 1948. This was certainly the most significant con tribution to numerical weather prediction since Richardson's magnum opus, and far exceeded Richardson's work in its profundity and implications. Briefly, in his paper "On the Scale of Atmospheric Motions," Charney made an ingenious analysis of the magnitudes of the various dynamical, kine matic, and thermodynamical effects that are reflected in the hydrodynamical equations. Even more important, he showed that the ostensible difficulties due to the "divergence error" and the almost exact mechanical balance between the pres sure gradient, gravitational, and Coriolis forces could be avoided by discriminate, but systematic, introduction of the geostrophic and hydrostatic approximations, and that these conditions characterize the large-scale meteorological mo tions in middle and high latitudes. Finally, he derived a single partial differential equation in one unknown—pressure or isobaric height—whose solutions demonstrably do not cor</page><page sequence="6">760 Vol. 64, No. 7, July 1983 respond to sound or gravity waves (Charney, 1949). Char ney's 1947 formulation, later known as the quasi-geostrophic model, simultaneously skirted two major difficulties: first, it imposed much less stringent conditions for computational stability, and second, it did not demand that the horizontal divergence or accelerations be computed as small differences between large and compensating terms, each of which is sub ject to sizable percentage errors. These features alone evaded the two fundamental difficulties inherent in the practical ap plication of Richardson's method. At this point I must not fail to recognize and commend a paper by Eliassen, published in the same 1948 issue of Geo fysiska Publikasjoner as the one in which Charney's paper appeared. Eliassen's paper was concerned not only with the elegant and systematic use of isobaric coordinates, but also includes the derivation of a "quasi-geostrophic" equation that is essentially equivalent to Charney's. Quite literally, something was in the wind, even if it blew from slightly dif ferent quarters. Charney arrived in Princeton in midsummer of 1948, and Eliassen followed by a couple of months. I was then slated to organize and direct one of the divisions of the newly estab lished Air Force Cambridge Research Laboratories(AFCRL) in Cambridge, Mass., and was due to leave Princeton late in the fall of 1948. That prospect was exciting in many ways: it was certainly challenging. I did, however, feel twinges of re gret for leaving the group at Princeton at a time when the ac tion was really starting. One of my first official acts at AFCRL in the fall of 1948 was to build up a small group of people to capitalize on some of the ideas of Rossby and Charney. More specifically, we were concentrating on the propagation of Rossby waves in 2-dimensional flows. Meanwhile, Charney and Eliassen initiated and completed a nice study of the effects of orography, based on linear the ory, but including the mechanism of Ekman "pumping" through the boundary layer. To my knowledge, this was the first time this effect had ever been applied in the analysis of a real geophysical problem. Their paper appeared in Tellus in 1949. Eliassen returned to Oslo in the fall of 1949. As if by prear rangement to maintain a steady flow of Norwegians through the Princeton Project, Fjörtoft appeared to take his place. Soon after his arrival, Fjörtoft, Charney, and von Neumann collaborated on the design of a numerical prediction experi ment, using the equations for the nondivergent barotropic model, but starting with real initial conditions at about the 500 mb surface. At that time the Princeton machine was still far from oper ational. Accordingly, the calculations were programmed for the computer EN1AC (Electronic Numerical Integrator and Calculator), the only extant electronic machine, at Aberdeen Proving Ground, where von Neumann was a consultant. The programming, done by plugboard and manual switching of a huge bank of keys, was supervised primarily by Prof. G. W. Platzman, Dr. Joseph Smagorinsky, and Dr. John Freeman. For reasons of which I am unaware, the whole operation had to be carried out continuously, on an around-the-clock basis, with the whole gang coming in and going off shift. The first job was pulled off successfully early in April 1950. The results looked good, certainly better than Richardson's prediction. Later there was a triumphal celebration. I have a rather bad reproduction of a photograph of some of the participants and visiting dignitaries (Fig. 1 ), but I'm sure you will recog nize most of them. Going from left to right: Harry Wexler, John von Neumann, M. H. Frankel, Jerome Namias, John Fig. 1. Visitors and participants in the 1950 ENIAC computations (left to right): Harry Wexler, John von Neumann, M. H. Frankel. Jerome Namias, John Freeman. Ragnar Fjörtoft, Francis Reichelderfer, and Jule Charney.</page><page sequence="7">Bulletin American Meteorological Society 761 Freeman, Ragnar Fjörtoft, Francis Reichelderfer, and Jule Charney. It was a great day—the day of the first successful numerical weather prediction. The results are summarized in a paper entitled "Numerical Integration of the Barotropic Vorticity Equation." published by Charney et al. (1950) in Tellus. A second expedition to ENIAC, organized by Platzman and Phillips, took place in June 1951. This was principally a test of numerical procedure in a case of analytical initial con ditions. The results displayed a new type of numerical insta bility, probably nonlinear in nature, due to aliasing errors. This was the first clear manifestation of this kind of phenom enon: as we shall see later, however, it took another five years to get it sorted out, and it isn't yet laid to rest. 4. The rapid proliferation of research in NWP Needless to say, the Tellus paper of 1950 excited considerable interest, but, even before its publication, the basic approach and numerical methods had spread through the grapevine. At the same time, everyone was aware that those calculations were based on the principle of absolute vorticity conserva tion for 2-dimensional flow, which precluded the intensifica tion of circulation centers and did not provide for the forma tion of new centers where none existed before. Accordingly, there was a general rush to develop baroclinic models—i.e., models whose vertical structure was simple enough that the equations could be solved without undue computational strain, but general enough that they could simulate cyclo genesis and conversion of available potential energy to the kinetic energy of growing disturbances. In a relatively brief span, 1951-53, no less than six simple baroclinic models were proposed, two of which were tested in a few real cases: all were variants of the general quasi geostrophic model developed by Charney in his paper of 1949. They all also contained some elements of adiabatic thermodynamics. The first of these was Phillips "two-layer" model of 1951. Eady (1952) and Eliassen (1952) discussed some propagation and stability properties of "two-parameter" models, whose states are characterized by the horizontal dis tribution of two variables—e.g., the heights of two geopoten tial surfaces, or the height and temperature of a single geopo tential surface. Independently, Charney and Phillips (1953), Sawyer and Bushby (1953), and Thompson (1953) formu lated essentially equivalent "two-parameter" models, and Charney and Phillips carried out tests in a single case of spec tacular cyclogenesis: that was the famous Thanksgiving Day storm of 1950. Their results were encouraging, but slightly suspect, since certain coefficients in the equations had been "tuned" or adjusted to yield optimum agreement with observations. In the following year, Bushby and Hinds (1954) published the results of tests of the Sawyer-Bushby model in a number of cases. In June 1954 Thompson and Gates completed the analysis of a series of 120 forecasts based on Thompson's ver tically integrated two-parameter model, using real initial data. This was the most comprehensive test of both baro tropic and two-parameter baroclinic models up to that time (Thompson and Gates, 1956). It was carried out by the Numerical Prediction Project at AFCRL, staffed jointly by Air Force and Weather Bureau people, military and civilian, in preparation for the imminent application of numerical methods to short-range weather prediction. Our findings at that time, to summarize them briefly, were that: 1 ) The general level of performance of the two-parameter baroclinic model in predicting 500 mb height fields was virtually indistinguishable from that of the non-diver gent barotropic model. 2) The quality of 1000 mb forecasts was slightly lower than that of 500 mb forecasts. 3) There was a strong indication that the accuracy of all forecasts was adversely affected by orographic effects over and in the lee of the Sierra Nevada and Rocky Mountains. 4) The arbitrary specification of boundary conditions around an area of continental proportions rapidly con taminates the prediction in the interior, seriously in fecting about one third of the area in a period of 24 hours. 5) Spatial truncation error results in a systematic under estimate of the eastward speed of propagation, particu larly for small-scale disturbances. To a considerable extent, these deficiencies are still with us. Perhaps one of the most significant facts which emerges from the foregoing description of the development of quasi geostrophic models is that, by 1952, there were no less than four sizeable research groups who were concentrating on the problem, namely: the Meteorology Project at the Institute for Advanced Study, the Atmospheric Analysis Laboratory of AFCRL, the Napier Shaw Laboratory of the British Meteorological Office, and the International Meteorological Institute of the University of Stockholm, working in coop eration with the University of Oslo. The development of nu merical prediction had become an organized and well-sup ported research movement, comprising a substantial fraction of the total meteorological research effort. Let us admit at once, however, that this would not have happened without collateral advances in the technology of computing, com munications, and numerical analysis. 5. The establishment of operational NWP By the summer of 1952, there was mounting evidence that the crudest of numerical methods was capable of attaining an average accuracy comparable with that of forecasts prepared by conventional methods. Recognizing the potential of more sophisticated numerical methods and the equally important advantages of data-processing by high-speed automatic computing machines, a number of well-placed scientists and military officers brought these new developments to the at tention of the Joint Meteorological Committee under the Joint Chiefs of Staff. As a result, this committee, composed of the heads of the Air Weather Service of the U.S. Air Force, the U.S. Weather Bureau, and the Naval Weather Service,</page><page sequence="8">762 Vol. 64, No. 7, July 1983 commissioned a special subcommittee in late 1952 to review the current state of development, to estimate the trend of de velopment, and to advise the Joint Meteorological Commit tee on the desirability of establishing an operational numeri cal weather prediction unit. An amended resolution of the Joint Meteorological Committee requested this same sub committee to investigate the requirements of a numerical prediction unit, to advise on the feasibility of activating such a unit, and to lay plans for its establishment. With excellent cooperation between the three U.S. weather services, the subcommittee completed its survey, made its recommenda tions, and drew up the plan for the first operational numeri cal weather prediction unit by late in the summer of 1953. Briefly, the subcommittee found that numerical methods of weather prediction had already advanced far enough to jus tify putting them into practice, that it was feasible to do so, and that the best way to form an effective organization of people and physical facilities was to pool the resources of the three weather services. A mark of the subcommittee's under standing and foresight was its recognition that the further development of numerical prediction methods would be a necessary, slow, and generally unspectacular process, and that it should go hand-in-hand with the daily routine of nu merical weather forecasting. Accordingly, the subcommittee recommended that a research and development group should be an integral part of the first operational numerical predic tion unit. The subcommittee's recommendations were put into effect immediately upon the parent committee's approval, and the Joint Numerical Weather Prediction (JNWP) Unit was offi cially established on 1 July 1954. By that time, a nucleus of key people had been assembled in Washington, and after per formance tests of several production models, a high-speed electronic computer was ordered for delivery in the following spring. From its beginning and up to 1961. the JNWP Unit was jointly staffed, financed, and supported by the three U.S. weather services, with Air Force and naval officers working side-by-side with their civilian scientific colleagues. In 1961, it was made a division of the National Meteorological Cen ter, then under ESSA and now NOAA. At the inception of the JNWP Unit, the Director was Dr. G. P. Cressman, now [1976] head of the U.S. National Weather Service. I was head of the R&amp;D Section, consisting originally of Dr. F. G. Shuman (now [1976] head of the Na tional Meteorological Center), Major H. A. Bedient (Air Force), Cmdr. Paul Wolff (Navy), and Lt. Cmdr. William Hubert (Navy). The early members of the Applications Sec tion were Dr. J. Smagorinsky (head), Charles Bristor, Dr. G. Arnason, Louis Carstenson, and Lt. Col. H. Zartner (USAF). The chief and general factotum of the Analysis and Opera tions Section was Edwin Fawcett. Although the original group is now widely dispersed, all have risen in the world. To see this, all you need to do is look at the top echelons in NOAA's administrative structure. The first of the JNWP models were Thompson's two parameter baroclinic model and a stripped-down barotropic version. Both had previously been coded up by Gates and Zartner in 1953 at AFCRL. Although the latter circumstance did not dictate the JNWP Unit's choice of the IBM 701 as its first computer, it had been anticipated that the 701 would be a strong contender. Accordingly, the AFCRL code was writ ten for and tested on the 701 at IBM Headquarters in New York during the winter and spring of 1954. Not long after the establishment of the JNWP Unit, we started programming a three-level, quasi-geostrophic model, whose design was begun by Cressman while he was in Princeton in 1953. This model became operational at the beginning of routine nu merical weather prediction on 15 May 1955. The practice of numerical prediction was launched and on its way. What was a gleam in the eye in 1945 was a working reality in 1955. 6. Later developments in deterministic prediction That isn't to say, however, that the forecasts were perfect. By and large, they accounted for about 65% of the variance in day-to-day changes of the large-scale circulation patterns, so there was plenty of room for improvement. At that time, it was natural to assume that a considerable part of the residual error was due to the physical approxima tions of the quasi-geostrophic model, and that the use of the original, unmodified hydrodynamical equations (or "primi tive" equations) would do much to correct the defects. Al though this was even then demonstrably not the whole case, an appreciable effort has been put into the formulation of the "primitive" equations over the past 20 years. In effect, the return to the primitive equations took us straight back to Richardson, but with one important differ ence: we then realized that even small errors in the initial data may generate gravity-inertial oscillations of large amplitude, which may obscure or severely distort the large-scale pertur bations of primary interest. The rather stringent conditions for computational stability must still be satisfied, but this is, after all, only an economic constraint. It is also necessary, however, to adjust or "balance" the initial conditions in such a way that they do not generate large-amplitude gravity waves. These new difficulties were clearly recognized in Charney's (1955) paper on "The Use of the Primitive Equa tions of Motion in Numerical Prediction." Later papers, dealing with some important technicalities of the integration problem, are by Platzman (1958) and Hinkelmann (1959). The problem of "balancing" the initial conditions was at tacked independently by Bolin (1956)and Thompson (1956). In the latter paper, it was shown that a necessary and suffi cient condition for the exclusion of gravity-inertial oscilla tions is the omission of the total derivative of divergence from the divergence equation. With this approximation, the divergence equation is a "balance" equation, a diagnostic re lation between the pressure and wind fields. The next big advance, at least in my opinion, was Phillips's introduction of radiative and convective input of available potential energy and dissipation of kinetic energy into a hemi spheric model. Starting with conditions of relative rest, Phil lips first calculated the time evolution of the zonally symmetric state and then superposed a random and zonally asymmetric perturbation at the time when the zonal motion should have become baroclinically unstable. After a long time-integra tion (about three weeks of simulated time), he then computed the zonally averaged statistics of the model. These were found to agree remarkably well with observed monthly and seasonal statistics of the real atmosphere. This experiment</page><page sequence="9">Bulletin American Meteorological Society 763 pointed the way for extending short-range prediction methods to the problem of medium-range forecasting and climate studies. This work was done virtually single-handed, and, for it, Phillips deservedly received the Napier Shaw Prize in 1956. Although Phillips (1956) was first on the scene in this par ticular field, it is only fair to point out that the refinement of the radiative and transport calculations in more sophisti cated "general circulation models" has been an arduous and time-consuming enterprise—in total about two orders of magnitude greater than Phillips'original effort. I refer specifi cally to the work of Smagorinsky (1963), Mintzand Arakawa (1964), Leith (1965), Kasahara and Washington (1967), and Somerville et al. (1974). I would also like to emphasize that this problem will not be solved at a single stroke, or by some kind of "breakthrough." It will, in fact, require a long-term, cooperative effort on the part of creative scientists from every branch of the atmospheric sciences. We've come a long way, but we still have a long way to go. At the present time [1976], the standard numerical predic tion model of the U.S. National Weather Service is a six-level representation based on the primitive equations and an in itialization based on the "balance" equation, developed by F. G. Shuman and his colleagues over a period of years. The finite-difference formulation is very complicated, with vari ous linear smoothing and unsmoothing operations which make its non-linear behavior extremely difficult to analyze. Perhaps it is a little uncharitable to say that the behavior of the six-level primitive equation model at 500 mb is not strik ingly better than that of the old barotropic model. In some special respects and in certain regions and situations it is bet ter, but only slightly so. Something is still amiss. Before leaving the evolution of deterministic prediction models, I should at least mention some of the important con tributions to the purely mathematical representation of the atmospheric system and to the technological aspects of nu merical equation-solving. With regard to the former, one must give due recognition to the early work of Haurwitz (1940) and later generalizations of orthogonal representa tions by Baer and Platzman (1961). Extensions to empirical orthogonal representations were effected by Oboukhov (1960). Lorenz (1963), and Holmström (1963). In the realm of numerical analysis, one cannot avoid citing the von Neumann perturbation method for establishing the necessary conditions for computational stability, work that is not clearly documented, but certainly dates back to at least the early '40s. As it turned out, however, the necessary condi tions were not always sufficient in the case of highly non linear equations. This was first shown by Phillips (1959), who exhibited some simple examples of computational instability due solely to aliasing errors in calculating the interaction be tween modes of different scales. Although everyone is now aware of the existence of this type of instability, no one has yet found a completely satisfactory prescription for curing it. 7. Predictability and stochastic-dynamic prediction Up to 1956. virtually all of the people involved in the devel opment of numerical methods took a strictly deterministic view of the prediction problem—i.e., that the future state of the atmosphere is completely determined by its present state. In 1956,1 found myself in the invidious position of defending this view in public debate. I may have won the argument then, but I certainly wasn't comfortable with my own rea sons. Even if the proposition were true, the detailed current state of the atmosphere is known only in some probabilistic sense, and the ensuing prediction of the future is also correct only in that same sense. The question then arises: Granting that the reconstructed initial value of a variable at each grid point is its most probable true value, but with a known error distribution, how does the most probable value and its asso ciated error distribution evolve with time through the course of the prediction? If, for example, the error distribution be comes more and more "smeared-out," the prediction may be no better than a sheer guess, and possibly even worse than a forecast based only on the climatological mean value. Later in 1956, I tried to devise a mathematical technique for dealing analytically with this problem, and slightly more than half succeeded. Fortunately, Oboukhov saw my paper (Thompson, 1957) on the predictability question and passed it on to one of his most brilliant students, E. A. Novikov, who added the mathematical rigor and refinements needed to make this a complete work. I gather that this was his thesis problem. It was published in 1959. The question did not surface again until 1963, when Lo renz examined the gradual departures of states evolving from near-analogs of the initial state of the actual atmosphere. Al though this study was not completely conclusive owing to the fact that no near-analogs were found, the observed rate of departure was in good agreement with Novikov's and my theoretical estimates. The question was revived again by G. D. Robinson in his Presidential Address to the Royal Meteorological Society in 1967. He pointed out that, assuming a nonlinear "transfer of uncertainty" from unresolvable small scales of motion to larger scales, all predictive value would be lost after about two days. This argument was not totally convincing, how ever, because it led to an estimate of predictability that is lower than the level that is actually achieved in practice. Moreover. Robinson's estimate was based on Kolmogoroffs famous "—5/3 power" energy spectrum of 3-dimensional isotropic turbulence, whereas it has been observed by Wiin Nielsen (1967) and Kao et al. (1966) that the spectrum of large-scale atmospheric "turbulence" closely approaches the "—3 power" law predicted by various theories of 2-dimen sional or quasi-geostrophic turbulence. A consequence of the stronger decrease of energy toward small scales is that the "transfer of uncertainty" to larger scales is slower, increasing the range of predictability by a factor of two or three. Questions of this kind have been investigated in a broader and more realistic theoretical framework by Lorenz (1969) and by Leith and Kraichnan (1972), using the techniques and formalism of recent theories of turbulence. Recognizing that predictions should ideally be stated in terms of probability distributions, Epstein (1969) proposed to compute at least their low-order moments—i.e., the en semble mean value and the variance around the mean. This approach to stochastic dynamic prediction leads to closure problems that are identical to those encountered in the statis tical theory of turbulence or in any theory of the statistical</page><page sequence="10">764 Vol. 64, No. 7, July 1983 behavior of an inherently nonlinear system. Incomplete as it was, I regard this development as being highly significant and very promising. It has been pushed further by two of Ep stein's students, Fleming (1971a, b) and Pitcher (1974), and by Leith, who has outlined a simple Monte-Carlo method for computing the evolution of the probability distribution. To many of the meteorologists I have talked with over the past two or three years, some of these newfangled statistical mechanical notions seem pretty esoteric. Let me point out, however, that probabilistic information is precisely what we need in filling out the "payoff table" and in determining the optimum economic strategy in the face of any kind of uncer tainty. Perhaps I should also add that these are not strange notions, but are peculiarly American. The concepts of classi cal statistical mechanics were laid down in the 1880s by Josiah Willard Gibbs, a professor at Yale, regarded by his European colleagues as the greatest American scientist of his time. 8. The outlook The popular view of history, I expect, is that of a chronicle of long-gone and rather dusty events. If that were true, my ac count is not a history. I regard the development of numerical weather prediction as a process of growth and evolution of ideas, the most important stage of which is the present. I therefore conclude my account with a few remarks about the current state of affairs. There are now and always will be three major sources of error in numerical predictions. They are: 1) not totally re movable errors in the specification of initial conditions; 2) defects of the physical model and its mathematical formula tion; and 3) approximations of numerical representation. At the present time, the errors arising from these three different sources are roughly comparable in magnitude and are not, therefore, easily isolated. With regard to the first category of error, I would like to draw your attention to the Global Atmospheric Research Program (G ARP) and its subprogram FGGE (the First Glo bal GARP Experiment), due to be launched in about 1978. This program was first proposed in 1961 (in the NAS-CAS report) and later in 1967 at the International GARP Planning Conference, with the principal objective of nailing down the errors of prediction due to incomplete initial data and poor parameterization of small-scale transport processes. It isn't history yet [1976], but it promises to reduce one of the major sources of error. With reference to the physics of current numerical models, it is evident that the very large-scale components of the circu lation patterns are handled badly—particularly wave numbers 2 and 3. These quasi-stationary modes are undoubtedly asso ciated in some way with the variable surface properties of oceans and continents—reflectivity, heat capacity, heat con ductivity, roughness, topography, and the like. These effects have been incorporated in a number of general circulation models, but not with notable success, possibly owing to the rather cavalier treatment of vertical transport. Finally, it is far from clear that finite-difference methods are best or even well-suited to the requirements of numerical weather prediction. Except in instances when processes op erate in a discontinuous fashion, the advantages evidently lie with spectral methods or quasi-Lagrangian finite-element representations. I have the impression that it is now time to back off to a prudent distance, shake off our earlier precon ceptions and later investments, and start afresh. In the phraseology of Herodotus, such were the customs and manner of numerical predictors of weather. Thus ends my story. Thank you. Appendix A. Initial proposal to establish the Meteorology Project THE INSTITUTE FOR ADVANCED STUDY Princeton, N.J. May 8, 1946 Office of Research and Inventions Attention Lt. Commander D. F. Rex, Room 3446 Navy Department Constitution Avenue Washington 25, D.C. 1. The Institute for Advanced Study is a New Jersey corporation with its seat in Princeton, New Jersey. The Institute would be pre pared to accept a contract to carry out a project with the objective and under the conditions as described in what follows. Objective of the project. 2. The objective of the project is an investigation of the theory of dynamic meteorology in order to make it accessible to high speed, electronic, digital, automatic computing, of a type which is begin ning to be available, and which is likely to be increasingly available in the future. It is also expected that these investigations will give in dications as to what further observations are necessary—both of the laboratory type and of the field type—in order to make theoretical work, that is supported by such high speed computing, more fully effective. The primarily relevant details of these ideas are as follows: 3. These are some typical problems of dynamic meteorology, which are also probably among the most critical ones from the point of view of the present status of fundamental theory: (a) What is the mechanism and the flow pattern of the general, planetary circulation of the atmosphere? Can such a circulation be at all defined in any zonal-average sense, with zonal symmetry, i.e. dis regarding(or rather averaging over) the actual irregular distribution of the continents? (b) Can (a) be significantly treated in the troposphere alone, or is it necessary to draw at least the lower stratosphere, too, into the discussion? (c) A stability analysis of the polar front, or of extended fronts in general? (d) What is the mechanism and the flow pattern of the major cy clones? What can be said about their formation, their progress and their stability? (e) What is the detailed, quantitative functioning of the release mechanism of local instabilities? Quite apart from observational difficulties, to which we will re turn, e.g. in 11. below, these problems are well known to lead to ana lytical difficulties of a prohibitive character. In other words: Even if one were certain which of the numerous possible mathematical physical formulations of these problems corresponds best to reality, the equations to which these formulations correspond are of a very difficult partial-differential or even integro-partial-differential type, further complicated by alternative distinctions defined by inequali ties. It is utterly hopeless to try to resolve problems of this type by</page><page sequence="11">Bulletin American Meteorological Society 765 general mathematical analysis, and it has been recognized for a long time that numerical computational methods are the only ones which offer any prospect of really informative and specific results. Numerical computation, on the other hand, has been very limited in its capabilities up to a generation ago. The improvements which have been introduced during the last generation—mechanical and electrical "desk" multipliers (these are rather generally arithmetical machines) and partially automatically sequenced electro-mechani cal punch-card machines—changed this picture somewhat, but not very radically. Quite recent equipment—of a fully automatically se quenced and purely electrical (relay) type or even of an electronic type—has more extensive potentialities, but it has not yet had an op portunity to make its influence fully effective on the computing situation. For these reasons the efforts in meteorological theory were in the main limited by what was practical in actual computing, i.e. with computing methods and with computing equipment of the periods preceding the present one. It is therefore essential to visualize what these limitations are. 4. The speed of most computing equipment is in the main deter mined by its multiplication speed. This statement is valid with defi nite qualifications, all of which are fulfilled in the cases that we now consider, but which should nevertheless be evaluated and kept in mind: (a) This applies only to digital machines, which solve a problem by resolving it into discrete arithmetical operations. It does not ap ply to analogy machines, which may work on entirely different prin ciples, and at any rate by continuous operation. However, the exist ing types of analogy machines are neither sufficiently precise nor sufficiently flexible to be adequate for problems of the type described in 3. above. The remarks which are valid for digital machines only do therefore apply in the present situation. (b) Even for digital machines this statement must be taken with definite limitations and interpretations. It is, of course, not true that multiplications are the only time-consuming operations in a calcula tion. It is true, however, that other arithmetical operations are either a good deal faster (addition, subtraction) or a good deal less frequent (division, square rooting) than multiplication. On the other hand, non-arithmetical operations may consume considerable time: e.g. storing results, gaining access to and effecting the use of stored re sults—both these operations forming what is known as transfers— and also general logical control and discrimination operations. It is usually true that the time required for all arithmetical operations to gether will not exceed twice the pure multiplication time. (c) For the non-arithmetical—transfer and logical—operations, this may be said: they may require a good deal more time than the arithmetical operations, if the problem is not primarily mathemati cal—that is, algebraical or analytical—but rather combinatorial or logical. (E.g. various forms of sorting.) For primarily mathematical problems, however, the non-arithmetical time should not exceed the multiplication time seriously, or else there is usually reason to believe that the components of the machine are not well matched, that its system is not properly planned, integrated and balanced. These re marks apply very definitely to the present situation. (d) For the reasons (a)-(c) it seems to be appropriate to assert this: for the meteorological problems of the type indicated in 3. above, and assuming a well planned and balanced machine, the total computing time should not be essentially more than, say three times the pure multiplication time. (e) In talking of multiplication time, it is necessary to specify what precision is intended. Indeed, if n-decimal precision is in tended, multiplication means n by n digit multiplication. If various n's are compared, the work involved in a multiplication varies in proportion to n squared. In most machines this requirement is di vided evenly between the equipment and the duration of the opera tion: Both are essentially proportional to n. In meteorological work 5 to 7 decimals have usually been consid ered necessary. In most scientific work in the more involved parts of fluid dynamics 8 to 10 decimals were used. Most modern machines have 8 to 10 decimals, the Harvard machine allows either 11 or 23 decimal digit operation. We may therefore consider 10 decimals as a reasonable standard. The machines which will be built in the future may be binary instead of decimal, the one to be discussed in 5. below will certainly be binary. 10 decimal digits are equivalent to 33 binary digits. Since these future machines are likely to allow somewhat higher precisions, 35 to 40 binary digits seem a more reasonable standard. With these qualifications in mind, the speeds of computing, past present and future, may be characterized as follows: (A) Fast "desk" machines, the basic organs of what might now be called "human" or "hand" computing, have multiplication speeds of about 10 seconds (10 decimal digits). (B) Semi-automatic equipment, as referred to in 3. above is not essentially faster. Thus the standard IBM multiplier has a multi plication speed of 7 seconds (8 decimal digits). (C) The modern fully electrical (relay) machines, referred to in 3. above, have higher speeds: multiplication speeds of3.5 to some thing like .7 seconds (varying between 6 and 11 decimal digits). (D) The only existing fully automatic, electronic machine (ENIAC. Army Ordinance Department, University of Pennsylvania) is faster by several orders of magnitude: Multiplication speed of 3 milliseconds (10 decimal digits). However, this machine has a stor age process which must be considered slow by such standards (punch-cards holding 1 to 8 numbers, available in linear order only, at a rate of one in .6 seconds); therefore it is not well balanced in the sense of (c), (d) above. (E) Electronic machines which are now being planned or built should obtain multiplication speeds of 1 millisecond to .1 milli second (varying between 30 and 40 binary digits). (Cf. e.g. 5. below.) They should be well balanced in the sense of (c), (d) above. Thus accelerations over past computing methods by factors of the order of 10 and possibly much more—order of 1000—are already possible, and accelerations by factors of 10,000 to 100,000 will be come possible in a few years, as the projects mentioned in (E) above are carried out. 5. Specifically, concerning the projects mentioned in 4. (E), this might be said: Several such projects are now in various stages of execution, at the University of Pennsylvania, at M.I.T., at the Institute for Ad vanced Study, and possibly also at other places. In what follows we will refer to the Institute for Advanced Study project only, since this could be directly integrated with this proposal. We propose to build, with help from other agencies, a fully auto matic, electronic, digital machine with the following characteristics: (a) Binary operation, with decimal-binary and binary-decimal conversions at the ultimate inputs and outputs. However, for purely scientific problems the conversions are of subordinate importance. (b) Precision between 36 and 40 binary digits. (c) Vacuum tube operation at about a megacycle rate. (d) Multiplication time of .1 to .2 milliseconds. Division time of .2 to .3 milliseconds. Addition and subtraction times of .01 to .02 milliseconds. (e) An electronic memory of about 4,000 numbers (36 to 40 bi nary digits each) with transfer speeds of .01 to .02 milliseconds. (f) Fully automatic, electronic logical and mathematical control, by coded instructions as in (e). (g) Ultimate inputs and outputs on several tapes—probably magnetic tapes of the sound-recording type. (h) Alternative outputs on oscilloscope screens, allowing direct graphing of the results—these graphs can be viewed or recorded by photographing. (i) Automatic checking, probably by running two identical ma chines in parallel, continuously compared at many points by elec tronic coincidence circuits. This allows an automatic identification of most malfunctions, and recognition of the part of the machine in which they occurred. Further automatic checking by arithmetical means. (j) Size of the machine (assuming doubling according to (i)): Less than 9,000 vacuum tubes, dissipating less than 20 kilowatts. (k) Sample logical control-code, which allows the "setting up" of problems with not essentially more work than for a human computer group. We anticipate that a preliminary model may work in about two years and a final model in three years. We hope to carry out this pro gram with considerable help from the Princeton Laboratories of R.C.A.</page><page sequence="12">766 Vol. 64. No. 7, July 1983 We expect that such a machine will change the conditions, methods and applications of computing fundamentally. It should compute 50.000 to 100,000 times faster than is possible at present (cf. 4. (A), (B) above and (d) here); it should therefore change the entire inner economy of computing. It will therefore make the developing of entirely new methods of approximation mathematics highly indi cated and profitable. It will accordingly cause a complete change in our estimation as to which problems can be solved by computation. For these reasons we propose to use this machine solely for ex ploratory and research work: To develop new approximation and computing methods, to test them, to explore new fields in applied mathematics and in mathematical physics, which become now ac cessible to computation. Among the fields which we intend to study in this manner, the one of dynamic meteorology is among the most important. Another field, which should have the highest priority, isthat of turbulent fluid motion—and this is also essential fora more fundamental approach to dynamic meteorology. 6. In carrying out this program considerable preparatory studies will be required, for which the 2 to 3 years needed to build the ma chine offer a welcome and natural opportunity—and one of not at all too long duration. The studies on new approximation and comput ing methods, referred to in 5. above, will be carried out during this period, in the sense indicated. Studies of meteorological theory are necessary in tire same sense. A careful analysis of the present status of meteorological theory, carried out in particular by Dr. John von Neumann of the Institute for Advanced Study and Dr. C. G. A. Rossby of the University of Chicago, indicates that even if computing equipment of the type in dicated in 5. above were immediately available, we would not be able to use it at once. This is even valid for some more limited, but never theless very interesting and important, problems, which might be solved by ENIAC. Indeed, the possibilities that are opened up by these devices are so radically new and unexpected, that the theory is entirely unprepared for them. There was no practical motivation in the past to work out those parts of meteorological theory on a mathe matical and analytical level, which, in order to become really effec tive. would require calculational methods that are 1.000 to 100,000 times faster than what seemed possible at the time! A complete reas sessment or revaluation of the theory is therefore an absolute prerequisite. 7. It is therefore our proposal to carry out this reassessment. This should be done by a group of about 5 or 6 first-class younger mete orologists. who should work in the closest possible association with the group which is planning and building the machine referred to in 5. and 6. above. For this reason we consider it essential that the group should be located in Princeton, and that it should work under the general direction of Dr. John von Neumann, who is directing all phases of the computer program of the Institute for Advanced Study. It is furthermore quite essential to provide for ample consult ing opportunities, in order to be able to secure the interest and the cooperation of the leading meteorologists of the country, as well as that of several physicists and engineers whose help is essential. The last mentioned circumstance deserves special emphasis: We antici pate that new physical measurements will gradually turn out to be essential for the adequate integration of our program, and that these w ill in their laboratory phase require the help and advice of various physicists, and in their field phase new instrumentations, and hence the help and advice of meteorologists with a broad administrative experience and of engineers. Time Schedule and Working Schedule of the project. 8. We anticipate that the project as outlined above will have to ex tend over several years. Indeed, the phase discussed in 2. and 6., 7. above is only a preliminary phase, while the principally fruitful phase begins only where the former ends: When a machine as de scribed in 5. above becomes available, the exploitation of the theo retical methods developed in the preliminary phase can begin. At this moment the main need is to discuss the preliminary phase. It seems reasonable to time it so as to have it coextensional with the period required for the building of the machine—i.e. to last 2 to 3 years. This duration, however, also seems to be reasonable and ap propriate per se. It is again desirable to elaborate this in somewhat more detail. 9. Assuming that an adequate group of meteorologists will be as sembled in Princeton about the fall of 1946, it should take about 6 months to carry out a preliminary analysis of the main representative problems of dynamic meteorological theory—e.g. of the problems 3., (a)-(e), or of some equivalent list. During this period these main physical and observational-meteorological uncertainties should be assessed, alternative mathematical formulations of the resulting possibilities should be developed, and the analytical structure of the theories which are thus obtained should be investigated. At this point an outside board of meteorological and physical consultants should be brought in, to evaluate the respective merits of the alterna tives which are evolved, to assess the relative difficulties of the experi mental and observational work which may be required in each case, and to advise the project as to the directions which hold most prom ise. This should require a few months' work. After that it will be pos sible to carry out a concentration of the project towards two or three definite problems and specific problem formulations—presumably certain alternatives derived from 3. (a)-(e) above. About another 6 months' work by the Princeton group may then be required to work out the analytical details of these specific problems. This would take the project well to the end of 1947. In the year which follows definite approximation and computing techniques for the selected problems should be worked out. During the second half of 1948 the first model of the machine should become available. This will make the testing of the techniques in question possible, at a gradually increasing rate as the first model and the methods to use it become familiar. With the end of this period, in the course of 1949, the final model of the machine should be completed and the project, as outlined, would have also achieved its objective. 10. The possibilities opened up by this work need only be referred to in a very general manner. Entirely new methods of weather predic tion by calculation will have been made practical. It is not difficult to estimate that with the speeds indicated in 5. above, a completely cal culated prediction for the entire northern hemisphere should take about 2 hours per day of prediction. A new. rational basis will have been secured for the planning of physical measurements and of field observations in meteorology, since complete mathematical theories and the methods to test them by comparing experience with the rig orously calculated consequences of these theories will have been ob tained. And finally the first step towards influencing the weather by rational, human intervention will have been made—since the effects of any hypothetical intervention will have become calculable. 11. It is much more difficult to predict what new measurements and observations will turn out to be desirable. One thing, however, seems to be very probable: that more radiation measurements, more in formation about the radiative properties of such components of the atmosphere as water vapor and carbon dioxide will be necessary. It seems also probable that more information about the air flows in the southern hemisphere will be desirable: Because of the relative pau city of large continental masses, the southern hemisphere would seem to be a better testing ground at least for theories of the general, planetary circulation (cf. 3. (a (above (than the northern hemisphere. Personnel. 12. As mentioned in 7. above, the nucleus of the proposed project would be a group of 5 or 6 first class younger meteorologists. Every effort should be made to secure the services of Dr. H. Wexler from the U.S. Weather Bureau to lead and supervise this group. There seems reason to hope that this will indeed be possible. Other very de sirable candidates for inclusion in this group are Dr. H. Pekeris, now at Columbia University. Dr. R. B. Montgomery, now at the Woods Hole Oceanographic Institution, and Captain G. Hunt of the Army Air Forces, now at Princeton. Further selections should be made as the group crystallizes. The level of this group should be definitely very high and aca demic. The general coordination of the various phases of this project with each other and with the Institute's computer project should rest with Dr. John von Neumann. 13. Another essential phase of the project would consist in securing the services of a prominent group of consultants and advisers, as mentioned in 7. and in 9. above. This group should include such mete orologists as Dr. C. G. A. Rossby of the University of Chicago. Dr. H. U. Sverdrup of the Scripps Oceanographic Institution at La Jolla. Dr. J. Bjerknes of the University of California at Los Angeles; phvsi</page><page sequence="13">Bulletin American Meteorological Society 767 cists with their main interest in questions of radiation, molecular physics and astrophysics, such as E. Teller of the University of Chi cago, and S. Chandrasekhar of the Yerkes Observatory; one or more aerodynamicists, including Dr. Th. von Karman of the California Institute of Technology; and various experts in other fields, such as W. Weaver of TDRC and the Rockefeller Foundation, and V. K. Zworykin of the Princeton Laboratories of RCA. There is reason to believe that a majority of the persons mentioned would accept. The total group whose advice might be sollicited at various times may consist of 8 to 10 persons. 14. The clerical help and the physical equipment required by this group is not likely to be considerable. About 2 clerk-computers may suffice for the first purpose, and nothing beyond ordinary office fa cilities is needed for the second. The services of the Institute's com puter project will be automatically available. Experimental and ob servational help should be secured through personal connections, and through the advisory group mentioned in 13. above. Equipment and Facilities available and needed. 15. This subject is partly covered in 14. above. Beyond that, this may be said: The Institute for Advanced Study can provide some office space in its present building, Fuld Hall. This, however, will not be adequate, and additional space will have to be secured by rental in Princeton. This is satisfactorily feasible. The Institute will provide the services of certain members of its staff: Dr. John von Neumann for the overall direction and coordina tion of the project, the members of its computer project for coopera tion and consultation when required. The Institute will take care of the general administration of the project, of securing personnel for the staff and the consulting and advisory body. Yearly Budget. 16. The yearly budget requirements of the project are as follows: Staff salaries: 6 members, averaging $5,500 $33,000 Overhead: 40% of the staff salaries. This includes rental of office space. 13,200 Consulting: 10 consultants, averaging 15 days per year, at $25 per day. 3,750 Travel: For 10 consultants, averaging 2 trips per year at $125 per trip, plus $1,500 per year for staff travel. 4,000 Clerical, computing: 2 clerk-computers, averaging $2,500 5,000 Miscellaneous expenses 2,000 $60,950. Thus a total yearly budget of about $61,000 would seem to be necessary. Frank Aydelotts Director, Institute for Advanced Study Appendix B. Letter from Jule Charney to Philip D. Thompson February 12, 1947 Lieutenant Philip D. Thompson Institute for Advanced Study Princeton, New Jersey Dear Phil, I thoroughly agree that the questions you propound lie at the very heart of the whole problem, not only of numerical forecasting but of the solution of the equations of motion by any means whatever, and I am very pleased to hear that you are now grappling with them. As you know, I have long been aware of these questions and have from time to time sounded off at some length about them. I am therefore not only willing but anxious to discuss them with you. Let us begin with your last question, "Why don't the large scale atmospheric disturbances move with the speed of sound?". One answer was given by a scientific pundit writing in the Readers Digest. It is obvious he says, that man exists only because of a very improb able concatenation of events. If the solar radiation were twice as great the oceans would dry up and man would simply find existence too uncomfortable. Or if the earth rotated at a much reduced speed he would freeze in winter and roast in sumer, etc., etc. Donc, Dieu existe. One could add in the same vein that if cyclones traveled with the speed of sound man would be whisked right off the earth, which is manifestly impossible according to our learned scientist. In case these anthropomorphic arguments leave you cold, and you do not believe in the Bible or even in the Readers Digest, I propose the fol lowing argument. In the terminology which you graciously ascribe to me we might say that the atmosphere is a musical instrument on which one can play many tunes. High notes are sound waves, low notes are long inertial waves, and nature is a musician more of the Beethoven than of the Chopin type. He much prefers the low notes and only occa sionally plays arpeggios in the treble and then only with a light hand. The oceans and the continents are the elephants in Saint-Saens' animal suite, marching in a slow cumbrous rhythm, one step every day or so. Of course, there are overtones; sound waves, billow clouds (gravity waves), inertial oscillations, etc., but these are unimportant and are heard only at N.Y.U. and M.I.T. To become literal we might say—the energy that goes into an at mospheric disturbance depends on the initial mode of excitation. A forced perturbation of long period produces a disturbance of long period. A perturbation in which energy is released so fast that the air does not have a chance to get out of the way could produce sound waves of very large amplitude to consume this energy. But with the exception of volcanic eruptions and atom bombs such agencies are never found. Even the atom bomb converts only a small part of its energy into waves of concussion. Let us illustrate by considering the motion of waves in a constant barotropic zonal current. The equations of motion are du du du du 1 dp — + u h v h w — = 2flv sin 6 — — 20m&gt; cos 6 dt dx dy dz p dx dv dv dv dv 1 dp h u h v h m —2flv sin 0 ât dx dy dz p dy dw dw dw dw 1 dp + u h v h w = 2flu cos d — g ât dx dy dz p dz and for waves of small amplitude and infinite lateral extent, propa gated in the x-direction, they become du du dn — + U — = fv dt dx dx dv dv dn — + U — = —fu dt dx dy dir dir du + U fUv — tj) dt dx dx where U is the zonal speed,/ = 2fi sin 8. u, v. n respectively the veloc ity components and barotropic pressure function ( fôp/p) of the dis turbance, 0 is the undisturbed value of dp/ dp and as usual we neglect the vertical components of acceleration and coriolis force as well as the horizontal component of coriolis force involving w. These equa tions were solved by Rossby for a disturbance of the form u = Ae v = Be 2iri/L(.x—ct) 2ni/L(x—ct) The solution gives the value of the velocity c: ßL2 LY c V — c — ■ 4tr 4tt &lt;t&gt; ~ (iU-cf where ß is equal to df/dy and is assumed to be constant. If we had assumed that the atmosphere were incompressible and homogene</page><page sequence="14">768 ous &lt;p would be the dynamic height. Now here is the important point. The last equation has three roots. One is very nearly equal to the solution of this equation without the (U — c)2 term in the denominator of the right hand side. The other two roots are nearly equal to those obtained by setting the denomi nator on the right equal to zero. This means that there are three modes of vibration, and it is easy to see that the first root corres ponds to iong waves and the remaining two to gravitational waves traveling in opposite directions. (Sound waves are eliminated by the assumption of no vertical acceleration (quasi-horizontal motion)). The general solution for a given initial disturbance would embrace both long inertial and gravitational waves. But if, for example, the initial disturbance were harmonic and had a period equal to that of the long waves no energy at all would go into the gravitational wave components. In general, of course, every disturbance, if broken down into harmonic components by Fourier analysis, would exhibit components with all periods, and therefore some of the energy would produce gravitational oscillations which, you will observe, have velocities of the same order of magnitude as that of sound. In an isothermal barotropic atmosphere, for example, two solutions of the above velocity equation are given approximately by 2 dP (U — c) = — = RT dp whereas for soiind we have (U-cf = —RT~RT Cv But since most of the energy of the initial disturbance goes into long period components very little of the energy will appear in the gravita tional wave form. This leads us to the next problem, namely, how to filter out the noise. Pardon me, but let us again think metaphorically. The atmos phere is a transmitter. The computing machine is the receiver. The receiver is a very good one indeed, for it produces no appreciable noise itself, i.e. all noise comes from the input. (I am supposing that you can compute to any desired order of accuracy.) Now there are two ways to eliminate noise in the output. The first is to make sure that the input is free from objectional noises, or the second is to em ploy a filtering system in the receiver. Translating, the first method implies that the unwanted harmonics shall be eliminated from the raw data by some type of harmonic analysis; the second that you transform the equations of motion and make approximations in such a way that the bad harmonics are automatically eliminated. Let us consider the second method and illustrate by means of the fore going example of wave motion. If, in the solution of the equations of motion, whenever a term containing the factor 1 — (U — c)2 /&lt;f&gt; ap pears. we replace if by 1, then the resulting equation for c would be* ßL2 L2f2 c 47t2 47r2 &lt;/&gt; instead of ßL2 LY c U-c- = 4/ 4n2 &lt;t&gt; — (U — c)2 Thus the equation would have only one root and that one would cor respond to the long waves. But this does not tell us what to do with the equations of motion themselves. If you work backward you find that the approximation is equivalent to ignoring the x-component of the acceleration i.e., to assuming that the north-south perturbation velocity is geostrophic. Now don't jump to the conclusion that the latter approximation may always be made. We can do it here because we have assumed no variation in the streamline pattern in the north south direction. I do not know what will happen if you consider waves of finite lateral extent as Haurwitz does. Here the problem be comes more complicated since Haurwitz assumes that V#'V = 0, •This approximation is justified since &lt;f&gt; is of the order of the square of the speed of sound. Vol. 64, No. 7, July 1983 which, of course, is tenable only for barotropic motion. In my paper on baroclinic waves I find that one has to make a number of approx imations of the type (U — c)2/(f&gt; &lt; 1 to arrive at a tractable system of equations, from which gravitational waves, Helmholtzian waves, sound waves and inertial oscillations are eliminated. But I also con sider only waves of infinite lateral extent. I still don't know what types of approximation have to be made in more general situations. On the other hand, don't think that compressibility is what botches up the works. Even if you were to replace the actual atmos phere by a non-homogeneous incompressible atmosphere with the same stability you would still have gravitational waves. However, if you accept the consequences of the above reasoning you will perhaps share my conviction that there is a general type of approximation or transformation or what have you that will eliminate the noise and the problem is now to find it! Enough of this. Let us change the subject. Do you remember my suggestion that you study simple types of finite amplitude motions as a preliminary step to attacking the general forecasting problem? In particular, the Rossby wave model? Well, for various reasons I do not think that that particular study will lead to anything very inter esting. If a barotropic system is stable, then the horizontal diver gence is negligible and the first order approximation is very nearly the exact solution. On the other hand, if the motion is unstable and developes into vortices, the successive approximations will be signifi cant. I have begun an attack on several such problems and the results look promising. I would like to discuss some of these things with you personally since the time scale of interchange of ideas by correspondence is just too great. If I had the dough I would hie myself to Princeton and have it out with you, but naturally I haven't. Our Quarter ends on the 21st of March and I am catching the boat for Norway on the 22nd so I will not even be able to stop over in Princeton. With all that Navy money lying around why don't you invite me to come to Princeton for a couple of days? In any case, write and let me hear your reac tions. Also give my best regards to Panofsky. Sincerely yours. Jule Charney References Baer, F., and G. W. Platzman, 1961: A procedure for numerical in tegration of the spectral vorticity equation. / Meteor., 18,393-401. Bjerknes, V.. 1904: Das problem der Wettervorhersage, betrachtet von Standpunkte der Mechanik under der Physik. Meteor. Zeits., 21, 1-7. Bolin. B., 1956: An improved barotropic model and some aspects of using the balance equation for three-dimensional flow. Tellus, 8, 61-81. Bushby, F. H., and M. K. Hinds, 1954: The computation of forecast charts by application of the Sawyer-Bushby two-parameter model. Quart. J. Roy. Meteor. Soc.. 80, 165-173. Charney, J., 1948: On the scale of atmospheric motions. Geofys. Publ, 17, 17 pp. , 1949: On a physical basis for numerical prediction of large-scale motions in the atmosphere. J. Meteor., 6, 371-385. , 1955: The use of the primitive equations of motion in numeri cal prediction. Tellus, 7, 22-26. . and A. Eliassen, 1949: A numerical method for predicting the perturbations of the middle latitudes westerlies. Tellus, 1, 38-54. , and N. A. Phillips, 1953: Numerical integration of the quasi geostrophic equations for barotropic and simple baroclinic flows. J. Meteor.. 10, 71-99. , R. Fjörtoft.and J. von Neumann, 1950: Numerical integration of the barotropic vorticity equation. Tellus, 2, 237-254. Eady, E.T., 1952: Note on weather computing and the so-called 2'/2 dimensional model. Tellus, 4, 157-167.</page><page sequence="15">Bulletin American Meteorological Society 769 Eliassen. A., 1948: The quasi-static equations of motion with pres sure as independent variable. Geofys. Publ.. 17, 44 pp. , 1952: Simplified dynamic models of the atmosphere, designed for the purpose of numerical prediction. Tellus, 4, 145-156. Epstein, E. S., 1969: The role of initial uncertainties in prediction. J. Appt. Meteor., 8, 190-198. Fleming, R. J., 1971a: On stochastic dynamic prediction: I. The energetics of uncertainty and the question of closure. Mon. Wea. Rev.. 99, 851-872. , 1971b: Stochastic dynamic prediction: II. Predictability and utility. Mon. Wea. Rev., 99, 927-938. Haurwitz, B.. 1940: The motion of atmospheric disturbances on the spherical earth. J. Mar. Res., 3, 254-267. Hinkelmann. K., 1959: Ein numerisches Experiment mit den primi tiven Gleichugen. In The Atmosphere and the Sea in Motion, edited by B. Bolin, Rockefeller Institute Press, New York, pp. 486-500. Holmström. I„ 1963: On a method for parametric representation of the state of the atmosphere. Tellus, 15, 127-149. Kao, S. K., L. L. Wendell, and D. A. Noteboorn, 1966: Longitude time power- and cross-spectra of atmospheric quantities. Re search Report on Atmospheric Turbulence and Transport, Uni versity of Utah, Salt Lake City, 240 pp. Kasahara, A., and W. M. Washington, 1967: NCAR global circula tion model of the atmosphere. Mon. Wea. Rev.. 59, 389-402. Leith, C. E., 1965: Numerical simulation of the earth's atmosphere. In Methods in Computational Physics, Vol. 4. Applications in Hy drodynamics. edited by B. Adler, Academic Press, New York, pp. 1-27. , and R. H. Kraichnan, 1972: Predictability of turbulent flows. J. At mos. Sei., 29, 1041-1058. Lorenz, E. N., 1963: The predictability of hydrodynamic flow. Trans. New York Acad. Sei., Series 2, 25, 409-432. , 1969: Atmospheric predictability as revealed by naturally oc curring analogues. J. Atmos. Sei., 26, 636-646. Mintz, Y., and A. Arakawa. 1964: Very long-term global integration of the primitive equations of atmospheric motion. Paper pre sented at the WMO-IUGG Symposium on Research and Devel opment Aspects of Long-Range Forecasting. Tech. Note 66, World Meteorological Organization, Geneva, pp. 141-155. Novikov, E. A., 1959: Contributions to the problem of the predict ability of synoptic processes. Izv. Geophys. Ser., 1721. (English translation: Am. Geophys. U. Transi, 1209-1211.) Oboukhov, A. M., 1960: The statistically orthogonal expansion of empirical functions. Bull. Acad. Sei. USSR, Geophys. Ser., 288-291 (English translation). Phillips, N. A., 1951: A simple three-dimensional model for the study of large-scale extratropical flow patterns. J. Meteor., 8, 381-394. . 1956: The general circulation of the atmosphere: A numerical experiment. Quart. J. Roy. Meteor. Soc.. 82, 357-361. , 1959: An example of non-linear computational instability. In The Atmosphere and the Sea in Motion, edited by B. Bolin, Rocke feller Institute Press, New York, pp. 501-504. Pitcher. E. J., 1974: Stochastic dynamic prediction using atmos pheric data. Ph.D. thesis. University of Michigan, Ann Arbor. Platzman, G. W.. 1958: The lattice structure of the finite-difference primitive and vorticity equations. Mon. Wea. Rev., 86, 285-292. Richardson, L. F.. 1922: Weather Prediction by Numerical Process. Cambridge University Press. London, 236 pp. (Reprinted by Dover Publication, New York (1965).) Robinson, G. D., 1967: Some current projects for global meteoro logical observation and experiment. Quart. J. Roy. Meteor. Soc., 93, 409-418. Sawyer, J. S., and F. H. Bushby, 1953: A baroclinic model suitable for numerical integration. J. Meteor., 10, 54-59. Smagorinsky, J., 1963: General circulation experiments with thfe primitive equations. Mon. Wea. Rev., 91, 99-174. Somerville, R. C. J.. P. H. Stone, M. Halem, J. E. Hansen, J. S. Hogan, L. M. Druyan, G. Russell, A. A. Lacis, W. J. Quirk, and J. Tenebaum, 1974: The GISS model of the global atmosphere. J. Atmos. Sei., 31, 84-117. Thompson, P. D., 1953: On the theory of large-scale disturbances in a two-dimensional baroclinic equivalent of the atmosphere. Quart. J. Roy. Meteor. Soc., 79, 51-69. , 1956: A theory of large-scale disturbances in nongeostrophic flow. J. Meteor., 13, 251-261. , 1957: Uncertainty of initial state as a factor in the predictabil ity of large-scale atmospheric flow patterns. Tellus, 9, 275-295. , and W. L. Gates, 1956: A test of numerical prediction methods based on the barotropic and two-parameter baroclinic models. J. Meteor., 13, 127-141. von Helmholtz, H. V., 1858: Ueber Integrale der hydrodynamischen Gleichungen, welche den Wirbelbewegungen entsprechen. J. für die reine und angewandte Mathematik, 55,25-55. (English transla tion—C. Abbe, 1891: The Mechanics of the Earth's Atmosphere. Smithsonian Miscellaneous Collection, Washington, D.C., pp. 31-57.) Wiin-Nielsen, A., 1967: On the annual variation and spectral distri bution of atmospheric energy. Tellus, 19, 540-559. •</page></plain_text>