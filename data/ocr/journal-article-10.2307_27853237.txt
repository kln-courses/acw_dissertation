<plain_text><page sequence="1">What Is Artificial Intelligence Anyway? Raymond Kurzweil Fans of Norbert Wiener will be familiar with the fol lowing German children's song: Knowest thou how many stars stand in the blue tent of heaven? Knowest thou how many clouds pass far over the whole world? The Lord God hath counted them, that not one of the whole great number be lacking (1). Wiener was fond of reciting this song partly because it combined two of his favorite topics?astronomy and meteorology?but also because he enjoyed pointing out that experts were capable, in the mid-twentieth century, of accomplishing those functions. From Wiener's writ ings, one senses some pride that scientists could perform at least one of God's chores. We might, in turn, take pride that we have now turned over the job of keeping track of our stars and clouds to computers, which are probably doing a more thorough job than the scientists of Wiener's day, although perhaps not yet up to God's standards. We might also ask the question that if computers can now take over one of God's many chores, what about human chores? Indeed, what about our most prized chore?that of thinking? Machines that think, or as a recent book title put it, machines who think, have been a topic of reflection and debate since Charles Babbage designed his "Analytical Engine" in the early nineteenth century and ruminated on the subject with his friend the Lady Ada Lovelace, daughter of Lord Byron (2). In those days there were few backers for such ambitious projects, and those few tended to be skeptical. In the last several years, however, the subject of thinking machines, or to put it more con servatively, machines that at least appear intelligent, has become of consuming interest to a widespread audience, Raymond Kurzweil is an inventor whose work centers on practical applications of pattern-recognition technology. In 1976 he introduced the world's first optical scanning system?the Kurzweil Reading Machine for the Blind?which scans printed documents in any typeface and converts them to synthetic speech. In 1984 one of his companies began manufactur ing the Kurzweil 250 electronic keyboard?a digital instrument that uses AI techniques to recreate the complex sounds of the grand piano and other orchestral instruments. Kurzweil is currently leading a team in the development of a voice-activated word processor with a 10,000-15,000 word vocabulary. This article is adapted from Kurzweil's keynote address to the annual ieee International Conference on Computer Design in October 1984. Address: Kurzweil Applied Intelligence, 411 Waverley Oaks Road, Waltham, MA 02154. As the techniques of computing grow more sophisticated, machines are beginning to appear intelligent?but can they actually think? including Wall Street and Madison Avenue. Along with the increased attention has come in creased controversy. Experts still debate about what constitutes an intelligent machine, and they still disagree about what constitutes the field of study that has been called artificial intelligence (or simply AI) since John McCarthy gave it that name at a Dartmouth conference in 1956. The uncertainty has always been there, but now there is more at stake. AI has been compared to bioen gineering in having brought an academic discipline together with venture capital and public attention. Yet, while bioengineering has its own controversies, it lacks this one. Articles and talks do not start with the question, What is bioengineering anyway? If it has to do with splicing genes, no one doubts that it is bioengi neering. The controversy surrounding AI is evidenced by a certain amount of discord within the field. Academic researchers have accused some industrial AI develop ment of being shallow and giving too high a priority to short-term commercial goals rather than long-term re search. Industrial researchers have accused academic AI of providing superficial demonstrations, of not devel oping robust systems that really work. Companies have accused their competitors of not using real AI tech niques, and so on. Unfortunately, we are not likely to find published any time soon the definitive work that will lay this confusion to rest, once and for all. Since that will not be the result of this article, either, I will refrain from mak ing such an assertion. But an examination of issues re lated to artificial intelligence may nonetheless help us to identify those questions that cannot be answered, as well as those that are worth answering. In the latter category, I would include an understanding of what the discipline of artificial intelligence is now capable of achieving, what it will be capable of achieving, and how we can best reach those goals. I will approach these questions and the central question?What is artificial intelligence anyway??from three perspectives: the past, the present, and the future. AI past Rather than recite the usual litany of early experiments, I will begin by examining instead some of the intellec tual roots of the AI movement, which I feel will be more revealing. One is tempted to go back to mechanical cal culating engines, which were proposed as early as the seventeenth century by Pascal and Leibniz. Perhaps the 258 American Scientist, Volume 73</page><page sequence="2">most famous was Babbage's analytical engine of 1833, the first machine designed to employ a stored program, which was to be read in from punched cards. Although the machine was never built, programs were written for it, and intense discussions by Babbage and Lovelace have been preserved which reflect on such issues as self modifying code, programs that would play chess or compose music, and how (or whether) automated in telligence might be related to human thought. While fascinating in retrospect, these early discus sions did not immediately produce the ferment from which computers and artificial intelligence were to spring. These emerged instead from some of the more powerful intellectual movements of the early twentieth century. One of them was the philosophical movement called logical positivism, which strove to examine ep istemology with the same rigor that was then coming into fashion in the world of mathematics. Epistemology is the study of the origins, methods, and limits of knowledge?what we can know and how we can know it. Although computers as such were not on the minds of the early logical positivists, it is not surprising that an attempt to define the nature of knowledge with math ematical precision would have relevance to the subse quent emergence of computers. One work that was influential in the development of logical positivism was the Tractatus Logico-Philosophicus by Ludwig Wittgenstein, published in 1921 (3). The book is interesting both in what it says and in its own internal structure. The treatise contains only seven primary for mal statements, numbered 1 through 7. To help us along, Wittgenstein also includes several levels of modifying statements to clarify the primary statements. For exam ple, statements 1.1 and 1.2 modify statement 1. In turn, 1.1.1 is provided to help explain 1.1, and so on. The reader has the choice of reading the book from left to right, from the top down, or even in reverse. The mod ular structure would, I am sure, please proponents of good programming style. Wittgenstein starts with statement 1, "The world is all that is the case," and ends with statement 7, "What we cannot speak about we must pass over in silence/' As you might gather from these starting and finishing points, it is an ambitious work. The statements in be tween deal with a formal concept of language?what can be said?and make a connection between what can be said and what can be thought or known. He states: 4.0.0.3.1 All philosophy is a "critique of language." 5.6 The limits of my language mean the limits of my world. 5.6.1 We cannot think what we cannot say. In Wittgenstein's world, there are certain elemen tary facts, there are propositions about relations between elementary facts, and there are certain allowable trans formations on such propositions that yield composite propositions. His model of human thought is that we can receive sense impressions which comprise elementary facts. We can then transform these elementary facts and derive relationships among them according to certain allowable logical processes. Any thought outside this scheme is either false or nonsensical. While I have oversimplified the theory, I have done so to bring out two important points. Both have a bearing on the intellectual roots of artificial intelligence. The first?and Wittgenstein makes this point in several different ways?is that what we can think is what com putes. He makes a direct link between human thought and a formal process that can be described only as com putation. To reorder Wittgenstein's statements, we cannot think what we cannot say; we cannot say, or at least we ought not say, what is meaningless in the lan guage we are speaking; and statements in any language are indeed meaningless unless they can be derived from a formal sequence of computation-like transformations on a database of elementary propositions. This description of human thought as a formal se quence of computation would be restated two decades later in the Church-Turing thesis, which I will discuss shortly. It is not a thesis that everyone familiar with it necessarily accepts, and it remains controversial in the philosophical literature. Wittgenstein himself ended up rejecting it, and in his later works had a lot to say about subjects that he had argued in the Tractatus should be passed over in silence. The second point made in the Tractatus which would have significance later to computational theorists was that thought is embedded in language. It is also in teresting to note that language as conceived in the Tractatus has more of the quality of the programming language lisp or even prolog than it does of Wittgen stein's native German. Although Wittgenstein ended up rejecting many of the ideas expressed in his first major work, his thoughts were elaborated into the formal theory of logical posi tivism by such philosophers as Alfred Ayer and Bertrand Russell. Russell also extended this new formalism in epistemology to the world of mathematics in the seminal work he wrote with Alfred North Whitehead, the Prin cipia Mathematica, which represented the founding of modern set theory (4). From the foundations of set theory came, in turn, a startling new theory by the mathematician Alan Tur ing, in 1937, which would lead to modern computational theory. In effect, Turing restated the assertion originally made by Wittgenstein, that true thought is computation, only this time he stated the idea explicitly as a formal definition of computation rather than of human thought. One purpose of Turing's pioneering work was to address a problem that the Principia had failed to an swer?the so-called twenty-third problem of the great German mathematician David Hilbert. This question, briefly stated, is the question of whether it is possible to devise a method that can establish the truth or falsity of any statement in a certain language of logic called the predicate calculus (5). In examining what Hilbert meant by the word "method," Turing came up with a formal definition of method as algorithm. He also devised an enduring concept of an algorithm as a program that could run on what has become known as a Turing ma chine. The Turing machine has persisted as our primary theoretical model of computation because of its combi nation of simplicity and power. Its simplicity derives from its very short list of capabilities. It can read a tape and determine its next operation, based on whether it reads a zero or a one; it can move the tape left or right; it can write a zero or a one on the tape; it can jump to another command; and it can halt. As for its power, 1985 May-June 259</page><page sequence="3">Turing was able to show that this extremely simple machine can compute anything that any machine can compute, no matter how complex. There are two reasons why the Turing machine created the stir that it did. First is this astonishing com bination of simplicity and power. Second, Turing dis covered something unexpected?that there are well defined problems for which we can prove that an answer exists but for which we can also prove that the answer can never be found. These are the so-called unsolvable problems. The most famous of these, the Busy Beaver problem, was discovered by Tibor Rado (6). It may be stated as follows. Given a positive integer N, we construct all the Turing machines that have N states, which is to say N distinct internal configurations (this will always be a finite number); eliminate those that get into infinite loops; and then select the machine that writes the largest number of ones on its tape. The number of ones that this Turing machine writes is called the Busy Beaver of N. Rado showed that there is no algorithm, that is, no Turing machine, that can compute this function for all Ns. The crux of the problem is sorting out those N-state Turing machines that get into infinite loops. If we pro gram what is called a universal Turing machine to sim ulate all the N-state Turing machines, the simulator itself goes into an infinite loop. The Busy Beaver function can be computed for some Ns, and it is also, interestingly, an unsolvable problem to separate those Ns for which we can determine Busy Beaver of N from those for which we cannot. Aside from its interest as an example of an unsolv able problem, the Busy Beaver function is also inter esting in that it can be considered to be itself an intelli gent function. More precisely stated, it is a function that requires increasing intelligence to compute for in creasing arguments. As we increase N, the complexity of the processes needed to compute Busy Beaver of N increases. With N equal to 6, we are dealing with addi tion, and Busy Beaver of 6 equals 35. At 7 the Busy Beaver lea'Tis to multiply, and Busy Beaver of 7 equals 22,961. At 8 it learns to exponentiate, and the number of ones that our eighth Busy Beaver writes on its tape is ap proximately 1043. By the time we get to 10, we are dealing with a process more complex than exponentiation, and to represent Busy Beaver of 10 we need an exotic nota tion in which we have a stack of exponents the height of which is determined by another stack of exponents, the height of which is determined by another stack of exponents, and so on. For the twelfth Busy Beaver, we need an even more exotic notation. It is likely that human intelligence is surpassed well before the Busy Beaver gets to 100. Turing showed that there are as many unsolvable problems as solvable ones, the number of each being the lowest order of infinity, the so-called countable in finity. Working independently during the 1930s, three mathematicians?Kurt G?del, Alan Turing, and Alonzo Church?each showed that the answer to Hilbert's twenty-third question, originally posed in the year 1900, is no. There is no method or algorithm that can deter mine the truth or falsity of any logical statement in the predicate calculus, nor can we even sort out those statements that can be proved from those that cannot be. The work of these three mathematicians created rever berations still being felt today. G?del's Incompleteness Theorem, for example, which showed that all formal systems of sufficient power are capable of generating propositions that cannot be decided at all, has been called the most important in all mathematics (7). G?del, Turing, and Church's work represented the first formal proofs that there are definite limits to what logic, mathematics, and computation can do. These discoveries strongly contradict Wittgenstein's statement 6.5: "If a question can be framed, it can be answered." In addition to finding some profound limits to the powers of computation, Church and Turing also ad vanced, independently, an assertion which has become known as the Church-Turing thesis?that if a problem that could be presented to a Turing machine is not solvable by a Turing machine, then it is also not solvable by human thought. Others have restated the Church Turing thesis to propose an essential equivalence be tween what a human can think or know and what is computable. The Church-Turing thesis can be viewed as a restatement in somewhat more precise terms of one of Wittgenstein's primary theses in the Tractatus. It should be pointed out that although the existence of unsolvable problems is a mathematical certainty, the Church-Turing thesis is not a mathematical proposition at all. It is a statement which in various disguises is at the heart of some of our most profound philosophical de bates. It has both a negative and a positive side to it. The negative side is that problems which cannot be solved through any theoretical means of computation also cannot be solved by human thought. Accepting this thesis means that questions exist for which answers can be shown to exist but which can never be found. The positive side is that if humans can solve a problem or engage in some intelligent activity, then machines can ultimately be constructed to perform in the same way. This is a central thesis of the artificial intelligence movement?that machines can be made to perform in telligent functions, that intelligence is not the exclusive province of human thought. We can thus arrive at one possible definition of artificial intelligence?that AI represents attempts to provide practical demonstrations of the Church-Turing thesis. In its strongest formulation, the Church-Turing thesis addresses issues of determinism and free will. Free will, which has been described as purposeful activity that is neither determined nor random, would appear to contradict the Church-Turing thesis. Nonetheless, the truth of the thesis is ultimately a matter of personal be lief, and examples of intelligent behavior by machines are likely to influence one's belief in at least the positive side of the question. Lady Lovelace's skepticism re garding the possibility of intelligent machines was no doubt related to the limitations of the mechanical com puter with whirling gears and levers that was proposed to her. Today it is possible to imagine building machines whose hardware rivals the complexity of the human brain. As our algorithms grow more sophisticated and machines at least appear to be more intelligent and more purposeful, discussions of the Church-Turing thesis will become more practical than the highly theoretical debate of Church and Turing's time. Up through the late 1940s, the link between thought and computation was necessarily theoretical. 260 American Scientist, Volume 73</page><page sequence="4">With the development of electronic computers during World War II, the discussion turned quickly to the reality of what might be done with the available hardware. The early AI efforts were enthusiastic, productive, and guilty, perhaps, of just a bit of "blue skying." For example, Herbert Simon and Allen Newell, in a paper published in 1958, say the following: "There are now in the world machines that think, that learn and that create. More over, their ability to do these things is going to increase rapidly until?in a visible future?the range of problems they can handle will be coextensive with the range to which the human mind has been applied" (8). The paper goes on to predict that within ten years (that is, by 1968) a digital computer will be the world chess champion. Eight years later, in 1965, Simon wrote in another article that "machines will be capable, within twenty years, of doing any work that a man can do" (9). Now I do not mean to pick on Simon. He has con tributed as much as anyone to the substantial progress that has in fact been made, and he is far from alone in making such unfulfilled promises. My point is only that the AI field started with a romantic energy that enabled it to achieve some impressive intellectual accomplish ments but at the same time caused a credibility problem, from which, to some extent, it still suffers. The romanticism of early (as well as some current) work in AI is also reflected in a strong tendency to use anthropomorphic terms to describe its techniques. Any discussion of artificial intelligence is likely to include references to experts, expert managers, demons, com munication through blackboards, learning, logical in ference processes, and knowledge sources. Personally, I have mixed feelings about such terminology. On the one hand, I enjoy using it as much as anyone. It can also be argued that such terms are reasonably descriptive of the methods being labeled. They are certainly far more relevant to what they are describing than truth, charm, and strangeness are to several recently discovered phe nomena in particle physics. The negative side of this tendency to anthropomorphize is the accusation, sometimes justified, that such terminology is vague and that its primary purpose is to make certain techniques appear more complex and mysterious than they really are. It can be pointed out, for example, that a logical in ference process, as used in recent expert systems, is no more complicated an operation than the comparatively dull-sounding fast Fourier transform. There have been attempts along the way to quell this perceived overenthusiasm (see, for example, ref. 10). Perhaps, in those early years, an unbridled enthusiasm was more important than restraint. I believe there is a consensus today, however, that establishing realistic standards as well as expectations is essential if the field's enormous potential is to be realized. The real accom plishments of AI technology are now substantial enough that the field no longer needs to live almost entirely in the future. AI present As we shift our attention to the present, I would prefer again to avoid a roundup of the usual suspects?not that a recitation of the triumphs and frustrations of the AI world of today is of no interest, but it is material avail able elsewhere. Instead, I will try to share with you a personal view of the state of the art. To do this, I will cite six assertions that have been made in recent months, and state whether or not I agree with each. Because AI re searchers such as myself are often accused of being vague, I will respond to each assertion with a short and precise answer. Having done that, I will succumb to the temptations of my training and also give a long and vague answer. The first assertion is that "AI is lisp." My short an swer is no. lisp is a computer-programming language con cerned with the evaluation of symbolic expressions. These expressions are comprised primarily of lists and functions. Lists in turn are defined as ordered sequences of items, which may be numbers, symbols, expressions, or other lists. By defining hierarchies of lists, very complex data structures may be created; these structures can be interactive. Before I explain my negative short answer, let me speak in praise of lisp. First of all, lisp is an elegant and satisfying language which, reflecting its mathematical roots, gains a great deal of power through recursion. Recursion, as many will remember from elementary number theory, allows us to make an infinite number of assertions by proving a proposition true in one case, for n = 0, and then deriving the proposition for n + 1 from the proposition for n. Much of mathematics relies on recursion, and this technique allows simple lisp pro grams to perform relatively powerful transformations. Second, lisp is one of the few high-level languages that allows self-modifying code, which is another powerful concept in the theory of computation. Third, lisp allows highly flexible information structures that are amenable to representing concepts more complex than numbers. lisp has also changed over the years more than most languages. Many of these changes have added signifi cantly to its sophistication and capability. There are two reasons why I answered no. First, I feel there is an excessive reliance and emphasis on lisp. A recent market intelligence report for the venture capital community, in advising prospective investors, declared that if the software they were considering in vesting in was not written in lisp, then it was almost certainly not "true artificial intelligence." While most serious AI researchers, even ardent supporters of lisp, would find fault with that statement, it is not an un common sentiment. My second reason is probably more salient and has to do with some of lisp's drawbacks. The same market report stated that lisp used to be inefficient and expen sive but that this is no longer true. It is certainly true that the advent of lower-cost microcoding has brought down the cost of lisp machines. They have come down from high tens of thousands of dollars to low tens of thou sands, and soon will be in the high thousands. As I will note shortly, however, the future of AI will depend heavily on parallel processing?the linking of many processors to perform many operations simulta neously?rather than step-by-step computing. Products that link a hundred small machines in parallel are practical and can be manufactured at a reasonable cost. No one would propose, however, manufacturing a product that included a hundred efficient lisp machines. Parallel processing was not a consideration when lisp was designed, and lisp is not particularly well suited for 1985 May-June 261</page><page sequence="5">it. In my opinion, parallel processing will be more im portant to the future of AI than list processing. There is no question that lisp will continue to be an important language in AI research and that it will con tinue to evolve. I do not believe, however, that it will continue to play the dominant role that it has to date. The second assertion I would like to examine is: "AI is AI techniques." My short answer again is no. The assertion appears to be a tautology, the type of meaningless statement that Wittgenstein advises us to pass over in silence. If we define some terms, however, it will begin to have meaning. By AI, I mean the art of creating machines that perform tasks considered to re quire intelligence when performed by humans. This is the definition that Minsky gave to AI in the mid-sixties. I have seen it repeated at least twenty times since, gen erally without attribution. By AI techniques, I mean what people generally refer to when they say "AI tech niques"?that is, those techniques that the AI journals will accept articles on. It is the same set of techniques that have been worked on in AI research centers over the past five years, but not those that were researched prior to that time. One problem with the second assertion, as my definition probably makes clear, is that the concept of AI techniques is somewhat arbitrary and vague. I suggest that we refer instead to inference engine techniques or concept associative techniques or high-level feature extraction techniques or expert management techniques rather than use the vague term "AI techniques." We would, at least, have a better idea of what is being re ferred to. There is, however, a more important problem with the assertion. Regardless of any confusion about the meaning of terms, it has been my experience that the bulk of the technology applied successfully to AI prob lems is not concerned with such AI techniques at all, but rather with what we call domain-related techniques. For example, my colleagues and I are working on a machine that will recognize human speech with virtually no re strictions on vocabulary. While there are so-called AI techniques involved, such as expert management, di rected search, context analysis, and others, the bulk of our technology is specific to the domain of inquiry. We draw it from linguistics, speech science, psychoacoustics, signal processing, information theory, human factors, computer architectures, very large scale integrated cir cuit design, and other fields. Another of Minsky's early definitions of artificial intelligence was "a grab bag of tricks." I could not agree more. Each specific task that we associate with an intel ligent machine will require a different set of techniques, with methods derived primarily from our under standing of each problem. We hear every now and then about "generalized perception algorithms" that can recognize any type of pattern, whether it be manifested in speech, printed characters, land terrain maps, or fin gerprints. It turns out that such claims are absolutely correct?such algorithms do, in fact, recognize every type of pattern. Only they do all these tasks very poorly. To perform any of them well, with satisfactory rates of accuracy, requires a great deal of knowledge deeply embedded in the algorithms and specific to the domain of inquiry, so much so that this aspect of the technology far outweighs the generic AI techniques. The third assertion is: "AI is parallel processing." My short answer is yes. I answer yes not because an AI implementation must be parallel by definition, any more than it must use lisp, but because I believe that parallel architectures are the wave of the future, particularly for the more complex systems which we tend to consider as artificial intelli gence. The brain, as we know, more than makes up for the inherently slower speed of nerve cells as compared to silicon with almost total parallelism, and no doubt with its superior algorithms. Parallel processing can represent a much more ef fective use of resources than serial processing. Our complete reliance on serial processing up to now can be compared to a society in which work can be done only by one person at a time. Another reason for using parallel processing is that applying sufficient brute force to a problem is often necessary to achieve the desired result. For example, the typical organization of an expert system consists of three components?a massive database relevant to the area of expertise; a set of rules as to how the database is to be searched, manipulated, and transformed; and a logical inference processor that can apply these rules to this organized base of knowledge. The bottleneck in such systems is not the memory cost of storing the informa tion and rules but the real-time requirements of the in ference operations. This can be overcome at reasonable cost through massively parallel architectures, which the next generation of expert systems will undoubtedly rely on. An example from the beginning of AI concerns game playing. The essential algorithm for playing games like chess is fairly simple and well known: generate a tree branch, push down, and prune. Interestingly, dur ing the thirty years that game-playing machines have been around, we have found that the most effective means of improving performance has been the brute force approach. Applying greater computation power has brought more improvement than has algorithmic sophistication. It is no coincidence that the best com puter chess is now played by the supercomputers. The computer that does end up winning the world chess championship will undoubtedly be a highly parallel computer. One approach to parallel processing is to wire a lot of microcomputers together and provide ways for them to communicate and coordinate their activities. Another, even more powerful, approach is to provide an array of what I would call subprocessors on a single chip that are dedicated to some particular task. A subprocessor is not a programmable computer but a dedicated circuit de signed to implement a desired algorithm. Examples of the functions such chips could perform would include signal processing, logical inference processing, time warping, pattern matching, and image processing, as well as implementing the algorithms specific to some particular system. Using custom very large scale inte gration, it is possible to put a dozen or more subproces sors on a single chip. A product with 100 such chips thus provides the equivalent computing power of over 1,000 microprocessors with a foundry cost of less than $1,000. This computing power is, of course, dedicated to a particular set of algorithms, which is why I consider 262 American Scientist, Volume 73</page><page sequence="6">system architecture to be as important as the algorithms themselves. A typical product architecture used to be a single computer with appropriate peripherals and software. Increasingly, we will see algorithms distrib uted instead throughout a complex and diversified parallel architecture. The fourth assertion is: "AI is interdisciplinary." My short answer is yes. This assertion may at first seem obvious. I feel, however, that the extent to which work in AI is neces sarily interdisciplinary, and the challenge that this presents, is not fully appreciated. As I mentioned earlier, in my experience the domain-related techniques con stitute a larger share of the technology used in successful AI systems than do the generic AI techniques. This has important implications for the type of effort required. One implication is that a major part of the challenge in solving an AI problem involves organization and communication. Research teams spanning a half dozen or more distinct disciplines do not come together easily, and once assembled, there is the problem that the team members speak different technical languages. One of the first persons actually to recognize this as a potential stumbling block to the development of intelligent machines was Norbert Wiener. His book Cybernetics, published in 1948, was a remarkably com prehensive look at the future of computing. In it he points out that no one since Leibniz has had "a full command of all the intellectual activity of his day." He goes on to state the inevitable result: "There are fields of scientific work . . . which have been explored from the different sides of pure mathematics, statistics, electrical engineering and neurophysiology, in which every single notion receives a separate name from each group, and in which important work has been triplicated or quadruplicated, while still other important work is delayed by the unavailability in one field of results that may have already become classical in the next field" (13). The fifth assertion is: "AI models human intelli gence." The short answer is no. It would be beneficial if it could, but we just do not know enough at present to gain a great deal from mod eling human cognition. There are exceptions. In our work on speech recognition we have taken advantage of what is known about the auditory processing of the cochlea and the auditory nerve. We find that our speech recognition algorithms perform better if we attempt to model human auditory front-end processing. I still an swered no, however, because this front-end processing represents only a small part of the overall processing involved in recognizing speech, whether by machine or by a person. One of the reasons we know something about au ditory front-end processing is that the cochleas and au ditory nerves of test animals are relatively accessible. The bulk of cognitive processing takes place, however, out of reach of our probes. Not only are most brain cells deeply embedded in the brain, but we now realize that most of the processing takes place within the cells themselves through a series of complex chemical transformations. Progress in understanding the brain will continue, but this is not our most promising source of ideas for new AI algorithms. My last assertion is the reverse of the previous one: "We can learn about human cognition from AI algo rithms." My short answer is yes. Before we can run experiments on humans to test alternative theories, we need to have theories to test. One of the best sources for those theories is techniques we have found to work in machines. The fact that an algo rithm works in a machine does not prove that the same technique is used in the brain, but it does prove that this is one way that the brain could work, and provides a potential theory that could be subjected to neuro physiological testing. AI future If we shift our focus to the future, it is important for us to understand precisely what it is that we are on the verge of. Perhaps the best place to start would be to state what I do not expect to see in the near future. We are still far from Simon's second prediction, of being able to replicate by machine the vast range of human intellec tual capability. It is doubtful, for example, that by the end of this century computers will be able to watch a movie and write a coherent review. What we are on the threshold of is nonetheless of major significance. We are gaining the ability to apply sharply focused machine intelligence, or perhaps I should say narrowly focused intelligence, to a wide range of problems. I would like to emphasize the word "narrow," because the computers that will be created over the next decade are not going to be wide-ranging intellectuals. Instead, we will see a proliferation of sys tems with well-defined areas of expertise?systems that have a mastery of our knowledge about a particular class of diseases, or that have an ability to develop certain types of financial investment strategies, or that can help guide a complex negotiation. Despite my emphasis on the word narrow, it is im portant not to underestimate the significance of auto mating this kind of expertise. Look at the impact com puters are having today on almost all areas of endeavor, despite the fact that virtually all are idiot savants. Add ing some well-focused intelligence to our computers' already well-demonstrated superiority in mastering vast amounts of information and conducting repetitive op erations at high speeds without tiring will be a powerful combination. Our goal, in fact, should not be to copy human intelligence in the next generation of computers, but rather to concentrate on the unique strengths of machine intelligence, which for the foreseeable future will be quite different from the strengths of human in telligence. At least as important as the emerging expert systems are the efforts to develop intelligent computer interfaces in such areas as speech recognition, the understanding of natural languages, and computer vision. The primary users of the next generation of intelligent machines are not intended to be computer experts but everyone, in cluding children. There are extensive efforts, for ex ample, to add some measure of intelligence to com puter-assisted instruction systems. The next generation of teaching machines will attempt to evaluate students to determine their strengths and weaknesses, as well as their interests, and will provide instruction that is both entertaining and pedagogically sound for each indi vidual. 1985 May-June 263</page><page sequence="7">To engage in some predicting of my own, I believe that by the end of this century AI will be as ubiquitous as personal computers are today. The majority of soft ware will be intelligent, at least by today's standards. It will interact with users through intelligent front and back ends; highly organized databases will be embedded within the software, as well as available through tele communications; and high-speed parallel computation engines will be able to manipulate the information as needed. If we consider what it is that makes certain behavior appear to be intelligent, one observation we might make is that it combines two attributes. First, the behavior appears to be reasonably appropriate to the situation. Second, it is not totally predictable. Computers are now on the verge of satisfying both these conditions. By being tied into vast yet rapidly changing databases and by applying highly parallel processing to generate the needed inferences and deductions, the answers gener ated by such systems will indeed combine appropriate ness with a measure of unpredictability, giving at least the impression of intelligent behavior. To what extent we will consider such systems ac tually to be intelligent is hard to say. Our standards as to what constitutes artificial intelligence are constantly changing. In the fifties there was a great deal of excite ment as computers began to play chess and checkers, albeit at a crude level. That was considered AI at the time, yet today we do not consider the $20 pocket-sized chess-playing machines, which play a much better game, to be examples of artificial intelligence. There was ex citement in the late fifties and early sixties when AI programs were able to prove theorems and solve calculus problems. Today there are far more powerful packages to manipulate equations that are considered quite useful, but are not generally pointed to as examples of AI. Ap parently, as we understand a process well enough, we begin to consider it just a rote technique and not an ex ample of intelligence. As Minsky pointed out recently, if a superior being were to analyze human behavior and understand in great detail how we operate, it might not consider us to be very intelligent either (12). Some observers have actually suggested that arti ficial intelligence is inherently on the moving edge of technical feasibility, that it should be defined as those computer science problems we have not yet solved. References 1. Norbert Wiener. 1961. Cybernetics; or, Control and Communication in the Animal and the Machine, 2nd ed., p. 30. MIT Press. 2. Charles Babbage. 1973. Babbage's Calculating Engines. Charles Bab bage Research Institute Reprint Series for the History of Com puters, vol. 2. Los Angeles: Tomash Publishers. 3. Ludwig Wittgenstein. 1961. Tractatus Logico-Philosophicus. New trans. D. F. Pears and B. F. McGuinness. Routledge and Kegan Paul. 4. Alfred North Whitehead and Bertrand Russell. 1957. Principia Mathematica, vol. 1. Cambridge Univ. Press. 5. Douglas M. Campbell and John C. Higgins, eds. N. d. Mathematics: People, Problems and Results. Wadsworth International. 6. John M. Hopcroft. 1984. Turing machines. Sei. Am. 250 (May): 86-98. 7. Douglas R. Hofstadter. 1979. G?del, Escher, Bach: An Eternal Golden Braid. Basic Books. 8. Herbert A. Simon and Allen Newell. 1958. Heuristic problem solving: The next advance in operations research. Oper. Res. 6: 7-8. 9. Herbert A. Simon. 1965. The Shape of Automation for Men and Management, p. 96. Harper &amp; Row. 10. Marvin Minsky and Seymour Papert. 1969. Perceptrons: An Intro duction to Computational Geometry. MIT Press. 11. Ref. l,p.2. 12. Marvin Minsky. 1984. Artificial intelligence. A. I. Reporter, April, p. 6. AfIBZ O?Viuut VfmVa? vW5tor\c v^-s^COMP ?lGcWr, VM5 Z_L&amp;&amp;A(*E Co ULO MOT gfc UQCP&amp;&amp;&gt;._ L_ ^WOs</page></plain_text>