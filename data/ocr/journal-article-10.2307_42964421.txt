<plain_text><page sequence="1">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS WITH EMPHASIS UPON ANALOGIES FOR LEARNING AND CREATIVITY by Gordon Pask, Richmond (Surrey) The research reported in this document has been sponsored by the Air Force Office of Scientific Research, OAR, under Contract AF61(052)-640 with the European Office of Aerospace Research, United States Air Force; by the Aeronautical Systems Division of the Air Force Systems Command, United States Air Force, through the European Office of the Office of Aerospace Research, under Contract AF61(052)-402, and by the US Departement of the Army, through its European Research Office, under Contract No. DA-91-591-E U C-3216. 1. Discussion of Complete Analogy [1] 1. 1. Aims and Basic Definitions In Section 1.2. of this paper, we outline the application of descriptive analogy in Cybernetics. The argument is extended, in 1.3. and 1.4., to systems of control and in 1.5. to systems of adaptive control. Section 2.1. describes a model M of the learning process. In contrast to the previous models, which were reducible to a common representation (essentially positivistic models) M is irreducible and (when applied) involves more than one universe of discourse. In 2.2. we deal with a special case of « Restricted Conversation » with a participant represented by M ; in 2.3. with some problems of an « internal analogy » that is part of M ; and in 2.4. with unrestricted conversation and parable. Finally, 2.5. makes a brief comment upon the creative process. We shall adopt Hesse's [1] definition of an analogy in « Models and Analogies in Science » as the four term relationship of DIA- GRAM 1 wherein R is a similarity and the relations F are systemic</page><page sequence="2">168 G. PASK or causal. The components a, b, c, and d are objects with Relevant properties. Insofar as R determines similarity these are positive analogical properties, insofar as it determines a difference they are negative and if no comment is made the properties of the objects are neutral. Any neutral analogical property may become positive or negative. So far as the F's are concerned it will be convenient to define them as relations that depend upon a dispositional statement or, in the sense considered by Pap, [2] a generalised counterfactual conditional statement. From Pap's discussion, a causal relation is a special case of F since it is asserted by a condensed form of generalised counterfactual conditional statement. It will be necessary to distinguish at least a pair of types of analogy. One of these, called a complete analogy, is characterised by the exemplars labelled (a), ( b ), and (c) in Chapter 2 of Hesse's Book « Models and Analogies in Science ». In each case the relation R can be asserted independently of F. The other type, called an incomplete analogy, is characterised by exemplar (d) in Chapter 2 of the same book. In this case R exists only because of F. _a - (R) ► _b (F) (F&gt; T ▼ c_ M (R) ► A DIAGRAM 1</page><page sequence="3">THE USE OF ANALOGY AND PABABLE IN CYBEBNETICS 169 Much of our discussion depends upon the concepts of descrip- tive language, object language, and metalanguage. The descriptive language is called L* and is a tongue familiar to a body of scientists and used with more or less precision. An object language is the language in terms of which a particular experiment is conducted ; for example, if the experiment rests upon a communication model the alphabet of the object language is the alphabet of signs avai- lable to the transmitter and the receiver and the syntax comprehends the syntactic constraints upon the channel whereby the transmitter and receiver convey signs. A metalanguage is a language that describes an object language or another metalanguage and ob- viously L* is always a metalanguage. Finally, the concept of an hierarchy is used in the sense of Braithwaite [3] and Popper. [4] The Braithwaite and Popper conception can readily be introduced, in the special case of adap- tive control systems, to describe an hierarchy of organisations characterised by an hierarchy of metalanguages. 1. 2. Descriptive Analogy in Cybernetics The simplest descriptive analogies of Cybernetics refer to mechanical simulations. The positive analogical properties are relations and the negative properties refer to the fabric of the model. Typical cases are computor programmes that simulate an industrial process or a sensory mechanism in a brain and for the present purpose the form of simulation (digital computor, analogue computor, specially built artifact) is irrelevant. Such analogous models are chiefly valuable as conceptual tools. They allow us to visualise the behaviour of a system and they suggest novel relationships; either dynamic relations or relations between functional components. It is also possible to introduce initial and boundary conditions that would be difficult to realise in practice and to observe the subsequent behaviour of the model. To some extent, models of this kind reveal that neutral pro- perties are, in fact, positive analogical properties. In most cases the unsuspected property is concealed by the mere elaboration of the modelled system. Thus, Beurle's[5] and Farley's [6] simulations 12</page><page sequence="4">170 G. PASK of a neurone network reveal some important and unsuspected statistical attributes and, at a more microscopic level, Harmon's [7] neurone models exhibit undiscerned attributes of the interaction between a few components. Occasionally, the fabric or material of the simulation contains neutral as well as negative elements. Thus my own thread systems [8] were chiefly of interest because their physical properties had an undetermined role in the analogy. Probably the most important discoveries obtained in Cy- bernetics by using descriptive analogy do rely upon a purely relational identification. Adopting the « Black Box» approach of operationalism (and modelling the operationalist « Black Box » by the relations isomorphic or homomorphic to those relating its obser- vables), Ashby [9] [10] [11] has shown that many apparently mysterious processes such as « Reproduction » and « Habituation » can, in fact, be accounted for by relational characteristics alone. [12] 1. 3. Analogy and control systems Apart from the descriptive applications we have briefly con- sidered, analogies of one sort or another are used in connection with control systems. Since Cybernetics is pragmatic in character (Couffignal, [13] for example, defines it as an art, namely « L'art d'assurer l'efficacité de l'action»), these control analogies are probably the most important of all (from the present and Cyber- netic point of view.) In the simplest case there is some entity, which may either be a real life experimenter or a constructed artifact, which is coupled to a system and aims, as its goal, to bring the states that are assumed by this system into conformity with a plan. A homeostatic device like a thermostat aims for a static, equilibrial, goal. Thus a successful thermostat maintains the attribute «water temperature» of the hot water system to which it is coupled within prescribed bounds. Its goal is a state subset in a one dimensional state des- cription of the physical apparatus. More elaborate goals entail dynamic equilibria in which some feature or symmetry of the state description of an object or a process is maintained invariant. Now this activity depends upon analogy and also maintains analogy for, (1) In order to design the control mechanism, it is necessary</page><page sequence="5">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 171 to assert the similarity relations R between a and b and between d and c of the paradigm case (shown in DIAGRAM 1) and (2) The controller itself is required to compute a function that creates or maintains a dispositional or systemic relation, F, between b and d that is the same as the dispositional or systemic relation between a and c (the latter being built into the controller, as part of its design, when the goal is specified). If R did not apply, the controller could not be coupled to the physical object it controls. There could be no state description and consequently there could be no system. Phrased differently, in the absence of R the object language, L°, in terms of which the controller communicates with its environment could not be I a i » CONTROLLER PROCESS •4 i r •4 p 1 Equivalent Representation Controller Process a . (R) ^ _b_ (F) (??) _c_ &lt; _d_ Controller drives Process until (??) = (F) DIAGRAM 2</page><page sequence="6">172 G. PASK specified in L* and neither communication nor control could take place. Conversely there is a perfectly good sense in which R guarantees that L° can be defined, at any rate whenever the notion of « defining L° » is pertinent. But the mere existence of an interaction in L° is quite insufficient to establish control or to justify our drawing a picture like DIAGRAM 2. As Wiener [14] has pointed out, significant control occurs when the feedback loop a, ß, (carrying signs in the alphabet of L°) involves a computor which recognises patterns or images that are capable of generalisation (hence the invariant preserved when the goal is achieved has the calibre of a universal). Now it seems pompous to talk in this fashion about DIAGRAM 2 because simple-minded control systems undeniably have this property. But the facile assumptions that serve well in the study of everyday feedback devices are untenable in a wider field ; particularly so in connection with brains and brain-like machines. Here, the possibility of generalisation cannot be assumed, it must be maintained ; An arbitrary retroaction is a candidate for the title « control system » only if the abstraction from the environment which applies, in L°, at its input can be generalised (if the would-be controller embodies some principle of inference, if it «computes a uni- versal »). [15] [16] Now this requirement is satisfied, in a rather restrictive sense, if the controller and the environment or process to which it is coupled in L° contain a common causal rule. In fact we shall later argue (when viewing evolutionary systems as control systems) that less stringent dispositional relations can serve in place of a causal rule. But, F, is exactly such a relation. Thus, given that R pertains between a and b and between d and c the existence of F between a and c and the possibility of inducing an analogous relation between b and d implies that abed has the property of a control system, or, if F has been induced between b and d a system in which the goal is reached. 1. 4. Analogy and biological systems These comments are particularly relevant to biological control and experiment. Here, of course, we can nearly always view</page><page sequence="7">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 173 the animal as a homeostat [17] or a control mechanism in its own right. But this is not uncommon in connection with a mechanical process and the chief distinction of biology (so far as simple systems are concerned) is that whereas a and ß are usually given by the mechanical specification of a process, so that L° is automatically defined, this object language must be discovered if the process is replaced by an animal. Thus Lettvin, Matturana, McCulloch and Pitts [18] have discovered the object language of the frog (at any rate for its visual modality). The object language pertinent to octopus is also known, as demonstrated by Young [19] and Sutherland, [20] and the same comment might be made, in view of Napalkov's work or [21] Skinner's [22] work, with the Pigeon and some other birds. Pretty bizarre languages they are from our point of view (in the case of a frog, the words are points in a four attribute space with co-ordinates evaluating the motion of small objects, the movement of specific kinds of shadow, and other quite curious appearances). Not surprisingly the frog is insensitive to the point stimulation applied by earlier workers. Experiments or control procedures that assume the R's and F's are always like our own, so that point stimuli, for example, seem atomic and obvious, usually fail and are certainly unsound. In a few cases L° can be discovered by an auxilliary set of experiments. But it is important to recognise that these auxilliary investigations refer to an ontological class of entities that are distinct from the ontological class sampled by the laboratory experiments (using « ontological class » in the sense of Harre). [23] Thus the auxilliary investigations concern the behaviour of frogs in their normal surroundings, in a pond. But the primary inves- tigations concern frogs on a disecting table. There is a «family continuity » between either set of experiments (in the sense, advanced by Harre, that the terminal observations of a sequence of experiments overlap one another). Hence the sampled entities belong, in either case, to a single ontological class and, in abstraction, to a single universe of discourse. But each set is distinct since the separate family continuities are, at the present state of know- ledge, incomparable.</page><page sequence="8">174 G. PASK The possibility of settling L° once and for all at the outset depends, in the first place, upon the adequacy of L° as a com- munication medium and next upon its invariance throughout the experiment or control procedure. So far as adequacy is concerned, many kinds of interaction call for more than one level of discourse, characterised by an hierarchy of metalanguages L1, L2 Lm. If the animal's mentation involves a corresponding hierarchy of organisations or, when viewed as an individual in the environment, if it has a corresponding hierarchy of control, then it may be possible to specify the hierarchy by auxilliary investigations providing, of course, the structure does not change. This is true of octopus, for example, where J.Z. Young has shown that the modality of pattern vision involves a form of computation above the modality of vision and that vision lies above the sensoria of touch and of taste. Here, the hierarchically structured apparatus has been exhibited in physiological terms and its development has been related to the evolution of the beast. Octopus, however, appears to be exceptionally invariant and amenable to analysis. Normally the experimenter cannot discern the hierarchical structure by auxilliary investigations and merely aims to achieve temporary constancy by preconditioning the subject (starving an experi- mental rat) or constraining the subject by instructions. The form of instruction depends upon the experimental objective. In a choice experiment, for example, it is chiefly im- portant to maintain the denotation of terms in L° (so that a human subject gives a known and consistent interpretation to stimuli and response alternatives). In this case the experimenter models his human subject as a simple control mechanism. On the other hand, any experiment that introduces a distinction between the « stimuli » and the « reinforcements » relies upon more than one level of discourse. [24] The subject is asked to agree, for experimental purposes, that certain L° signs are stimuli, as before. But he is also asked to accept a goal, so that other L1 signs, indicating proximity to this goal, can act as reinforcement. In this case the experimenter models his subject as an adaptive controller.</page><page sequence="9">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 175 1. 5. An Adaptive Control System Providing that the instructions or the preconditioning maintain the hierarchy of organisation without any significant change whilst the experiment or the control procedure takes place it is possible to use the generic model shown in DIAGRAM 3. The notation has been culled from control engineering where the large arrow depicts a parametric coupling. The act of « turning » the large arrow which is either performed by a next higher level component or by an input signal (at this next higher level) changes the parameter value (over a set of n possible values) and thus determines the function computed by the « Box » concerned. The L1 input may be reduced to a specific property of the product of the L° input and the L° output. In this case the L1 output alone (rather than the L1 input-output-product) changes the L° para- meter. This construction can be iterated for all L' (so that the Lm input is the product of the Lm_1 input and the Lm_1 output and the Lm_1 parameter is changed by the Lm output) to yield the familiar image of an hierarchically organised adaptive control mechanism which interacts with the environment it controls only in terms of L0. The present construction includes this familiar case but allows for interaction at several different levels of discourse. Any interaction with C0 takes place in L0 and might, for example, represent a subject's response to stimuli that are L0 signs. Any interaction with Cx takes place in Lx and might, for example, represent a subject's response to a reinforcement procedure me- diated by L1 signs. Similarly an interaction with Cm involves Lm and uses signs that could have the status of «Questions» and « Commands » in the sense of the analysis of Questions and Commands advanced by MacKay. [25] Since the entire picture depends upon an invariant structure the ordered set L°, Ļ1 Lm is a set of closed formal languages (of the kind used in computor programming) 1. Hence the 1 Only 2 levels of description are logically essential, namely an object language and a descriptive metalanguage like L. However, in order to express the M model the object language and the descriptive metalanguage must be stratified, for example, by embodying a structure of logical types. The requirement is discussed in a recent publication, Intension and Deci- sion, by R. M. Martin, Prentice Hall, New Jersey, 1963.</page><page sequence="10">176 G. PASK The System G ļM_^r- c, -7-1: 1: Input ļ ļ Output |M- 1 |M-1 To |M- lo|_ |M-1 - - - - ! I I I I • I I I i I From From v -p; L T" c, 1 ~H~~ ^ l: Input v ^ 1 v Output v - )_7 - ļ. - . - C, - - - 1; 12 Input ^ Output Product Combination ^ Parametric Coupling DIAGRAM 3</page><page sequence="11">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 177 innovation of an adaptive control system does not really alter our paradigm analogy. It may be convenient to regard R as an ordered collection of m distinct relations, but we can always identify R with a single m adic relation. Similarly, although it may often be convenient to view the several levels of adaptation separately and thus to consider an hierarchy of n valued functions /) where i - 0, 1 m and / = 1, 2, n (an adaptation of the i-th level implying that / assumes a particular value in the term /1) it will always be possible to equate F with an m-th order set of n valued functions so that R, F, a,b,c,d, still describes the system. Now it is possible to contend that the linguistic distinction is no more than a formal expedient that can safely be taken for granted when using the model in DIAGRAM 3. Further this model is widely applicable in psychology and the related behavioural sciences (it is certainly capable of embodying the entire gamut of goal directed adaptive behaviours). Hence, there is a tendency to assume that this model should always be applicable and consequently that the linguistic distinction need rarely if ever become explicit. I believe this assumption is mistaken and confusing. If a subject learns in the non-trivial sense that apart from indulging in goal directed adaptation (which is, of course, a pre- requisite for learning) he also acquires concepts, then the hierarchy of organisations is no longer immutable. We shall discuss the form of change later. For the moment we comment that certain organisms are Cybernetically characterised by the property that they must learn or equivalently that, in Von Foerster's [26] [27] sense, they are self-organising systems. It can be argued that conscious men belong to this class insofar as men are apparently constrained to maintain a given rate at which they reduce their uncertainty about their environment [28] or to continually « search for meaning » [29]. It can also be shown that any « Neo- phyllic » [30] organism is a member of this class of systems. Now if this characterisation of man is valid no experiment which entails instruction or preconditioning that does maintain the adequacy of C = C0, Q, Cm can sample the behaviour of man (or of a neophyllic organism or any other system of this</page><page sequence="12">178 G. PASK class) since these instructions must disallow the relevant type of behaviour (namely non-trivial learning). The experiment may, of course, yield valuable information about a subsystem of a self- organising system just as psychophysical experiments glean data about subsystems of reflexes in the system of human mentation. I have argued in several papers that human psychologists do, in fact, define their domain of interest as a self organising system ; for example, by asserting that men indulge in a continual search for meaning. On the other hand the experimental methods of psychology subscribe, in many cases to C. Insofar as they do, the derived experiments must be uninformative regarding systems in the avowed domain of interest and to this extent the underlying methodology of the science is open to criticism on grounds of inconsistency. [31] [32] [33] [34] 2. Incomplete Analogy in Cybernetics 2. 1. Analogy and Learning Systems If an organism learns, it certainly does indulge in goal directed adaptation. But also, according to almost any cannon of « learning », it forms « concepts » or engages in some equivalent though differently phrased gambit. To avoid argument let us take Hovland's [35] view of a concept as definitive (from the psychological viewpoint this is probably a minimel definition). In this case a concept is the process involved in naming the denotation of a class (and « acquiring a concept » is the acquisition of the necessary organisation). Suppose that a minimal element of « learning » is some goal directed adaptation in the conduct of which at least one concept is acquired or cons- tructed. It follows that we are dealing with an hierarchical system like C each level of organisation being characterised by a system metalanguage L'. It also follows that C, as such, is an inadequate vehicle for modelling the learning process since, if learning occurs at the i-th level, then at least one axiom is adjoined to L' as the result of activity described at the i+l-th level of discourse (for this is just an apposite way of saying that learning occurs, according to our previous definition).</page><page sequence="13">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 179 Consequently, let us define a different hierarchy of models A = Ao, A1( Am which are conveniently regarded as collections of computor programmes written in L°, L1 Lm, the collection of programmes A¡ being written in L'. The structure A,- is some kind of « Artificial Intelligence » [36] used for « problem solving » or inference and it might, of course, be part of a control mechanism or the object of experimental enquiry. The problem solving programmes in A¡ are constructed, to some extent, by programmes in Ai+1 and, as pointed out by Newall, [37] there is an isomorphism between the process of problem solving in A¡ and in Ai+1 for all values of i. But, if we communicate with the « artificial intelligence » in L! the activity of problem solving in Ai+1 which has programmes in A¡ as its domain (for these are signs in Li+1) will be called « learning ». Alternatively A¡ can be interpreted as the introspective account given by a co-operative subject in L1' of how he solves problems and Ai+1 as his account, given in L,+1, of how he learns to change his problem solving methods. Thus A is a description that is given by a co-operative subject about how he solves and learns to solve problems and one admirable case of such a description is Polya's Book [38] heuristics and other procedures for problem solving which has been used extensively in constructing various artificial intelligence programmes. One of the most familiar forms of axiom adjunction is a process whereby the language Li+1 in an alphabet Zi+1 is extended by incorporating a part, say 2 of Lř, as a member of Zi+1. Transition of this kind; Z(~*Zi+1u2; T.'+1 , L'+2 ; and transitions which, in general, permit the extension of the L', are difficult to represent and perhaps the most convenient notation is obtained if we regard the L' as « open ended » mechanical languages where « open endedness » permits (I) The extension of the Z¡ or the substitution and generation rules of the L1 and (II) The creation of Lm+1 given Lm in the hierarchy. Now we have argued that « learning » entails changes that involve « open ended » languages L' (or some equivalent des- criptive expedient). But (I) and (II) which define « open endedness » imply that the hierarchy of the L' need not be invariant (and if</page><page sequence="14">180 G. PASK r ! L" A„ «- * B„ !_ t- - ' i ' ! ! ! Í ! I ! j LJ 1 1 i ļ L i aJ^jJ l_ J - 1- - j- - - ļ L° A0 B, I ! L I The System M DIAGRAM 4 learning really occurs, that this hierarchy will not be invariant). Hence it is impossible to model the learning process in C as we asserted at the outset and A is characterised as a variable structure of problem solving programmes. On the other hand A is an insufficient account of the learning process, on its own. The general reason for this is that when we make enquiries in connection with learning we are asking for more than the description of learning that A might provide. In</page><page sequence="15">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 181 addition we ask for an analogous mechanism of learning (typical mechanisms, not all of them « learning mechanisms » are the « Filter Mechanism » advanced by Broadbent, [39] the models advanced by Hull, [40] and, at a more detailed level, the almost physiological mechanisms proposed by Hebb, [41] George, [42] and Milner). [43] The demand for a mechanism corresponding to the description is not a matter of whim. We could, instead, have asked for the intension of each concept and, in a certain sense, the mechanism for A is the intension of A. Less dramatically, the mechanism is needed in order to interpret the description (at any rate as part of a control system). Hence the basic model for a learning system is shown in DIAGRAM 4 where A¡^=^B¡ is a relation of analogy which it will be convenient to call an internal analogy and where Aj+i=^ A¡ or B¡+1=» B¡ is a many to one preserving correspondence. Whereas B = Blf B2, ... Bm are members of the same universe of discourse and A = Aj, A2, . . . Am are members of the same universe of discourse, these universes of discourse are distinct because the systems A and B denote members of distinct ontological classes in the sense that there is one kind of family continuity amongst the experiments done to determine A and another kind of family continuity amongst the experiments done to determine B (the phrase « universe of discourse » could be replaced by « reference frame »). The necessity for A and B in DIAGRAM 4 is most conveniently exhibited by showing special cases in which the demand for A and B is made. 1. To determine the L' it is necessary to perform auxilliary investigations (like the pond experiments for the frog L°) which have reference to an ontological class of entities distinct from the referents of the primary experiments that entail communication in L{. Since the L' are open ended languages, these auxilliary investigations must be performed continually if we aim to use the learning system as a control mechanism (which is tantamount to constructing a momentarily tenable C model). Hence for each of the Aj in A there must be some analogous Bř in B (and Bř is used to interpret A, by defining the immediate form of L{ which allows the control engineer or the experimenter to communicate at the i-th level of discourse with the learning system).</page><page sequence="16">182 G. PASK 2. As Chomsky [44] has pointed out, an abstract calculus such as the descriptive calculus for a Transformational grammar, is only a framework in which to construct a model for linguistic behaviour. In particular it does not constitute a model of the individual that makes and perceives utterances (the latter entails some motivated automaton). Our learning model could represent the individual that makes utterances, the A component being the calculus (or an hierarchy of systems of calculus) and the B component being the motivated automaton. Similar comments apply to the TOTE hierarchies proposed by Miller, Gallanter and Pribram [75] as models for mentation. The TOTE hierarchy as it stands is an A model that must be completed by adjoining a more or less explicit B model, for example, in terms of a system of drives. 3. In the systems of artificial intelligence devised by Amaral [45] and also by Minsky [46] there is an hierarchy of programmes and open ended languages L'. But in order to develop programme trees that specify novel programmes in A¡, hence, in order to mediate a learning process, it is necessary to introduce one or another of a pair of restrictive expedients. The first is some kind of « economic » control whereby a programme tree has a « cost » assigned to its construction and its maintenance and a « value » in the economic system that depends, for example, upon criteria of simplicity or verification. We might call this expedient «control by the con- notation of a concept ». The other possibility is some kind of « heuristic ». But effective heuristic rules refer to entities in the common environment or the common experience of whoever makes use of this artificial intelligence. Hence we might refer to this expedient by the phrase « Control by the denotation of a concept ». In fact most systems, for example Fogel's [47] system, use « Control by Denotation » as well as using « Control by Con- notation » and jointly these restrictive devices have the status of the Bj in B. 4. The « m level n goal » systems described by Mesarovic [48] are special cases of our construction in which the distinction between levels is made in A and the distinction between goals is made in B.</page><page sequence="17">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 183 5. Finally, we can ask « What sort of system would generate the hierarchy of mechanisms B¡ with the property that each is a self-organising system in the sense of Beer, [49] von Foerster, [27] or Mesarovic » ? [50] (the latter requirement is added to suit the present discussion but it is also at least one way of ensuring that A&lt;=&gt;B will learn). One reply is «A system of evolving entities, such as automata or computing devices » (typical cases are one of Loefgren's infinite automata, one of Baricelli's systems, [52] one of the automata populations computor simulated in this laboratory, or a physical medium, such as a brain, in which modes of organised activity can be shown [53] [54] [55] to evolve). Various restrictions must be imposed in order to avoid absurdity but these have been adequately documented and will not be considered in detail. Briefly, the evolution of automata takes place in a simulated medium wherein the individuals must compete for a commodity that is limited in supply and required for reproduction and sur- vival. If the evolutionary rules assign survival value to co- operative interaction those variants that aggregate tend to develop into stable entities that are reproduced as higher order individuals. Further, an hierarchy of modes of interaction or of sign systems is evolved to mediate the communication between the individuals at a given level that is a pre-requisite for their co-operative activity. These sign systems are related as the L', the levels of organisation as Bf+i=^B ; and insofar as the Ař describe the computations per- formed by the Bř they bear a relation of similarity A¡ &lt;==&gt; B,. Let us call a system of this kind M = (A#=^B). The system C is the special case of M obtained when the A¡ and the B¡ are not only analagous but isomorphic 1. Although many assembleges such as social organisations and developing languages are probably analogous to « M systems » we shall confine our attention to the more obvious cases of a brain or a learning artifact and show that M necessarily leads to ela- boration of the simple analogy paradigm R,F, a, b, c, d. 1 It appears that (1), (2), (3) and (4) can be derived as special cases of (5). Hence we take M to have a B component of the evolutionary form for the present discussion.</page><page sequence="18">184 G. PASK i i A ¡., &lt;i=^=î&gt; B , ty v aí p- fft i jr JEquivalently represented as Control System maintaining Stability of by adjustment of Interval Analogy M State iT t? ^ * ► - 4 - r* - iï L&gt; ! Satisfy Vyy/% Condition ^ ' ' -i», pf Vyy/% » . Condition =» i - - &gt;- « ' ■« V TT DIAGRAM 5</page><page sequence="19">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 185 2. 2. Control Analogy and Teaching In DIAGRAM 5 the relation « » can be construed as a similarity and the relation « =¥ » as a dispositional relation (it is dispositional since the algorithms of Ař+1 in Li+1 determine the computations of A¡ in L'). Hence M involves an internal analogy which may also be construed (in the control sense) as an internal stability; maintained by the analogous relation and characte- rising M with the property of being a « Learning System ». Now M exists insofar as DIAGRAM 5 is satisfied. But it is far from obvious that M can exist, in the sense that the internal analogy can be maintained, if M communicates with some other system ; if, for example, M learns or if M is instructed. Hence, any analogy with M is conditional upon the internal analogy that is part of M. In the case when M represents a man and when the skill to be instructed satisfies some rather restrictive axioms that render it a « structured skill » it has been possible to completely mechanise the teaching process in a fashion that is compatible with the idea that teaching is the control of learning, in other words, a process that imposes a control analogy upon M. To illustrate the pecu- liarities of M, let us review the mechanised instruction of a single concept in a structured skill, over an interval t-»t + At. [56] [57] [58] [59] [60] ( 1 ) If M exists in the sense that the internal analogy is maintained, (2) If the concept belongs to a structured skill (which implies that the form of the L1 is known and that the relations between the L1 are constrained sufficiently to assert R independently of F) then it will be possible to provide a training routine such that a proposition «Ai+1(F) Aj» is true of M at an instant t+At whereas it was not true at an instant t (this proposition might be interpreted «given the name of the operation inversion at Ai+1 in Ll+1, M will invert all matrices provided as input at Aj in L' »). The training routine amounts to a control procedure, the form of which does not immediately concern us, applied to M between t and t-'-At so that the control analogy of DIAGRAM 6 is satisfied at t+At . The term a in DIAGRAM 6, a^word in L'+1, 13</page><page sequence="20">186 G. PASK Satisfy 1 Condition ► n n I Al+i (F) Af L^-PŤ- ^ A,., &lt;=o B¡., until S the condition A¿+i (F) A ¿ ļ A,., u &lt;=o B¡., until the condition A¿+i (F) A ¿ ļ is satisfied | J DIAGRAM 6</page><page sequence="21">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 187 is a terminal segment of operation names in the training routine and the term ß, a word in L' is a terminal segment of problem or input names in this training routine. It is possible to guarantee that such a training routine exists providing that (1) and (2) are true. Now (1) entails the proposition. (3) « Man is a self organising system, for each B¡ in B .» But this proposition refers to a class of mechanisms, namely B which might be dignified by the term « mentation ». However, since man is represented by M and since for each level in M we have A¡ «=*■ Bř proposition (3) can be interpreted in terms of the At and, in par- ticular, it is possible to define computational criteria that must be satisfied if B is a self-organising system. Broadly these are of the form. (4) « The rate of learning must exceed some given average value if the interest and attention of M are to be maintained by the relevant training routine », the supposition being that if (4) is not satisfied then M will attend to some other than relevant data (it is possible to assign a numerical value to the learning rate and thus to use this criterion only if the skill is a structured skill). The appearance of terms like « attention » and « interest » gives away the fact that we are not talking about the A¡ (these are properties of processes, like mentation, they are not properties of a programme or a mechanical language). Since evolution of each Bj in B is a sufficient index of each system being a self- organising system, an A, interpretation of this index is a proposition like. (5) « Ař+1(G) A¡ », which is, «problem solving programmes applicable to the training routine in A¡ are generated at a sufficient rate in Ař+1 » and, given «Af«==»B,» this implies B{+1(y)B{, or verbally, « the B¡ evolve ». Obviously (5) entails certain constraints upon the form of training routine. It must supply a sufficient variety of problems for their solutions to be learned at the given rate and these must be represented in L' so that they are intelligible (in other words they must be denoted by expressions in the language concerned). Now, in order to maintain (1) and (2) which is necessary in order to achieve the goal F, there must be an auxiliary control procedure</page><page sequence="22">188 G. PASK ř Satisfy ķ Condition - ►- n r Až+i (G) Aj '7 T7 * l'I i !.' A,., «=&amp; B„ I jfeST v V T7 , L* A. &lt;=^&gt;B- u , [&gt;- I L* u fr H-L, I &lt;- 1 I V V M Controller adjusts M State until the condition AÍ+1(G) A¿ and thus Bi+i(g)Bi is satisfied DIAGRAM 7</page><page sequence="23">THE USE ANALOGY AND PARABLE IN CYBERNETICS 189 I : I r^r^-pn I [ _ il 11 I - i - &gt;( _ ¡ - Ai+1 &lt;3zz=î&gt; Bi+1 Satisfy n n Conditions II II : &gt;( -- A, &lt;=&gt;Bi f&gt; ft fl [ I* 1 I li JI M Controller adjusts M State until Joint Condition F, G, is satisfied DIAGRAM 8 that aims to maintain the control analogy of DIAGRAM 7 with goal G (the term auxiliary is used in exactly the sense of 1.4. and 2. 1 . (2)). The control procedure involves a variation in the type of problem posed in the training routine and its degree of sim- plification and is determinable if the skill is a structured skill.</page><page sequence="24">190 G. PASK The term &lt;5 in DIAGRAM 7 is a class of expressions in Li+1 and the term y is a class of expressions in L1. Hence the teaching process involves a pair of interacting control loops, as suggested in DIAGRAM 8. One of these control loops is concerned with instruction and the other is concerned with the auxilliary process of maintaining the internal analogy in M. The controller aims to achieve the joint goal, F,G and, broadly speaking, all teachable skills and concepts have the property that F is a member of a subset of acceptable G. Since satisfaction of the joint goal F,G, entails compromise and since, insofar as the L' are open ended it is necessary to change the level of discourse when M learns, the control process of DIA- GRAM 8 has the logical form of a conversation (rather than merely a process of communication). [61] [62] However, since R is defined independently of the goal pair, F,G we shall call it a restricted conversation. 2.3. Status of Internal Analogy Any restricted conversation depends upon the tacit acceptance of « A ļ &lt;==&gt; B i » as part of the acceptance of M as an analogue for the controlled assemblage. It can be argued on various grounds, using auxilliary data, that the similarity «A^^Bj» can be accepted (in the sense that a pertinent isomorphism can be specified). But there is no simple argument that demonstrates that we must accept this relation or that indicates the sense in which it should be construed. This depends upon whether, amongst the numerous neutral properties of A¡ and B¡, we believe a particular collection to constitute positive analogy properties. Our decision in this matter must be made against a specific background, that assigns some value on pragmatic grounds to the acceptance of certain neutral properties as properties of positive analogy. The point is important because, in subscribing to « A¡ &lt;^=^&gt; B¡» and also conducting a restricted conversation with M that we call teaching, or dignifying our interaction with some other anthropomorphic name, we give some evolutionary process the status of mentation, and this is to perform the action that MacKay [63] refers to as giving M a personal name.</page><page sequence="25">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 191 Thus, in routine experiments, we simulate M as a computor programme and pit it against a simulation of the adaptive teaching machine (which mediates the restricted conversation of teaching) in the same way that a real life student is pitted, in the laboratory, against a real adaptive teaching machine (on the assumption that M represents his learning process). In the case of the pro- gramme I am inclined to assert that « A(4=^B¡» in M is purely relational, and a special isomorphism. On the other hand, when M represents the student, I am inclined to believe that different properties are involved in the internal analogy and thus to give the student a personal name. Perhaps the issue is reversed, and these additional properties are asserted because the student has been named. In either case the choice is my own and it is chiefly influenced by the auxiliary data of 1.4. and 2.1. (2) rather than the immediate experiment or the immediate behaviour of this student. 2.4. Conversation and Incomplete Analogy So far we have assumed that R can be asserted independently of F and G and thus that the control analogy is complete. We shall now briefly examine the case in which the analogy is in- complete when the associated control procedure is an unrestricted conversation wherein a controller aims to persuade M about F. Since R depends upon F,G and is undefined initially, the discourse must take place in a mixed language like L* and at the outset a more broadly specified goal, say G*, is required, to replace the goal G of a restricted conversation (we can replace G* by G if rapport is established between the participants). Although no mechanised simulation of an unrestricted conversation has yet been attempted, it is possible to advance a G* in place of G, namely, that the controller should be able to resolve what Gorn [64] calls the « Pragmatic Ambiguity » of the discourse in L* needed to achieve F, in the other participant represented by M (we con- jecture that the idea of « the controller », which implies an initially decided dominance, may be misplaced. In an unrestricted con- versation the ultimate condition may be a compromise, that is preferred by the other than dominant participant).</page><page sequence="26">192 G. PASK Pragmatic Ambiguity is introduced by Gorn in his consideration of the mechanical languages used as part of a data processing system. He points out that whenever any mechanical language is embodied in such a system it gains an intensive definition (which, incidentally, is a special case of a B system) over and above its extensive definition (as the set of strings of signs that are producible and permissible in this language). The intensive specification of the language is the set of data processing entities that act upon the expressions in this language and also upon its structural constraints. Pragmatic Ambiguity is a confusion between these entities or between one input domain and another. In principle the data processing can be unambiguous if the control programme for the system is stratified, using an hierarchy of metalanguages like the L1'. However, stratified control pro- grammes readily become impracticable and the requirement of open ended L' becomes incompatible with postulates like (I) and (II) of 2.1. if we also insist upon universal decidability of the expressions that are being processed. In pratice, the control programme is expressed in a Mixed Language like L*, but as a result of this, Pragmatic Ambiguity is bound to occur (in the sense that some words may be interpreted in several ways, as control instructions, as data to be processed, as the requirement for creating a processing entity or in yet other fashions). The Mixed Language system is, incidentally, capable of paradox and incon- sistency (which can, of course, be removed by a distinction between logical types [65] which is tacitly involved in the hierarchical L' distinction). On the other hand, Pragmatic Ambiguity can be removed in in a local context, for example, in the context of achieving F, in a way that can be mechanised and does not lead to a replacement of L*. Broadly, this local decidability entails local « understanding » of L* and we require, by G*, that the controller is able to « understand » those L* expressions pertinent to achieving F. In particular, this controller must be able to give emphasis to pertinent ex- pressions in L*, to interpret expressions that assign specific con- notations to pertinent words in L*, to give value to terms and to recognise classes of relations and rules.</page><page sequence="27">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 193 One important special case of an unrestricted conversation involves a kind of incomplete analogy that I shall call a parable1. Suppose there is an incomplete analogy of the kind shown in DIAGRAM 9. The particular characteristics of this analogy are that a controller Y can appreciate a dispositional proposition P which, if accepted by a recipient, X, would have certain beha- vioural consequences T. Let us assume that X is unable, at a given instant U to appreciate P but that we can appreciate P which also leads to T. Now Y can aim to achieve either of the situations shown in DIAGRAM 9 by dint of an unrestricted conversation with X, each of which gives rise to the behaviour T on the part of the recipient X. If, as a result of the unrestricted conversation with Y, we obtain a complete analogy with P* dependent relation R*, then it is fair to call Y's control activity coercive (until proved to be other than coercive) for he has obtained T on the basis of P*. On the other hand, if Y believes that X will evolve, so that at some instant t+At (perhaps later than the moment when his control procedure ceases), recipient X will become able to appreciate P and that P* ostensively defines P and that T induces X to presuppose a P oriented behaviour, then we say that Y has used a parable . If X,Y, happen to be subsystems in the same organism only the latter possibility is admissible though the parable may not be realised because A t is too great. If X = M and Y is a controller the situation is that if Lm is the highest level metalanguage in M at an instant t and if Lm+1 evolves before t+At then ft is a word in Lm+1, d is a word in Lm, a is a word in Lř+1, m &gt; i and c is a word in L*. 1 I intend « Parable » as the word is used in religious teaching and discussion, for example in Buddism and above all, in Christianity. A similar form of analogy appears in political control and in connection with fairy stories, though it is less well developed. A brief survey of the literature reveals that our usage is not altogether eccentric. Manson, [66] for example, conjectures that the F in a parable analogy is a « Concrete Universal ». Jeremias [67] lays emphasis upon the use of parable analogy in specific control situations. The idea that the recipient does not immediately appreciate the universal proposition but « becomes able » to appreciate it in some later state is asserted by Swete [68] and even Jullicher, [69] who was the first to discount the allegorical interpretation ot the specifically Christian parables, was careful to preserve this concept of « becoming » able or of evolution on the part of the parable recipient.</page><page sequence="28">194 G. PASK 1 I I a_ (R*) JD I Controller Recipient i (pi (p*) - - .£ (R*) d_ (Relation | #P I such that . I"/ T occurs) ^ Coercion ' c_ Y X A (R) b (P) fP) ç_ (R) d. Parable t t + A t DIAGRAM 9</page><page sequence="29">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 195 2.5. Internal Conversation and Creativity There is nothing to prevent internal conversations between subsystems of an organism or components of M. It appears that an internal restricted conversation has most of the attributes ascribed to rational or closed thinking and that an internal and unrestricted conversation can have most of the attributes of either rational or irrational, closed or open thinking. Finally if there is a parable that is used as part of a control process with the specific identification suggested by DIAGRAM 10 then the resulting appreciation of a principle may, if it occurs, be called the result of Creativity. [70] In DIAGRAM 10 the creative process is illustrated in a slightly restricted form (and depends upon four plausible assumptions). We assume that Bm+1 is defined before Am+1, that Am+1 becomes available as it can, from (I) and (II) of 2.1. and that P* is a lower level interpretation of P in the hierarchy of metalanguages. We conjecture that this creative process is the analogue in A for the B process of induction [71] (in the biological sense) which is an attribute of many evolutionary systems. At a more grandiose level the M Model can be fruitfully identified with various schemes of developmental psychology such as Piaget' s, [72] Luria's [74] and Vygodtsky's [73] and it unifies certain of their divergent features. Indeed it is possible to view the M Model as a Cybernetic inter- pretation of Vygodtsky's conceptual scheme which provides an explicit statement of the admissible experiments. Thus the contention of 2.2. that experiments upon learning must involve a conversation in which the student benefits from instruction (which lies at the basis of all our work) was propounded, many years ago but on somewhat different grounds, by Vygodtsky (like others in this country I was unaware of this fact until a translation very recently became available). In this connection the matu- rational development of linguistic or A system control programmes, which is considered by Vygodtsky and by Luria, has a mechanistic or B system, analogue in the shift that is exhibited in biological systems from a « tissue level » to a « cellular level » of control and which is one consequence of induction.</page><page sequence="30">196 G. PASK , 1 t a t0 t * t| ļ t - t2 I , 1 , , 1 r bm+1 bm+1 I iii i iii i i $ * i I ' I g i I ^ g -ļ ^ ' fy C B. ļ A„^ B. - H- h^-h+ļ-h-K IT Am.&lt;^^Bm_1 ļ n n i i i O O &lt;y &lt;y j '&gt; '/ , i I i Analogy for Creative Process in M. mafy is P and a^is P* and t2 &gt; tx &gt; t© DIAGRAM 10</page><page sequence="31">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 197 References 1. Hesse, M.B., Models and Analogies in Science . London: Sheed and Ward, 1963. 2. Pap, A., An Introduction to the Philosophy of Science . New York : Free Press, 1963. 3. Braithwaite, R. B., Scientific Explanation . Cambridge Univ. Press, 1954. 4. Popper, K. R., The Logic of Scientific Discovery. New York : Basic Books, 1959. 5. Beurle, R. L., Properties of a mass of cells capable of regenerating pulses, Phil. Trans. Roy. Soc. B, 240 , 1956. 6. Farley, B., Aspects of behaviour in neurone network model, 3rd Bionics Symposium, Dayton, Ohio, 1963. To be published. 7. Harman, L. D., The artificial neurone, Science , 129, 159. 8. Pask, G., The natural history of networks, in M. C. Yovits ans S. Cameron (eds.) : Self-Organising Systems. London : Pergamon Press, 1960. 9. Ashby, W. Ross, The mechanism of habituation, in The Mechanisation of Thought Processes. London : H.M.S.O., 1959. 10. Ashby, W. Ross, The self-reproducing system, in C. A. Muses (ed.): Aspects of The Theory of Artificial Intelligence. New York : Plenum Press, 1962. 11. Ashby, W. Ross, Introduction to Cybernetics. London : Chapman and Hall, 1956. 12. Pask, G., An Approach to Cybernetics . London: Hutchinson, 1961. 13. Couffignal, M. L., Essai d'une definition generale de la cyber- netique, in Proc. 2nd Int. Conf. on Cyber- netics, Namur, 1958. 14. Wiener, N., Cybernetics. 2nd Edition, New-York: Wiley, 1962.</page><page sequence="32">198 G. PASK 15. Pask, G., A discussion of artificial intelligence and self-organisation, in M. Rubinoff (ed.), Advances in Computers , Vol . 4 , New-York: Academic Press, 1964. 16. Pask, G., Learning Machines, in Proc. Int. Fed. of Automatic Control (IFAC), Basle, 1963. To be published. 17. Ashby, W. Ross, Design for a Brain. London : Chapman and Hall, 1962. 18. Lettvin, J.Y., What the frog's eye tells the frog's brain. Maturana, H. R., Proc. I. R. E., 47, 1959. McCulloch, W. S., and Pitts, W. H. 19. Young, J. Z., Some essentials of neural memory systems. Proc. 10th Int. Congress on Electronics, Rome, 1963. To be published. 20. Sutherland, N. S., Stimulus analysing mechanisms. In The Mechanisation of Thought Processes. London : H. M. S. 0., 1959. 21. Napalkov, A. V., Information Processes of the brain. In N. Wiener and J. P. Schade (eds.), Nerve , Brain and Memory Models . London : Elsevier, 1963. 22. Skinner, B. F., A case history in scientific method. In S. Koch (ed.), Psychology : A Study of a Science , Study 1, Vol. 2, New York: McGraw-Hill, 1959. 23. Harre, R., Theories and Things. London : Sheed and Ward, 1962. 24. Pask, G., Self-organising systems involved in human learning and performance. In 3rd Bionics Symposium, Dayton, Ohio, 1963. To be published. 25. Mackay, D. M., The informational analysis of questions and commands, in Communication Theory (C. Cherry, ed.), London : Butterworths, 1962.</page><page sequence="33">THE USE OF ANALOGY AND PARABLE IN CYBERNETICS 199 26. Foerster, H. Von., On Self-organising systems and their environments. In M. C. Yovits and S. Cameron (eds.), Self-Organising Systems . London : Pergamon Press, 1960. 27. Foerster, H. Von., Biologic. In E. E. Bernard and M. R. Kare (Eds.), Biological Prototypes and Synthetic Systems . New York : Plenum Press, 1962. 28. Pask, G. A Model for concept learning, Proc. 10th Int. Congress on Electronics, Rome, 1963. To be published. 29. Bartlett, F., Thinking. London : Allen and Unwin, 1958. 30. Morris, D., The Biology of Art. London : Methuen, 1962. 31. Pask, G. Man as a system that needs to learn. In S. Beer (ed.), Advances in Cybernetics. New York : Academic Press. To be published. 32. Pask, G. The simulation of learning and decision making. In C. A. Muses (ed.), Aspects of the Theory of Artificial Intelligence. New York: Plenum Press, 1962. 33. Pask, G. Comment on an indeterminacy that cha- racterises a self-organising system. In V. Braitenberg and E. Caianiello (eds.), Cybernetics and Neural Processes. New York : Academic Press, 1964. 34. Pask, G. A discussion of the cybernetics of learning behaviour. In N. Wiener and J. P. Schade (eds.), Nerve, Brain and Memory Models. London : Elsevier, 1963. 35. Hovland, C. I., A communication analysis of concept learning, Psych. Rev., 59, 1961. 36. Minsky, M., Steps towards artificial intelligence, Proc. I. R. E., 49, 1961. 37. Newall, A. Learning and problem solving. In R. Pop- plewell (ed.), Information Processing. North Holland Publishing Co., 1962. 38. Polya, G., How to Solve it. New York: Doubleday, 1957.</page><page sequence="34">200 G« PASK 39. Broadbent, D. E., Perception and Communication . London : Pergamon, 1958. 40. Hull, C. L., Principles of Behaviour. New York: Appleton- Century-Crofts, 1943. 41. Hebb, D. 0., The Organisation of Behaviour. New York: Wiley, 1949. 42. George, F. H., The Brain as a Computer. London : Pergamon, 1961. 43. Milner, P. M., The cell assembly: mark II, Psychol . Rev., 64, 242-252, 1957. 44. Chomsky, N., On the notion : « Rule of Grammar ». Proc. Sgmpos. Appl. Maths , Vol. XII, 1961. 45. Amarei, S., On the automatic formation of a computer program which represents a theory. In M. C. Yovits, G. T. Jacobi and G. D. Goldstein (eds.), Self-Organising Systems 1962 . Washington : Spartan Press, 1962. 46. Minsky, M., and Randon nets, in C. Cherry (ed.) : Selfridge, 0. D., Communication Theory. London: Butterworths, 1962. 47. Fogel, L. A., Towards inductive inference automata. In R. Popplewell (ed.), Information Processing. North Holland Publishing Co., 1962. 48. Mesarovic, M. D., On self-organisational systems. In M. C. Yovits, G. T. Jacobi and G. D. Goldstein (eds.), Self-Organising Systems 1962. Washington : Spartan Press, 1962. 49. Beer, S., Toward a cybernetic factory. In H. Von Foerster and G. W. Zopf (eds.), Principles of Self-Organisation. London : Pergamon, 1962. 50. Mesarovic, M. D., Towards the development of a general systems theory, Nouvelles Techniques, Vol. 7, 1961. 51. Loefgren, L., Tesselation models of self-repair. In E. E. Bernard and M. R. Kare (eds.), Biological Prototypes and Synthetic Systems. New York : Plenum Press, 1962.</page><page sequence="35">THE USE OF ANALOGIE AND PARABLE IN CYPERNETICS 201 52. Baricelli, N., Numerical testing of evolution theories, Acta Biotheoretica, 1 and 2, 1963. 53. Pask, G., A proposed evolutionary model. In H. Von Foerster and G. W. W. Zopf (eds.), Principles of Self-Organisation. London : Pergamon, 1962. 54. Pask, G., Physical analogues to the growth of a concept. In The Mechanisation of Thought Processes „ London : H.M.S.O., 1959. 55. Pask, G. and Research on cybernetic investigation of learning and perception. A.R.No.4 , U.S.A.F. Contract AF61(052)-640 , 1963. 56. Lewis, B.N., The rationale of adaptive teaching machines. In M. Goldsmith (ed.), Mechanisation in the Classroom. London : Souvenir Press, 1963. 57. Lewis, B. N. and The theory and practice of adaptive teaching Pask, G., systems. In R. Glaser (ed.), Teaching Machines and Programmed Learning. II. Data and Directions. Nat. Ed. Assoc., 1964. 58. Pask, G., The adaptive teaching machine, Control, 1964. 59. Pask, G., Adaptive teaching systems and a model of learning applicable to systems stabilised by an adaptive teaching machine. Tech. Note No. I U.S.A.F. Contract No. AF61(052 )- 402, , 1963. 60. Pask, G., Adaptive teaching systems. In K. Austwick (ed.), Teaching Machines. London : Pergamon Press, 1964. 61. Pask, G., The logic and behaviour of self-organising systems as illustrated by the interaction between men and adaptive machines. I.S.I.T., Brussels, 1962. 62. Pask, G. Teaching Machines. Proc. 2nd Int. Conf. on Cybernetics, Namur, 1958. 14</page><page sequence="36">202 G. PASK 63. Mackay, D.M. Contribution to discussion on definition. In C.A. Muses (ed.), Aspects of the Theory of Artificial Intelligence. New York: Plenum Press, 1962. 64. Gorn, S. The treatment of ambiguity and paradox in mechanical languages. In Recursive Function Theory. Proc. Symposia in Pure Mathematics, Vol. 5. Amer. Math. Soc., 1962. 65. Rüssel, B. and Principia Mathematica. Cambridge Univ. Whitehead, A. N., Press, 1927. 66. Manson, T. E., The Teaching of Jesus. Cambridge Univ. Press, 1934. 67. Jeremias, J. Parables (transi, by S.C. Hooke), London: S.C.M. Press, 1954. 68. Swete, H. B., Parables of the Kingdom. London : Macmillan, 1920. 69. Jullicher, A., Die Gleichnisreden Jesu. Tübingen, 1899. 70. Pask, G., The conception of a shape and the evolution of a design. In J. C. Jones and D. G. Thornley (eds.), Conference on Design Methods. London : Pergamon, 1963. 71. Auerbach, R., The organisation and reorganisation of embryonic cells. In M. C. Yovits and S. Cameron (eds.), Self-Organising Systems. London : Pergamon, 1960. 72. Flavell, D., The Developmental Psychology of Jean Piaget. Van Nostrand, 1960. 73. Vygodtsky, L., Thought and Language . The M.I.T. Press 1962. 74. Luria, A. The role of speech in the regulation of normal and abnormal behaviour. Pergamon Press, 1961. 75. Miller G. A. Plans and the Structure of Behaviour. Gallawter E. Holt, 1960. and Pribram K.</page></plain_text>