<plain_text><page sequence="1">THOMAS D. COOK AND DONALD T. CAMPBELL THE CAUSAL ASSUMPTIONS OF QUASI-EXPERIMENTAL PRACTICE* The Origins of Quasi-Experimental Practice 1. TWO TYPES OF EXPERIMENTATION It might be desirable if the procedures for gaining knowledge in a particular field were developed only after exhaustive and critical discussion of a broad range of epistemological options, from which a subset was then selected as appropriate for determining practical method choices. While superficially attractive, this scenario would have at least two disadvantages. First, some of the practices so developed would almost certainly need revision in light of the consequences practitioner-scientists experienced in using them, with some techniques falling out of the repertoire, others being modified, and others being added as the products of unexpected discoveries achieved through trial-and-error learning by scientists on the job. Second, no science has evolved so systematically from first principles, certainly not the set of techniques called "quasi-experimentation" whose history reflects the operation of happenstance as well as deliberate choices. As its name suggests, quasi-experimentation aspires to approximate the "experimental method", usually in settings where "full experimen tal control" is not possible because researchers are trying to identify the consequences of social changes in naturalistic contexts. The drive to generalize research results to naturalistic settings precludes use of one tradition of experimentation that we call the tradition of experimental control and isolation. In this tradition, discrete interventions are applied in laboratory settings to an ideally pure sample of test materials, often to probe precise numerical predictions. The laboratory setting allows re searchers to control pressures, temperatures, and atmospheres, and to isolate materials within lead-shielded and degaused chambers, sterilized test tubes, and the like. It is out of this experimental tradition that the successful pure sciences arose which have captured most of the attention of philosophers of science. A second ideal type of experimentation replaces laboratory control and experimental isolation with statistical control, achieved largely through the random assignment of multiple samples to contrasting ex Synthese 68 (1986) 141-180. ? 1986 by D. Reidel Publishing Company.</page><page sequence="2">142 THOMAS D. COOK AND DONALD T. CAMPBELL perimental treatments, some of which may be control conditions where no treatment is explicitly made available. In what follows we shall use the shorthand random-assignment experiment to describe this tradition and to differentiate it both from experiments based on laboratory isolation and control and from situations where samples are randomly selected to represent a predesignated population but where there is no deliberate assignment to different treatments on a randomized basis. When biologists moved out of the laboratory into the agricultural experiment station where rain, temperature and soil composition were uncontrolled, the lack of control led them to invent or elaborate the random-assignment experiment (Fisher, 1925). Interestingly, the psy chologist E. L. Thorndike and his associates (McCall, 1923) simul taneously proposed the same technique for moving the study of learning out of the laboratory and into the classroom. As elaborated by many other statistical theorists, Fisher's methods rapidly became the orthodoxy for social psychology, educational psychology, pharma ceutical research, and medical trials, as well as agriculture. While under attack from Bayesian statisticians, this impressive innovation in scientific method and theory of inference is used when causal inference is at stake and hypotheses cannot be generated in the form of numerical point estimates. The developers of quasi-experimental methodology were trained in this second experimental tradition based on random assignment and, by and large, have continued to accept it as optimal where feasible. However, they recognize that it is not always feasible and sometimes breaks down after being initially implemented. Hence, alternatives to the random-assignment experiment are needed. Quasi experiments provide the closest alternative, since they are like random assignment experiments in all respects except that the various treat ment groups cannot be presumed to be initially equivalent within limits of sampling error. 2. THREATS TO VALIDITY For psychology, at least, the random-assignment experiment had become ritually orthodox in the teaching of experimental statistics and most of its practitioners have forgotten what they were controlling for. A first step in developing quasi-experimental methods (e.g., Campbell 1957) was to make explicit some of the specific threats to validity for which random assignment does not usually control and those threats</page><page sequence="3">CAUSAL ASSUMPTIONS 143 that are often plausible if random assignment has not occurred. (We outline some of these later.) A second step [taken in Campbell and Stanley (1963,1966) and Cook and Campbell (1979)] was to detail a dozen or so quasi-experimental designs, each of which was presented with a discussion of the extent to which it usually controls for particular validity threats. To develop these designs required assembling an older tradition of research efforts than those built upon the innovations of Fisher and his successors, thus ambivalently increasing the respec tability of these early efforts by providing formal methodological arguments about their strengths even while making their weaknesses excruciatingly explicit. In their lineage and focus on nonlaboratory research settings, quasi experimental methods are much closer to the random-assignment experiment than to the experimental tradition based on laboratory isolation and control. However, there is still one important sense in which quasi-experimentation is closer to the latter than the former. Random-assignment has been seen from Fisher on (Paul Humphreys, personal communication) as controlling for all of an unspecified set of threats to validity. In contrast, for both laboratory control and quasi experimentation, controls are used to deal with specific threats to validity that are judged to be plausible in disputes within the scientific community at a particular time. This is not to suggest that attention to specifically articulated validity threats is totally absent from the overall methodology accompanying the random-assignment experiment. Such attention appears in the choice of "blocking" or "matching" variables employed to make up a priori-equivalence sets from within which random assignments to treatments are then made. While such blocking is usually justified in the Fisher tradition on grounds of precision (i.e., to make smaller samples more decisive), in the Bayesian tradition a blocking variable is considered to be more plausible as a rival explana tion to the degree it reduces variance in outcome measures. A second example, from the tradition of control group selection, further illu strates the "historicist" emphasis on argument within the scientific community as to the origin of claims about the plausibility of rival hypotheses. In the 1970's, in brain ablation studies, surgical shock was regarded as enough of a rival explanation of some effects to justify the "sham operation" control group in which comparable injury was inflicted without the removal of the specified area of the brain under study. By the 1960's, the "known" and predicted effects of brain</page><page sequence="4">144 THOMAS D. COOK AND DONALD T. CAMPBELL ablation were so specific that surgical shock was no longer considered a plausible rival explanation, and the sham operation had disappeared from the literature. Contrast this with most modern pharmaceutical research where several placebo effects still remain very plausible rivals for most general therapeutic effects, justifying the current best orthodox practice in which placebo control groups are used that are implemented in double-blind fashion. As has been emphasized in quasi-experimental presentations, there are many threats to validity that randomized experiments do not control. These include the threats to external validity that create uncertainty about the generalizability of relationships; the threats to construct validity that create uncertainty about how variables should be labelled theoretically; and the threats to statistical conclusion validity that creates uncertainty about how strongly two or more variables are related. Randomized assignment appears to control for most threats to internal validity - i.e., threats to the conclusion that a relationship between variables is causal in the manipulability sense. However, even Cook and Campbell (1979) have unearthed some threats with respect to internal validity for which random assignment does not control. Moreover, the implementation of randomization is never as perfect in practice as the statistical models assume, and situation-specific knowl edge is necessary to evaluate the plausibility of any threats to validity that result from the imperfect implementation of random-assignment procedures. Given all these limitations, the randomized experiment cannot be counted on to rule out all plausible alternative interpretations that are relevant to make decisions about* "facts" or theory choices. Rendering specific rival hypotheses implausible still remains a central task of science, whether random assignment has occurred or not. A major feature of the post-positivist consensus is a recognition that the best of theories go far beyond observations; they are "under determined" by scientific "facts" (theory-laden or otherwise). Quine's (1951,1963) rejection of the second dogma of empiricism (reduction) is a convenient locus classicus for this truism, describing both our epis temological predicament and the coherence strategy of belief revision which we employ in the face of that predicament (without thereby necessarily adopting a coherence definition of "truth"). Quine's paper has recently been criticized both for a holism which postpones valid theory choice until the whole of science is completed (e.g., Millikan 1984; Putnam 1983, pp. 229-247) and for an exaggeration of</page><page sequence="5">CAUSAL ASSUMPTIONS 145 science's impotence in the face of the Quine-Duhem problem. However, neither criticism is just. Quine describes our epistemological predicament correctly, and his holism is but a restatement of the unavoidable underdetermination of theory choice. More importantly, his description of scientific practice contains in it the means by which the more successful sciences prevent the underdetermination of theories from paralyzing practical scientists' search for improved belief. In The Roots of Reference (1974) and especially in 'The Nature of Natural Knowledge' (1975), Quine makes it clear that he believes scientific strategies of belief revision are faute de mieux rational, given our predicament as knowers. Explicitly assuming (rather than pretend ing to prove) the success (albeit corrigible) of both ordinary knowing and science at its best, he presents what can be called a "hypothetically normative" epistemology for both vision and science (Campbell, 1986; Giere, 1985). Restating his point of view under the two headings below should help dramatize its relevance to the quasi-experimental program and may also make clear why the criticisms of Quine are incorrect. A. Panfallibilist Trust Given our evolutionary and epistemological predicaments, the avail able strategy in science is to try to revise some beliefs even while trusting most of what we already know. We must realize that no part of the belief and presupposition structure that we have to assume is itself immune from revision, not even logic itself (see also Campbell 1977, 1978). While the coherence strategy of belief change depends on what is already (fallibly) known, it also depends on the ability to match patterns of observed relationships with theory-related relationships. Needless to say, such pattern matching (Campbell, 1966) in practice never involves the whole of current science, and so is inadequate if one assumes with Quine that "our statements about the external world face the tribunal of sense experience not individually but only as a corporate body" (1961, p. 41), and "the unit of empirical significance is the whole of science" (1961, p. 42). Given the necessity for trust and for theory specificity, we can never answer the skeptic on his or her own terms, i.e., without presuppositions. Indeed, there are probably some presup positions that we humans are most reluctant to revise, especially automatic posits of physical objects and events which our sense organs provide at the periphery and the very logic and presuppositions that are</page><page sequence="6">146 THOMAS D. COOK AND DONALD T. CAMPBELL central to the enterprise of knowing as a whole. It is presumably the intermediate portions of this web that are most readily revised, i.e., scientific theory. But even here pressures exist to minimize these changes, even through "swelling ontology to simplify theory" (1961, p. 45). B. The Ramification-Extinction of the Plausibility of Rival Hypotheses At the level of epistemological logic, the Quine-Duhem equivocality in the relationship between those peripheral posits designated as "facts" and those intermediate posits designated as "theories" is never over come. However, in the history of successful sciences, the specific Quine-Duhem type challenges raised at a given historical period have not proved to be paralyzing to practitioners. Instead, in a process that is distributed across research teams and depends heavily on the discretionary interests of individual scientists, the focally relevant scientific community of a given historical period has usually considered the plausibility of challenges to conventional knowledge - accepting some and rejecting others by comparing their plausibility with that of the conventional theory under test. Such collective discretionary decisions do not constitute proof and, being sometimes wrong [e.g., Bechtel (1984), for the early theory of the cell], do not contradict the Quine-Duhem thesis. However, they do provide pragmatic probes of rival theories and they may be the best substitute for "proof" we can achieve - a substitution that is not totally worthless and may be responsible for the widespread belief that some sciences have indeed progressed in the "empirical adequacy" (Van Fraassen, 1980) or approximate validity of their theories. In comparing the specific rival interpretations available to the historically embedded scientific community, plausibility judgments often take into account any new "facts" that may have originally provoked discussions of the merits of different interpretations. But plausibility judgments can also be based on considerations relevant to the many other ramifications of the rival interpretations under con sideration, i.e., their implicative compatibility with other trusted "facts," fundamental presuppositions, and theories. In the two British 1919 eclipse observations and the subsequent debate [so well described by Moyer (1979)], many rival explanations were considered in addition to the gravitational theory coming from Einstein's general theory of</page><page sequence="7">CAUSAL ASSUMPTIONS 147 relativity. These rivals included the possibility of higher lens tem perature for the noneclipse comparison photos, lens effects from distributed matter, the attribution of gravitational mass to photons, and even slightly modifying the gravitational attraction law of lid2. The ramifications of each of these rival explanations were explored and found to be less plausible in light of the pattern match between the predictions specified in the ramifications and the multiple, but separately fallible, observations the scientists actually made. As a result, the community of British astronomers and physicists changed between 1917 and 1923 from one in which few accepted general relativity to one in which almost all accepted it. A complementary case is provided in Clausner and Shimony's (1978) review of the dozen or so experiments then available on Bell's theorem. While each of the experiments left open one or more seemingly plausible Quine-Duhem explanations for a hidden-variable theorist, overall the results were convincingly nega tive, even to those reviewers who had tentatively favored a hidden variable interpretation of the quantum effects in question. In laboratory and especially quasi-experimental methodology, the threats to validity considered are not usually rival formal theories of comparable scope. Rather, they are "nuisance" theories, often mun danely concerned with possible "side effects" of the apparatus. Since the status of these nuisance theories is as contingent hy potheses rather than logical requirements, their plausibility can be a matter of empirical/theoretical determination as judged by the state-of-the-art at the time. In our earliest presentations of the threat to valid generalization from conducting research on pretested samples (Campbell, 1957; Campbell and Stanley, 1963), we argued as though generalization to nonpretested samples was illogical because such samples had not been observed. In subsequent presentations (e.g., Campbell, 1969a) we have since recognized that such logicism would preclude all generalizations, and that the relevant basis for evaluating the threat of pretest sensitization comes from empirical research on the issue. 3. RANDOM ASSIGNMENT AND EXPERIMENTATION IN OPEN SYSTEMS The model of experimentation borrowed from agriculture and psy chology is far from perfect. Random assignment allocates treatments</page><page sequence="8">148 THOMAS D. COOK AND DONALD T. CAMPBELL as though by lottery. But some phenomena worth study cannot be so controlled, e.g., natural disasters. Moreover, logistical and ethical considerations sometimes preclude chance assignment, e.g., when studying laws that have to apply to all citizens, or when social mores dictate that a resource in short supply be allocated by need or merit rather than by chance. Random assignment also creates probabilistic group equivalence at the wrong time. Causal inference would be easier if, on everything except exposure to a treatment, the various experimental groups were still manifestly equivalent at the end of a study. Yet some treatments are more desirable than others, and the persons receiving them are less likely to drop out of a study. When attrition differs across treatment groups, the purpose of random assignment is vitiated, for the groups now differ not only in theoretically meaningful exposure to a treatment but also in their theoretically irrelevant average group composition. When random assignment occurs the individuals receiving one treatment sometimes know or suspect that other individuals are receiv ing different treatments. Reactions to this knowledge are then con founded with the researcher's treatment assignments. Using past research as a guide, Cook and Campbell (1979) listed four ways in which knowledge of treatment differences might limit causal in ferences. Compensatory rivalry occurs when respondents receiving a less desirable treatment attribute this to being less deserving and respond with increased efforts in order to demonstrate that they are indeed deserving. Resentful demoralization occurs when individuals respond to treatment differences with despondency and decreased effort. Compensatory equalization occurs when administrators anti cipate that persons in an experiment will react negatively to knowledge of treatment differences and so they distribute nonexperimental resources in ways designed to reduce the differences the treatment assignment was supposed to create. Finally, treatment diffusion occurs when the respondents in one group observe the treatment in another group and decide to adopt some or all of the observed treatment components, thereby reducing the planned experimental contrast. None of these responses are limited to situations of random assign ment. The crucial event is learning about treatment differences, and this can occur whatever the assignment procedure. However, random assignment may especially increase the likelihood of respondents noting and responding to treatment inequities. For reasons of cost and sample</page><page sequence="9">CAUSAL ASSUMPTIONS 149 size, it is easier to achieve initially comparable treatment groups the lower the unit of aggregation studied - viz., when students from the same school class are assigned as opposed to when intact classes are; or when intact classes are assigned rather than whole schools. However, inter-unit communication about treatment differences is also more likely with aggregates of lower order, since children in the same class speak to each other and see each other more than children in different classes, and children from different classes in the same school see each other more often than children from different schools. The second rationale for suggesting that atypical reactions may be more common when treatment assignment is at random is that few resources in our society are so distributed. Merit or need are more common allocative principles and may generate less attention and social comparison than assignment by lottery. Quasi-experimentation has been proposed as an alternative form of experimentation for cases when random assignment is not possible, has broken down, or will result in responses that cloud interpretation of a causal agent. In addition to the prototypic components of all experi mentation (an abrupt intervention at a known time and the quantitative measurement of outcomes) quasi-experimentation retains the usual features of Fisherian agricultural experimentation - viz., one or more comparison groups and multiple units assigned to each treatment. But it differs from such experimentation principally in that assignment to treatments is not at random, the units studied are more likely to be people than animals, and data collection is more likely to occur outside of a laboratory. This last point is important, for in our reconstruction the origins of quasi-experimentation lay in the desire to escape from the laboratory as well as in the desire to examine the causal consequences of phenomena for which random assignment was not considered possible. Apprehen sion arose about studying social behavior in the laboratory, first, because of the concern that the laboratory might be a unique social culture with its own norms about appropriate behavior. So, theories were developed and tested of how subjects thought they were supposed to behave in the laboratory and of the conditions under which subjects would want to corroborate an experimenter's hypothesis or give data that might deliberately infirm it (Weber and Cook, 1972). A second concern was that laboratories limit the range of human phenomena that can be studied, particularly since for mundane reasons few</page><page sequence="10">150 THOMAS D. COOK AND DONALD T. CAMPBELL laboratory experiments with humans can last more than 50 minutes. The third concern was that no algorithm exists for generalizing from laboratory experiments either to behavior in nonlaboratory settings or to the behavior of populations rarely studied in the laboratory where college students predominate. While the laboratory could make many forces independent that are normally correlated, the fear was that in so doing human phenomena would be removed from the very contextual and social arrangements that give them human meaning. Moreover, the extent of such situational dependence could not be readily assessed if all the studies on a particular topic occurred in the laboratory. 4. PURPOSE Below we summarize the current systematization of quasi-experiment al design, presenting in some detail two quasi-experimental investi gations so that readers can gain some feel for quasi-experimental practice. After this we explore the assumptions of the best practice in quasi-experimentation, suggesting that the ontology is realist and predicated on a world of probabilistic multivariate causal agency in which some manipulable events dependably cause other things to change. The purpose of quasi-experimentation is to identify such causal agents, not to achieve the total prediction or complete explana tion specified in other conceptions of cause. The Structure of Quasi-Experimentation A. The Notational System for Individual Designs In the tradition of quasi-experimentation, a notational system is used to describe research designs that depends on knowing three things: (1) when the units in a study were observed; (2) which treatment was assigned to a particular group of units; and (3) how the assignment of treatments occurred. In this system, O stands for observation and a numerical subscript indicates temporal sequence (Ox precedes 02, etc.). Implementation of a treatment is designated by an X, with a blank space indicating a no-treatment control group. (When several treat ments are contrasted in the same study each is indicated by a numerical subscript.) The manner of treatment assignment is shown as R for random assignment and as a dotted line between the various</page><page sequence="11">CAUSAL ASSUMPTIONS 151 groups if this assurance of equivalence before treatment is lacking (a "nonequivalent" but still potentially useful control group). This simple notational system generates many versions of an ap parent OX O game, each of which corresponds to a particular quasi experimental design. Thus, if only a single group is studied and measurement occurs both before and after a treatment, the design would be: Ox X 02. If a nonequivalent control group was added, the design woul become: oxxo2 ?i ?2 If the control and treatment groups had been formed at random the (non-quasiexperimental) design would then be: RO,X02 ROx 02 An interrupted time series study would be described as below, with the X indicating where we would expect an interruption in the plotted data if the treatment made an immediate difference: Ox 02 03 04 05 06 07 08 09 O10 Oxx 0X2 0X3 014 X If a control series were added to the simple interrupted time series, the design below would result: Ox 02 03 04 05 06 On 08 09 O10 Oxx 012 013 X Ox 02 03 04 05 06 On 08 09 O10 Oxx 0X2 0X3 Cook and Campbell (1979) added some structural elements to those of Campbell and Stanley (1963). They noted that it is sometimes possible to deliberately withdraw a treatment to test whether this results in the disappearance of any changes that happened when a treatment was first introduced; X was used to designate treatment removal. Cook and Campbell also noted that some treatments can be reintroduced, making multiple X's possible for a single group at different times and for multiple groups at multiple times. They also called attention to the possibility that some treatments might affect a certain class of obser vations without affecting other observations that relevant substantive theory suggests should be affected by the most plausible alternative interpretations of an apparent treatment effect. This was called a "nonequivalent dependent variable design" and designated with letter</page><page sequence="12">152 THOMAS D. COOK AND DONALD T. CAMPBELL subscripts. Finally, Cook and Campbell reasoned that some comparison groups reduce the degree of initial group nonequivalence, especially when individuals who receive a treatment are compared to, say, their own siblings rather than to strangers, or when the same grade levels in a particular school are analyzed, as compared to when the same grades are compared across different schools. The presumption is that siblings and same grade/same school cohorts share environmental and genetic environments that are less noncomparable in the aggregate than the environments shared by different families or by children from different schools. To indicate such "cohorts", Cook and Campbell used wavy lines between groups instead of the dotted lines signifying larger degrees of nonequivalence. Unique quasi-experimental designs can be created, not only by inventing novel structural features of experimental design, but simply by combining the already available features in novel ways. Knowing this, researchers can come to any task requiring causal inference and can explore a large number of different design options before deciding on one that seems most suitable in terms of the time and resources available and the degree of uncertainty reduction required. B. Criteria for Evaluating Research Designs In the quasi-experimental tradition, designs designated by a series of X's and O's are evaluated against ever growing lists and typologies of "threats to validity" or "plausible rival hypotheses". Initially (e.g., Campbell and Stanley, 1963) two validity types were distinguished: Threats to "internal validity", i.e., forces that can systematically bias inferences about causal connections; and threats to "external validity", or forces that can lead to spurious conclusions about the generality of causal effects. The individual threats were mostly identified from prior studies in which a particular source of systematic bias had been postulated to explain the data. In this sense the threats constitute empirical products whose relevance to a particular study depends on the details of that study. The lists of validity threats were never intended to be exhaustive, and many additions have been made over the years. Threats to "internal validity" are confoundings with other agents that, potentially, could cause changes in outcome measures and be mistaken for effects of the implemented treatment. The goal of</page><page sequence="13">CAUSAL ASSUMPTIONS 153 quasi-experimental design is to create structures of observation and treatment implementation that rule out such threats, which initially Campbell and Stanley (1966) listed as: 1. History. This refers to specific events occurring contemporaneously with the treatment. 2. Maturation. Processes within respondents that operate as a function of the passage of time per se, including growing older, hungrier, or more tired. 3. Testing. The effects of taking a test upon the scores obtained in a later testing. 4. Instrumentation. Changes in the calibration of a measuring instru ment irrespective of whether the instrument be mechanical or human. 5. Statistical regression operates where persons (or other units) have been directly or indirectly selected for treatment on the basis of extreme scores at a specific time period. This bias operates where matching has been employed in a misguided effort to make com parison groups more equivalent. 6. Selection biases result from the average composition of the re spondents being different in each of the treatment groups being compared. 7. Differential mortality results from differences between comparison groups in the rate and nature of respondents lost from the measurement framework of the study, thus creating a selection bias. 8. Selection-maturation occurs when different groups are spon taneously changing at different rates over time. (This is only one of the many ways in which selection may interact with the other threats to internal validity to produce spurious causal inferences.) The threats to "external validity" are factors that might limit the generalizability of causal relationships, making them specific to parti cular settings, kinds of people, or historical time periods. Such threats</page><page sequence="14">154 THOMAS D. COOK AND DONALD T. CAMPBELL specify some of the contingencies on which a causal relationship might depend. Since all analyses of causation require that a presumed cause and effect covary, Cook and Campbell (1979) took the statistical criteria by which covariation is usually inferred in the quantitative social sciences and identified seven ways in which incorrect conclusions can be drawn about such covariation, largely because of the incorrect use of statistics or the low power of statistical tests. Although the list deals with quantitative assessments of concomitance, it could easily be made relevant to the sources of systematic bias associated with more qualita tive assessments of convariation (cf., Crocker, 1981). Cook and Campbell (1979) also noted that while internal validity is concerned with whether the change obtained in a treatment group would have occurred in the absence of the treatment, it makes no reference to how the treatment and effect operations should be labelled in theoretical terms. So, Cook and Campbell developed a fourth list, this time of ten threats to the construct validity of causes and effects. As with the threats to internal validity, the threats listed for these other kinds of validity are products of practice, and reflect sources of bias identified from puzzling data, the self-critical commentary of individual re searchers, and peer review in journals and elsewhere. Campbell and Stanley (1966) used their threats to internal and external validity to evaluate a number of quasi-experimental designs, producing checklists of the threats generally ruled out by each of the structural designs they analyzed. These checklists allowed readers to see which designs are preferable because they usually allow researchers to rule out more threats. In general, designs were stronger for internal validity (and hence causal) purposes if a pre-test measure was available; if there were a long string of pre-treatment observations rather than a single one; if there were a no-treatment control group; if the controls were selected so as to minimize group noncomparability; and if the treatment was administered on more than one occasion to more than one group, with the occasions being free from any systematic relation to factors that might influence responding (e.g., particular days or seasons of the year). Although a pedagogic aid, such checklists are dangerous if they lull readers into believing (1) that because a particular design usually rules out a particular threat, it always do so; or (2) that the only threats needing critical analysis are on Campbell's lists. All threats are ten</page><page sequence="15">CAUSAL ASSUMPTIONS 155 tative and context-dependent. Experience can modify them, and new threats can be added. Moreover, each threat is couched so abstractly that the forces it describes can be manifest in many ways, some novel. Thus, Campbell and Boruch (1975) discussed the problems arising in simple nonequivalent group designs when a measure differs in reli ability between pre- and post-treatment measurement waves. While the spurious causal conclusions that can arise in this situation are subsumed under "instrumentation" (that is, the measuring tool has changed), this particular form of instrumentation was new at the time. Ultimately, it is researchers' and their critics' understanding of the subject matter under study at that time which determine whether a threat should be treated as plausible. While the structure of a quasi-experimental design is highly relevant to such judgments, it does not totally predict all the threats needing consideration in a particular research project. An Example from Quasi-Experimental Practice A. Evaluations of "Sesame Street" Ball and Bogatz (1970) evaluated the educational television series for preschoolers called "Sesame Street". Cook and his associates (Cook et al., 1975) have made an extensive reanalysis of their data, and the present analysis draws upon selected aspects of both reports. Had Ball and Bogatz just taken a single group of children, measured their knowledge, exposed them to the show, and measured again, (i.e., Ox X 02), the resulting data would have been uninterpretable causally. If scores had increased, it would not have been clear whether this was due to the television series or to (1) the spontaneous maturation of children; (2) historical events that occurred between the two testings; (3) prior exposure to the test items; (4) statistical regression because children were deliberately selected for further study if they scored low at the first test; or (5) selection due to the post-treatment group differing in composition from the pre-treatment group because some children dropped out of the study. Hence, the first mean was com puted using all children while the second was based on those who remained. Mindful of these difficulties, Ball and Bogatz conducted a ran domized experiment in which the treatment contrast consisted of children being encouraged or not-encouraged to watch the series on a regular basis, with the encouragement coming from research staff</page><page sequence="16">156 THOMAS D. COOK AND DONALD T. CAMPBELL members who left toys and books in the home in the hope of reminding children and adults about the show. But "Sesame Street" was so popular that many children in the nonencouraged control group viewed the show and the original research plan broke down. So, the researchers used measures of the amount children had watched the series to create four ex post facto groups of children who differed in the amount of reported viewing. Since both pre- and post-vie wing knowledge scores were available for all the children, the resulting design is as described below, with the X subscripts how much "Sesame Street" was reportedly watched: Ox Xx o2 ?XX2?2 Ox X3 ?2 O?X4O2 Data analysis showed that larger learning gains were associated with heavier viewing, suggesting an effect of "Sesame Street". Simple maturation is probably not a viable alternative interpretation in this case, since all the children presumably matured over the course of the study. Nor do simple historical events constitute a viable alternative, since they should have affected each group of viewers equally. Prior testing would not be a problem since all the children were pre-tested. Nor does simple regression present an inferential difficulty because the children were not assigned to groups by virtue of their pre-treatment scores. Finally, selection can be ruled out as an explanation of the differences since the data analysis was restricted to children who provided both pre- and post-treatment data. (This, of course, un avoidably restricted external validity.) However, more complex threats to internal validity bear closer scrutiny. The pre-treatment means reveal that children who sub sequently watched more "Sesame Street' were more knowledgable to start with, and descriptive research has consistently shown that children who are initially more knowledgable increase their knowledge at a faster rate. Thus, the differential gains may be due, not to "Sesame Street", but to the nonequivalent viewing groups maturing at different rates - i.e., a selection-maturation interaction. The possibility also exists that, due to differential home environments, heavier viewers attended kindergarten in greater numbers, or attended kindergartens</page><page sequence="17">CAUSAL ASSUMPTIONS 157 emphasizing pre-literacy and pre-numeracy skills. This suggests the potential problem of a selection-history interaction, for the different viewing groups could have been experiencing unique theory-irrele vant treatments classified under the threat of "history" and likely to effect learning gains. Finally, the testors may have known which children had viewed more heavily, and this may have led them to treat the children in each viewing group differently, more so after the treatment than before, creating an interaction of selection and in strumentation. Note what adding the comparison groups of lighter viewers has done. The viable alternative interpretations are no longer general ones like testing, selection, regression, and the like. They are now more complex and particularistic, involving statistical interactions with some aspect of selection. If the quasi-experimental design could not be made initially stronger to deal with the threats above, the analyst would have three other recourses. One would be to collect additional data to check on whether these threats did indeed operate. While selection-maturation has been documented so many times in education that it is a priori plausible, it can be further checked by examining pre-treatment data to probe whether age differences in knowledge are greater for children who would eventually go on to be heavier viewers than for those who would become lighter viewers or nonviewers. For the selection-history alter native one would consult other available data sets to probe whether the viewing groups differed in kindergarten attendance or in the type of kindergarten attended. And for the selection-instrumentation threat one would need at a minimum to check carefully to find out if the testers knew of a child's viewing score before the final knowledge testing took place. The second recourse would be to conduct specialized analyses of the original data. Among other ways, the selection-maturation threat might be examined by conducting an analysis of covariance with pre-test scores as the covariate. The simplest form of this analysis assumes, however, (1) that any differences in expected growth between the four viewing groups mirror any differences in growth obtained between children who view different amounts even within the narrow range of a single viewing group; (2) that the pre-test is the sole pre-treatment variable responsible for selection differences; (3) that the pre-test is measured with perfect reliability; and (4) that the observed differences at the post-test are not due to factors other than</page><page sequence="18">158 THOMAS D. COOK AND DONALD T. CAMPBELL selection-maturation (i.e., to selection-history or selection-instrumen tation). The strategy of conceptualizing, measuring, and statistically adjusting for confounds always requires that multiple assumptions be accepted before the analyst's purported conclusions can be tentatively accepted. Hence, multiple analyses are often desirable that make quite different, plausible assumptions about theoretical irrelevancies. Employing any of these strategies makes recourse to arguments about plausibility. Plausibility is inherently "squishy," for different commentators have their own standards and little ingenuity is usually required to develop alternative interpretations to any finding. Most will not be plausible, however, in light of what is already known. Thus, it is possible to argue that the heavier viewers of "Sesame Street" knew more because certain rays emanate from television sets that stimulate the brain the more frequently viewing occurs. While this belief could be probed empirically, few of us would want to. We know of no such ray; we know that viewing television is negatively related to knowledge in older children; and we know of no anecdotal remarks from the earliest days of television which claim that young children suddenly became smarter. Since plausibility is such a necessary but "squishy" concept, quasi-experimentalists are enjoined both to be self-critical and to let their work be critically analysed from many different theory and value perspectives. This is both to force out more alternative interpretations and to estimate the plausibility of those already identified. Once plausible alternatives have surfaced, multiple options some times exist for dealing with them. To deal with selection-maturation interaction, Ball and Bogatz (1970) conducted the type of analysis of covariance mentioned earlier, the efficacy of which depends on a number of untestable assumptions. So, for other analyses they crafted a revised quasi-experimental design that took advantage of the fact that some children's ages ranged between 53 and 58 months at the post-test, while others were in the same age range at the pre-test. The first of these groups was called the post-test cohort and the second the pre-test cohort. Considering just the post-test scores of the post-test cohort (then aged 53-58 months) and just the pre-test scores of the pre-test cohort (then also aged 53-58 months), the revised design is: xo2 With this design, maturation cannot easily explain differences be</page><page sequence="19">CAUSAL ASSUMPTIONS 159 tween the two means because the cohort groups are constructed to be of equivalent age and so are presumably at comparable maturational stages. A selection effect is also not likely, since the analysis is based on data from all the available children in each age cohort who provided both pre-test and post-test data. But as a check, one could probe whether the two cohorts differed on measured background variables. However, this simple version of a cohort design is vulnerable to the alternative interpretations we have classified above under "history". Something that affects learning might have occurred when "Sesame Street" was introduced that influenced only the younger post-test cohort because the older pre-test cohort already knew the information presented. Alternatively, the older cohorts may have experienced some unique event at a particularly sensitive maturational stage that was less meaningful for their younger cohorts who did not learn as much from it. Also, the design has a testing problem, since the scores of the pre-test cohort came from a first pre-test measurement wave while the scores of the post-test cohort were from a second wave with the first wave of data going unanalyzed. Therefore, it would not be clear whether any obtained difference between cohorts were due to the treatment or to differences in the frequency of measurement. The problems of history and testing can be eliminated by following Ball and Bogatz's own extension to their basic design. They used viewing measures to partition each cohort into four separate groups that differed in the reported frequency of watching "Sesame Street". Statistical analysis showed that differences in knowledge were greater among the various viewing groups within the post-test cohort than among the pre-test one. A construction of the results is in Figure 1. Since the cohorts were of the same mean age, of comparable social background within each of the constructed viewing groups, and had experienced the same history and testing sequences (all post-test cohorts were pre-tested), the outcome in Figure 1 can account for all the threats to internal validity that we have discussed thus far, except one. This is that the testers knew how much the children had viewed and behaved differently with the heavier viewers, perhaps eliciting less test anxiety. Such a threat would influence only post-test scores and create the obtained achievement results - larger differences between viewing groups in the post-test than the pre-test cohort. Several of the analyses reported above used measures of the amount "Sesame Street" was watched. Four measures of viewing were obtained, either from the children or their parents. They were to check on the</page><page sequence="20">160 THOMAS D. COOK AND DONALD T. CAMPBELL I_I_ Pre-test Post-test cohorts cohorts Fig. 1. Interpretable outcome of a selection cohorts design with pre-test and post-test cohorts. implementation of the original encouragement treatment and to reclassify children into viewing categories based on the amount they reportedly viewed rather than the amount they were supposed to view. While quasi-experimentalists welcome measures used for both of these purposes, they become apprehensive if the measures of treatment implementation are collected from the persons providing outcome data at a time that is closer to one measurement wave than another. The preference for independent measures collected equidistant from the pre-test and post-test is on the grounds that theoretical irrelevancies in the measurement process are more likely to be correlated with outcome measures the closer together measures of viewing and outcome are taken and the more those who know about viewing can influence how knowledge is measured. Although Ball and Bogatz commendably had four viewing measures, those collected from parents did not apply to all the sample and so can not be used in many analyses, while the most comprehensively collected measure from children used a format that required children to recognize "Sesame Street" characters. Since a recognition format was also used for measuring knowledge, it seems reasonable to assume that a recognition measure of viewing might be just another measure of the knowledge induced by "Sesame Street", spuriously affecting the correlation between viewing and post-test knowledge when compared to the correlation between viewing and</page><page sequence="21">CAUGAL ASSUMPTIONS 161 pre-test knowledge. Fortunately, data analyses using non-recognition viewing measures indicated the same direction of effects as the recognition measure collected from the total sample. This cor respondence suggests that a putative recognition bias did not spuriously cause a viewing effect though it may have inflated the magnitude of the estimate obtained. B. Effects of Participative Decision-Making Figure 2 gives interrupted time-series data from a quasi-experiment by Lawler and Hackman (1969) where the threats to internal validity are somewhat different from the previous example. The treatment was the introduction of a participative decision-making scheme to three groups of men doing janitorial work at night. The dependent variable was absenteeism, the proportion of possible work hours actually worked. Even though its results were ambiguous, we present this as an example of quasi-experimental practice because it describes how quasi experimentalists operate and indicates how critical are processes for forcing out alternative plausible interpretations to those that come to mind once the data have been first examined. The data from Lawler and Hackman are presented in Figure 2. Consider only the treated group. A noteworthy feature is the false With bonus Without bonus 12 1110 9 6 7 6 5 4 3 2 1 1 2 3 4 5 6 7 8 9 10 11121314 1516 Before Weeks After Fig. 2. Percent of scheduled hours worked for janitors with and without the new bonus scheme..</page><page sequence="22">162 THOMAS D. COOK AND DONALD T. CAMPBELL conclusion we might have drawn with a single pre-test and a single post-test observation point. The total pre-test data reveal that the measure immediately prior to the treatment has an atypically low value so that statistical regression will almost certainly be inflating the mean difference between it and the first post-test measure. Had all the pre-treatment percentages been consistently low, statistical regression would not of course have been a problem. A strength of time-series designs is that they allow assessment of the pre-test time trend, thereby permitting a check on the plausibility of a number of alter native explanations, including statistical regression. The major threat to internal validity with most realizations of a simple interrupted time-series design is history - the possibility that forces other than the treatment exercised an unusual influence in the period between when the treatment was introduced and when the first post-test measure was made. In the present example, a police drive may have occurred to clean the local streets of criminals who hang about in the late evening, or the janitors may have begun a car pool. Several controls for history are possible, perhaps the best being to add a no-treatment control group. But, though advisable, this is not always necessary. Lawler and Hackman's unobtrusive measure of absenteeism was calibrated into weekly intervals, and the historical events that can explain an apparent treatment effect are fewer with weekly than with monthly or yearly intervals. Also, if records are kept of all plausible, already identified effect-causing events that could influence respon dents during a quasi-experiment, it should ibe possible to ascertain whether any of them operated between the last pre-test and the first post-test. Another threat is instrumentation. A change in administrative pro cedures will sometimes lead to a change in the way records are kept. In particular, persons who want to make their performance look good can simply change bookkeeping procedures to redefine performance or satisfaction. Alternatively, persons with a mandate to change an organization may interpret this to include reforming the way records are kept or criteria of success and failure are defined. When Orlando Wilson took charge of the Chicago police, he redefined how crimes should be classified, appearing to cause an increase in crime that was really a manifestation of changes in record-keeping (Campbell, 1969b). In time-series work it is dangerous to assume that a constant definition undergirds a series of numbers.</page><page sequence="23">CAUSAL ASSUMPTIONS 163 Another threat with simple interrupted time-series designs is the selection that occurs when the composition of a group changes abruptly at the time of intervention. Usually, this will be because the treatment causes attrition from the measurement framework, making it difficult without further analyses to disentangle whether an interruption in the series is due to the treatment or to different persons being in the pre-treatment when compared to the post-treatment series. When available, the simplest solution to this problem is to restrict at least one of the data analyses to the subset of units for which measures exist at each time period. If such repeated measurement is not possible, the background characteristics of units can be analyzed to ascertain whether a sharp discontinuity in unit profile occurred when the treat ment was introduced. If no such discontinuity is found, then a shift in selection is not likely to be a problem. Time series are subject to many influences of a cyclical nature, including seasonal varition in performance, attributes, or com munication patterns. Such "influences" (classified, to avoid multiplying categories, under "maturation") can inadvertently masquerade as treatment effects. It is not clear at what point in the year the Lawler and Hackman quasi-experiment was conducted or how the performance of janitors is normally related to temporal cycles. Just suppose, however, that the study began in December when the weather is cold, illness more prevalent, and absenteeism higher. Twelve weeks later, when the treatment was implemented, it would be March or April when the weather is better, health improved, and absenteeism lower. Under these conditions the decrease the authors obtained could be due to a recurrent seasonal change in absenteeism rather than to the novel decision making scheme under analysis. Since controlling for cyclicity obviously requires estimating what the cyclical pattern is, a long time series is required and the data should be displayed with cyclical variations statistically removed. Lawler and Hackman (1969) actually incorporated an untreated control group into their experiment since another set of work groups received the same bonus as the janitors without participating in any decisions about setting their own rates. The relevant data from this control group are also in Figure 2 along with the data from the experimental group. Visual inspection shows that for 7 of 12 pre-test observations the exp?rimentais outperform the controls, while after wards they do so 16 times out of 16. This suggests that absenteeism may</page><page sequence="24">164 THOMAS D. COOK AND DONALD T. CAMPBELL have declined more in the group with participative decision making than in the group without it. But the inference is not at all clear due to the possibility that attendance was slightly on the increase in the experimental group even before the new bonus scheme was introduced. In "close calls" like this one it would be useful to have a statistical test to probe whether the post-treatment time trends in each group are merely continuations of prior different time trends. However, given the current state of the social sciences, data such as Hackman and Lawler assembled are of value even if the analysis is left at the level of graphic presentation and discursive discussion, such as sampled here. While some statistical analyses could provide valuable supplements, they would not, in our judgment, eliminate the need for graphic presentation and the discursive discussion of threats to validity. Statistical analyses, as a rule, cannot help in evaluating the threats discussed for this case, and most of the older statistical usages are usually wrong. Thus, comparing linear fits to the before and after intervention segments leads to spurious findings if the true underlying trends are curvilinear, and extrapolating higher-order polynomial fits for comparisons turns out to be irresponsible. Moreover, the effective degrees of freedom in time series are reduced by the autocorrelation of adjacent points, which are difficult to estimate, and the regression approaches employed in economics too often implicitly and wrongly assume that the pre intervention values (independent variables) have been measured without error. We judge that the Box and Tiao (1975; McCleary and Hay, 1980) "transfer function" variant oti Box's general approach to time series analyses lacks these weaknesses, but that it is still apt to produce distorted results where the selection of those treated has been triggered by an extreme value just prior to the intervention. While a transfer function analysis has not been applied to the Lawler and Hackman data, the shortness of the series and their instability make it very unlikely that a statistically reliable effect would be found. We would be left with a statistically uncorroborated effect for which there is only tantalyzing visual support. Ontological and Epistemological Assumptions A. Of Grandiose and Modest Conceptions of Cause We want now to explore some of the assumptions made in quasi experimentation and hinted at in the more descriptive presentation</page><page sequence="25">CAUSAL ASSUMPTIONS 165 above. Quasi-experimentation is an evolving entity with indistinct boundaries, and its practitioners do not all make the same assumptions about ontology and epistemology. This makes it difficult to decide where to look to uncover assumptions. But in general, writings on quasi-experimentation appear to make the realist assumption that a world exists independently of human knowers, even though obser vations of this world inevitably co-mingle the perceptions and ap perceptions of individual observers. The real world we fallibly per ceive is assumed to consist of discrete objects and of forces that relate some of these objects to others. While the ultimate task of science might be to provide a complete prediction or explanation of these objects and forces, quasi-experimentalists do not expect to attain such knowledge soon. This is partly because of the low quality of current social science theories and methods; partly because of the difficulties inherent in conceptualizing social and cognitive objects and in dealing with consciousness and intention as causal forces; and partly because of the belief that society and people are ordered more like multiple pretzels of great complexity than like any structure captured by a parsimonious mathematical formulas such as e = mc2. Most social scientists assume that the world of complex, multivariate, particularis tic causal interdependencies they seek to understand is ordered in probabilistic rather than deterministic fashion (Suppes, 1970), making it impossible in principle to predict particular behavior in particular units at particular times. A final assumption most quasi-experimen talists make is that the social world is overdetermined, creating a perennial need to separate out the set of conditions responsible for a particular outcome from the many other conditions that might plausi bly have brought it about. Quasi-experimentalists do not assume that all parts of this external world are equally important. They place little emphasis, for example, on the forces that maintain steady states within normal bounds. Their interest is in describing the consequences of a particular form of disturbance which they try to achieve by assessing any impacts it might have had over and above the fluctuations that would have occurred had it not operated. Although quasi-experimental methods have been used to study many types of disturbance, they are most plausibly and most often used when the disturbances have a sudden and known time of onset, when the delay period required for a particular effect is either known or "short", and when the disturbance can be deliberately intro duced at the experimenter's will. Since effects of gradual or long</page><page sequence="26">166 THOMAS D. COOK AND DONALD T. CAMPBELL delayed onset turn out to be almost undetectible in the nonlaboratory world of multiple change agents, quasi-experimental research em phasizes the study of manipulable social forces rather than the study of nonmanipulanda (although natural disasters that are precisely timed, and whose occurrence is not plausibly a symptom of prior conditions may be so analyzible (Cook et al., 1977)). To use Mackie's (1974) language, quasi-experimentation seeks to identify, in the context of a particular research setting, whether a manipulated variable is a neces sary but insufficient part of a sufficient condition that is itself not necessary for producing a particular outcome. Alternatives exist to such a conception. The traditional emphasis is to search for the complete cause as constituted by factors that are both necessary and sufficient for an effect and so make it inevitable. This conception requires considering molar causes and effects as inexact constructs and trying to identify (1) the causally efficacious components of a treatment (where applicable), (2) the causally impacted com ponents of an effect; and (3) the micro-mediating processes that unfold after the causally efficacious component of the molar cause has varied and before observing any change in the causally impacted component of the more molar effect. In extreme form, such "essentialist" theories of causation require simultaneous causation, and lead to a blurring of the conceptual distinction between a particular cause and effect. Alternatively, they require knowledge of the generative processes inherent in objects and of the circumstances that initiate these (Harre and Madden, 1975). A flavor of essentialism is captured in J. S. Mill who wrote that a cause is the sum total of the conditions, positive and negative taking together; the whole of the contingencies of every description, which being realized, the consequent inevitably follows. (1906, p. 222) Positivist conceptions of causation are like essentialist theories in requiring total prediction; but they differ in attempting to avoid reference to unobserved or imperfactly observed causes. Positivism equates causation with some antecedent state of the total system, which provides the initial conditions' for the computation, by means of a theory, of the later state to be explained. (Hempel, 1965, pp. 351-2) The crucial feature here is a mathematical function that fully predicts transitions from one state to another and that predicts later from earlier</page><page sequence="27">CAUSAL ASSUMPTIONS 167 states just as well as it predicts earlier from later states. Unlike positivists, quasi-experimentalists believe it is necessary to postulate hypothetical causes and believe them to be so "irreducible" (Scriven, 1966, 1971) that the language of observables can never capture some of the prototypic components of the causal construct. In particular, quasi-experimentalists regret the positivists' surrender of causal asymmetry, for this loses the notion that causes can be manipulated to make something desirable happen later. Quasi-experimentalists believe that in emphasizing the mathematical functions that predict how conjoint events are related, positivists "throw the baby out with the bathwater". Essentialists and positivists both aspire to statements about causation that transcend the unique contextual arrangements of a particular study. Quasi-experimentalists also aspire to such generality, as is evident in their concern with threats to construct and external validity, by their recommendations to design studies so as to sample multiple populations, settings, constructs, and construct definitions, and by their growing interest in meta-analysis and other forms of synthesiz ing the results of multiple related studies (Cook and Levit?n, 1980; Light and Pillemer, 1984). But these developments notwithstanding, quasi-experimentalists discuss internal validity more often than the other types of validity; and internal validity is explicitly defined in terms of the particular representations of a possible cause and effect in a particular study. Since no cognizance is taken of how these variables are labelled in theoretical terms or of the populations of people, settings, and times to which a causal conclusion might be limited, internal validity clearly places priority on singular and not generalized causal relations. This problem of generalization is perhaps less severe for quasi experimental research than for experiments based on randomized assignment in laboratory settings. This is because quasi-experimental research is usually done in settings and with populations typical of those to which generalization is desired. However, generalization in quasi-experimental research differs radically from the pure single variable strategy epitomized in experimental science based on labora tory control and isolation where universal principles free of context and interaction effects are sought and often apparently approximated. The causal complexity of treatments, measures, settings of research, and settings of application/generalization are forced upon quasi-experi</page><page sequence="28">168 THOMAS D. COOK AND DONALD T. CAMPBELL mentalists by their research and the causal density of social contexts, making it clear that generalization is to be empirically explored, not assumed, and that generalization going beyond what has been obtained is a matter of theory or conjecture rather than logic. There are, to be sure, some settings in which formal statistical sampling procedures designed to facilitate generalization can be combined with experimental interventions, usually when generalization to a particular population of research sites or respondents is at issue. More typically, generalization is explored in a less formal fashion dependent on a patchwork of purposive replications that vary dimen sions suggested by theory, hunch, or urgency of need. Characteristic of the generalization achieved with quasi-experimentation (and with random-assignment experiments) are meta-analyses in which an average causal effect is computed for a particular treatment class across multiple available studies. The crucial assumptions here are that (1) the effect sizes from individual studies have not been influenced by a source of bias consistently operating in the same direction; (2) the studies sampled constitute a representative sample of all the studies that could be done on the research topic; and (3) it is useful to compute an index of average causal efficacy (i.e., a general causal statement) across a large number of forces, many of which may condition the magnitude of a causal connection, with some perhaps even influencing its direction. Quasi-experimental theorists do indeed devote more space to generalizing across moderator variables than to detecting how they might statistically interact with E to influence the magnitude of effect on C This reflects their belief in the greater utility of identifying robust main effects rather than describing the statistical interactions out of which explanatory "pretzels" might one day be built. Besides formal sampling and haphazard generalization, a third stra tegy for generalization is to assume the generality of the causal connections established in a study and to revise them only if future probes suggest such a need. Such probes are better if based on explicit conjectures about the conditions under which a causal connection does and does not hold and if experimental arrangements are created that correspond to these conditions. Quasi-experimentalists would label as general all relationships that persist despite such deliberate "falsification" attempts even while remaining ready to correct this knowledge claim if later research requires this. Given his real (but not</page><page sequence="29">CAUSAL ASSUMPTIONS 169 uncritical) debt to Popper, Campbell (e.g., 1969a) in particular has written of causal generality in this last mode that has to be practiced with awareness of Quine-Duhem equivocality. The conception of causation we have just sketched is predicated on the identification of manipulable, active causal agents, although it is admitted that these do not constitute the universe of all causal agents. It is also predicated on the possibility, and utility, of discovering causal connections that occur rapidly or with a suspected delay, that are dependable but not inevitable, singular rather than general, and coarse grained in the specification of moderator variables rather than fine grained. This conception is very modest when compared to the more complete conceptions of causation based on some form of essentialism or positivism developed to describe practice in physics rather than the social sciences. B. Justifying the Modesty If the conception of causation is so modest and theorists of quasi experimentation hold other conceptions as ideals (particularly some version of essentialism), why do they persist in trying to improve methods based on a theory of causation acknowledged to be so limited? One reason has to do with potential utility. The identification of manipulable causal agents should help develop a body of research findings that can be used to effect outcomes of presumed value. Other conceptions of cause do not have as direct implications for practical action. Little can be done at present about many of the nonmanipulable causal agents (e.g., biological sex or age), and conceptions based on predictions are likely to confuse causation and correlation so that we may eventually have to deal with any mischievous consequences arising from false knowledge about what will change what. A second reason for preferring such a modest conception of cause is that, since the work of Collingwood (1940) and Gasking (1955), a conception in terms of manipulability, activity, or recipes has been regularly defended not only on pragmatic grounds but also because it is commensurate with one major understanding of causation in ordinary language where we denote causation in terms of manipulating X in order to bring about Y. [For similar discussions see Mackie (1972), Suppes (1970), Whitlock (1977), and vonWright (1971).] Current interest in evolutionary epistemology (e.g., Campbell, 1966, 1977)</page><page sequence="30">170 THOMAS D. COOK AND DONALD T. CAMPBELL also reinforces the notion of manipulation as the protypical component of causation, postulating that a predisposition to infer causal con nections may be part of our biological inheritance, an ex posteriori product of evolution for our species although a priori for all humans now alive. The crucial assumption is that survival in our space-time environment has long been promoted by inferring regularities and by operating on the environment, leading to selection for the ability to classify the things we can do that often make other things happen. For such a heritage may follow the deeply ingrained psychological ten dency among humans to "see" causal relationships (Michotte, 1944), even when we watch inanimate objects moving on a screen (Heider, 1958). Hacking (1984) has made a third claim for the manipulability theory of causation, asserting that it is not possible to attribute entitativity to nonnatural objects until they can be manipulated so as to bring about something else. He takes his examples from particle physics where, from an anthropomorphic perspective based on our space-time notions, material objects with defensible boundaries do not exist even - he asserts - in principle. Yet, physicists talk about such things as entities. Hacking thinks this is because the status of some hypothesized causal agent changes for physicists when they have made a tool out of it that can be dependably manipulated to make something else happen. Only then do they assume they have something real. This materialist theory of entitativity is obviously harsh on concepts like the astronomer's black holes that can never be manipulated; and it might not be too easy on many of the complex treatment packages with which quasi-experimen talists work. They usually consist of nonnatural combinations of natural elements, and can be so poorly understood that their consequences are unstable from one application to another. Hacking might argue that such irreproducibility provides prima facie evidence that such non natural packages of forces do not yet constitute definable entities. However, that is not our major point right now. Instead, we want to stress how the manipulability theory of causation has been used in epistemology to infer entities as well as to justify the search for pragmatically useful relationships that correspond not only with a major meaning of causation in ordinary language but also perhaps with one of the cognitive products of human evolution. A fourth justification for preferring such a modest conception of cause is that manipulation has a long history in epistemology as a means</page><page sequence="31">CAUSAL ASSUMPTIONS 171 for improving causal inferences. J. S. Mill wrote: It thus appears that in the study of various kinds of phenomena which we can, by a voluntary agency, modify or control, we can in general satisfy the requirements of the Method of Difference, but that by the spontaneous operation of nature these requisitions are seldom fulfilled, (cited in Boring 1922) Control over a treatment allows the researcher to introduce it at a known time, facilitating decisions about temporal precedence. Moreover, the treatment need not be introduced to all units at once, so that a counterfactual no-cause baseline can be created in comparison units that can then be observed over the course of a study. Manipulat ing a treatment helps researchers meet the major requirements of Mill's canons. The manipulability conception of cause is also consonant with related thinking that accords a special status to abnormal interventions into normal processes. Hanson (1955) talks of there being (at least) two varieties of cause: (1) that without which some given effect (usually a breach of routine) will not occur, and (2) that delicate cluster of circumstances all of which conspire to bring about some effect (usually quite routine). Dietle (1979) makes a similar point arguing that it usually makes little sense to answer "Oxygen" when asked what caused a fire. Oxygen is always around, a fixture of the steady background state. We seek an answer in terms of an "interference" with this background state such as matches, lightning or an electrical short-circuit. However, in an atmospherically controlled laboratory from which oxygen had sup posedly been excluded, it would make sense to answer "Oxygen" when the fire question is posed, for oxygen is abnormal in such a setting and is a known cause of fire in Mackie's INUS sense. Quasi-experimentalists study deliberate disturbances of routines, with routines understood as a complex of interacting objects and forces that cause an outcome to vary in apparently haphazard fashion within indeterminate limits. Such objects and forces are always present in the outdoor studies quasi-experimentalists conduct, so that deliberately manipulated variables have to cause a change that is distinct from the routine changes in the background. Usually, we look for changes of greater magnitude than would normally occur, but sometimes we look for changes that constitute qualitatively unique breaches that few alternatives could have brought about. Detectives use this last prin ciple to attribute crimes to specific perpetrators based on what they</page><page sequence="32">172 THOMAS D. COOK AND DONALD T. CAMPBELL know about their modus operandi; pathologists and special educators do the same when they attribute tissue abnormalities or speech defects to the known modus operandi of viruses or organic brain disorders. Scriven (1976) has discussed this modus operandi approach to causal inference, and Cook and Campbell discuss something similar as "signed causes", i.e., they leave their signature behind as unique breaches in some routine from the past. We must be careful not to claim too much for the manipulability theory of causation and its variants. Critiques of the recipe concept reveal its generic weaknesses (e.g., Aronson, 1971; Haack, 1967; Hanson, 1955; Rosenberg, 1972). First, the theory is anthropomorphic, assuming that no causation of importance existed before humans or exists today in substantive areas where manipulation is impossible. Second, from the standpoint of theories of complete causation, the manipulability theory provides manifestly incomplete knowledge of how and why relationships occur. This incompleteness leads to prob lems in transferring to new settings knowledge about manipulanda achieved in other settings. Third, the manipulability theory is tau tological because to use manipulations to infer cause presupposes a theory of cause. Fourth, the theory is not explicitly sensitive to multiple perspectives on causal origin, as when a driver attributes a car crash to driving too quickly, a highway engineer to the road's camber, and an automobile engineer to the car's suspension. Fifth, the recipe version of manipulability downplays trial-and-error as a source of causal knowledge, although other versions of the theory emphasize it and see it reflected in the very origins of the verb "to manipulate". Sixth, for social scientists the recipe formulation may be too stringent. Most recipes specify multiple ingredients, particular quantities, a preordained order of mixing, and preordained oven temperatures and cooking times. While some deviations from the specifications can be tolerated, they are not without limits. Exceed any one, and the final dish may be inedible. Given the current poverty of most social science theories, our recipes will have to be simpler than what we find in cookbooks, specifying a smaller number of variables that have to be "somehow" combined to "sometimes" produce a desired effect. C. The Creaky Foundations Theorists of quasi-experimentation assume that science needs obser vation and that all observations are theory-laden. This was recognized</page><page sequence="33">CAUSAL ASSUMPTIONS 173 long before Hanson (1955) and Kuhn (1962, 1970), and is inherent in a multiple operationalist measurement that requires collecting data using more than one measure of a construct, each of which contains unique sources of both relevant and irrelevant variance. Multiplism is not pursued for its own sake; rather a critical multiplism is espoused (Cook, 1985) in which researchers and their critics attempt to uncover any inadvertent communalities of bias in the definitions and measures available. The aim is to imbue observation with many latent theories and not just a single one, and to test whether similar readings are made despite the different theories inevitably represented in any one method of observation. Quasi-experimentalists assume that new theories supercede the old and that, in so doing, will have to reuse the stubborn, dependable causal connections on which earlier theories were built. It is just such connections that they hope eventually to identify, treasuring as "fact-like" those cause-effect relationships that are repeatedly suggested across multiple methods and multiple investigators with different theoretical predilections. The theory of quasi-experimentation leans heavily on Popper's (1935) falsification for guidance in probing theories, even though the alternatives to be ruled out - the threats to validity - are more mundane than the grand theory clashes about which Popper writes. However, falsification is not without limitations, especially as concerns the construction of ad hoc hypotheses to deal with disconfirming obser vations. The low level of specificity in nearly all social science theories, and the low sensitivity of most measures, make it simple to construct such hypotheses and provide one reason why plausibility has to play such a large role in quasi-experimentation. Moreover, in many in stances of social science research disconfirmation requires accepting the null hypothesis that a cause and effect are not related. But as statistics texts repeatedly warn, accepting the null hypothesis can never be logically justified. Thus, social scientists hear some logicians telling them in one ear that they can only know what is false, and they hear other logicians and all statisticians telling them in the other ear that they can never know what is false! Given this situation, one can readily understand why more social scientists are insisting that theory probes should be based, not on statistical significance tests, but on magnitude estimates (e.g., Meehl, 1978; Cronbach, 1982). The refutation of alternatives is even more difficult in the quasi experimental context. Most experimentation uses Mill's Method of Difference to justify control groups. When the latter are created at</page><page sequence="34">174 THOMAS D. COOK AND DONALD T. CAMPBELL random, a counterfactual is created based on an observed world that validly represents what would be the case if the world in the treatment group stayed the same except for the treatment. In the quasi-experi mental case, the observed world is only an approximation to what the world in a treatment group would have been without the treatment; and a multitude of possible alternative interpretations arise from this nonequivalence. They force quasi-experimentalists either to assume that a particular threat was implausible, or to measure the threat and show it did not occur, or to rule out the threat using some procedure of experimental design or statistical analysis. Note that some of these methods of refutation depend on measurement. This puts one face to face with another apparent paradox, for the lower the quality of measurement the more likely one is to "refute" a threat to validity and to conclude that it did not operate. Even when measurement is better and is used in statistical analyses to "control" for threats, the extent to which threats are controlled depends on numerous subsidiary assump tions. Among other things, these touch upon the reliability of measures and the appropriateness of a given statistical test in light of the substantive model of the world it assumes and the distribution of data it requires. Ruling out alternatives is an especially assumption-riddled process in quasi-experimental research where researchers have to compare worlds that differ in multiple ways, most unknown. Why do theorists of quasi-experimentation persist in their task in light of such weak foundations for knowledge growth? One part of the answer relates to the importance they attribute to their subject mat ter, which is often of immediate and pressing social relevance. A sec ond relates to the current impossibility of conducting research on these topics in the laboratory in ways that give confidence about the generalizability of results to other settings. A third relates to the impossibility of conducting randomized experiments on these topics outside of the laboratory. A fourth relates to the relative superiority of quasi-experiments over other forms of nonexperimental research aimed at justifying causal inferences outside of laboratory settings. The major alternative goes under a variety of names, all of which imply causal modeling, e.g., path analysis or structural equation modeling. Such methods are not as objectionable when nonmanipulable causal forces are studied, when nothing else is possible, or when they are used to probe the more micro-level processes that might have mediated a link between a molar cause and effect studied experimentally or quasi</page><page sequence="35">CAUSAL ASSUMPTIONS 175 experimentally. Indeed, when these methods are used and the principle elements of quasi-experimentation are also present, e.g., pre-tests, control groups, etc., careful structural equation modeling in a multiple operationalist mode that deliberately pits multiple plausible models against each other will provide a better data analysis than simple forms of multiple regression, including analysis of covariance. Our objection to causal modeling, shared by some philosophers acquainted with it (e.g., Ellett and Ericson, 1983; Hausman, 1983), is to when it is used as an alternative to deliberate manipulation. As we have outlined elsewhere (Campbell and Boruch 1975; Cook and Campbell, 1979, Chapter 7), causal modeling depends on even more subsidiary, untestable and plausible assumptions than preferred quasi-experimental designs, especially when no pre-test data are available on the same measures as used for a post-test and when the groups being compared differ in many ways presumptively related to the performance on the outcome measures (e.g., when comparing Catholic and public schools where the model of selection differences requires substantive knowl edge that is simply not yet available (see Murnane, Newstead and Ol sen, 1985)). Also, many advocates of causal modeling eschew realism of any form and acknowledge that their conclusions are dependent upon a model whose truth cannot be independently probed (e.g., Blalock, 1971; Blalock and Costner, 1972). In making statements about what might follow if a model were true, conclusions from causal modeling have a different ontological status than the singular truths about the real world to which quasi-experimentalists aspire. Causal modelers also operate from a different theory of causation, searching for the full causal model of a particular ultimate effect, according no special status to assessing the marginal impact of manipulable factors. It follows from this difference in implicit theory of causation that most causal models will be more multivariate than their quasi-experimental counterparts, that there will be a greater reliance on measurement rather than design, and that many more alternative models can be fit to the data with causal modeling, though not all of them will be plausible. It is in the number of unexamined alternatives that causal modeling and quasi-experimentation most differ, with quasi-experi mentalists trying to reduce the number through pre-tests and control groups whose performance is responsive to many of the theoretically irrelevant background causal forces. Without pre-tests and/or control groups, causal modelers have to make each of these forces explicit,</page><page sequence="36">176 THOMAS D. COOK AND DONALD T. CAMPBELL have to measure them validly, and then have to include them in causal models that are "true". Causal models have to insert more knowledge into research than quasi-experimenters typically think is warranted by what we already know with confidence. 5. CONCLUSIONS We speculate that philosophers see quasi-experimentation as an eclec tic collection of borrowings from their history. From the early posi tivists comes the notion that cause and concomitance are closely related, although quasi-experimentalists deny the Humean assertion that cause is psychological and not "real" in senses we do not yet fully understand; from J. S. Mill comes the use of the Methods of Agreement and of Difference; from Popper comes the emphasis on falsifying alternative theories, albeit in the mundane form of empirically derived and consensually validated threats to various forms of validity, especi ally internal validity; from Quine comes an exquisite sensitivity to the limitations of refutation and a sense of the necessity of constructing probes of Nature based upon critical trust in our past knowledge; from Collingwood, Gasking, Mackie, Hacking and others come major epistemological and pragmatic justifications for the emphasis on manipulable causal agents; and from evolutionary epistemology comes the hypothesis that humans may have evolved with a special sensitivity to objects that can be operated upon to make something else happen. All of these themes lead to a concept of causation like that embodied in the concept of experimentation. But the correspondence between experimentation and the manipu lability theory of cause depends on numerous other assumptions. Among the most important of causal relevance are these: causal assertions are meaningful at the molar level even when the ultimate micromediation is not known; molar causal laws, because they are contingent on many other conditions and causal laws, are fallible and hence probabilistic; the effects in molar causal laws can be the result of multiple causes; effects follow causes in time, even though they may be instantaneous at the level of ultimate micromediation; some causal laws can be reversed, with cause and effect interchangeable; and a paradig matic assertion in causal relationships is that the manipulation of a cause will result in the manipulation of an effect. Since many of quasi-experimentation's borrowings involve assump</page><page sequence="37">CAUSAL ASSUMPTIONS 177 tions that have recently come under intense attack in the meta-science literature, it is useful to summarize how the practice of quasi-experi mentation has evolved to be partly responsive to some of these attacks. Relevant beliefs include: multiple operationalism decreases the de pendence of observations on single theories; critical interchange be tween persons with different theoretical and methodological values can reveal most plausible alternative hypotheses over and above those already incorporated into empirically derived but still corrigible lists of threats to validity; many quasi-experimental designs are potentially available which differ in the number of alternatives they usually leave unexamined; designs that usually leave a particular alternative un examined can be improved by expanding the sampling and measure ment framework so as to address these alternatives; all quasi-experi mental data analysis should be exploratory so as to probe causal hypotheses in ways that incorporate different plausible assumptions about any alternatives that cannot be directly examined; while quasi experimentation prioritizes on singular causation, the generality of causal connections can be probed using past studies and tests of robustness when there has been a heterogeneous sampling of settings, populations and times in individual studies; social science theory and other forms of relevant knowledge can guide the collection of data for use in causal models that complement quasi-experimental findings by decomposing molar treatments and exploring some of the micro mediating processes that might have occurred after a treatment varied and before its effects were observed. If we date quasi-experimentation from Campbell and Stanley (1963), it is only slightly more than one human generation old. This, plus the social science subject matter with which it deals, may help explain its somewhat primitive nature. But by the same temporal token, one might want to admire the progress made in so short a time, particularly since little of the research done by social scientists uses these methods - with randomized experiments being preferred by psychologists and non experimental methods by anthropologists, sociologists and political scientists. Consequently, quasi-experimentalists have fewer oppor tunities to generate the self-critical feedback from practice and to identify through trial-and-error or other indigenous mechanisms novel practices that improve on the old. Perhaps it is from this necessity that theorists of quasi-experimentation have taken philosophy of science more seriously than theorists of most other social science methods,</page><page sequence="38">178 THOMAS D. COOK AND DONALD T. CAMPBELL even while disciplining the insights of philosophy with common sense reflections on practice. NOTE * The support to the first author of the Center for Applied Psychological Research at Memphis State University made this paper possible. We are grateful to the Center's directors and members for the stimulation they provided. REFERENCES Aronson, J. L.: 1971, 'On the Grammar of 'Cause", Synthese 22, 414-430. Bechtel, W.: 1985, 'The Evolution of Our Understanding of the Cell: A Study of the Dynamics of Scientific Progress', Studies in the History and Philosophy of Science, 15(4), 309-356. Ball, S. and G. A. Bogatz: 1970, The First Year of Sesame Street: An Evaluation, Educational Testing Service, Princeton, NJ. Blalock, H. M., Jr. (ed.): 1971, Causal Models in the Social Sciences, Aldine, Chicago. Blalock, H. M., Jr. and H. Costner: 1972, 'Scientific Fundamentalism and Scientific Utility: A Reply to Gibbs', Social Science Quarterly 52, 827-844. Campbell, D. T.: 1957, 'Factors Relevant to the Validity of Experiments in Social Settings', Psychological Bulletin 54, 297-312. Campbell, D. T.: 1966, 'Pattern Matching as an Essential in Distal Knowing', in K. R. Hammond (ed.), The Psycholog^ of Egon Brunswik, Holt, Rinehart, &amp; Winston, New York, pp. 81-106. Campbell, D. T. and H. L. Ross: 1968, 'The Connecticut Crackdown on Speeding: Time-Series Data in Quasi-Experimental Analysis', Law and Society Review 3(1), 33-53. Campbell, D. T.: 1969a, 'Prospective: Artifact and Control', in R. Rosenthal and R. Rosnow (eds.), Artifact in Behavior Research, Academic Press, pp. 351-382. Campbell, D. T.: 1969b, 'Reforms as Experiments', American Psychologist 24, 409-429. Campbell, D. T.: 1977, 'Descriptive Epistemology: Psychological, Sociological, and Evolutionary', William James Lectures, Harvard University (unpublished, duplicated copies available). Campbell, D. T.: 1978, 'Qualitative Knowing in Action Research', in M. Brenner, P. Marsh, and M. Brenner (eds.), The Social Contexts of Method, Croom Helm, London, pp. 184-209. Campbell, D. T.: 1986, 'Science Policy from a Naturalistic Sociological Epistemology', PSA 1984, Vol. 2. Campbell, D. T. and R. F. Boruch: 1975, 'Making the Case for Randomized Assignment to Treatments by Considering the Alternatives: Six Ways in Which Quasi-Experimental Evaluations Tend to Underestimate Effects', in C. A. Bennett and A. A. Lumsdaine (eds.), Evaluation and Experience: Some Critical Issues in Assessing Social Programs, Academic Press, New York. Campbell, D. T. and J. C. Stanley: 1963, 'Experimental and Quasi-Experimental Designs</page><page sequence="39">CAUSAL ASSUMPTIONS 179 for Research on Teaching', in N. L. Gage (ed.), Handbook on Research on Teaching, Rand McNally, Chicago. (Also published as Experimental and Quasi-Experimental Designs for Research, Rand McNally, Chicago, 1966.) Clausner, J. I. and A. Shimony: 1978, 'Bell's Theorem: Experimental Tests and Implications', Reports on Progress in Physics 41(12), 1881-1927. Collingwood, R. G.: 1940, An Essay on Metaphysics, Clarendon Press, Oxford. Cook, T. D.: 1985, 'Post-Positivist Critical Multiplism', in L. Shotland and M. M. Mark (eds.), Social Science and Social Policy, Sage Publications, Beverly Hills, CA. Cook, T. D. and D. T. Campbell: 1979, Quasi-Experimentation: Design and Analysis Issues for field Settings, Houghton-Mifflin, Boston. Cook, T. D. and L. C. Levit?n: 1980, 'Reviewing the Literature: A Comparison of Traditional Methods with Meta-Analysis', Journal of Personality 148, 449-472. Crocker, J.: 1981, 'Judgment of Covariation by Social Perceivers', Psychological Bulletin 90, 272-292. Cronbach, L. J.: 1982, Designing Evaluations of Educational and Social Programs, Jossey-Bass, San Francisco. Dietl, P. J.: 1970, 'Abnormalism', Theoria 36, 93-99. Dunn, W. N.: 1982, 'Reforms as Arguments', Knowledge: Creation, Diffusion, Utilization 3, 293-326. Ellett, F. S., Jr. and D. P. Ericson: 1983, 'The Logic of Causal Methods in Social Science', Synthese 57, 67-82. Fisher, R. A.: 1925, Statistical Methods for Research Workers, 1st ed., Oliver &amp; Boyd, London. Gasking, D.: 1955, 'Causation and Recipes', Mind 64, 479-487. Giere, R. N.: 1985, 'Philosophy of Science Naturalized', Philosophy of Science 52, 331-356. Haack, R. J.: 1967, 'Recipes and Causes', Mind 76, 98-102. Hacking, I.: 1984, 'Experimentation and Scientific Realism', in J. Leplin (ed.), Scientific Realism, University of California Press, Berkeley, CA. Hanson, N. R.: 1955, 'Causal Chains', Mind 64, 289-311. Harre, R. and E. H. Madden: 1975, Causal Powers: A Theory of Natural Necessity, Blackwell, Oxford. Hausman, D.: 1983, 'Are There Causal Relations Among Dependent Variables?', Philosophy of Science 50, 58-81. Heider, F.: 1944, 'Social Perception and Phenomenal Causality', Psychological Review 51, 358-374. Hempel, C: 1965, Aspects of Scientific Explanation and Other Essays on the Philosophy of Science, The Free Press, New York. Kuhn, T. S.: 1970, The Structure of Scientific Revolutions, University of Chicago Press, Chicago. Lawler, E. E., Ill and J. R. Hackman: 1969, 'Impact of Employee Participation in the Development of Pay Incentive Plans: A Field Experiment', Journal of Applied Psychology 53, 467-471. Light, R. and D. B. Pillemer: 1984, Summing Up: The Science of Reviewing Research, Harvard University Press, Cambridge, Mass. Mackie, J. L.: 1974, The Cement of the Universe, Oxford University Press, Oxford.</page><page sequence="40">180 THOMAS D. COOK AND DONALD T. CAMPBELL McCall, W. A.: 1923, How to Experiment in Education, Macmillan, New York. Meehl, P. E.: 1978, 'Theoretical Risks and Tabular Asterisks: Sir Karl, Sir Ronald and the Slow Progress of Soft Psychology', Journal of Consulting and Clinical Psychology 46, 806-834. Michotte, A.: 1963, The Perception of Causality, 1st English edn., Basic Books, New York. Mill, J. S.: 1906, A System of Logic, Longman Greene's &amp; Co., London. Millikan, R. G.: 1984, Language, Thought, and Other Biological Categories, Broadford Books of MIT Press, Cambridge, Mass. Mover, D. F.: 1979, 'Revolution in Science: The 1919 Eclipse Test of General Relativity', in A. Perlmutter and L. F. Scott (eds.), On the Path of Albert Einstein, Plenum Press, New York, pp. 55-150. Popper, K. R.: 1959, The Logic of Scientific Discovery, Basic Books, New York. (Originally Die Logik der Forschung, 1935.) Popper, K. R.: 1972, Objective Knowledge: An Evolutionary Approach, Clarendon Press, Oxford. Putnam, H.: 1983, Realism and Reason, Cambridge University Press, Cambridge. Quine, W. V.: 1963, 'Two Dogmas of Empiricism', in From a Logical Point of View, Harper &amp; Row, New York, pp. 20-46 (originally 1951). Quine, W. V.: 1969, 'Epistemology Naturalized', in Ontological Relativity and Other Essays, Columbia University Press, New York. pp. 90-99. Quine, W. V.: 1974, The Roots of Reference, Open Court, LaSalle, Illinois. Quine, W. V.: 1975, 'The Nature of Natural Knowledge', in S. Guttenplan (ed.), Mind and Language, The Clarendon Press, Oxford, pp. 67-81. Rosenberg, A.: 1983, 'Causation and Recipes: The Mixture as Before', Philosophical Studies 24, 379-385. Scriven, M.: 1971, 'The Logic of Cause', Theory and Decision 2, 49-66. Scriven, M.: 1975, 'Causation as Explanation', Nous 9, 3-16. Scriven, M.: 1976, 'Maximizing the Power of Causal Investigation: The Modus Operandi Method', in G. V. Glass (ed.), Evaluation Studies Review Annual, Vol. 1, Sage Publications, Beverly Hills, CA. Suppes, P.: 1970, A Probabilistic Theory of Causality, North-Holland, Amsterdam, van Fraassen, B.: 1980, The Scientific Image, Clarendon Press, Oxford. Weber, S. J. and T. D. Cook: 1972, 'Subject Effects in Laboratory Research: An Examination of Subject Roles, Demand Characteristics, and Valid Inference', Psy chological Bulletin 77, 273-295. Whitbeck, C: 1977, 'Causation in Medicine: The Disease Entity Model', Philosophy of Science 44, 619-637. Wright, G. H. von: 1971, Explanation and Understanding, Cornell University Press, Ithaca, NY. Department of Psychology Northwestern University Evanston, IL 60201 U.S.A.</page></plain_text>