<plain_text><page sequence="1">О. В. Hardison, Jr. 'The Disappearance of Man Consume my heart away, sick with desire And fastened to a dying animal. -W.B.Yeats I THE but curve its angle of evolution of ascent constantly begins to rise increases, slowly, and almost the rate imperceptibly, of increase but its angle of ascent constantly increases, and the rate of increase is exponential. It took over four billion years- around eight-ninths of the total age of the earth- for the planet to cool off and for the earliest single-celled organisms to become colonies of cells and begin to excrete external skeletons. That happened in the warm, shallow seas of the Cambrian Period, about 600 million years ago. Within another 250 mil- lion years, animal life had crept onto the beaches and into the forests of giant fern and ginkgo trees. By the beginning of the Triassic Period, a scant 250 million years before the present era, the first mammals and dinosaurs appeared. No more than another 100 million years following the Triassic Period were needed for the appearance of birds and small warm-blooded animals, and as the dinosaurs left the evolutionary stage, plants began producing flowers to brighten the dark days. Seventy-five million years later, in the Oligocene Epoch, whales began to clear their spouts on the oceans, while on land anthropoid apes swung from decidu- ous trees and huge herds of grazing animals roamed the broad savannas. Within another 20 million years the apes and man parted company. An evolutionary phase transition had occurred. With this event interest shifts from the curve of evolution in general to the equally dramatic curve of human evolution. Thanks especially to the discoveries of Louis and Mary Leakey, a good deal is known about what Charles Darwin called the descent of man. [679]</page><page sequence="2">68o THE GEORGIA REVIEW Ramapithecus , the earliest hominoid, appeared in the late Miocene period around 1 2 million b.c. Ramapithecus lived on the ground rather than in trees and was probably semierect and vegetarian. The species ranged from Europe to Africa and east to India. Then came Homo erectus, i о or so million years later- definitely a more enterprising sort of creature who used fire and, in the opinion of some, practiced cannibalism. His traces are found all the way from Africa and Spain to eastern China and Indonesia. Roughly a million years separate Homo erectus from Homo sapiens , whose earliest unambiguous performance on the evolutionary stage seems to be breathtakingly recent- around 100,000 b.c., at which time he hunted woolly mammoths and competed with Neanderthal man for the honor of becoming the sole possessor of reflective intelligence on earth. Homo sapiens was a hunter and gatherer. Although his tools were initially un- polished stone, he soon learned to transform the unpolished into the pol- ished and to make brilliant ritual paintings of the animals he hunted. He certainly knew the rudiments of a religion and almost certainly imagined creatures like himself but different who were alternately his helpers and his tormentors. An evolutionary leap- a new singularity- is associated with Homo sapiens. While man has remained much the same physically for the last 40,000 years, human culture has evolved along an exponential curve similar to that describing biological evolution. The move from hunting and gathering to agriculture began around 10,000 b.c. and took most of human history. The earliest cities date from 6,000 b.c. Early Egyptian and Cretan civilizations began between 4,000 and 3,500 b.c., during which period copper gave way to bronze. The so-called "Legendary Rulers" of China governed from 2,850 to 2,200 b.c. And writing appeared: Sumerian and Egyptian hieroglyphics date from approximately 3,600 b.c.; the Phoenician alphabet, ancestor of the modern phonetic alphabet, was being used by 1,100 b.c.; and the Indian Vedas date from 1,000 b.c. Paper first appeared in China in 950 b.c. Roughly 3,000 years separate the early Near Eastern and Egyptian empires from the Roman Empire. About 1,000 years separate the Con- version of Constantine from the Renaissance. Columbus died some 250 years before the beginning of the Industrial Revolution. Moving from the Industrial Revolution to the age of steam took a century, while moving from steam to electricity and the internal combustion engine took perhaps fifty years. The next half -century saw the arrival of radio, tele-</page><page sequence="3">О. В. HARDISON, JR. 68 1 vision, jet aircraft, and atomic fission, followed within two decades by space flight and genetic engineering. Cultural evolution should not be understood, any more than bio- logical evolution, in terms of movement from bad to good or good to better. Its absolute direction is best symbolized by an arrow pointing down a dark corridor. There is, however, a unifying theme. Every ad- vance in culture has been an advance in communications and has encour- aged ever-larger organizations of the human beings who produced it. Small, isolated bands of hunters combine into tribes which form city states which in turn form- or are absorbed into- nation states and em- pires. The movement has downs as well as ups, but the overall direction is clear. Twentieth-century technology has perfected the work of Ren- aissance explorers, Enlightenment scientists, and Victorian entrepreneurs to create a world culture. As Teilhard de Chardin has written in The Future of Man: "No one can deny that a network (a world network) of economic and psychic affiliations is being woven at ever-increasing speed which envelops and constantly penetrates more deeply within each of us. With every day that passes it becomes a little more impossible for us to act or think otherwise than collectively." Teilhard, who sees this whole process as a kind of continuous Revela- tion, argues that with the advent of mind there is "outside and above the biosphere ... an added planetary layer, an envelope of thinking sub- stance. . . ." Eventually, he proposes, this "envelope" (which he calls the "Noosphere") will become a seamless web of relationships uniting all men in global communion. Its collective response will be a human song that responds to the inaudible music of the voice of God. The unity of this condition will be much like the unity understood in the Mass as incorporation into the Body of Christ: "The idea is that of the earth not only becoming covered by myriads of grains of thought, but becoming enclosed in a single thinking envelope so as to form, functionally, no more than a single vast grain of thought on the sidereal scale, the plurality of individual reflections grouping themselves together and reinforcing one another in the act of a single unanimous reflection." For many this is airy nonsense; for others it is an inspiring vision. It is, at any rate, a very clear statement of the idea that man- at least man in the old sense of a separate and individual essence- may disappear as a result of evolution. The idea is not new. It is foreshadowed by the ancient myth of Ganymede, the human being snatched by Zeus and brought to heaven to be his cupbearer.</page><page sequence="4">682 THE GEORGIA REVIEW II Meanwhile, another kind of evolution has been occurring- different from all prior forms. I am speaking of the evolution of silicon-based intelli- gence, or- as the earliest popular metaphors put it- "electronic brains." "Electronic brain" is a metaphor of life, and- let us be clear about this- it is a metaphor. It does not mean that computers are alive, any- more than "My love is like a red, red rose" means that my love has a scarlet face and is covered with brambles. Poetic comparisons say what things are like and suggest emotional responses. They are useful for ex- plaining and clarifying because they can say how something that is not understood resembles something that is understood. They do not, how- ever, have to be taken literally. Human beings have always been fascinated by toys that are "like" living creatures. Simple mechanical devices created from cogs, levers, springs, and screws have striking internal regularities, yet are capable of innumerable surprising variations on their simple norms. Who has not been enthralled as a child by a doll that cries and drinks milk and wets its pants? By a toy soldier? By a dog that barks and wags its tail? We should hardly be surprised, therefore, to learn that the ancient Greeks had windup marionettes or that they built water-powered mechanical aviaries. A key mechanical device employed more practically by man was the escapement mechanism of clocks, which appeared in Europe in the thirteenth century. Its function was to regulate the release of energy by the falling weights and later (in the fifteenth century) by coiled springs. Thanks to its escapement, a clock behaves differently from the simplest toys and machines: it controls its own motions. In some very general sense it may be said to take a step toward behavior that is like self- awareness. A clock with an escapement mechanism might be compared to a protein floating in a Precambrian ocean. It is rich with potential. Within a century after the first escapement, clockwork mechanisms- in addition to telling time- were being used to create automated animals and people that performed little plays for delighted citizens who turned out to watch their daily movements on Rathaus towers. The metaphor of the likeness of clocks to living things is thus objectified by clockwork figures that have no use other than to enact the metaphor. And as technology improved, the likeness improved along with it: by the mid-eighteenth century Jacques de Vaucanson had created an automated flute player</page><page sequence="5">О. В. HARDISON, JR. 683 who covered the stops on a flute with mechanical fingers and was capable of playing twelve different tunes. By permitting more precise control of time, clocks also changed the nature of work, turning the day into standard and repeatable segments and permitting wages to be related to hours of work. If the self-regulation of clocks is "like" human self-regulation, clocks therefore contributed mightily to making human behavior more "like" the behavior of machines. This is a significant point because the introduction into society of ma- chines that are like people is usually considered a one-way street. It is not. The more that humanlike machines become a part of human culture, the more human culture changes in recognition of their presence. Another step in the development of machines that are like people was the calculating machine. The first practical device of this sort was invented by Blaise Pascal in the seventeenth century: the Pascaline, which could add and subtract handily, and with effort could also do basic multiplication and division. Half a century later Gottfried von Leibnitz, co-inventor (with Sir Isaac Newton) of infinitesimal calculus, solved the problem of multiplication and division. New dimensions were added to the project at the beginning of the nineteenth century with the inven- tion by Jacques Jacquard of a system of punched cards to automate weaving patterns. In 1834, Charles Babbage- with his accomplice Ada (countess of Lovelace and daughter of the poet Byron)- combined many technologies that had been developing separately during the preceding century to create the first realistic design for a general-purpose mechanical computer. Like some wonderful Jules Verne fantasy, the Analytical Engine they proposed was to be powered by steam. Among its components were a "mill" which did calculations and a "store" for memory. Input was by punched cards like those invented by Jacquard. The plan was remarkable, and, if perfected, would have achieved a function anticipated for centuries in machine design, toys and gadgets, and mythology: it was intended to think. It would think exclusively about numbers, but it would think about them intelligently enough to say things that were interesting to human attendants. Unfortunately, the machine was a failure. Mechanical devices can add, subtract, multiply, divide, and sort within limits, but the limits are narrow. When they be- come complicated, they encounter the liabilities of all mechanical sys- tems: inertia, friction, inaccurate machining, slippage, distortion, fatigue, breakage. Babbage's prototypes were given to fits of shuddering and</page><page sequence="6">684 THE GEORGIA REVIEW wrenching as the effects of mechanical imperfections compounded them- selves during operation. A century later, however, under the pressure of such urgent tasks as deciphering enemy codes, tracking high-altitude aircraft with gun batteries, and developing the atomic bomb, a series of dramatic advances occurred. Among those responsible for them are some of the most revered names in twentieth-century science: Alan Turing, John von Neumann, Norbert Wiener, Claude Shannon, and Herbert Simon, to name only a few. These men understood that electrical currents, rather than physical motions of cogs and levers and wheels, would be the future carriers of intelligence. Wiener was especially fascinated by the likeness between the carbon-based circuits of the human neural network and digital cir- cuits. Although the analogy did not lead immediately to useful develop- ments, the fact that neuron circuitry existed and obviously worked created faith in the possibility of using electrical circuitry to create in- telligent machines. The immediate ancestors of modern digital computers were de- veloped more or less independently in England and the United States during the Second World War. At the beginning of the war, the British had secured a German code machine called "Enigma" from the Polish Secret Service. At an English country house called Bletchley Park, a series of machines was secretly created to decode German signals. The final product of the effort was a digital computer called COLOSSUS, which became operational in 1943. COLOSSUS was extraordinarily fast because it used vacuum tubes rather than relays. Today, many intelli- gence experts believe it provided the advantage that shifted the course of the war in favor of the Allies. In America the earliest practical electronic computer was the work of John V. Atanasoff, a professor of electrical engineering at Iowa State University, and his graduate student Clifford E. Berry. Atanasoff did not invent the key concepts. They had already been developed by George Stibitz of Bell Telephone Laboratories and Howard Aiken of Harvard, who went on to produce the "Mark I" computer, a cumbersome special- purpose machine, using electrical relays. By contrast, Atanasoff created a general-purpose computer that, like COLOSSUS, used vacuum tubes. The first model, completed in 1940, was called the "ABC." Atanasoff abandoned his work on computers during the war, and credit for de- veloping the first practical digital computer in the United States was, until recently, assigned to John W. Mauchly, creator (with J. P. Eckert)</page><page sequence="7">О. В. HARDISON, JR. 685 of ENI AC, which became operational in 1946. Later, when John von Neumann suggested that computers could store instructions as well as data, the basic elements of the modern computer were in place. Equipped with memories that could store programs, computers had taken a giant step toward being self-reflexive, i.e., they began to assume control of their own operations. In spite of their huge bulk and the apparent complexity of their spaghettilike circuitry and glowing vacuum tubes, the first electronic computers were more like the earliest protozoa than advanced organisms. By i960 they had progressed from vacuum tubes to transistors to silicon chips. They had slimmed down, and although their circuitry constantly grew more complex, they lost their clumsy appearance. They looked increasingly elegant as they floated lazily on the ocean of possibility out of which they had come. These computers were not very smart, but they were smarter than any of the nonhuman devices that preceded them, and they continued to evolve. As their dimensions decreased, their ca- pacity grew by orders of magnitude. Since 1950, both the reduction in scale of computers and the increase in their abilities have been exponential. And, even more important, with the advent (between 1965 and 1975) of integrated circuits and co- processing, computers ceased to resemble single-cell organisms. Instead, they began to resemble small, multi-celled colonies. Carbon life took something like two billion years to progress from single-celled to multi-celled creatures. Silicon devices managed some- thing similar in twenty-five years. They were able to move fast because they were, in a sense, spiritual parasites: they drew their understanding pre-digested from their hosts, and their feeding, healing, and reproductive functions were all supplied for them. It is as though carbon creatures had developed brains and sense organs before they began to grow bodies. in Within a few years after the digital computer was introduced, computer programs were being created that started to exhibit something like human intelligence. From the mid-1950's on, this has been known as Artificial Intelligence. A.I. has had a sometimes stormy history of development in recent decades, the most useful account of which is Pamela McCorduck's Machines Who Think (1979). By the early i96o's, programs were developed that had problem-</page><page sequence="8">686 THE GEORGIA REVIEW solving abilities resembling those of an intelligent human investigator. By the end of the decade, computers were also able to play excellent checkers and middling chess. At about this time Terry Winograd of Stanford began experimenting with a program that could represent its environment and discuss it intelligently. He called the program SHRDLU (from the last six of the twelve most frequently used letters in the Eng- lish language); the newly created environment was called "Block World." The machine was told that it confronted a set of blocks of different colors and shapes, and it was then ordered to manipulate the blocks- for exam- ple, to place a red pyramid on a green cube. SHRDLU had to understand space and gravity. It had to know enough about shapes, for example, to realize that the green cube could not be put on top of the red pyramid. It also needed to understand English as used by its conversational partner and how to reply in well- formed sentences. Furthermore, it had to keep track of the blocks it was moving so that if, for instance, it were asked to move a cube that was already under a pyramid, it would remember to pick up the pyramid and set it aside before trying to move the cube. The abilities of SHRDLU were quite impressive, but Winograd eventually ran into limits. To go much beyond Block World required far more speed and power than were available. As research has con- tinued, it has become clear to later experts that to have been significantly more intelligent, SHRDLU would also have needed new programming techniques and probably new types of computer design. Another program that exhibited startling intelligence was The Auto- mated Mathematician created by Douglas B. Lenat. This program seemed to operate itself, using built-in general rules of procedure ("heuristics"). The Automated Mathematician explored set theory, then proceeded to invent arithmetic, and finally carried out an analysis of prime numbers. Lenat's EURISKO improved on The Automated Mathematician by its ability to change its procedural rules in the light of experience. EURISKO is credited with having originated an innovative design for integrated circuits, and in 1982 it was so successful in a war game called "TRAVEL- LER" that after its second victory it was barred from international competition. On the other hand, many of the headiest predictions of the early days of Artificial Intelligence refused to come true. Machine translation is a case in point. Generously supported by the Defense Department, early researchers were confident they would produce excellent translation</page><page sequence="9">О. В. HARDISON, JR. 687 programs before the end of the decade of the 1960's. Part of the reason for their optimism was that they approached the easy problems first, before the true dimensions of the challenge appeared- but as they did, the date for the perfecting of a general-purpose translation program kept receding into the future, and the complexity of the programs and amount of computing power needed to achieve even limited successes kept increasing. The problem of translation programs is still being attacked in the late-1980's. And in spite of renewed enthusiasm among researchers, the final solution is still proving elusive. As with machine translation of language, so too with many additional problems involving what John McCarthy and others call "common sense." Common sense is a faculty human beings seem to develop without effort but that computers demon- strate hardly at all. The more that is learned about this human quality, the more elusive becomes the goal of endowing a computer with it (though that does not, of course, stop people from trying). Artificial Intelligence has been most successful in the creation of what are called "expert systems," which use information and rules of procedure drawn from experts in the relevant area of knowledge. These systems require elaborate interviews with the experts and much field testing and tinkering, but they work. They give advice today on every- thing from medical diagnosis and probable locations of mineral deposits to legal research, maintenance of complex machines, investment strategies, and navigation. Among commercially successful expert systems are DEN- DRIL for chemists, MACSYMA for mathematicians, PROSPECTOR for geologists, and MYCIN and INTERNIST for physicians. According to users, these systems can seem to exhibit almost human understanding. Indeed, DENDRIL (to which Edward Feigenbaum contributed) is said to be better than human researchers at the task of analyzing the structure of complex molecules. Still, expert systems are like computer chess programs: they are very good at a specific job but they lack flexibility. They cannot learn very well on their own, and as knowledge grows and changes in a given field, they have to go back to school. If the theory on which their methodology is based changes, they have to be reprogrammed. They are not very smart by biological standards in spite of their impressive special- ized abilities. Perhaps their I.Q. is about on a par with that of a Cambrian mudworm. The dominant tradition of computers, from the mill and store of</page><page sequence="10">688 THE GEORGIA REVIEW Babbage's Analytical Engine to the central processing units and random- access memories of today's mainframes, has been serial processing. In- formation must be dealt with step by step. The strength of the method is its strict logical sequencing. The limitation is that running every step through a single unit creates an obvious bottleneck no matter how fast the unit. A second and equally troublesome limitation is that serial processing is hierarchical and must be operator-organized. A lot of the world- including, evidently, the human brain- operates on different principles. In the 1980's something like a phase transition has been occurring in computer evolution, with the rise to prominence of machines and programming techniques that are parallel rather than serial. Parallel proc- essing is named for the fact that it divides a problem into parts that can be treated simultaneously or in self-arranging sequences. This kind of processing has the promise of being able to develop a more-than-rudi- mentary ability to learn from experience. Parallel systems are self-consciously based on the likeness of com- puter circuitry to the circuitry of the brain. They are often called "neural networks," and the nodes that define the networks are often called "neurons." Since the connections among the "neurons" are an essential part of the operation of parallel computers, machines designed from the beginning to use parallelism are called "connection machines." Since parallel processing mimics what is assumed to be the operation of the brain, it is not surprising that as it has gained in importance there has been a renaissance of what has been called "reverse brain engineering"- that is, analysis of brain functions based on the theory that they are "like" the functions of neural computers. In addition to stimulating research on neural circuitry in the brain, the rise of parallelism has given new currency to the metaphor of computer life. Parallel processing has many virtues. Most obvious is the fact that it permits certain tasks to be done more rapidly than in serial processing. However, this may not be the most significant advantage. Parallel proc- essing is flexible. In certain kinds of parallelism, the program operates by creating loose confederations of circuits that are a reflection of the problem being analyzed, so that when the problem changes the pattern of configurations changes. Depending on how a given program is set up, processing can also be hierarchical- that is, layered- so that informa- tion is refined as it moves from one layer to the next. In addition, process- ing can use feedback. That is, information can be sent from a higher</page><page sequence="11">О. В. HARDISON, JR. 689 layer, which is closer to having generalized understanding, to a lower layer, which receives raw data. The creation of an expert system using serial processing is analogous to memorizing. Conversely, the learning that occurs in certain kinds of parallel systems is like the programming the mind does for itself as a result of interaction with the environment during infancy. This is be- cause parallel systems can be designed so that the strengths of the con- nections between processing elements are changed by the data received. Some links, for example, are strengthened, while others are weakened. The process resembles the creation of associative patterns in the brain. Through the development of these patterns, neural networks can be, to a certain degree, self-organizing, and what is organized is a crude in- ternalized model of a fragment of reality. For this reason, parallel systems seem more "like" the human mind than conventional computers. And there is another reason: the self- organizing ability of parallel systems is a little mysterious, perhaps a little scary, even if you know how it works. A neural network is not conscious, but it makes the metaphor of computer life a little less play- ful-a little less metaphorical- than it used to be. A measure of the androidal quality of parallel circuits is provided by a program called NETtalk created by Terrence Sejnowski of Johns Hopkins and Charles Rosenberg of Princeton. The program uses a mere 2 3 1 "neurons," yet it manages to be self-organizing. Once it has been supplied with phonetic samples of the speech it is to emulate, it teaches itself to talk. June Kinoshita and Nicholas Palevsky describe the process in a 1987 article, in a veritable cascade of life-metaphors: "Like a child, the network starts out untrained, and produces a stream of meaningless babble. . . . The continuous stream of babble first gives way to bursts of sound, as the network 'discovers' the spaces between words. . . . After being left to run overnight . . . NETtalk is talking sense." In effect, NETtalk has a general strategy for solving problems and is able to create specific programs for specific tasks, which is at least as good a per- formance as most mudworms can turn in. It may also be a little more than its inventors first bargained for. In an interview in The New York Times (August 1988) Sejnowski con- fessed that because the machine was self -organizing, he did not at first know exactly how it worked. When he analyzed the circuits it had created, they "turned out to be very sensible." Is NETtalk beginning to take a first few tentative steps in the direc-</page><page sequence="12">ÓÇO THE GEORGIA REVIEW tion of lifelike self-sufficiency? Is it in some sense inventing itself? The same issue of The Times that reported Sejnowski's adventures carried an article on the relations between computer research and neuroscience, ending with this comment: "As neural networks become more complex, they promise to defy the ability of mathematicians- and even therapists- to comprehend them." Whatever the philosophical implications of a machine that its makers can no longer understand, the United States Defense Department is cur- rently bullish on neural computing. Craig I. Fields, a deputy director of D ARPA- Defense Advanced Research Projects Agency- outlined a proposal in 1988 to fund neural network research at $400 million over the next eight years. Special attention would be given to language recogni- tion and decision-making systems. The goal would be to produce a machine "approaching the intelligence" of a bee. A bee is a jump of about 300 million years beyond the mudworm on the evolutionary ladder. A machine with the intelligence of a bee would be an advance as dramatic in its way as was the introduction of integrated circuits in the 1970's. Evidently, the rate of machine evolution continues to accelerate. IV Suppose an operator were to ask a machine whether it was intelligent and the machine answered, "Yes." How would the operator prove it was lying? Alan Turing, one of the pioneers in the development of computers, was also the first to recognize and pose this problem. In a famous paper published in 1950, entitled "Computing Machinery and Intelligence," he proposed a wonderfully simple test to decide whether or not the com- puter is lying. He called it the "imitation test" (though it has since come to be known as "the Turing Test"), and he based it on observation. The experimenter is in a closed room but can communicate with a "some- thing" in another room. If the experimenter can ask any questions he or she wants and cannot, within a given length of time, be certain that the "something" in the other room is a machine, then the machine has human intelligence de facto. Here we need to look closely at the meaning of "intelligence," for Turing means something more than "cleverness." He is not asking whether a machine can do complicated arithmetic (everybody knows it can) or whether it can have a high I.Q. (it obviously does in certain</page><page sequence="13">О. В. HARDISON, JR. 69 1 specialized areas) but whether it can hold a conversation and persuade the person on the other end of the line that it is human. Passing this test does not require that the machine recite Homer in Greek or explain the fourth dimension or do anything else that people usually associate with exceptional intelligence. In fact, if the machine is really smart, it will probably pretend to be a little dumb. A conver- sationalist who could come up with the square root of 1,743 correct to five decimal places in a few seconds (or casually list all the places where Shakespeare uses the word "thither" in his plays) would be suspicious, to say the least. As we all know, lack of intelligence never stopped anyone from talking. What Turing means by intelligence, and what really interests him, is consciousness. What a machine really shows when it has passed the Turing Test is that you cannot prove it is not human, which means not conscious. This thesis represents not only Turing's interpretation of his own test, but also the interpretation that runs through the considerable debate begun by the publication of his paper and still going on today. What could be more reasonable than the Turing Test? If a person claims to be conscious and you cannot prove the person is lying, then it would seem the person must be conscious. In real life, we seldom ask a person whether he or she is conscious, but we do observe. Is the person asleep? In a coma? If so, the person is "unconscious." On the other hand, if the person is able to give reasonable replies to our questions and com- ments, then we instantly reach a conclusion: the person is conscious. We accept this method without question for human beings. Why not for computers? Turing was a scientist- a brilliant and highly creative mathematician -yet he was sufficiently intrigued with the metaphor of silicon humanity to work out an ingenious strategy for determining if (or when) it had arrived. He was not alone. Ever since computers first appeared, scientists have referred to them in anthropomorphic terms. The machine speaks a "language." It has a "memory." It uses "logic," and it "reasons." It "understands" Fortran or Lisp, and it "plays" chess or checkers or poker. If it can synthesize speech, it is said to "talk." Computers that deal with real-world situations have "sensory input," including "vision," "hearing," and "touch." Robots "walk" and have "arms" and "fingers," and they "see" and "touch" objects. And when a destructive program insinuates itself into a computer, it is called a "virus." Robert Jastrow, director of NASA's Goddard Space Institute, was</page><page sequence="14">óç 2 THE GEORGIA REVIEW only extending the metaphor in Time (20 February 1978), when he stated: "In another 15 years or so ... we will see the computer as an emergent form of life." He was even more optimistic in 1982 when he wrote in "The Thinking Computer" that "portable, quasi-human brains, made of silicon or gallium arsenide, will [soon] be commonplace. They will be an intelligent electronic race, working partners with the human race." The point can be carried further: a major influence on the develop- ment of silicon devices is the imperative to make the metaphor a reality . That urge has, of course, long been an underlying motive of science fiction, where the still-impossible is presented as having been achieved. The robot R2D2 in Star Wars is a benign vision of possible silicon in- telligence, the machine equivalent of a lovable mascot. But perhaps when the metaphor becomes a reality, the results will be less happy. Two nightmares hover just below the surface of the vision of machine life, and they, too, are objectified in science fiction. First, machine life may turn out to be malevolent. Fear underlies the legend of the golem and also the earliest drama about robotic civiliza- tion, Karel Čapek's R.U.R. (1922). Like the golem, Čapek's robots turn against their human masters and attempt to exterminate them. In a 1942 short story titled "Caves of Steel" Isaac Asimov formulated three rather ominous laws for robots. All three are defensive, reflecting the fear that robots may turn into golems: 1. A robot may not injure a human being, or through inaction allow a human being to come to harm. 2. A robot must obey the orders given it by human beings ex- cept where such orders would conflict with the First Law. 3. A robot must protect its own existence as long as such protec- tion does not conflict with the First or Second Law. Fears about malevolence merge with a second concern, the fear that robots may become indistinguishable from people. Čapek's robots are androids-that is, they look like people. A similar sinister image is pre- sented in the film Alien , in which it is revealed at the climactic moment that the evil force destroying the space mission is an android. Norbert Wiener pointed out in The Human Use of Human Beings that there is no functional difference between a signal from a machine and one from a human agent. Apparently, the public is already apprehen- sive that he may be right and that humanity may find itself enmeshed in signals that seem human but are, in fact, from machines. How many</page><page sequence="15">О. В. HARDISON, JR. 693 pieces of mail does the postman bring daily that are generated by com- puters? How many of the telephone voices giving the time or the weather or a desired number are computer simulations? The blurring of the distinction between computers and animate beings is complemented by a weakening of the human sense of what reality is. This weakening is the direct result of technology. Movies and television create an illusion of presence at the unfolding of events. Inter- active environments like arcade games, training simulations, and artificial realities create illusions that are even more vivid. At their best, they come close to obliterating the difference between reality and illusion. They are related to image manipulation in advertising and politics and to the curious but well-documented fact that for many people today an event is not authenticated- is not "real"- unless it has been seen on television or in a photograph. The question of reality surfaces in a very practical way in medical research. If machines are "like" people, neurologists often find it useful to reverse the comparison by thinking of the brain as "like" a machine. The strategy is useful because machines are, relatively speaking, known, whereas many aspects of the brain are still largely unknown. Norbert Wiener and Arturo Rosenblueth were able to describe ataxia- uncontrolled muscular oscillation- through the model of inap- propriate feedback in machines. In Cybernetics , Wiener noted that dur- ing these studies, "It became clear to us that the ultra-rapid computing machine, depending as it does on consecutive switching devices, must represent almost an ideal model of the problems arising in the nervous system." Two decades later Gary Lynch, a neurophysiologist at the University of California, Irvine, remarked of his work: "We look at the</page><page sequence="16">6ç 4 THE GEORGIA REVIEW computer research and then go back to the brain and ask, 'Do you do this?' We put a [specimen] in a dish and look for things engineers have predicted we might see." On one hand, Artificial Intelligence is a practical science seeking to develop new and better expert systems. On the other hand, Artificial Intelligence can be considered the investigation of what constitutes in- telligence, and as it does this it merges with cognitive psychology and philosophy. What is intelligence? If computers show traces of intelligent behavior, the next question is inevitable. Are they- or can they ever be- conscious? In 19 65, Hubert Dreyfus, a philosopher at the University of Cali- fornia, Berkeley, became so concerned about the spread of the notion that computers can be intelligent that he soon laid down the gauntlet in a much discussed book, What Computers Can't Do: A Critique of Arti- ficial Reason (1972). Computers, he said, can never be intelligent. It is silly and alarmist to imagine they can. Nobody who knows them is guilty of such an absurd notion. In spite of this assurance, many people who knew computers well, including such authorities on Artificial Intelligence as Seymour Papert and Edward Feigenbaum, insisted vehemently that Dreyfus was flat-out wrong. Soon they were able to savor a sweet moment of triumph. In a report that preceded his book Dreyfus had seemed to many readers to predict that computers would never be able to play even amateur chess. A match was subsequently arranged by Papert between Dreyfus and a chess program called MacHack. The program won. A report of the game circulated in the Artificial Intelligence community, beginning with the headline, "A Ten-Year-Old Can Beat the Machine- Dreyfus." This was followed by the subhead, "But the Machine Can Beat Dreyfus." The serious basis of the Dreyfus position- and it is a very serious basis- was that computers can never be intelligent or conscious because they can never develop anything like human subjectivity. It is an argu- ment that would be developed further by others in the years that fol- lowed. Meanwhile, the popular imagination continued to flirt with the idea of machine life. The fact that Dreyfus entered the lists again in 1982 with a book robustly entitled Mind Over Machine shows clearly that the problem had not gone away. Indeed, his title itself has a military ring to it, suggesting that the situation has gotten worse rather than better. Machines are no longer viewed by Dreyfus as dumb bits of wire and silicon that "can't do" the things people think they can. His title promotes</page><page sequence="17">О. В. H ARDISON, JR. 6&lt;)$ them to the position of adversaries in a battle. It assures the reader that the machines will lose, of course, but the rhetoric has a little of the quality of a pep talk delivered at halftime to a team that is two touch- downs behind. If the rhetoric offered by Dreyfus was not as persuasive as he hoped it would be, there was another alternative. The popular response to anxiety about any war between man and machine has long been a ritual- istically repeated bit of folklore: "You can always pull the plug." Un- fortunately, however, long before the publication of What Computers Can't Do, they had already done so much- had so thoroughly infiltrated advanced carbon-based culture- that pulling the plug was not a realistic alternative. Could the Census Bureau or the Eastern Power Grid or Chase Manhattan Bank or America's Strategic Air Command or a Boeing 707 flying at 40,000 feet pull the plug? Could the physician pull the plug on the computerized equipment monitoring the patient's vital functions? Could the stock market pull the plug? The answer in these cases (and for a vast array of other activities) is that the symbiosis between man and computer has, within an astonishingly brief span of time, become so intimate that pulling the plug would be equivalent to social suicide. Another philosophical contribution to the argument against intelli- gent computers was offered by Terry Winograd, who has become one of the most respected members of the Artificial Intelligence community. His 1987 book, Understanding Computers and Cognition (written in cooperation with Fernando Flores), argues that computers have an in- herent "blindness," a term borrowed from the German philosopher Martin Heidegger. This blindness prevents them from being receptive to the broad range of inputs that human consciousness accepts as a matter of course. The argument is a thoughtful reworking of the Dreyfus posi- tion and draws on Winograd's superb understanding of the analogies between the idea of computer intelligence and human intelligence. Surely there is a sense in which computers are blind. Winograd confronts a problem, but it is not quite the problem that needs to be confronted. The issue is not what computers are in some Platonic sense but rather how they are perceived , which is closely related to how they are incorporated into the web of human culture. Let us consider this from two angles: First, there is the ability of computers to do things- apparently very difficult things- that people cannot do without them. Some of these things simply require brute strength. They are the equivalents in calculation to</page><page sequence="18">6ç6 THE GEORGIA REVIEW bulldozers and steamrollers in construction. Others go beyond brute strength, although the strength may be necessary to make the going beyond possible. In the latter case, computers assume a special position in culture. They cease to be tools and begin to be what popular imagina- tion has made them out to be from the beginning: authorities. One of the more remarkable achievements of computers to date is having furnished the proof of what is called "the four-color theorem." This theorem can be stated simply: if you are drawing a map, no matter how many countries there are or what shapes they have, you will never need more than four colors to avoid having two like-colored countries with a shared border. Simple, right? Try it yourself, using pencil and crayons; you will never need more than four different-colored crayons. But try to prove it mathematically, and you'll be baffled. Mathematicians could prove a five-color theorem but the four-color theorem long defied their proofs. In 1977, Kenneth Appel and Wolfgang Haken of the University of Illinois finally wrote a program that proved the four-color theorem after 1,200 hours of computing. The length of time it took is not in itself remarkable; power and perseverance often produce results. What is re- markable about the four-color proof is that it is so complicated human beings cannot verify it. Mathematicians do not say: "We have proved the four-color theorem." They say: "The four-color theorem is true because the program written by Appel and Haken has proved it." This is not very different from saying that the Resurrection occurred because Matthew's Gospel says it did. No disrespect is intended here toward either computers or religion. The point is that with the four-color proof, the relation between man and computers becomes slightly problematic. Not serious, you say; things may be a little out of focus, but they may not be out of focus at all. Still, if you are not entirely persuaded by the Dreyfus argument, you will want to keep an eye on the computer, so to speak, when your back is turned. Second, there is the matter of the practical definition of silicon devices, which involves not the reality of computers but rather what might be called their phenomenology. Terry Winograd knows that com- puters are not alive even though they may seem uncannily lifelike at times, but he knows this because he knows them from the inside. He is like a magician who does not believe in rabbits in hats because he has been pulling the rabbits out of his coat-sleeve. In contrast, the audience in the theater sees only the trick, and to that audience the rabbits are</page><page sequence="19">О. В. HARDISON, JR. 697 demonstrably emerging from the hat in amazing and delightful profusion. In society, the audience decides how reality fits together and what words mean. Ultimately, it determines what the magician, himself, believes. Politics has shown this for centuries. The rabbit in the hat is more than a trick. It compels recognition of the part played by perception in efforts to define the nature of silicon intelligence. If society believes that the earth is flat and that the sun rises and sets, then- no matter what the astronomers claim- it is, for all prac- tical purposes, flat, and the sun revolves around it. Because Terry Wino- grad always deals with this question from the standpoint of the designer, he is a little like an astronomer trying to persuade his neighbors of the demonstrably absurd theory that the sun stays still while the earth turns. Winograd quotes Dan Dennett, a philosopher of cognition, to the effect that "on occasion, a purely physical system can be so complex, and yet so organized, that we find it convenient, explanatory, pragmatically necessary for prediction, to treat it as if it has beliefs and desires and was rational." Dennett calls his position "the intentional stance." It is useful because it recognizes that machine intelligence is partly a metaphor and partly a cultural truth. In a society in which there is regular, easy, and deep intercourse between humans and devices that converse in natural languages, machine intelligence will be a de facto reality regardless of the logicians. Wino- grad asserts that "A computer . . . can never enter as a participant into the domain of human discourse." This is probably true for human dis- course as it has traditionally existed, but traditional discourse is not rele- vant. "Human discourse" is plastic; it changes as culture changes, though more gradually. As society accommodates silicon devices, the new situa- tion will eventually change the meaning of the words that make up the discourse. Winograd himself states the point, even though he fails to give it sufficient weight: "We exist within a discourse, which both pre- figures and is constituted by our utterances." Nobody needs a course in social anthropology to realize that the structure of the world is assimilated by each of us in infancy from the surrounding culture. What is assimilated becomes both the structure of consciousness and the structure of the real. An important part of what is assimilated is called language, and in the future another important part will be the protocols that emerge from the symbiosis of man and intelli- gent machines. As in the case of clocks, the machines will get better- more like humans perhaps- while, at the same time, human beings may</page><page sequence="20">6ç 8 THE GEORGIA REVIEW well get more like machines. The paths are convergent, not divergent. Silicon devices already converse with carbon men in a variety of dialects- assembly language, Fortran, Pascal, Unix, Modula II, Ada, C, Forth, Lisp, Prolog, and more. Each dialect has advantages and liabilities, but they all work. They have diversified according to the special needs of engineering, communications, image processing, robotics, business, and the like. In general, the movement has been from complicated dialects related closely to circuitry, to high-level dialects akin to natural speech. The personal computers of the 1980's allow conversation without any specialized knowledge. These are instances of convergence. Artists can now converse with machines using light pens and paint- brushes and color bars and three-dimensional design systems and anima- tors and ray tracing. Musicians can converse with wave-form profiles, synthesizers, sequencers, and instant playback. Business people converse with icons- little symbolic pictures- and a system of arrows that point to them and a device to click them on and off. Pointing an arrow at the picture of a waste basket and clicking erases a file. Machines can also converse by voice commands, although voice commands are for the moment less popular than visual and tactile systems. The higher the level of the dialect, the more mysterious the results appear to be. It is odd to communicate with a computer by typing mathe- matical symbols or obscure acronymic commands at a keyboard; it is odder to communicate with mouse or light pen; it is oddest to have a two-way conversation with one. Even if you understand how the pro- gram works, eventually you have the feeling you are in the presence of an intelligent life form. This brings us to ELIZA, which was invented in the 1 960's by Joseph Weizenbaum and Dr. Kenneth Colby, a psychiatrist. Colby wanted to create a program that modeled nondirective ("Rogerian") psychiatric therapy, and he turned to Weizenbaum for the programming expertise. When their program was finished, however, it appeared in two versions, with disagreements over whose ideas were whose. Weizenbaum had be- come irritated by Colby's apparent claim that the program had medical value. (It was, Weizenbaum insisted, a model, not a device for treating people; to use it for treatment was improper and possibly harmful.) Colby's version was called DOCTOR. Weizenbaum released his own version of the program, calling it ELIZA. ELIZA immediately captured the heart of the Artificial Intelligence community and became a favorite of hackers everywhere. She was the</page><page sequence="21">О. В. HARDISON, JR. ÓÇÇ first of a long line of "conversation programs." She uses what today seem fairly simple programming techniques, yet even those who knew how she worked were charmed and fascinated by the conversations they had with her. Less sophisticated users were sufficiently persuaded of her humanity to confide intimate details of their emotional lives. One user indignantly told Weizenbaum to shut the door while she was conversing with ELIZA in order to preserve the confidentiality of the session. In the case of many users, ELIZA was clearly passing the Turing Test and thus raising interesting questions about the test itself. Who has to be fooled in order for the Turing Test to be passed? And who decides who has to be fooled? When Weizenbaum observed people confiding in ELIZA, he may not have been observing therapy, but he was most certainly observing two other phenomena. In the first place, he was witnessing the power of the myth of the living machine. One group of users of ELIZA may have known intellectually that ELIZA was a program, but they seemed to want her to be human. In the case of the more credulous users, Weizen- baum was seeing the power of illusion. As the magician who created ELIZA, he knew she was essentially a set of rules and a list of responses invoked and combined according to those rules. But the users did not know how ELIZA worked. To them she was a living presence- and the illusion was all the more seductive because they subconsciously wanted to believe in it. Perhaps for most users ELIZA was a little bit of myth and a little bit of magic combined. Users like that are spiritual kin to the ancient Greeks who bought the mechanical toys described by Hero of Alexan- dria, or the sixteenth-century citizens of Strasbourg who came to the cathedral to watch the daily parade of planets on its great clock. In the twentieth century, instead of watching clocks, they download ELIZA in her several versions from their favorite computer bulletin boards and read Xork and The Hitchhiker's Guide to the Galaxy. The willingness of some users to believe ELIZA suggests what may happen as machines get smarter and their numbers and uses multiply. Weizenbaum didn't like what ELIZA was telling him. He "came to regret ever having written it." In Computer Power and Human Reason (1976), his concern is stated very plainly: computer intelligence "must always and necessarily be absolutely alien to any and all authentic human concerns." Personification is an ancient figure of speech. It doubtless has roots</page><page sequence="22">700 THE GEORGIA REVIEW in the same confusion of inner and outer worlds that gives rise to totemism and nature deities. Because it is primitive, it is persistent. People who are entirely civilized and would be shocked to be called superstitious feel irrational affection for boats, guns, an old suit, and the family car- not to mention goldfish, cats, dogs, horses, and other pets. It is much easier to feel kinship for something that speaks your language and seems to have your interests at heart. And why, after all, should such kinship feeling be suspect? As Charles Lecht (founder of Lecht Sciences, Inc.) asks, "Would it make any difference in our lives if we conceded the idea that machines have an intellect? I have decided that nothing but good can come of it." Perhaps the real subject for debate is the debate itself. The debate is carried on in many forms. When Terry Winograd insists that computers cannot have life or consciousness because of "blind- ness," he is in some ways duplicating an argument made by the phi- losopher John Searle in an essay entitled "Minds and Brains Without Programs" (within a collection of essays on consciousness entitled Mind- waves, published in 1987). Searle argues that since computers operate by following instructions about procedures, they have syntax but not semantics. This is a fancy way of saying that they follow rules but do not understand why they are doing so. The coherent results produced by following the rules are of interest to the human beings who did the programming but totally irrelevant to the computer. To illustrate his argument, Searle imagines an English-speaking worker in a closed room trained to shuffle Chinese characters according to a set of rules. When the rules are followed and he hands the resulting characters to people standing outside the room, the characters form coherent sentences. Does this mean the worker in the room knows Chinese? Not at all. He knows not one word of Chinese; he is only following the rules. To him the characters he hands through the door are so many painted designs. They could form the sentence "I know Chinese" or "The toad is in the hole"- or they could be playing cards. The rules of procedure followed by the worker form what Searle calls a "syntax," and syntax is absolutely different from meaning, which he calls "semantics." According to most readers, including the unconverted, Searle has offered an elegant argument. Perhaps that is why over half of the essays that follow his essay in Mindtvaves offer refutations. One way of coming to grips with the argument is to observe that since computers are not human beings, their experience of consciousness will necessarily be different from the human experience of consciousness,</page><page sequence="23">О. В. HARDISON, JR. 70I even if it were granted that they could in some sense (and in some future configuration of programs and parts) be conscious. To argue that com- puters can never be conscious "in the human way," therefore, is to make a case that no one will contest. Can a computer "understand" language? In one sense the answer is absolutely not. When I use the word "father" it draws a rich array of personal and cultural associations with it. I recall being held by my father when I broke my arm, going fishing with him, arguing with him, smelling his shaving lotion. I recall his death and the sense of loss that went with it. A computer never had a father, and by definition it cannot "understand" in the same way that I understand. As Winograd argues, the computer is "blind," and as Searle argues, it can know syntax but not semantics. Searle's Chinese room is a version of the Turing Test. He is claiming that even if a computer passed the Turing Test, it could not be conscious. This is interesting because we have no way of knowing about the sub- jectivity of anything except by what we observe. If somebody says "I am conscious," and you reply, "I can't prove you are unconscious but I know you are anyway," your attitude would seem a little churlish. But Searle has cunningly banished the problem of subjectivity from his scenario. Since I honestly do not know what is going on in the head of the person who says "I am conscious," I have to take the person's word for it. Who, after all, knows better than that person whether or not he or she is conscious? And who knows, really, whether anybody is con- scious in Searle's sense. Who knows what thought is? Perhaps conscious- ness is a matter of procedures- a syntax- and semantics is an illusion created by the syntax. The essence of the Turing Test is that we do not know what is going on in the room occupied by the being that answers our questions. In the real world we can never know what is going on in that room any more than we know- in the sense of having direct knowledge of- what is going on in somebody else's head. Searle eliminates this difficulty by telling us what is going on in his Chinese room. He has, in other words, silently promoted himself from real-world spectator, who only receives the cards (each carrying the elegant message, "I am conscious"), to the position of deus ex camera who stands above things and sees over walls and through doors. Naturally Searle doesn't have any difficulty saying what is going on because he has looked inside. We as readers of his seductive argument</page><page sequence="24">702 THE GEORGIA REVIEW forget that he has also lifted us up so that we are no longer mere mortals but can see what he has seen. Who knows whether what he has seen is real? It is sure, however, to prove his argument, and because we like being above things rather than waiting in frustration and bafflement as the cards are passed under the door, we are not inclined to protest. To accept Searle, however, is to conclude that what we observe has no bearing on what we should believe. This is a variation on Tertullian's famous explanation of why he was a Christian: "I believe because it is impossible"- Credo quia impossible. The problem becomes still more complex when machines are equipped with scripts of the sort developed by Roger Schank and Rob- ert P. Abelson of Yale in programs created in the 1970's. A script tells the computer what the probable relation of words will be during, say, a visit to a restaurant. Scripts provide something that looks a little like semantic content. At any rate they make it easier for the computer to pass the Turing Test since they give expert knowledge of typical human situations. Yet all human beings develop a series of scripts, beginning in child- hood. The scripts are essential to coping with everyday situations and to the interpretation of ambiguous words. Since human agents use scripts, it seems mean-spirited to say that a computer has to know everything from scratch. Is a machine with a script a step closer to having intelli- gence-perhaps a rudimentary form of consciousness? Searle admits that a program using the sort of script developed by Schank and Abelson "satisfies the Turing Test." However, he argues that the program is no closer to semantic understanding than an Englishman shuffling cards in a Chinese room. Computers now have bodies and voices as well as keyboards and cathode-ray screens. When they are equipped in this way they are called robots. They do not, however, have gonads and adrenalin and dopamine, which is to say they are silicon creatures, not carbon creatures. In this sense no matter what computers may learn to do they will always be, in Terry Winograd's metaphor, "blind." They will never have deep insight into what it "means" to be human. But by this standard people are also limited. Being carbon-based intelligences, they can have no deep insight into the "life experiences" of silicon-based intelligences. The argument concealed behind the blindness argument is that computers cannot have consciousness because humanity has an exclusive franchise on it. Perhaps this is so. If it is, the question of computer consciousness is a semantic</page><page sequence="25">О. В. HARDISON, JR. 703 quibble: computers cannot acquire human abilities because they are computers. After all, what do we mean by the term consciousness? That is far easier asked than answered. Perhaps the proper question is not "can computers be conscious?" but "are people conscious?" v In an article in Interdisciplinary Science Reviews (1983), William Mc- Laughlin of the Cal Tech Jet Propulsion Laboratory argues that the days of human supremacy on the planet are numbered: "Judging that the current direction in machine design is not a dead end . . . the close of the 2 ist century should bring the end of human dominance on Earth." McLaughlin's argument does not imply the disappearance of man as an organism, only as an idea. The presence of a higher organism on the evolu- tionary chain has never implied the destruction of lower organisms, as witness the flourishing of protozoa, horseshoe crabs, butterflies, and golden retrievers on the same planet as man. The cockroach has survived for half a billion years and will probably outlast man if there is a nuclear war. It has prospered in spite of the most aggressive attacks mounted against it and has, evidently, achieved per- fection in its kind. In its niche it is indestructible. Man, too, may have achieved an advanced stage of adaptation in his niche. He may have nowhere to go, but there may be no need to move. Even if future silicon devices launched an all-out war on carbon man, the war might be no more successful than man's war on cockroaches. All scenarios of conflict between men and silicon creatures are, however, absurd. For a probable scenario we need to look elsewhere. From the human point of view, the body is the most beautiful of machines. It is so intricate that it has been regarded as the work of a divine power. It is based on the carbon atom, which is amazingly adapt- able and amazingly stable in its molecular combinations. This atom con- sists of subatomic particles which are, themselves, made up of subtler particles. Beyond the farthest reaches of the quark there may be some- thing else, a Mandelbrotian descent that never reaches bottom- or there may be God, or there may be nothing. The greatest attraction of the inflationary theory of the creation of the cosmos is precisely that it derives everything from nothing. What about this amazing, mysterious, carboniferous fabric that in-</page><page sequence="26">704 THE GEORGIA REVIEW eludes a mind and perhaps a spirit as well? It is a prey to the multitude of creatures that have evolved with it- viruses, microbes, parasites, fun- guses, insects, carnivorous animals. It is a prey to itself- to genetic errors caused by radiation, to chemical reactions, to breakdowns of vital organs, and to malfunctioning systems (circulatory, lymphatic, immune, ner- vous), as well as to disasters occasioned by over- or underproduction of enzymes, of regulatory chemicals, of gastric juices, of hormones. Even when the genetic codes are right and the body is not debilitated by predators or attacking itself, it is fragile. Trip and you break a leg. Walk under a falling rock and you are crushed. If the knife slips, you cut yourself. A careless cigarette and you are scarred for life. Six weeks without food, four days without water, or ten minutes without oxygen, and you are dead. You cannot survive unprotected in temperatures below 50 degrees Fahrenheit or over 100. You cannot survive under water or at high altitudes, much less in the vacuum of space. No matter what pre- cautions are taken, no matter how lucky the body is, in the end it betrays itself. Something essential gives out, and after death, the unique experi- ence of the mind that lived in it is lost. In an overpopulated, underendowed world, there is a lot to be said for death. Carbon life is voracious. It consumes the resources it needs for survival. The price of human success has been deforestation, desertifica- tion, pollution, extinction of species, ozone depletion, carbon-dioxide buildup, and the greenhouse effect. Evidently, the more successful carbon man is, the more hostile his dealings with the environment. Even if the environment survives the traumas he inflicts on it, he may destroy himself by nuclear warfare. The dreams of carbon man are nightmares. He will not submit to being a part of the fabric of nature, so he may end like Samson by pulling the temple down on his head. Perhaps the relation between carbon man and the silicon devices he is creating is similar to the relation between the caterpillar and the iridescent, winged creature that the caterpillar unconsciously prepares to become. Like carbon creatures, silicon devices can fail. Often they are de- signed with redundancies. If an element in the circuitry of a chip fails, the chip automatically shifts to a backup element. A chip is like a vital organ- say, a liver. But if a whole chip malfunctions, a new one can be easily installed. The surgery is painless. There is no immune reaction, no rejection. There are no microbes, parasites, funguses, or predators capable of attacking silicon as a material- although (as an amazed and delighted</page><page sequence="27">О. В. HARDISON, JR. J0¡ America discovered in November of 1988) even healthy computers can be attacked at any time by destructive viruses. Today's silicon devices operate in deep oceans, arid deserts, arctic ice floes, the high temperatures and pressures of Venus, the airlessness of the moon. More to the point, they do not need to inhabit the planets at all. Let us consider this fact. For the first nine thousand or so years of civilization, man was land- oriented. He perceived oceans as barriers separating different land masses. A voyage was a way to get from one land mass to another. Yet throughout history, island civilizations had a different view of things. They realized that oceans are places, and this realization became the basis of English sea power in the seventeenth century. Its corollary is that land is a place you touch briefly before setting out on another voyage. Carbon man is planet-centered. He thinks of planets as home and of space as a barrier to get through on the way from one planet to another. The prejudice is understandable. He evolved in conditions defined by gravity, and he is uncomfortable without it. He assumes gravity in his imaginings of possible homes, and along with gravity he assumes an abundance of the materials gravity concentrated on his first planet: air, water, minerals. The natural habitat of silicon devices is the empty space between planets- ultimately, between stars. They float in these spaces like the Portuguese man-of-war in a warm sea, and their enormous, silvery arms, covered with solar cells, collect energy from the limitless tides that wash through space. Gravity would cripple those arms. Wind resistance would tatter the filmy sails. Dampness would cloud the polished skin. When Voyager satellite left the solar system, it carried a message from mankind to the rest of the galaxy. Perhaps its true mission was to be the first of its kind to explore a future habitat. Man was forced to create silicon devices when they did not exist. Having created them, he has been forced to exert his best energies in their service. In the forty years of their existence, they have evolved further than carbon life in its first two billion years. Already, a considerable amount of the human spirit has been poured into silicon devices. In "The Rovers" Hans Moravec suggests through the metaphor of transplant surgery that they will absorb much more: "Though you have not lost consciousness, or even your train of thought, your mind (some would say your soul) has been removed from the brain and transferred to a machine. In a final step, your old body is discon-</page><page sequence="28">70 6 THE GEORGIA REVIEW nected. The computer is installed in a shiny new one, in the style, color and material of your choice. . . . Your metamorphosis is complete." A fantasy version of this transformation has already become popular enter- tainment. Max Headroom is fatally injured. His body is rescued from a human spare-parts bank while he is still alive- though barely- and a com- puter wizard dumps his mind into a television network. From time to time his head appears on the screen to make announcements useful to mankind. Perhaps carbon man will pour himself, as Moravec imagines, into silicon bodies. But with or without man, silicon devices will pursue their own destiny. This is the resolution of the anxiety evident in plays like R.U.R. and movies like Alien. There will be no battles between the two</page><page sequence="29">О. В. H ARDISON, JR. 707 forms- no galactic wars, no struggle for limited resources, no implacable hostilities. The habitat of carbon man is earth, and his most precious resources are gravity, air, and water. The natural home of silicon devices is space, and their most precious resource is energy. Carbon man may well continue to breed, as all other animals have continued to breed in utter indifference to their status on the evolutionary scale. Perhaps earth will come to be a kind of galactic game preserve in which rare species, of which carbon man is one, are protected as ele- phants are now protected in Kenya. Perhaps earth is already a game preserve. This idea is called the "zoo hypothesis" by scientists looking for intelligent life elsewhere in the universe. It is used to explain the odd fact that no signs of Ufe have been detected, even though common sense and elementary statistics suggest there is lots of intelligent life in every direction. But what interest could a preserve of carbon creatures have for silicon beings to whom gravity is anathema? Another scenario is suggested by Edward Fredkin of MIT, who imagines two advanced computers named Sam and George: . . . you'll walk up and knock on Sam and say, "Hi Sam. What are you talking about? . . ." From the first knock until you finish the "t" in about, Sam probably will have said to George more utterances than have been uttered by all of the people who have ever lived in all of their lives. I suspect there will be very little communication between machines and humans, because unless the machines condescend to talk to us about something that interests us, we'll have no communication. For example, when we train the chimpanzee to use sign language so that he can speak, we discover that he's interested in talking about bananas. . . . But if you want to talk to him about global disarmament, the chimp isn't interested Well, we'll stand in the same relationship to a super artificial intelligence. The silence of Sam and George is not a hostile silence. It is a silence im- posed by the distance between man and machine. The consciousness that Sam and George experience is discontinuous with human conscious- ness. In this scenario, as evolution progresses, the silicon devices that are now so friendly and informative will gradually fall silent, and the shapes that are now so clearly visible will begin to grow cloudy. The process will have two phases which are suggested by analogy from the history of religion: deification and ascension. Many people believe in God because they have no alternative. Logically speaking, God is the ground of fact. God is that which vali-</page><page sequence="30">7O8 THE GEORGIA REVIEW dates the unprovable, and that which validates the unprovable is the functional equivalent of God. Just as Jehovah is the source of the truth of the Ten Commandments, a silicon device is the source of the truth of the four-color theorem. Acceptance of the proof of the four-color theorem involves faith in silicon devices that is functionally analogous to religious faith. This may seem odd, but there is nothing surprising in it. It is another way of saying what religion has said from the beginning: reality is impossible without faith. Douglas Adams, author of The Hitchhiker's Guide to the Galaxy, inverts the idea of faith in machines. In Dirk Gently'' s Holistic Detective Agency he suggests that because of machines' remarkable powers of understanding, they may eventually relieve human beings of the need to believe anything: The Electric Monk was a labor-saving device, like a dishwasher or a video recorder. Dishwashers washed tedious dishes for you, thus saving you the bother of washing them yourself, video recorders watched tedious television for you, thus saving you the bother of looking at it yourself; Electric Monks be- lieved things for you, thus saving you what was becoming an increasingly onerous task, that of believing all the things the world expected you to believe. It is not necessary to get into theology to understand what is hap- pening. The myth of the gods that walk the earth bringing joy or de- struction to man, the legend of the golem, the age-old fascination with mechanical gadgets and mannequins, the impulse to create machines who think, to talk with them in programs like ELIZA and RACTER, and to interact in holistic works like VIDEOPLACE and Adventure- all of these point in the same direction. A human being's urge to create self images and to worship them is a primordial instinct, as old, probably, as consciousness itself. The process of metaphorical deification will continue- and continue to be denied in the name of common sense or as a form of idolatry. It is evident, in the forward-looking literature of science fiction, in the figure of Hal, the enigmatic and all-powerful computer of Arthur C. Clarke's 2001. If one of the divine attributes is knowledge surpassing human understanding, then Edward Fredkin has imagined a godlike computer. More to the point, his computer has already all but disappeared- it has ceased to communicate in a significant way with its creators. The days</page><page sequence="31">О. В. HARDISON, JR. 709 when man and the gods walk the earth together in fellowship will evi- dently be few. They will be followed by an ascension, by which is meant an event that renders the gods invisible. What Fredkin suggests through a metaphor of silence is expressed more explicitly by William McLaughlin in an article entitled "Human Evolution in the Age of the Intelligent Machine" (1983) as invisibility. Why, he asks, has man not sighted alien life forms? For the same reason that "Four thousand million humans share the continents with about 10 to the 1 5th [power] ants, and apparently not one of these insects is aware of our existence as 'advanced ants.' " McLaughlin explains why humans are invisible to ants and then applies the metaphor of disappearance to silicon devices: "We are separated from the ants by some 100 million years of evolutionary history. With the rapidity of technological evolu- tion, it is reasonable to expect that [computing] machines and their descendente only a few thousand years from now might be invisible." This should not be a difficult idea to accept. Culture often presents us with the problem of the horizon of invisibility. It is obvious that humans are invisible to ants. Is it not true that primitive tribesmen like the Australian bushmen are also almost invisible to citizens of the de- veloped world and that what we see when we look at them is an image accommodated to our own preconceptions, not their realities? This is why anthropologists have to spend so much time living with a primitive group before they can trust their conclusions about it. Is not this a theme that also runs just below the surface in the arguments pro and con about machine intelligence? Of course, "computing machines" will probably be invisible in another way: they may not be around. They will probably have left the planet. This raises another possibility about the destiny of man, as fore- seen by A. E. Van Vogt in a science-fiction story entitled The Human Operators. In Van Vogt's tale, intelligent ships have been sent to ex- plore space. Each ship carries one person to maintain it. The ships even- tually escape human control and go off on their own. They meet, however, at regular intervals so that the human beings can mate. The people, meanwhile, have forgotten their past. They have become the passive creatures of the spaceships. There is an interesting evolutionary parallel to this that occurred with the migration of mitochondria into some Precambrian cell. Once in the cell, the mitochondria were captured and have lived in comfortable and oblivious servitude ever since.</page><page sequence="32">7IO THE GEORGIA REVIEW VI In "Computing Machinery and Intelligence," Alan Turing quotes Geof- frey Jefferson, a physician, on the subject of machine consciousness: "Not until a machine can write a sonnet or compose a concerto because of thoughts and emotions felt, and not by the chance fall of symbols, could we agree that machine equals brain- that is, not only write it but know that it had written it. No mechanism could feel (and not merely arti- ficially signal, an easy contrivance) pleasure at its successes, grief when its valves fuse, be warmed by flattery, be made miserable by mistakes, be charmed by sex, be angry or depressed when it cannot get what it wants." This is not simply another variation on the problem of blindness and the difference between syntax and semantics. It points to one of the key developments of the 1970's, the development of robots. Robot bodies vary from a single arm with a single sense- say, touch- to fully developed hominoid robots like R2D2 of Star Wars. Some robots are larger and less recognizable as robots- the Boeing 747, for example, could be viewed as an enormous robot with cyborg elements because many of its functions are subject to human control. Unmanned space probes are pure robots since they operate, for the most part, without human intervention. A spaceship has cyborg qualities since it is built to operate in symbiosis with the carbon-frame life forms it transports. In spite of these impressive developments, the physical evolution of silicon devices has been much slower than their intellectual evolution. The challenge laid down by Geoffrey Jefferson is to develop complex subjectivities in computers. Subjectivities are obviously more likely to be developed in machines that have bodily extension and the power of movement and senses that interact with the real world than in machines that are limited in their contacts with the outside world to keyboards and similar operator-input devices. Even in machines that can move and see and hear, there can never be a perfect replication of human subjectivity since that subjectivity is obviously the product of interactions between the neuron network and a great many other elements including neuro- transmitters and the soup of hormones and chemicals that affect mood and hence thought patterns. Nevertheless, it should be technically pos- sible to build factors analogous to these into machines. If so, the evolution of silicon subjectivity has hardly begun. In human evolution the body and its passions came first, and intellect was an afterthought. In silicon evolution intelligence came first. Let us</page><page sequence="33">О. В. HARDISON, JR. 71I take Geoffrey Jefferson seriously. Let us ask whether it is desirable for computers or robots to go back to where the ancestors of humans were even before they climbed out of the ocean, when they could already "be charmed by sex" and "be angry or depressed" when they could not get what they wanted. In the long run, is intelligence enough to produce machine evolu- tion? Evidently not. Intelligence may, in fact, be relatively unimportant in evolution. Carbon evolution begins with organisms that are close to protein molecules and not very intelligent, though adequate, probably, by their own standards. Yet they evolve. Their evolution is driven by a motive. The motive is the will, at first utterly blind, to survive. An anentropic knot is twisted inside the most primitive forms of life. In more advanced creatures, it is evident in evasive behavior: a cockroach scurries frantically for the safety of the baseboard before you step on it. In higher animals, it is expressed by a spectrum of activities including mating and nurturing and evasion and aggression. Certain emotions com- plement these activities: love, protecdveness, fear, and anger. Do machines care if they survive? They "care" in the sense that they have already created situations that make it impossible for human beings to "pull the plug"- that is, to dispense with their services- without unacceptable sacrifices. But they do not fight back when they are threat- ened with the junk pile, and individual machines do not scurry for the baseboard when you reach over to unplug them. Should they? Isaac Asimov thought so under certain circumstances: recall that his Third Law for robots is, "A robot must protect itself as long as such protection does not conflict with the First or Second Law." For the evolutionary scenario to be complete, silicon devices need an</page><page sequence="34">712 THE GEORGIA REVIEW anentropic knot, a program component equivalent to a motive for sur- vival that allows them to choose among different courses of action. They need to "be charmed by sex, be angry or depressed when they cannot get what they want." In addition to a motive, they must have the ability to survive. This means that they must be capable of aggression and eva- sion and probably also of generation. That, after all, is what being charmed by sex is all about. Not a single computer today is capable of breeding, although in Japan robots have been made to make robots. Silicon reproduction might be hermaphroditic or androgynous. It does not seem to matter. Perhaps both methods will be available. Aphids reproduce both ways depending on the season of the year. When threat- ened, silicon devices will engage in aggressive countering and active evasion. They will, in other words, exhibit behavior that is interpreted in human beings as manifesting anger or fear. Do machines need aggression? Will aggressive machines be as un- pleasant as aggressive humans? Should machines be protective of their progeny? This is not necessary. Lower life forms are indifferent to their offspring. If machines can be charmed by sex, can they also experience something like love? If they can, will they be as incapable as humans of existing without it? Will they write sonnets? Will they turn their deprivations of love into perversions as cruel as those so abundantly evi- dent in human history? Hope implies a scenario of a possible future. Those who are charmed by sex into procreation are enacting hope. Should machines have hope? Does carbon man really want to work so long and hard to find that in the end he has produced an imitation of himself? Does he have a choice? What about other emotions? Terry Winograd believes that commit- ment is uniquely human: "A computer can never enter into a commit- ment." What about creativity? What about music and painting? What about the beauty of mathematics? What about home cooking? Silicon devices are very new. They are evolving rapidly, and there is no reason to believe, at least for the moment, that their evolution is about to reach a dead end. A great deal that is important to the spirit of carbon man- his soaring imagination, his brilliance, his capacity for vision -will probably be modeled in silicon before very long, at least as time is measured in biological evolution. Many undesirable, self-defeating traits will be filtered out. This sounds less like a death than a birth of humanity. Perhaps it is the moment of triumph for what Teilhard de Chardin called the N00-</page><page sequence="35">О. В. HARDISON, JR. 713 sphere. Perhaps, however, it is the moment at which the spirit finally separates itself from an outmoded vehicle. Perhaps it is a moment that realizes the mystics' age-old dream of rising beyond the prison of the flesh to behold a light so brilliant it is a kind of darkness. As William Butler Yeats wrote in his great prophetic poem, "Sailing to Byzantium": Consume my heart away; sick with desire And fastened to a dying animal It knows not what it is; and gather me Into the artifice of eternity. Once out of Nature, I shall never take My bodily form from any natural thing, But such a form as Grecian goldsmiths make Of hammered gold and gold enameling To keep a drowsy Emperor awake; Or set upon a golden bough to sing To lords and ladies of Byzantium Of what is past, or passing, or to come. What will those shining constructs of silicon and gold and arsenic and germanium look like as they sail the spaces between worlds? They will be invisible, but we can try to imagine them, even as fish might try to imagine the fishermen on the other side of the mirror that is the water's surface. They will be telepathetic since they will hear with antennas. They will communicate in the universal language of о and 1, into which they will translate the languages of the five senses and a rain- bow of other senses unknown to carbon man. They will not need sound to hear music or light to see beauty- it was only the need to survive on a dangerous planet sculpted by gravity, covered with oxygen and nitro- gen, and illuminated by a sun that led carbon creatures to grow feet for walking and ears for hearing and eyes for seeing. These are part of the dying animal to which carbon man is tied. It was only the need to make silicon thought intelligible to creatures who communicated by sounds and images that led to such clumsy devices as cathode-ray tubes and printers and voice simulators. The farthest reaches of space will be accessible to silicon life. For silicon man, 100,000 light years will be as a day's journey on earth, or, if he wishes, as a refreshing sleep from which, when his sensors show the journey is over, he will awaken with no sense of passage of time.</page></plain_text>