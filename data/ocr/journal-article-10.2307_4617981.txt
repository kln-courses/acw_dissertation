<plain_text><page sequence="1">Jorge Solis, Keisuke Chida, Koichi Taniguchi, Shinichiro Michael Hashimoto, Kei Suefuji, and Atsuo Takanishi Mechanical Engineering Department, Waseda University Takanishi Laboratory 59-308 3-4-1 Ookubo, Shinjuku-ku 169-8555. Tokyo, Japan solis@kurenai.waseda.jp kei-c@fuji.waseda.jp kouichi-tani.0@asagi.waseda.jp {michaeltc2-ny, s_kei)@ruri.waseda.jp takanisi@waseda.jp The Waseda Flutist Robot WF-4RII in Comparison with a Professional Flutist The development of anthropomorphic ("humanoid") robots is inspired by the ancient dream of humans' replicating themselves. However, human behaviors are difficult to explain and model. Owing to the evolution of computers, electronics, and signal pro- cessing, this ancient dream is becoming a reality. In fact, current humanoid robots are able to perform activities such as dancing (Ogura et al. 2004) and playing musical instruments (Kato et al. 1987; Kaji- tani 1989), etc. However, these mechanical devices are still far from understanding and processing emo- tional states as humans do. Research on musical robotics seems like a partic- ularly promising path toward helping to overcome this limitation, because music is a universal com- munication medium, at least within a given cul- tural context. Furthermore, research into robotic musical performance can shed light on aspects of expression that traditionally have been hidden be- hind the rubric of "musical intuition." Several researchers have developed different mu- sical performance robots. One of the first attempts to develop a musical robot was undertaken by Waseda University, in 1984, with the WABOT-2 (Kato et al. 1987). This robot was capable of playing a keyboard instrument and reading a standard musi- cal score. Kajitani (1989) developed the Musician Robot (MUBOT), which can play a recorder, violin, or cello automatically. The MUBOT was developed under the premise that music should be played by a robot without remodeling the musical instruments in any way. Alford et al. (1999) developed a robot for playing the Theremin, which is an electronic musi- cal instrument that is played without physical con- tact. Singer, Larke, and Bianciardi (2003) introduced the GuitarBot, which was designed to be a respon- sive robotic stringed instrument controllable via MIDI. This robot is composed of four independent MIDI-controllable, single-stringed movable bridge units. Dannenberg et al. (2005) introduced the McBlare, which is a robotic bagpipe player. This ro- bot plays a standard set of bagpipes using a custom air compressor to supply air and electromechanical fingers to control the chanter. In 1990, research into an anthropomorphic robot flutist began at Waseda University. The main goal of our research is the understanding of the motor- control process required to play the flute (from an engineering point of view). Additionally, we propose novel ways of interaction between human and the robot, such as real-time interactive performance (Okuma et al. 2003) and transferring flute skills to beginners (Solis et al. 2004). Development of the Waseda Flutist Robot The first version of the anthropomorphic robot flutist, the WF-1, was developed in 1990. This ver- sion reproduced the human lung system by using a bellowphragm with a piston and cylinder mecha- nism. The "lungs" are actuated by a DC servomotor and a ball screw to control the velocity of the air beam (see Figure 1). The robot could synchronously perform with MIDI accompaniment data by com- bining its control system with a MIDI-processing unit. In 1992, the WF-2 included fingers that con- trolled the relative position of the mouth (i.e., the embouchure). Here, a linear actuator and a ball Computer Music Journal, 30:4, pp. 12-27, Winter 2006 ? 2006 Massachusetts Institute of Technology. 12 Computer Music Journal</page><page sequence="2">Figure 1. The Waseda Flutist Robot series. WF-1 WF-2 WF-3RIV 1990 1992 1998 WF-3RIX WF-4 WF-4R 2002 2003 2004 screw were used to control the width and thickness of the mouth mechanism. The human tongue was also reproduced by an actuated rod and a voice-coil motor. In 1994, the WF-3RIV was introduced, and it improved the lip mechanism, enabling the con- trol of thickness, width, and angle of the air beam (Takanishi, Sonehara, and Kondo 1996). The play- ing attitude mechanism was improved by fixing the flute on a holder and moving it with three de- grees of freedom (DOF, where degrees of freedom refer to the number of directions a robot can move a joint). In 1998, the WF-3RIX (Okuma et al. 2003) was de- signed to reproduce, as realistically as possible, every single human organ involved in playing the flute. This includes lungs for breathing, lips for shaping the air stream, the relative positioning mechanism between the embouchure hole and the lips, fingers that can create a trill, and the ability to double-tongue and produce vibrato. A newer ver- sion, the WF-4, was built in 2003; this version im- proved many aspects of the human-like shape and the flute-playing attitude. The lips (five DOF) and neck mechanism (four DOF) have been implemented to enhance the flute-positioning accuracy (Chida et al. 2004). Using a voice-coil motor for the vibrato system, the amount of vibration added to the air beam was increased. The lung system was improved by using two piston cylinders, each of them driven by ball screws installed in the cylinders. Finally, in 2004, the WF-4R was developed (Sue- fuji et al. 2004; Solis et al. 2005), and the arm mech- anism was added to assure the repetitiveness of the flute positioning. A volume-level control system was implemented so that the robot could expres- sively perform by using a human performance as a reference. In the beginning of 2005, the newest version of the Waseda Flutist Robot, the WF-4RII, was success- fully implemented. In the following sections, we de- scribe the mechanical components of the WF-4RII in relation to its human counterparts. We then ex- plain how the robot is programmed to perform a musical score. In particular, the algorithms used to generate what we call music data and robot data are explained. Music data are obtained by extracting musical parameters from the performance of a hu- man player. Robot data are obtained by finding the so-called "General Position," whereby the system searches for the optimal parameters of the robot's mechanical components to play each note using the same lip shape and lung velocity as the human per- former. Finally, an experimental setup is proposed to compare, using a statistical analysis of performed acoustical sounds, how well the WF-4RII can mimic the performance of a professional human flutist by using the proposed algorithms. Waseda Flutist Robot Number 4 Refined II For the Waseda Flutist Robot Number 4 Refined II (WF-4RII), we improved the design of the humanoid organs involved in flute playing, such as the robotic lips, lungs, arms, neck, tongue, oral cavity, and vi- brato system (see Figure 2). This robot has a total of 43 DOF, and each robotic body part has been de- signed to mimic its human counterpart (see Table 1). Solis et al. 13</page><page sequence="3">Figure 2. The Waseda Flutist Robot Number 4 Refined II (WF-4RII). p ,' f t ?;~ I ` :UI' a Table 1. Comparison of the Sizes of Human (Japan- ese Adult Male) and Robot Parts Body Part Measurements (mm) Human Body Robot Face Length 239 240 Width 162 150 Depth 188 200 Arm Upper 307 355 Lower 239 300 Hand 189 231.2 (wrist-fingertip) Foot Upper 397 320 Lower 465 490 Shoulder Height 1371 1500 Width 398 705.5 Trunk Height 1026 1120 Width 256 380 Depth - 200 Total Height 1698 1850 Figure 3. The lips. C~ r5 coQ The Mechanical System The robot's mechanical system comprises nine pri- mary parts: the lips, lungs, arms, neck, fingers, tongue, eyes, ears, and vibrato system. The Lips The shape of the lips controls the air stream that is directed into the flute embouchure. Ando (1970) de- scribed five parameters that control the air stream: width, thickness, angle, length, and velocity. Slight variations of these parameters are reflected in the pitch, volume, and tone of the flute sound. The lip system has a total of five DOF: an upper lip with three DOF (inside, outside, forward) and the lower lip with two DOF (forward and inside). This mecha- nism enables control of the width, thickness, angle, and length of the air stream with a high degree of accuracy (see Figure 3). The Lungs The lung system is composed of two vane mecha- nisms, each of them with one DOF. This system 14 Computer Music Journal</page><page sequence="4">Figure 4. The lungs. Figure 5. The arms. Vane Mechanisms Air flow valves Front view Rear view Figure 4 flS Roll ou er rist Roll Pitc h houlder ris ' Pitch Yaw ouicie Yaw PtOW Pitchh Figure 5 is actuated directly by an AC motor to increase the control accuracy and the inhalation speed (see Figure 4). The mechanical noise during the breath- ing process has been considerably reduced com- pared to the previous version. Furthermore, an air-valve mechanism (with one DOF) has been designed to control the opening and closing of the air-flow valves located on each vane mech- anism. The Arms The correct holding and positioning of the flute dur- ing performance enables the player to produce a clear sound in tune. Humans use their arms and hands to hold and position the flute. Thus, the ro- botic arm system has been implemented like a hu- man's, with a total of seven DOF: the upper arm has four DOF, the forearm has two DOF, and the hand has one DOF (see Figure 5). Solis et al. 15</page><page sequence="5">Figure 6. The neck. Ir UFer itch Yaw Lower pitch Roll Front view The Neck The neck helps to correct the posture during flute performance. The neck system has a total of four DOF (upper pitch, lower pitch, yaw, and roll) to ex- pand the range of positioning and attitude control of the lips (see Figure 6). A harmonic drive is used to assure the positioning control. The Fingers The correct combination of fingers while pressing the flute keys is required to produce the desired note. Twelve DOF compose the finger system of the robot: the left hand with five DOF and the right hand with seven DOF. Each finger is composed by a wire drive coated in Teflon (designed to have a low coefficient of friction) from a pulley connected to the finger, which is controlled by a DC servomotor (see Figure 7). The white color in the robot's finger- tips is silicon RTV rubber (Shin-Etsu Chemical Company KE12; Tokyo, Japan). The Tongue Tonguing is an important articulation feature dur- ing flute playing that helps the player to shape notes and produce a clean sound (especially when playing high notes). In particular, we were interested in sim- Figure 7. The fingers. Figure 8. The tongue. Fingertip Flute Mounting d Wire drive Figure 7 .. . ........... . ...... Air stream Double Tonguing Mechanism I Side view mira ...i Figure 8 ulating the double-tonguing technique. Double tonguing uses two parts of the tongue to produce E "T" sound followed by a "K" sound. Therefore, th tonguing system was implemented inside a case with acrylic plastic, similar to the oral cavity of a human (see Figure 8). A rod fixed on the axis of the rotation pushes the cam follower of the tongue (mounted on the slide table) to convert circular mc tion into linear motion. The Eyes The robot's eye system is composed of three DOF, corresponding to the yaw direction of each eye anc the pitch, which is common to both eyes (see Fig- ure 9). A gimbal mechanism was selected to reduc the mechanical size. The range of motion of the ey 16 Computer Music Journal</page><page sequence="6">Figure 9. The eyes. Figure 10. The ears. Figure 11. The vibrato system. Front vie Ramer Pitch Right Ya Left Yaw Figure 9 GT33 lAudioSport Duol Figure 10 mechanism was designed to match that of most hu- mans. Two charge-coupled device (CCD) cameras were attached to enhance the interaction between the robot and the human. A human face-tracking al- gorithm was proposed to maintain visual contact with the audience. The Ears The robot flutist is equipped with a dynamic micro- phone (Groove Tubes GT33; San Fernando, CA) connected to the M-Audio Audiosport Duo audio recording system (see Figure 10). The incoming sound is recorded with 16-bit resolution at 44. 1k Hz. The Vibrato System Vibrato can be implemented in terms of frequency or intensity. Girtner (1980) experimentally deter- mined that the throat has more influence than the Voice coil motor To oral cavity SAir stream Rubber tube Side view diaphragm in producing vibrato. Therefore, the throat method was chosen and implemented using a voice-coil motor (see Figure 11). Vertical motion produced by the coil motor is used to press directly on a rubber tube where the air stream flows from the lungs to the oral cavity. Solis et al. 17</page><page sequence="7">Figure 12. Musical perfor- mance system. b P. Music Data Robot Data Performance Posture Parameter Control Data Control Data MIDI Tone Generator Module Timing Clock MIDI Data Synchronize WF-4RII PC Sequencer PC Controller TCP/IP PC Vision Musical Performance System The musical performance of the robot flutist is fa- cilitated by a computer controller, a software se- quencer, and a vision system. The computer controller sends all command instructions to the ro- bot's hardware. The software sequencer produces both the timing clock to manage all music informa- tion and the accompaniment data sent to a MIDI tone-generator module. Finally, the vision system is used to process all visual information acquired through the CCD cameras (see Figure 12). The computer controller has several data acquisi- tion boards to control each of the degrees of freedom of the robot and to acquire the information from en- coders and sensors (photo sensors and limit switches) of the robot. The control software was developed us- ing Borland C++ Builder, and the resolution time of the control software is 5 msec. In particular, the controller is made up of the following hardware: six Interface Corporation (Hiroshima, Japan) PCI 6205C boards for the acquisition of data from encoders; four Interface Corporation PCI 3346A analog output boards for controlling each of the simulated organs; and one Interface Corporation PCI 2752A digital I/O board for the acquisition of the information from the photo sensors (used to detect the home position of the robot) and limit switches (used to detect the limits of the motor-axis position). From the sequencer, MIDI signals are sent via a Yamaha UX256 USB MIDI Interface connected to the host PC. The tone generator, a Yamaha MU2000, is connected to several amplifiers and speakers. Communicating the MIDI sequencer with a MIDI driver for Windows (MDW) and the PC controller enables synchronization of all musical information. The sequencer controls a MIDI tone-generator mod- ule that manages the time information of the mu- sic. The performance "start" signal generated by the sequencer is sent to the MDW, and it generates the start/stop signals and timing clock messages that are sent to the computer controller (see Figure 13). The computer receives an interrupt message from the MDW, and if the message is "start," "stop," or "timing clock," then the computer controller pro- cesses the message and updates the position of each mechanical component of the robot. To perform a desired score, the robot requires both music data and robot data, which should be processed off-line before the performance (see Fig- 18 Computer Music Journal</page><page sequence="8">Figure 13. Synchronization process of the musical in- formation. Figure 14. Example of (a) music data and (b) robot data. PC Controller Roland MIDI Driver for Microsoft Windows START Open MIDI Input Device Re uirce Open Device MIDI Callback Function Setup SeJp / 11START LOAD Robot Control Data Performance / OPEN Sequencer I L .1Start MIDI In Start PPerfomance SMIDI START MIDIMessages TimingClock IPerformance I Timing Performance Handlerocess - Pin Process Cloc Process NooEnd-flag On Cp , END Performance PerformanceStop Stop Sequence IJ1" JL Data MIDI In Stop &amp;Reset CLOSE 1 - 7]MIDI Sound Close MIDI Input Device I END Module (I I Figure 13 Lip 1 Lip2 ...Lung C I x X X  i C# r* * 1 .. . . D x X i Time Status Key Velocity D# V,r x 1 X )i J -Y 4 3 )1 X ? 4 E F# , . . . . G G# X Y G i x . I MIDI Data Robot Data Figure 14 ure 14). Music data include musical parameters of the score (i.e., timing, key, velocity, breath, vibrato, and tonguing), and robot data include the parame- ters of each mechanical component of the robot (i.e., lip position, tonguing and breathing points, lung velocity, etc.). Music data are obtained by extracting musical features from the recording of the performance of a professional flutist of the desired score. In this way, the expressiveness of the robot's performance is en- hanced (see Figure 15). This task is performed by an- alyzing the recording in the frequency domain using Fast Fourier Transform (2,048 points, 50% overlap with a Hanning window). The Sound Analysis Tool developed in 2002 (Okuma et al. 2003) performs the analysis. The Sound Analysis Tool extracts musical features of the desired score off-line using a record- ing of a professional flutist's performance (recorded at 16-bit, 44.1k-Hz monaural) and the correspon- ding MIDI data (which are created by hand using the CakeWalk music editor). To extract features from the recording of the hu- man performance (see Figure 16) involves four steps: 1. First, we analyze the MIDI data of the score. The timing, key, notes, and velocities are searched on the acoustic data recording us- ing the Sound Analysis Tool. At the same time, the volume of each identified note is calculated. This corresponds to the root mean square (RMS) value of the FFT coeffi- cients computed in each frame. 2. The breath points of the performance are then denoted when the volume level of the Solis et al. 19</page><page sequence="9">Figure 15. Diagram of the autonomous mimetic data generation. Figure 16. Screen shot ob- tained from the Sound Analysis Tool. H umSequencer Robot Human PC Performance Performance Music Control PC Data Timing Synchronize Key No. Music Data Wevale Deita I Velocity Sound Breath Robot Data Analysis Vibrate Tool F[Tonguing MIDI Data Figure 15 Length ' 2 ? XAAA 32 2a 12 AA 22A x4a a E4 'M 94 05 A4'ES I FU .05 CisExM .4M I IMU Key No. Aw :E4 4 I c4  Es i a as 4 a6 C 4 eM:ad a:" T im e lag ........................... ....................................................................................... Sound IN 0 a26 265 30a 62a 22 2 3 34 23 1024 1x Quality ... V elocity ..........7............. .. .......... 4 .. ....8....'7--. ...............5 -7.......... ............. 42...- 4 ....... 42--4,...7-4.... 0liMVelocity ......6............2 2M 0 86w4645 Volume Avg. dv I dt dv2/dt2 Tonguing Tonguing i ViVibro Vibrato 1 1 00 Figure 16 performance is located below a fixed thresh- old (see Figure 17). 3. Vibrato is then searched within the zone where the note was detected. The amplitude and frequency of the vibrato is then decided by computing the variation of amplitude of the volume level (see Figure 17). 4. Finally, the tonguing points are decided only when the volume level is below a threshold (see Figure 17). Robot data, on the other hand, are obtained by finding a basic position where the robot can blow a the tones with high quality using the same shape c lips and lung velocity. Such a position is called the "general position," inspired by the way a profes- sional flutist prepares before a performance. Huma players reach such a position by adjusting the pa- rameters of the lips and lung while continuously blowing a simple etude until producing notes with uniform quality. Inspired by this principle, the rob 20 Computer Music Journal</page><page sequence="10">Figure 17. The analysis of the vibrato, breathing, and tonguing features. 80 -- -- -- -- - ----- .- --------------- o 60 80 E 40 &gt; 20 Vibrato o 0 500 1000 1500 2000 Time [msec] 80 -( -ion uin 0 500 1000 1500 2000 Time [msec] flutist performs a calibration phase to find the opti- mal parameters of the mechanical components. The algorithm used to find the general position is shown in Figure 18a. First, a phrase (see Figure 18b) is programmed and performed by the robot using a set of initial blowing parameters (the "home posi- tion"). The parameters used by this algorithm in- volve the lung, the three DOF of the upper lip (inside, outside, and forward), and one DOF of the lower lip (inside). The remaining degree of freedom of the lower lip (forward) cannot be changed, as it should always remain in contact with the flute. The values of the other parameters of the robot (i.e., arm, finger, and neck position) are fixed during the initialization of the control system (home position), which is per- formed prior to the start of each general-position procedure. After the phrase is performed, the musical score is processed and evaluated using the Sound Analysis Tool. This tool is useful for extracting different sound features by analyzing the pitch, loudness, and harmonic structure content of acoustic audio data. Finally, from such features, all information is inte- grated into a sound-quality evaluation function, which is based on the experimental results from Ando (1970): Eva (M-H)+(Le - Lo) V Figure 18. (a) Flow diagram of the calibration phase; (b) etude used during the calibration phase. Start Select Phrase I Step 1 Set of Beginning Parameters to Search Play Phrase Ste 2 according to Orthogonal Table Step 2 Evaluate Using Sound Analysis Tool No . S3ep 3 Play 27 Times ? . . Determine Suitable Parameters Step 4 End a) b) where M is the average value in dB of all harmonics, Le - Lo is the amplitude difference in dB between the average values of even and odd harmonics, H is the average amplitude in dB of the "semi-harmonics" (0.5fo, 1.5 fo, 2.5 fo, etc.), and V is the average ampli- tude of the entire signal in dB. To search the optimal parameters, the robot mod- ifies the blowing parameters by following an orthog- onal chart (see Table 2) to reduce the searching time (where only 27 combinations are used). At the con- clusion of the search, the set of blowing parameters with the highest value of the sound-quality evalua- tion function is selected as the general position. Experiment and Results In this article, an experimental setup was proposed to verify how well the WF4-RII could imitate a hu- man performance. For this reason, we have per- formed a statistical analysis of the sound from the performances of a professional flutist and the robot to identify the main differences between them. Sta- tistical analysis of the sounds was performed using Solis et al. 21</page><page sequence="11">Figure 19. (a) A simple scale; (b) principal theme of Mozart's Flute Quartet, KV 298. Table 2. Orthogonal Chart Used to Find the General Position Experiment Repetition Robot Parameters Lungs Upper Inside Lip Upper Outside Lip Upper Forward Lip Lower Inside L 1 0 0 0 0 0 2 0 0 +A +A +A 3 0 0 -A -A -A 25 -A -A 0 +A 0 26 -A -A +A -A +A 27 -A -A -A 0 -A +A = Increment parameter value; 0 = No change; -A = Decrement parameter value. the Signals and Systems Using MATLAB (SSUM) program developed by Sturm and Gibson (2004). SSUM was developed as a tool to demonstrate es- sential principles and concepts of media signal pro- cessing to students. In particular, we used the signal feature explorer to compare both performances. The signal feature explorer of SSUM extracts several statistical proper- ties of the sound (such as those proposed by Pfeiffer and Vincent 2001): RMS, pitch, spectral centroid, spectral rolloff, the number of zero crossings, and the sonogram. Such features are obtained by per- forming an analysis on the frequency domain of the signal based on the FFT. Statistical features of the sound computed by SSUM have been used success- fully as features for audio and music classification (e.g., Tzanetakis, Essl, and Cook 2001). Therefore, such features should provide us valuable informa- tion to compare and detect possible required im- provements of the robot's performance. Each of these features has a specific meaning in musical terms. The RMS feature is a measurement of the intensity of the sound, which can be repre- sented on a logarithmic scale. The spectral centroid describes the center of gravity of the spectrum. It determines the frequency area around which most of the signal energy is concentrated, and it is fre- quently used as an approximation for a perceptual brightness measure. The spectral rolloff is a mea- sure of the distortion of the spectral shape, and it refers to the frequency under which 85% of the power distribution is concentrated. It is used to dis- a)0 IL A AP k ' " _ , - _ .. ... . ,, ..'_,. . _ . _ - "p, oI P'' Iii . .. . b,~Ph=;;" F~fL~""-dp ~eahin op~ F IVeCF~-~PFF==i tinguish between tonal and non-tonal components The zero-crossing count refers to the number of times that the time-domain signal passes the zero level. It is one measurement of the sound's noise b havior. Finally, sonograms are a way to visually rel resent musical information with frequency on the x-axis and time on the y-axis, where intensity is represented by shades of color. This is useful in de- tecting sustained notes (horizontal lines) and rhyti mic elements (vertical lines). We recorded a set of melodies, shown in Figure 19: a simple scale and the principal theme of a flut quartet by Mozart. A female professional flutist pe formed both melodies. From the recordings, the m sic features of each performance were processed. 22 Computer Music Journal</page><page sequence="12">The proposed algorithm (using the Sound Analysis Tool) produced the music data from each recording, and the proposed calibration phase of the robot pa- rameters (robot data) was done before the recording. Finally, the robot's performance was recorded for each melody, using its correspondent music data and robot data. Each of the recordings obtained from the human player and the robot flutist were then processed and analyzed using SSUM with a window size of 2,048 samples and a 50% overlap Hanning window. The results of the scale are shown in Figure 20. By observing the RMS values, the sound intensity of both performances were nearly identical, but the dynamic transitions of the signal were slightly dif- ferent (primarily on the highest notes of the scale). In the case of the spectral centroid, the professional flutist a exhibited larger frequency range (1-4 kHz) compared to the robot (2-3.5 kHz). This means that the performance of the robot flutist lacks the variability of brightness compared to the human player. Regarding the spectral rolloff, the professional flutist distributes the power of the signal in a larger frequency range (1-9 kHz) than the robot (4-9 kHz). This means that the sound produced by the robot is more concentrated in higher frequencies (perhaps related to the introduction of mechanical noises). In the case of the zero-crossing feature, both perfor- mances presented the same pattern, although some peaks were found in the case of the robot's perfor- mance (perhaps related to some noise components found while transitioning from one note to an- other). The pitch feature obtained for both perfor- mances was nearly identical, although, as can be seen on the sonogram, the robot's performance still lacks a certain sound clarity. In the case of the principal theme of the Mozart flute quartet, KV 298, the sound features from both performers are shown in Figure 21. By observing the RMS feature, we can see that both sounds exhibit about the same intensity. However, the dynamics of the robot were slightly different in some areas, mainly when there was a transition from one note to another (see the pitch feature). The spectral cen- troids of both performances were similar; however, in some cases, the professional flutist presented a larger frequency distribution. Regarding the spectral rolloff, although both performances presented simi- lar patterns, the robot's sound still lacks some high- frequency musical components during the transitions from one note to another. In the case of the zero-crossing feature, the professional flutist's sound presented more dynamic transitions (as can be seen on the RMS feature). The pitch feature pre- sented a similar pattern in both performances, which means that the proposed algorithm for the calibration phase of robot data-the general posi- tion-was effective in mimicking the pitch of the sound produced by the human. Finally, in examin- ing the sonogram feature, we see that the profes- sional flutist still presents strong accents on the horizontal and vertical lines, whereas the robot presents strong horizontal lines but some distortion on the vertical lines. This means that the robot still has some difficulties in mimicking some dynamic properties that are strongly related to the expres- siveness of the human performance. Conclusions and Future Work In this article, we presented the WF-4RII robot, which can mechanically mimic the performance of a human flutist. An experimental setup was pro- posed to compare sound features from the perfor- mances of a professional flutist with those of the WF-4RII. From statistical sound analyses of the re- sults, we demonstrated similarities between the performances of a professional flutist and the WF- 4RII. However, further improvements are required to increase the sound clarity, brightness, and dy- namic properties of the robot's performance. In the future, we will introduce further perfor- mance rules (e.g., phrasing, intonation, etc.) based on a human's performance as well as refined algo- rithms to control the sound clarity and the vibrato patterns. Tools from artificial intelligence, such as neural networks, will be also considered. Further- more, from an applications point of view, we will focus our research in developing a robotic musical teaching system using the robot flutist, where Solis et al. 23</page><page sequence="13">Figure 20. Statistical sound features of the scale. -3 RMS Feature -1Ws - Professional Flutist - WF-4RII mo. Spectral Roll-Off gso mm ---- Professional Flutist - WF-4RII 0 a i .L I TIm* [scl Pitch Feature Professional SFlRutist - WF-4RII son r,,, (m,) Spectral Centroid 4000 -- f n u 25 1500 1000 - WF-4RII Zero-Crossing E ISO 100 ----- Professional Rutist -- WF-4Rll lT* [We) Professional Rlutist Player WFI-4Ri 01 3 4 5 7 8 9 a [ 24 Computer Music Journal</page><page sequence="14">Figure 21. Statistical sound features of the Mozart ex- cerpt. RMS Feature Spectral Centroid -I00 -110 -4 - 0Professional Flutist lo - WF-4RII - Professional Flutist - WF-4RII Spectral Roll-Off 12M Zero-Crossing 10000 EI 20100 Pro ssional Flutist - WF-4R - Professional Flutist -WF-4RI I61I 0 5 1 15 20 3 010 is 2 t025 10 , rofessional Flutist Pleer 900 Pitch Feature Prof esional Won WF-4RIl 10000 To o 0 5 [10 15020 2 3Mr7fassaa ~ d Solis et al. 25</page><page sequence="15">longer experiments will be required to verify its real potential (Solis et al. 2005). Our robot has performed duets with human flutists in Japan, Germany, and France. We would like to attempt more advanced performances in which the robot can perform with more human players, such as an entire symphony orchestra. Finally, we have thus far performed several basic experiments in which the robot flutist is capable of performing complex performances (e.g., Rimsky- Korsakov's Flight of the Bumblebee ) that can be performed only by skillful human players. How- ever, we believe the robot flutist will be useful also in experiments where the robot flutist performs music that could not possibly be performed by hu- mans (similar to Conlon Nancarrow's music for player piano). The robot flutist may be also useful in mediating among humans and electronic systems in live performance. Acknowledgments A part of this research was done at the Humanoid Robotics Institute (HRI), Waseda University. This research was supported in part by a Gifu-in-Aid grant for the WABOT-HOUSE Project by Gifu Pre- fecture and by the Japan Society for the Promotion of Science (JSPS). The authors would like to express thanks to Okino Industries, Osada Electric Com- pany, Sharp Corporation, Sony Corporation, Tomy Company, and ZMP for their financial support of HRI. Finally, we would like to express thanks to Ms. Akiko Sato, a professional flutist, for her valu- able help with the experiments for this article. References Alford, A., et al. 1999. "Music Playing Robot." Proceed- ings of the 1999 International Conference on Field and Service Robotics. Pittsburgh, Pennsylvania: Carnegie Mellon University, pp. 174-178. Ando, Y. 1970. "Drive Conditions of a Flute and Their In- fluences Upon Sound Pressure Level and Fundamental Frequency of Generated Tone." Journal of the Acousti- cal Society of Japan 26(6):253-260. Chida, K., et al. 2004. "Development of a New Anthropo- morphic Flutist Robot WF-4." Proceedings of the 2004 International Conference on Robots and Automation. New York: Intistute of Electrical and Electronics Engi- neers, pp. 152-157. Dannenberg, R. B., et al. 2005. "McBlare: A Robotic Bag- pipe Player." Proceedings of 2005 International Confer- ence on New Interfaces for Musical Expression. Vancouver, Canada: University of British Columbia, pp. 80-84. Gartner, J. 1980. Das Vibrato unter besonderer Beriick- sichtigung der Verhiiltnisse bei Fl6tisten, 2nd ed. Re- gensburg: Gustav Bosse Verlag. Kajitani, M. 1989. "Development of Musician Robots." Journal of Robotics and Mechatronics 1:254-255. Kato, I., et al. 1987. "The Robot Musician WABOT-2." Robotics 3(2):143-155. Ogura, Y., et al. 2004. "Realization of Knee Stretch Walk- ing by a New Biped Robot, Wabian-2LL." Proceedings of the 2004 CISM-IFToMM Symposium on Robot De- sign, Dynamics and Control. St. Hubert, Quebec: Canadian Space Agency, pp. 4-28. Okuma, I., et al. 2003. "Time-Scale Performance Control of a New Anthropomorphic Flutist Robot." Proceed- ings of the 2003 Annual Conference of the Robotics So- ciety of Japan. Tokyo: Robotics Society of Japan, p. 3H15. Pfeiffer, S., and T. Vincent. 2001. "Formalization of MPEG- 1 Compressed Domain Audio Features." CSIR 0O Mathematical and Information Sciences 1(196): 18. Singer, E., K. Larke, and D. Bianciardi. 2003. "LEMUR GuitarBot: MIDI Robotic String Instrument." Proceed- ings of the 2003 International Conference on New In- terfaces for Musical Expression. Montreal: McGill University, pp. 188-191. Solis, J., et al. 2004. "Learning to Play the Flute with an Anthropomorphic Flutist Robot." Proceedings of the 2004 International Computer Music Conference. San Francisco, California: International Computer Music Association, pp. 635-640. Solis, J., et al. 2005. "The Anthropomorphic Flutist Robot WF-4R: Toward an Automated Transfer Skill System from Robot to Human."Proceedings of the 2005 Inter- national Computer Music Conference. San Francisco, California: International Computer Music Association, pp. 423-426. Sturm, B. L., and J. Gibson. 2004. "Signals and Systems Using MATLAB: An Effective Application for Teaching Media Signal Processing to Artists and Engineers." Pro- ceedings of the 2004 International Computer Music Conference. San Francisco, California: International Computer Music Association, pp.645-649. 26 Computer Music Journal</page><page sequence="16">Suefuji, K., et al. 2004. "Development of the Robot Arms and Method of Searching General Position by Etude." Proceedings of the 2004 Annual Conference of the Ro- botics Society of Japan. Tokyo: Robotics Society of Japan, p. 1L21. Takanishi, A., M. Sonehara, and H. Kondo. 1996. "Devel- opment of an Anthropomorphic Flutist Robot WF- 3RIII." Proceedings of the 1996 International Conference on Intelligent Robots and Systems. New York: Institute of Electrical and Electronics Engineers, pp. 37-43. Tzanetakis, G., G. Essl, and P. Cook. 2001. "Automatic Musical Genre Classification of Audio Signals." Pro- ceedings of the 2001 International Symposium on Mu- sic Information Retrieval. Bloomington, Indiana: University of Indiana, pp. 102-108. Solis et al. 27</page></plain_text>