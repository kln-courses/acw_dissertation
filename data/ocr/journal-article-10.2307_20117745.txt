<plain_text><page sequence="1">P. P. ALLPORT ARE THE LAWS OF PHYSICS 'ECONOMICAL WITH THE TRUTH'?* ABSTRACT. It has been argued that the fundamental laws of physics are deceitful in that they give the impression of greater unity and coherence in our theories than is actually found to be the case. Causal stories and phenomenological relationships are claimed to provide a more acceptable account of the world, and only theoretical entities - not laws - are considered as perhaps corresponding to real features of the world. This paper examines these claims in the light of the author's own field of research: high energy physics. Some of the distinctions upon which the above conclusions are based are found not to be tenable in practice. Examples from experimental particle physics are presented which suggest an important role of the underlying theoretical structure which cannot be overlooked. It is argued that the fundamental theories must, in fact, be treated as being as worthy or unworthy of ontological commitment as the entities they postulate or the phenomenological relationships they inspire. Whilst it is conceded that aspects of the current theoretical formalism belie literal interpretation, it is maintained that revision in these particular areas need not affect the symmetry principles, particle spectra, or coupling strengths that largely determine the empirical content of the theory. Various other reasons for believing that the laws of physics may be dishonest are examined and in particular attention is drawn to the current tendency in particle physics to judge a theory's merits by aesthetic criteria. The difficulty of providing a sound justification for this behaviour does lead to the suspicion that our abstract representation of the world may in fact be coloured by anthropomorphic prejudices. The question then remains as to whether this merely points to a certain embellishment of the truth or the masquerading of a lie. 1. INTRODUCTION It is a profound and fundamental good fortune that scientific discoveries stand up under examination and furnish the basis, again and again, for further discoveries. After all, it could be otherwise. Friedrich Nietzsche (1974, p. Ill) I wish to examine critically the claim that the laws of physics lie; i.e., that they do not tell the truth, the whole truth, and nothing but the truth. Clearly even the most ardent proponent of, say, the SU(3)C x SU(2)L x t/(l) 'standard model' of particle physics would be reluctant to claim that this provides a complete picture: the whole truth. How ever, many experimental and theoretical particle physicists maintain that, after two decades of research inspired by this theory, they are Synthese 94: 245-290, 1993. ? 1993 Kluwer Academic Publishers. Printed in the Netherlands.</page><page sequence="2">246 P. P. ALLPORT warranted in asserting that the standard model states nothing but the truth. We are again living in a time when the quest for a unified picture of the forces recognized as fundamental in physics seems to be nearing fruition. The question is whether such a theoretical triumph warrants any assertion of the truth of the principles employed, or even, whether on closer inspection, the unity of explanation will itself prove to be a pernicious illusion. Nancy Cartwright in her recent book How the Laws of Physics Lie (1983) provides a good summary of anti-realist arguments against belief in the supposed fundamental principles of physics. To start with, it is not at all obvious how the abstract formalism can be said to 'explain' the observed regularities of nature. It must be conceded that the laws of mathematical physics do not easily map onto more familiar causal explanations. The Weinberg-Salam theory is expressed as a Lagrangian density, which even in already rather compact notation runs to over fifty terms (Commins and Bucksbaum 1983, p. 62), and path integrals over space-time need to be taken to yield the Green's functions from which a perturbative expansion of terms that map initial into final state vectors can eventually be derived. Further steps are needed to provide testable predictions (cross-sections, decay rates, branching ratios, etc.) and even the derivations of these quantities gives little insight into any underlying causal mechanisms in the observed processes. (Indeed the similar treatment of the spatial and temporal co-ordinates within the model makes the common explanations of scattering phenomena in terms of the emission of a virtual photon which 'then strikes the target' at least a little misleading.) However, we often do need to tell ourselves causal stories to feel we understand the processes under study and these are not easily derivable from the mathematical formalism. Cartwright goes further by asserting that the causal stories provide both a deeper explanation of what is going on and inspire the pheno menological laws which, for specific calculations, prove of greater prac tical value than relationships purportedly derived from fundamental principles. These phenomenological laws are not to be understood as 'models', functioning either as 'impoverished theories' giving strictly speaking false but at least tractable approximations to a purportedly true theory or as supplements, providing the details needed to enrich an austere theoretical framework sufficiently for unambiguous predic tions to result.1 Such an interpretation begs the question, since it is the validity of the 'deductive-nomological' account which Cartwright is</page><page sequence="3">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 247 challenging. What Cartwright is asserting is that the 'covering laws' mode of explanation, arguing from general principles to specific re lationships, can never, in fact, be justified. It seems that we are to conclude that physics can and should be understood as a patchwork of autonomous but overlapping partial theories for which no grander con text is to be anticipated. Effectively, Cartwright is arguing that there can exist no real applications for 'covering laws', except as possibly providing useful mnemonics or, at most, a convenient classification scheme. I will attempt to demonstrate that the examples she has chosen to illustrate this assertion may not be representative of the entire scientific enterprise. That the covering laws cannot uniquely determine the re lationships expected to hold in many situations is not in question; boundary conditions, assumptions about pertinent variables, etc., are certainly needed. Nevertheless, I will argue below that Allan Franklin has successfully demonstrated that where an experiment is set up to test a particular conjunction of auxiliary hypotheses and fundamental principles, conclusions concerning both the auxiliary hypotheses (through calibration techniques, for example) and the underlying theory can be drawn (Franklin 1986). Therefore, I maintain, 'critical experi ments' are possible for the covering laws, and the question of their applicability as descriptions of the world remains an empirical, rather than solely metaphysical, issue. Of course Cartwright may be thinking of applicability in a different context. Certainly she draws many examples from what is often termed 'applied' physics where the criteria for usefulness may differ from that found in fundamental research. In this essay, I attempt to demonstrate that there is a sense in which two traditions, 'pure' and 'applied', can be distinguished, and that Cartwright's position of scepticism with regards to theoretical laws but acceptance of causal stories, phenomeno logical laws, and theoretical entities may not be tenable in the former case. In fact, to attempt to understand physics as a single undiffer entiated enterprise does seem mistaken. To understand the consequences of the popular pragmatic attitude, one can look to the ascendant tradition, no doubt influenced by the success of instrumentalism as the orthodox philosophy of quantum mechanics, for which the task of physics is in fact to understand the world as 'standing reserve', to use the apt phraseology of Heidegger (1977, p. 17). In terms of this world view, an exhaustive set of accurate</page><page sequence="4">248 P. P. ALLPORT phenomenological laws provides the goal for the scientific enterprise. Covering laws are either totally superfluous or at most they can prove valuable for remembering and classifying the 'true' (i.e., useful) practi cal laws. The contrary tradition, which I will dub 'monotheorism', makes the finding of an absolute minimum of empirically adequate principles the hallmark of 'great' science. According to this tradition, the discovery of a few abstract laws of great generality (and the protagonists of theoretical science would also say of great beauty), which are sufficient in principle to account for all known 'fundamental' phenomena, pro vides the surest route to a thorough understanding of the workings of the universe. (It is in addition assumed that a thoroughgoing reduction ism is tenable; i.e., all science is ultimately derivative of these few fundamental laws of nature.) Of course the rationality of this project stands or falls with the aptness of the underlying metaphysical assumption that the incredible richness and diversity of phenomena can be essentially derived from a few rather abstract mathematical formulae. Past successes of this immodest enterprise certainly do not imply its future viability. But only if its failure can be guaranteed a priori does it follow that the laws of physics can be said to lie. Lacking such a proof, the current existence of anomalies, dubious mathematical rigour, 'ad hoc manoeuvres', and deficiencies in predictive power may simply illustrate a transitional 'economy with the truth'. 2. CAN A LITERAL INTERPRETATION BE MAINTAINED FOR OUR CURRENT BEST THEORY OF THE SUB-NUCLEAR WORLD? Yet all the knowledge on earth will give me nothing to assure me that this world is mine. You describe it to me and you teach me to clarify it. You enumerate its laws and in my thirst for knowledge I admit that they are true. You take apart its mechanism and my hope increase. At the final stage you teach me that this wonderous and multi coloured universe can be reduced to the electron. All this is good and I wait for you to continue. But you tell me of an invisible planetary system in which electrons gravitate around a nucleus. You explain this world to me with an image. I realize that then you have been reduced to poetry: I shall never know. Have I time to become indignant? You have already changed the theories. So that Science that was to teach me everything ends up in a hypothesis, that lucidity founders on metaphor, that uncertainty is resolved in a work of art. Albert Camus (1975, p. 25)</page><page sequence="5">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 249 I wish to start by stating where I agree with the position Cartwright outlines in How the Laws of Physics Lie. For one thing, I agree that for many theoretical entities (such as electrons, ions, etc.) it seems almost perverse to deny them as much reality as other components of any equipment in which they play a decisive role. Hacking nicely ex presses such a practical approach to the issue of the ontological status to be attributed to theoretical entities in his account of an experiment looking for fractional charges (Hacking 1983, p. 23): Now how does one alter the charge on the niobium ball? 'Well, at this stage,' said my friend, 'we spray it with positrons to increase the charge or with electrons to decrease the charge.' From that day forth I've been a scientific realist. So far as F m concerned, if you can spray them then they are real. However, I hope to show in the following sections through examples from particle physics that the theoretical entities cannot always be untangled from the theoretical laws describing their behaviour, even when adopting this practical approach. Indeed the law/entity distinction is in need of greater clarification before sense can be made of attributing greater reality to the entities than to the laws appearing in a theory. Another important distinction to be found in Cartwright's book is that between the 'covering laws' and 'phenomenological laws'. Here I agree that phenomenological laws do have prima facie greater plausi bility than the covering laws intended to explain them. However, once more, the distinction is perhaps not tenable, at least given accepted usage of the term 'phenomenological' in particle physics. The difficulties with these distinctions will be discussed in much more detail in the following sections. What is important here is that it is agreed that we can treat the derivations of testable predictions from the so-called standard model of fundamental particle physics as pretty much paradigmatic examples of the 'covering law' mode of reasoning. Clearly I am assuming that we have not confined our definition of 'covering laws' to what Lakatos would call the 'hard core' (Lakatos 1970, p. 133) of the current research programme (which in this case might include the Dirac and Klein-Gordon equations plus quantum field theory, although perhaps not the gauge principle), but that we also include the gauge theories of weak, electromagnetic, and strong interactions along with such putative contenders for acceptance as superstring or technicolour theories. I shall here be content with show ing how our currently accepted theoretical account of fundamental</page><page sequence="6">250 P. P. ALLPORT physics (treated as established truth by many workers in the field) contains features which are difficult to reconcile with a thoroughgoing realism. Something of a hierarchy exists in the theoretical structure which I will here take as constituting the standard model. In outline (roughly from the top down) one has: the axioms of quantum mechanics and special relativity, the Dirac and Klein-Gordon equations, quantum field theory and Feynman rules, gauge invariance, spontaneous symmetry breaking, and explicit symmetry breaking. The resultant theory gives a description of the world in terms of fundamental fermions grouped in families of 4 members (2 leptons plus 2 quarks) interacting via exchange of spin-1 gauge bosons (8 + 1 massless, 3 massive) with 3 gauge coupling constants and, for 3 families, 3 weak mixing angles and 1 phase angle. Van Fraassen states (1980, p. 74): For theory construction experimentation has a 2-fold significance; testing for empirical adequacy of the theory as developed so far, and filling in blanks, that is guiding the construction or completion of the theory. In the case of the hierarchy just described, the required empirical input filters up as far as the choice of the Lorentz invariant couplings and that of the gauge groups. Here one meets the boundary with the somewhat unassailable 'hard core'; although some principles used in quantum field theory do seem to have direct empirical consequences, as we shall see below. There are three places at different levels in this hierarchy where I think a thoroughgoing realism is difficult to maintain. The first, on which I will not dwell much here, although in many ways it is probably the most influential, concerns the conceptual foundations of quantum mechanics itself. This enters both at the top level and at the interface with experiment where cross-sections, decay rates, etc., are calculated from the predicted matrix elements. At the fundamental level there are the difficulties with blurring the distinction between subject and object. Bell writes (1987, pp. 40-41): Whenever necessary a little more of the world can be incorporated into the object. In extremis the subject - object division can be put somewhere at the 'macroscopic' level, where the practical adequacy of classical notions makes the precise location quantitatively unimportant. But although QM can account for these classical features of the macroscopic world as very (very) good approximations it cannot do more than that. The snake cannot</page><page sequence="7">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 251 completely swallow itself by the tail. This awkward fact remains: the theory is only approximately unambiguous, only approximately self-consistent. There are also difficulties with the techniques described in most text books for calculating the available phase-space in scattering experi ments. A fictitious 'normalization volume' has to be introduced, which is only cancelled out in the final stages of the derivation. As Cartwright (1983, p. 142) writes of Merzbacher's (1970, p. 82) justification of 'box normalization' in terms of the inevitable presence of some confinement as a feature of the apparatus: Here is a clear distortion of the truth. The walls may interact with the particle and have some effect on it, but they certainly do not produce an infinite potential. Just such principles must be employed to perform the integrals over phase-space needed to give experimental predictions in particle physics. The second difficulty for the standard model which I wish to raise is also a familiar problem, that of renormalization. Cartwright notes this difficulty in the context of her discussion of the Lamb shift. In the most popular treatment of renormalization the couplings and masses appearing in first-order perturbation theory are taken as the 'bare' values, which, when calculation proceeds to higher order, must tend to infinite values if the divergences coming from loop diagrams, etc., are not to lead to infinite predictions.2 Of this procedure and its general acceptance Dirac wrote (1978, p. 36): Hence most physicists are very satisfied with the situation. They say 'Quantum electrody namics is a good theory, and we do not have to worry about it any more'. I am very dissatisfied with the situation, because this so-called 'good theory' does involve neglecting infinities which appear in its equations, neglecting them in an arbitrary way. This is just not sensible mathematics. Sensible mathematics involves neglecting a quantity when it turns out to be small - not neglecting it because it is infinitely great and you do not want it. But as Cartwright says of the measured Lamb shift (1983, p. 119): We can 'derive' a result very close to this in quantum electrodynamics using the result of mass renormalization of the electron. But the derivation is notorious: the exact details, in which infinities are subtracted from each other in just the right way to produce a convergent result, are completely ad hoc, and yet the quantitative results that they yield are inordinately accurate. The point being that the renormalization procedure is tightly con strained by the requirements of finite results for calculation to all orders</page><page sequence="8">252 P. P. ALLPORT of perturbation theory. The higher-order calculations of the Lamb shift do converge well to the measured value (Halzen and Martin 1984, p. 158). In fact the most spectacular agreement between experimental measurement and theoretical prediction found in particle physics is that between the third-order quantum electrodynamics (QED) calculation of the anomalous magnetic moment of the electron and the high accuracy measurement of this value (Perkins 1982, p. 238). The results agree to at least seven decimal places. This spectacular success is bought at the cost of divergent values of the 'bare mass' and 'bare charge' of the electron and a treatment of infinities which to me at least seems rather tricky to reconcile with a realist position. (For a more comprehensive discussion, see, for example, Teller (1988, p. 71).) The third feature on which I wish to focus attention is the mechanism whereby symmetries are broken spontaneously to become hidden sym metries. A good discussion of this mechanism can be found in Lee's book Particle Physics and Introduction to Field Theory (1981, p. 378, Ch. 16). The essential idea is that the vacuum itself carries non-zero quantum numbers and can therefore act as the source of symmetry breaking even where the fundamental Lagrangian of the theory respects the symmetry. A simple illustration is provided by the 'wine bottle' potential. Consider a two-dimensional potential looking like the base of a wine bottle with the axis of symmetry passing through the origin. The rotational symmetry of this potential (and by implication the Lagrangian) will not be discernible from the local dynamics of any particle sitting in a minimum of this potential. In the case of the standard model, one finds a potential of this form expressed as a function of scalar fields, and it is the ground state of the universe, the vacuum, that sits in one of the available minima. This then implies a non-zero vacuum expectation value for these scalar fields. The simplest version of the standard model requires only one Higg's doublet to break the gauge symmetry and allow the weak vector bosons to become massive; but extensions are not excluded and may be required in super string models. Aspects of the strong interaction sector of the standard model (quantum chromodynamics: QCD) also call for non-zero vacuum expectation values and a very rich structure for the QCD vacuum. All of which, I contend, belies a literal interpretation of the model unless one can really accept the existence of such all-pervading fields extended across the entirety of space and time. In addition, it is now widely appreciated that the cosmological consequences of such fields cannot</page><page sequence="9">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 253 be ignored. The predicted energy of the vacuum implied by these models, given that a miraculous cancellation does not occur, would be so great that space-time curvature effects would be manifest on the scale of miles (Abbott 1988). This hardly accords with experience. I believe that these problems are genuine, but I am not convinced that the positive merits of the standard model are endangered by revision in any of the three areas discussed above. Certainly the standard model has provided a unified picture of all the fundamental particle and field interactions accessible to present-day accelerators, and for ten years it has proved perfectly empirically adequate. For the anti-realist this suc cess is hardly telling. Cartwright writes on the unifying role of covering laws, which tie together many different phenomena (1983, p. 85): Explanatory laws summarize phenomenological laws; they do not make them true. Coinci dence will not help with laws. We have no ground for inferring from any phenomenolog ical law that an explanatory law must be just so; multiplying cases cannot help. And on the issue of spectacular empirical adequacy van Fraassen writes (1980, p. 40): I claim that the success of current scientific theories is no miracle. It is not even surprising to the scientific (Darwinist) mind. For any scientific theory is born into a life of fierce competition, a jungle red in tooth and claw. Only the successful theories survive - the ones which in fact latched on to actual regularities in nature. However, it may be, as most particle physicists would contend, that much of the current edifice can not only be understood realistically but also considered true; at least in the sense that it should prove resilient under future theory shift. But can we assert anything about what future theories will say? In his chapter "Do Unborn Hypotheses Have Rights?" (1985, p. 155), Sklar asks how much of our current theories we are warranted in accepting as true, given the likelihood of theoretical upheavals in the future. [S]uppose that our future scientists finally see the wisdom of the Leibnitzian view that all the world is a construct of spiritual monads. Would any of this change our opinion that genes are DNA molecules? Or that we were warranted in so believing in 1975? Would it change our estimate of our earlier warrant that electron sharing holds some molecules together? Or that superconductivity is the result of long-range pairing which bonds fermion electrons into the suitable constituents of a degenerate boson gas? I think not. Only as far as these can be considered examples of causal stories might</page><page sequence="10">254 P. P. ALLPORT Cartwright be in agreement with these sentiments. In the specific case we have been examining, it is anyway not at all clear how far up the theoretical hierarchy one could proceed substituting causal stories for mathematical formalism or whether such a venture would prove any thing but counter-productive. It will be argued below that there are cases where successful mathematical predictions are available but con sistent causal stories cannot be formulated. Rather than fall back on causal stories, a more positive approach must surely be to try and amend those features that currently cause us conceptual disquiet. As recently as 1975 Dirac said (1978, p. 20): People are, I believe, too complacent in accepting a theory which contains basic imperfec tions, and a true advance will be made only when some fundamental alteration is made. Whilst Bell and Nauenberg (1987, pp. 26-27) optimistically predicted that, regarding the conceptual difficulties in quantum mechanics, [w]e can look forward to a new theory which can refer meaningfully to events in a given system without requiring 'observation' by another system. The critical test cases requiring this conclusion are systems containing consciousness and the universe as a whole. Actually the writers share with most physicists a degree of embarrassment at consciousness being dragged into physics, and share the usual feeling that to consider the universe as a whole is at least immodest, if not blasphemous. Not that this disquiet alone is a sufficient basis for abandoning scientific realism. Nevertheless, it still remains something of a pious hope that the difficulties outlined earlier in this section will one day be solved. It has been argued that there do exist features in our current theories which cannot be literally true and therefore these cannot be satisfactorily incorporated into a realist account of the sub-nuclear world. This is, however, a far cry from establishing that such a realist account is impossible. 3. DO CAUSAL STORIES TELL MORE OR LESS OF THE TRUTH THAN COVERING LAWS? I think perhaps the strongest argument on Hume's side is to be derived from the character of causal laws in physics. It appears that simple rules of the form 'A causes B' are never to be admitted in science, except as crude suggestions in early stages. The causal laws by which such simple rules are replaced in well-developed sciences are so complex that no one can suppose them given in perception; they are all, obviously, elaborate inferences from the observed course of nature. I am leaving out of account</page><page sequence="11">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 255 modern quantum theory which reinforces the above conclusion. So far as the physical science are concerned, Hume is wholly in the right; such propositions as 'A causes B' are never to be accepted, and our inclination to accept them is explained by the laws of habit and association. Bertrand Russell (1979, p. 643) In the last section we saw that there may be difficulties for a literal interpretation of the standard model as currently formulated. Of course these difficulties may be resolved by future improvements within the theory such as Dirac desired. For many, a theory which unified a wide variety of different phenomena within a single elegant theoretical structure where underlying symmetries provide the basis for explanation would be a strong contender for acceptance as true. As noted above, to Cartwright, such a position smacks of a poor induction (1983, p. 75). Even if justified, Cartwright points out that with abstraction comes the problem of non-uniqueness of the theoretical treatment of specific problems. The method of inference to the best explanation is subject to an important constraint, however - the requirement of non-redundancy. We can infer the truth of an explanation only if there are no alternatives that account in an equally satisfactory way for all the phenomena. In physics nowadays, I shall argue, an acceptable causal story is supposed to satisfy this requirement. But exactly the opposite is the case with the specific equations and models that make up our theoretical explanations. There is redundancy of theoretical treatment, but not of causal account. Cartwright then proceeds to illustrate this assertion with the example of quantum damping in laser cavities for which there exists one 'causal' explanation (i.e., that the atom reacts with the radiation field) and several mathematical treatments, all giving the correct line broadening. It is this underdetermination of mathematical treatment by the data which is here being used to argue that explanation from covering law to phenomenological law (according to the deductive-nomological model) is hardly achieved by simply drawing out the consequences of the covering law. In the example quoted it is the different sets of auxiliary hypotheses that are invoked by the approximation methods used that distinguish the treatments. Whether in fact some of these treatments can be shown to only differ in 'theoretical formalization' (a concept discussed by Sklar in the context of special and general relativ ity (1985, p. 73)) or whether some auxiliary hypotheses can receive independent appraisal is not discussed. Without such input it is difficult to gauge whether there is a genuine underdetermination issue at stake</page><page sequence="12">256 P. P. ALLPORT here. Certainly, in the case of the anomalous magnetic moment of the electron discussed above,3 different renormalization schemes may be employed, but this is hardly sufficient to argue in that example for underdetermination of the theoretical treatment by the data. In her examples from laser theory Cartwright shows how much 'bot tom up' reasoning is required if useful predictions are to result from the quantum formalism ('filling in blanks' in van Fraassen's sense). Even so; [e]ngineers at Spectra Physics construct their lasers with the aid of the quantum theory of radiation, non-linear optics, and the like; and they calculate their performance charac teristics. But that will not satisfy their customers. To guarantee that they will get the effects they claim, they use up a quarter of a million dollars' worth of lasers every few months in test runs. (Cartwright 1983, p. 98) Are we to conclude from this puzzling passage that QED, as the funda mental theory of electromagnetic radiation and its interactions with matter, is not worth the text books it is written in, since Spectra Physics' customers do not regard it as a sufficiently reliable theory to risk hard cash on? Or, rather, it is not the case that so many factors influence the successful construction of any sophisticated piece of apparatus that even very competent people cannot be sure in advance that no 'bugs' remain in a design? I will return to this issue of the different role of experiments and theories in fundamental and applied science later. I would like to note here that whilst allowing that the influence of funda mental theory on technology development is difficult to define, it is not true that the theory is irrelevant in the sense of lacking tangible physical consequence. However, it is true that the ceteris paribus clause that necessarily accompanies any such law makes experimental design in fundamental physics a real challenge. The ability to refine the apparatus to isolate a single effect of interest is certainly an essential ingredient for successful experimentation. I now wish to turn to an example from high energy physics which I believe illustrates some of the weaknesses of causal stories in fundamen tal science. I wish to discuss deep-inelastic scattering, the process whereby high energy beams of leptons (electrons, muons, or neutrinos) are shone on a target and the tracks from interactions in the material are analyzed to provide information on the structure of the nucl?ons (the protons and neutrons) themselves. The theoretical model proceeds from analysis of the Feynman diagrams for the amplitude squared of</page><page sequence="13">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 257 the scattering process with the electron coupling to one virtual photon and the virtual photon also coupling to one point-like fractionally charged quark (these are the so-called 'handbag' or 'four-point' dia grams (Brodsky et al. 1972)). The lines appearing in these diagrams represent 4-momenta not physical trajectories, and many causally dis tinct orderings of the interaction are represented by a single diagram. Nevertheless, it is commonplace and often convenient to talk of the incoming electron knocking a quark out of the struck nucle?n which subsequently fragments to give the observed shower of secondary part icles. Here the difficulties raised by the interpretation of quantum mechanics are completely overlooked and the description is phrased solely in language appropriate for a mechanistic, corpuscular theory.4 The problem with such an approach comes precisely where one con fronts irreducible quantum effects, such as coherent phenomena (Sto dolsky 1970) in scattering experiments. Then the impossibility of provid ing an adequate causal story results in confused and contradictory accounts of these effects, whilst the mathematical predictions of the theory reproduce well the experimental distributions. It is precisely because of the way quantum mechanics has been grafted on to a classi cal, mechanistic world picture that explanation is not forthcoming in the form of causal laws but only through attention to the mathematical consequences of the underlying theory. (Most discussions of conceptual difficulties with the '2-slit experiment' emphasise the absence of a single adequate causal story.) Finally I want to address another aspect of Cartwright's critique of covering laws in this context, her apparent identification of forces with causes. Before proceeding I would like to outline why I think the Lagrangian formalism in classical mechanics deserves to be treated as more than a neat reformulation of Newtonian mechanics. The motiva tion comes primarily from the path integral approach to quantum me chanics pioneered by Feynman; but also I believe the formalism has merits when viewed from strictly within the classical perspective. The main advantage of the path integral approach is that it provides both a common language and a bridge principle between classical me chanics and quantum mechanics. When the action integral, S, is large with respect to Planck's constant, h, as it is for macroscopic systems, only one path between initial and final co-ordinates has non-negligible probability and this is the path for which S takes an extremal value. This gives us Hamilton's principle of least action for classical systems.</page><page sequence="14">258 P. P. ALLPORT At the other extreme, where S ~ h, many paths contribute with compa rable probability giving the characteristic interference effects of wave mechanics. I cannot claim to find such an acausal principle as Hamilton's difficult to take as a candidate for being a literally true feature of the world. When Cartwright (1983, p. 74) quotes Russell as saying: [t]he law of gravitation will illustrate what occurs in any exact science .... Certain differential equations can be found which hold at every instant for every particle of the system .... But there is nothing that could properly be called 'cause' and nothing that could properly be called 'effect' in such a system. . . , I find I am in sympathy with Russell. Cartwright, however, takes a dim view of such abstract mathematical reasoning (1983, p. 19). Things are made to look the same only when we fail to examine them too closely. Pierre Duhem distinguishes two kinds of thinkers: the deep but narrow minds of the French, and the broad but shallow minds of the English. The French mind sees things in an elegant, unified way. It takes Newton's three laws of motion and turns them into the beautiful, abstract mathematics of Lagrangian mechanics. The English mind, says Duhem, is an exact contrast. It engineers bits of gears, and pulleys, and keeps the string from tangling up. It holds a thousand different details at once, without imposing much abstract order or organization. The difference between the realist and me is almost theological. The realist thinks that the creator of the universe worked like a French mathematician. I think that God has the untidy mind of the English. (I propose to delay any further discussions of theology until the penulti mate section.) I would now like to discuss Cartwright's comments on the vector addition of forces. Cartwright considers the case of a charged, massive body in the presence of both an electric and gravitational potential (1983, p. 57). For bodies which are both massive and charged the law of universal gravitation and Coulomb's law (the law that gives the force between two charges ) interact to determine the final force. But neither law by itself truly describes how the bodies behave. No charged objects will behave just as the law of universal gravitation says; and any massive objects will constitute a counterexample to Coulomb's law. These two laws are not true; worse, they are not even approximately true. This has the consequence for Cartwright that for equal and opposite forces (1983, p. 60), [i]n interaction a single force occurs - the force we call the 'resultant' - and this force is</page><page sequence="15">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 259 neither the force due to gravity nor the electric force. On the vector addition story, the gravitational and electric force are both produced, yet neither exists. These extreme conclusions seem to me to follow from emphasising the role of forces, particularly when these are conceived as causal agents. This emphasis, I believe, is mistaken and the disturbing conclusions may be avoided if instead we consider the fundamental entities to be the potentials and treat the force on a particle as given by V2 g?V?, e.g., the gradient of the sum over the product of potentials and couplings. In the Lagrangian formalism the contributions of different potentials naturally enter the equations in this way and give the correct trajectories for test particles in the presence of two or more such fields.5 The main objection though to forces so conceived, as with other putative causal agents, is that there is something a little too anthropo morphic and suggestive of analogies with acts of the will (Nietzsche 1986, p. 295). There is no such thing as cause; some cases in which it seems given to us, and in which we have projected it out of ourselves in order to understand an event, have just been shown to be self-deceptions. Our understanding of an event has consisted in our inventing a subject which was made responsible for something that happens and for how it happens. Whilst one may not wish to accept the full sceptical import of this passage, I think such considerations are sufficient to suggest that the 'gears and pulleys' models really are untenable as literal accounts of the world. I agree with Russell that this should motivate us to look further than a naive cause and effect account of natural processes. The Lagrangian mechanics has the virtue of formulating the development of physical systems in terms of the simultaneous evolution of all the constituents. This I believe accords better with a flux of mutually depen dent changes, a view of causality which seems more acceptable. I believe that such a conception lies behind Schopenhauer's (1974, p. 55) assertion that first and foremost the law of causality relates solely and exclusively to changes of material states, and to nothing else whatever. Indeed, to Nietzsche and Schopenhauer, forces, in Newton's sense, only feature as qualitates occultae. Although, no doubt, the same can be said of potentials as they feature in our current theories, it seems possible that the language of counterfactuals could be made to fit the discussion of potentials in physics - a potential being defined through</page><page sequence="16">260 P. P. ALLPORT the trajectory a test particle would have taken if it had had a given initial position and velocity. These considerations lead me to disagree with Cartwright's assertion that causal stories provide a sound basis for scientific explanation. Their role is important in scientific discourse but I believe that these accounts are too often permeated with the tacit acceptance of unarticulated metaphysical prejudices (concepts such as 'powers' and the like) which more properly belong to the 'metaphysics of the New Stone Age'. One could even go so far as to argue that it is just where our theories depart from such anthropomorphic principles that we find the best evidence that Kant's transcendental idealism may be mistaken in the limits it imposes on what is conceivable. Given the belief that the universe is not created in the image of man, it even seems reasonable to treat the abstract acausal principles as better candidates for being true than the more intuitive causal relationships. This then implies that the covering laws could in fact provide a better guide to the true nature of things than the causal stories. 4. DO THE COVERING LAWS HAVE EMPIRICAL CONSEQUENCES? Ultimate scepticism. - What then, in the last resort, are the truths of mankind? - They are merely the irrefutable errors of humanity. Friedrich Nietzsche (1974, p. 219) In Cartwright's examples, the experimental work is not aimed at refut ing or corroborating bold conjectures in the style of falsificationist (Popper 1976) or fallibalist (Lakatos 1970, p. 91) methodologies. However, it is apparently not her claim that the covering laws are of necessity unfalsifiable but rather that they are irrelevant in the cases cited above. Fundamental theories are not to be treated as (provision ally) true or false since they represent, at best, useful organizing prin ciples amongst phenomenological laws, and, at worst, excess metaphys ical baggage. Let me repeat a point I have made often before. If we are going to argue from the success of theory to the truth of theoretical laws, we had better have a large number and a wide variety of cases. A handful of careful experiments will not do; what leads to conviction is the widespread application of theory, the application to lasers, and to transistors, and to tens of thousands of other real devices. Realists need these examples, application after application, to make their case. But these examples do not have the</page><page sequence="17">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 261 right structure to support the realist thesis. For the laws do not literally apply to them. (Cartwright 1983, p. 161) Apart from the suggestion that in fact we can argue from theory success to the truth of the theoretical laws, given a sufficient range and quantity of data (in apparent contradiction with the assertions to the contrary quoted previously), I must confess that I find this passage begs many questions. For a start, it would be odd to consider only such techno logical wonders as transistors and lasers as 'real devices' whilst treating the even more macroscopic tools of particle physics research as not. Or maybe it is the issue of utility, or the number of such objects to be found in the average Western workplace or household that counts? (How many compact disc players is an accelerator experiment worth in epistemological currency? Less than in dollars?) It seems that the argument we are supposed to adopt is 'It seems to be useful so it may be true' as opposed to 'It seems to be true so it may prove useful', despite the fact that the latter position is in better accord with most scientists' own view of their joint activity as a disinterested search for knowledge. Cartwright has concentrated much on fields where failure to fit the experimental findings with theory does not reflect on the theory because the situation is too complex to allow unambiguous predictions to be drawn and approximation techniques of unknown accuracy and assump tions of dubious validity must be employed. These are just not the conditions in which to challenge the empirical adequacy of the theory. Only with careful experiment design can one gain confidence that the ceteris paribus clause is satisfied, and therefore be assured of actually asking the question of the theoretical principles under study. Franklin (1986) gives several illuminating examples of experimental science in action. He chooses to emphasise particularly the very impor tant experimental findings on the discrete symmetries: P, symmetry under parity inversion (globally interchanging left and right as in a mirror image), and C, symmetry under charge conjugation (globally replacing particles with anti-particles in the theory and vice versa). These symmetry principles do have a rather indirect bearing on the properties of physical particles and were mainly important for constrain ing the theoretical models under consideration (much as Lorentz invari ance is a constraint that the Lagrangian must respect in any theory which purports to be true). In quantum mechanics, the operators P</page><page sequence="18">262 P. P. ALLPORT and C which produce these transformations can only have eigenvalues ?1, and these values will be conserved quantities in decays or interac tions. The problem of attributing these eigenvalues to the observed particles through study of their different decays and interactions turns out to be overconstrained such that some predictions concerning decays, etc., can be derived (Perkins 1982, p. 81). It is the study of the failings of these principles in the case of weak interactions, contrary to theoretical expectations, that occupies much of Franklin's book, and he clearly demonstrates that experimental pure science is not always led by the nose by theory. Of importance to the discussion here is that these fairly deep rooted theoretical prejudices were dismissed by 'a handful of careful experi ments' (and acceptance of the results by the theoretical community was swift) and that only through carefully contrived experiments could the scientific community have explored the general validity of these symmetry principles. Franklin concludes (1986, p. 190) that in his ex amples he presents a set of strategies that I believe provide grounds for rational belief in experimental results. Several of these strategies use the argument that observation of predicted behavior or of known phenomena, argues in favor of proper operation of the apparatus, and this validates the observations or measurements. Only given such carefully checked results are we then able to start to 'fill in the blanks' in van Fraassen's sense, and it is through such an interplay between theory and experiment that the standard model has evolved with the structure it has. (In particular, the presence of both vector and axial vector couplings in the weak sector gives the explicit violation of C and P for weak interactions, whilst CP violation is accommodated through the presence of a complex phase in the quark mixing matrix.) The way that a very general theory like the standard model adjusts to a specific experimental finding always has many empiri cal consequences in other sectors of the theory. The whole phenomenol ogy of neutrino physics, W? and Z? decay, y - Z? mixing, and many other fields reflect the symmetry breaking first discovered in the decays of ?T-mesons. If one were content with phenomenological laws in each of these fields, then this unity of explanation would be lost and the consequent predictions, due to the fundamental connection of these different phenomena, would be missed. In conclusion I cannot accept Cartwright's insistence on proof by</page><page sequence="19">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 263 technology. The truth or falsity of physical laws is best investigated through the careful design of experiments aimed specifically at address ing the issue. With such experiments the covering laws can indeed be evaluated empirically. If Cartwright's views of explanation from cover ing laws were valid, then much fundamental experimental work would take on the character of tilting at windmills. 5. WHAT ROLE DO PHENOMENOLOGICAL LAWS PLAY? We must love and cultivate error: it is the mother of knowledge. Friedrich Nietzsche (as quoted in Vaihinger 1973, p. 89) In the previous section it was argued that one aspect of experimental science is the careful design of equipment to allow isolation of phenom ena relevant for testing theoretical predictions. Although, in practice, every prediction may be accompanied by some ceteris paribus clause, in practice one may often be confident that this condition has been met (Franklin 1986, p. 192). Cartwright makes much of the extensions to the covering laws which are often needed to cope with a less than perfect set-up. For example, Snell's (Descartes's) law cannot by itself give an adequate account of the refraction of light by non-isotropic media such as Icelandic Spar. However, given the quantitative successes with isotropic media, such discrepancies suggest rather the possibility of a 'progressive problem shift' (Lakatos 1970, p. 134) (say, an explana tion of the optical properties of the material in terms of its structure) than a flaw in the original formalism. It is nevertheless true that phenomenological laws are often formu lated where theoretical developments are found necessary to account for unexpected phenomena. It is also frequently true that, although maybe rendered plausible by causal stories suggested by more funda mental principles, these laws are in no strict sense derivable from the covering laws. Indeed, any idea that seems to work in practice is preferable to not having a guiding principle to suggest new measure ments or re-evaluation of old data. Whilst such laws as are able to save the phenomena are all that is strictly needed for technological applications, in fundamental research such an approach is only acceptable pro tempore or where shown to be a justifiable simplification. As evidence of this one may contemplate the vast industry, currently underway, aimed at deriving the known</page><page sequence="20">264 P. P. ALLPORT masses of the baryons and mesons from those of their quark constituents and the theory of quark interactions, QCD. Many 'CPU days' on large computers are absorbed in attempting the lattice gauge calculations needed to produce answers where no perturbative calculations are pos sible. Although relationships between the masses and magnetic mo ments of these particles were derived using plausible assumptions, within the accepted framework of the quark-parton model (Halzen and Martin 1984, p. 55), these did not answer the question 'Does QCD get the right values?'. Cartwright gives the impression that she would find this whole enterprise ill-conceived. She writes (1983, p. 102): Without God and the book of Nature there is no sense to be made from the idea that one law derives from another in nature, that the fundamental laws are basic and that others hold literally 'on account of the fundamental ones. The indications are, however, that QCD may be able to give a 'deeper' understanding of the relationships between the masses of quark-bound states (the hadrons) than that found using just the parton model plus additional plausible hypotheses. There is within the theoretical particle physics community a sizeable group of researchers who label themselves 'phenomenologists'. These individuals are not directly concerned with matters such as 'intentional ity and subjectivity' but rather with producing predictions which are empirically accessible, starting from the fundamental theories. Whilst auxiliary hypotheses are often required in this enterprise, giving rise to an imperfect 'protective belt' (Lakatos 1970, p. 133), experimental results on these predictions can often prove very damaging to the covering laws. The SU(5) grand unified theory is now more or less abandoned since the proton has been measured to be even more stable than this theory would have predicted (&gt; 3 x 1032 years). (The develop ment of 'non-minimal' SU(5) models bears witness to the ad hoc ma noeuvres of a degenerating research programme.) Phenomenological laws need not be suggested by the covering laws. One example from particle physics, again taken from the realm of strong interactions, is that of the vector meson dominance theory (VMD). This theory was originally posited as the underlying theory of strong interactions; these being described in terms of meson exchange. Long since displaced by QCD, it is ironic that in the field of 2-photon interactions (where QCD was once advertised to actually be vulnerable to critical tests (Gotsman et al. 1988; Olsson 1988)) the VMD theory</page><page sequence="21">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 265 is required for a coherent description of the observed phenomena. However, this predecessor, and one-time rival of QCD, is now termed a phenomenological theory whose limited realm of applicability will one day have to be covered by the more fundamental theory. Here the phenomenological law is a refuted covering law which is maintained because it is uniquely successful in a limited range of applications. In fact it is quite a general feature of phenomenological theories that they are not considered as candidates for universal validity. In this sense, Newtonian mechanics could also be treated as phenomenological, in that within a well-defined scope this theory is remarkably successful. On the other hand, it would seem perverse to prefer a refuted covering law of limited applicability to a successor which admits of no such limitations. Often in the 'pure' sciences, those laws dubbed 'phenomen ological' are precisely those whose applications are not universal and for whom a justification based on more general principles is awaited; a justification that must also be able to explain why the phenomenological law has limited scope. Another class of principles often labelled as 'phenomenological' re lationships and of particular significance in those applied fields of re search where saving the phenomena clearly suffices are those suggested inductively by perceived regularities. Instances of such Baconian induc tion are not nearly as unimportant as previously supposed (Medawar 1984, p. 96). However, it is difficult to find examples of such reasoning discussed in the published accounts of fundamental research.6 The extent to which experimentalists in all fields need to rely on just such relationships is therefore quite surprising. It is sometimes argued by philosophers of a positivistic frame of mind that these phenomenological relationships are all that one requires and that elevating an observed regularity into a putative law, or seeking explanation in terms of existing laws, means merely accruing extra metaphysical baggage. Then not only covering laws but all laws are deemed superfluous. Armstrong attacks this 'regularise position on several fronts (Armstrong 1983). Some of his attacks rely on science as currently practised providing paradigm cases of laws, so a prescriptive principle, which lays down the criteria for determining what is to consti tute correct practice, need not be affected by these arguments. There remain, however, the problems of counterfactuals and Hempel's para doxical findings. The first problem, according to Armstrong's analysis, is that whilst law-statements can support counterfactuals, regularity</page><page sequence="22">266 P. P. ALLPORT statements cannot (1983, p. 46). It is then difficult to see what role counterfactuals can play in a regularist system. The second problem centres on statements of the form 'all Fs are Gs' (1983, p. 44). If this is to be expressed as (x)(Fx D Gx), then not only for an individual a does Fa &amp; Ga corroborate this relationship but so do all instances of ?Fa. However, if we take it as a law that all Fs are Gs, then (putting the matter a little differently from Armstrong) we can say for an individ ual a that Ga potentially corroborates the law and ?Ga potentially refutes it. But only if also Fa can the law be actually corroborated or refuted; otherwise the case has no bearing on the validity of the law. For the reasons Armstrong outlines, it is in fact very difficult to put the regularist position on a firm footing, and he concludes that even the simplest phenomenological regularities need to be formulated in terms of putative laws. Whilst laws expressing such simple regularities are often the only principles available in a new line of experimental research, the range of applicability of such phenomenological laws is not determined until they can be better understood in terms of more general principles. In fact, in choosing from the myriad of possible variables, those which actually show some functional relationship, one must already have exploited a large reservoir of background knowledge. It is also this background knowledge of more general theories which allows one to determine the limited scope of such laws. For example, it is often the case that one plays with a form of reductio ad absurdum; trying out the cases where certain variables become very large or very small to see where the model 'breaks down'. Here to 'break down' simply means to contradict accepted theories. Therefore in the case of such simple 'regularity laws', it is also true that one is guided by the covering laws and that, without these more fundamental principles, the reliability and scope of the phenomenological laws could never be adequately determined. 6. DO THEORETICAL ENTITIES HAVE GREATER PLAUSIBILITY THAN THEORETICAL LAWS? By convention there is sweet, by convention there is bitter, by convention hot and cold, but in reality there are only atoms and the void. Democritos (1966, p. 183)</page><page sequence="23">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 267 To most of us it would seem absurd to deny the existence of objects viewed through an inexpensive microscope or telescope. After all, these devices just provide extensions of our visual faculties in much the same way as spectacles do for those with defective vision. However, science employs many small steps, equally as plausible, in arguing from the familiar to the totally bizarre. If we don't want to accept the bizarre findings as literally true of the world, we must look a little more closely at the reasoning used to derive these results. If we concede that the reasoning is essentially valid, then we encounter problems at the dis tance scales explored in particle physics where it becomes difficult even to discriminate the entity postulating and law postulating aspects of the theory. Here the distinction between fields and particles become blurred and much of what constitutes the 'covering laws' of particle interactions may be re-expressed as postulates about the existence of certain 'virtual particles' with certain couplings. It is shown below that strong argu ments exist for treating even virtual particles as real entities postulated by the theory. A useful distinction is often drawn between those entities which could in principle be directly observable to us if our perceptual apparatus were greatly refined (e.g., bacteria) and those which would be observable to us if we had different components to our perceptual apparatus (e.g., electric fields). In the first case the development of apparatus to extend our existing faculties may be cross-checked directly by the calibration technique which renders the unfamiliar plausible by demonstrating, with the same instrument, the observation of the familiar. This tech nique can further proceed by building on the familiarity established at an earlier stage. The development of astronomy illustrates this proced ure. We find with Galileo that his early studies of the Moon's surface reveal in much more detail familiar features, and this lends credibility to his unexpected results concerning the hitherto undetected three little stars which followed Jupiter across the heavens, slowly shifting their positions relative to that planet. The estimation of cosmological distance scales in modern astronomy provides a further illustration of the use of calibration technique.7 Starting with the measured distances of bodies within the solar system obtained via time of flight radar techniques, the diameter of the earth's orbit may be accurately determined. Using this information the dis tances of nearby stars (within ~50parsecs) can be determined from parallax measurements. (The limiting factor being atmospheric turbu</page><page sequence="24">268 P. P. ALLPORT lence for terrestial measurements; much better results should have been obtainable with the ill-starred Hubble large orbiting telescope.) Another technique, using the measured Doppler shift and proper mo tion of stars assumed to lie in the same cluster, allows the sample of stars with known distance to be increased from a few hundred to several thousand. Relationships between absolute magnitude and composition (absorption spectra), temperature (colour), etc., can then be derived. Using such classifications the absolute magnitudes of more distant stars can be estimated, and, from this and the relative magnitude, the dis tance is easily derived. Using this 'standard candle method' the distance of more exotic objects can be estimated if they can be shown to lie at the same distance as a familiar type of star (in a binary system or the same cluster). Some stars, Cepheid variables and RR Lyrae stars, are found to show fluctuating luminosity and the period is closely related to the stars estimated absolute magnitude (where the distance can be determined from that of more conventional neighbours). Where these stars are also found in nearby galaxies, the distance of those galaxies is given by the variable stars, relative magnitude and period. From this sample of galaxies a classification scheme again emerges which allows the standard candle method to be used on a much larger scale. Then from Hubble's observed relationship between estimated distance and red-shift (Doppler shift), the fantastic distances of such exotica as quas ars, gravitational lenses, and the putative remnants of cosmic strings are found. None of these steps seems implausible and yet many who could accept each link in this chain of reasoning may hesitate about accepting its conclusions. Maybe it is best to bear in mind van Fraassen's warning against mistaking a difference of degree for no difference at all (1980, p. 16). It might even be possible to argue for a gradual degradation in plausibility, going from each step to the next. Without such a move, though, a sceptic must clearly decide where the argument presented is defective and what further conditions need to be met for it to ever be considered satisfactory. For those theoretical entities which are not, even in principle, per ceptible via an extension of any existing human faculty, the sceptic is on much stronger ground. Although it may be argued that some degree of direct detection of gravitational, electric, or magnetic fields is pos sible for humans (or at least some fellow species), the physiological effects may be of the nature of secondary influences. An example of what I mean here would be the heat experienced from a metal ring</page><page sequence="25">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 269 when it is worn on a hand which is passed through a high intensity, oscillating magnetic field, say, a de-gaussing coil. (I suppose we could consider such jewelry as a form of primitive scientific instrument in this context, but, as with fire, there is an instinctive tendency to regard something's being a source of pain as sufficient grounds for ontological commitment!) As already indicated above, for the discussion of fields I would tend to sympathize with the approach, implicit in the Lagrangian formalism, whereby the trajectories that charged test particles would follow given initial positions and momenta fully define the influence of a potential. ('Charge' being understood as whatever is the appropriate coupling for that field.) These trajectories in the presence of other charged bodies are then described by the appropriate gravitational or electrostatic equations. This treatment then clearly distinguishes charged bodies from the fields through which they move, and it seems possible to allocate different ontological status to the particles and the fields (the latter only having meaning through their influence on the motion of the former). It seems to me that Cartwright must also adopt a distinction, similar to the above, between things and the laws governing their motion. She writes (1983, p. 99): I believe in theoretical entities. But not in theoretical laws. But is the entity/law distinction really so clear? The study of electrody namics illustrates the interConnectivity of fields and those propagating electromagnetic waves whose influences over many parsecs were dis cussed earlier in the context of astronomy. Is it not now necessary to treat these self-sustaining fluctuating fields as entities in their own right? With quantum electrodynamics the position is even less clear. Why should a beam of high energy photons have any different ontological status from that of any other beam of neutral particles? The effects on nuclear targets are far from dissimilar. Even though it is clear that QED is not a theory that Cartwright would endorse, it is still necessary to determine which are the theoretical entities and which are the theo retical laws in this context. There is the suggestion from Hacking,8 supported by Cartwright, that the entities are the causal agents, the possible tools for use in further investigations; but where do virtual photons, Ws and Zs, which are used to probe the nucle?n sub-structure, fit into this picture (see Redhead 1988, p. 9; Harr? 1988, p. 59)? In</page><page sequence="26">270 P. P. ALLPORT fact, once one is prepared to grant 'entity' status to virtual photons and the like, then, given that these couple to some of the rest of the gamut of elementary particles, it is possible to reconstruct many of those features of the standard model that one had previously taken as being fundamental laws (Aitchison and Hey 1982, p. 29). But probably the reader feels virtual particles are a paradigm case of theoretical fictions. Let me explain below why to me this does not seem obvious. Virtual particles are particles for which the magnitude squared of the energy-momentum 4-vector, (v,q), is not equal to the mass of the particle squared. Such a particle is said to be 'off mass shell' and the special relativity relationship of rest-mass, energy, and momentum is violated. The existence of such particles is, however, allowed in quan tum field theory over time scales, At, such that the Heisenberg relation AvA? ~ h is satisfied (A(v2) being the difference between the magnitude of the 4-vector squared and the mass squared). The virtual mass squared, (-Q2) is then defined to be the magnitude squared of the energy-momentum 4-vector, m2 - A(v2), which will always be negative for virtual particle exchange in scattering experiments. The duration of the fluctuation can then be seen to equal ~ft(2v/A(v2)) = (2?v/(Q2 + m2)), which is only large for small Q2 (in fact small Q2lv) and small m. In experiments using high energy beams of real photons (Q2 = 0), the nuclear cross-section (the probability of interacting with a nucleus expressed as the effective area each nucleus presents to the incoming beam) is found not to simply increase linearly with the number of nucl?ons in the nucleus. Instead, it increases more slowly, in an anal ogous way to that found for beams of strongly interacting particles (hadrons) (Glauber 1969). The analogy with beams of hadrons is under stood in terms of the vector meson dominance model (Stodolsky 1967; Grammer and Sullivan 1978) mentioned previously. In effect the photon interacts as if it were a strongly interacting vector meson for which the absorption length in nuclear matter (typically, ~3x 10~15m) is less than the dimensions of most nuclei. The latter explains the departure from a linear dependence on the number of nucl?ons as due to a 'shadowing' effect, with the nucl?ons on the down-stream side of the nucleus not contributing to the cross-section (they are 'shadowed' by the nuclear matter between them and the beam). In deep inelastic scattering experiments involving charged particles, the interaction between the projectile and the struck nucleus is de</page><page sequence="27">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 271 scribed by the exchange of a virtual photon between the two which carries both energy and momentum, but for whose fleeting existence energy conservation is violated. These experiments are said to probe the sub-structure of the nucle?n with spatial resolution given roughly by cAi (= c?/Av). These experiments provide the main source of infor mation on the putative quark sub-structure of matter. No departure from the expected linear dependence of the total cross-section on the number of nucl?ons is found as is expected for such hard scattering processes where the virtual photon couples directly to the 'struck quark'. However, for soft scattering processes, where the spatial reso lution, cAi, is only of the same order as the nucle?n dimensions (10~15 m), a remarkable effect is observed. As the fluctuation duration, A/, becomes long enough for the virtual photon to traverse many nu cl?ons, the cross-section again displays the shadowing phenomena seen with real photons (Caldwell et al. 1979). In fact as Q2 -&gt;0, i.e., as the virtual photon becomes more 'real' (on mass shell), its cross-section on different nuclear targets approaches closer and closer to the values measured for beams of real photons (Goodman et al. 1981). Virtual photons really do seem to behave like real photons, which only last for as long as the Heisenberg indeterminacy principle allows. The above illustrates the difficulties involved in distinguishing theo retical fictions from objects possibly worthy of ontological commitment and, on the basis of the prior discussion, the difficulties in cleanly separating features of a theory which are to be designated laws from those which seem to postulate entities. There really seems to be no decisive divide between the plausible and the implausible in theories concerned with matters unfamiliar. Sklar writes in his chapter "Seman tic Analogy" (1985, p. 229): Perhaps it is not too unreasonable to view bacteria as, among other things, tiny objects too small to see. Even for molecules the analogy is 'good enough' to allow us to at least plausibly maintain that we understand the kinetic theory of gases on the analogy with a box filled with rapidly moving billiard balls. But when we get to electrons, quarks, photons, or worse yet, virtual intermediate massless bosons, charm etc. what earthly use can we make of analogy as a source of meaning? I think there are two things that can be said. First, even in some pretty recherch? cases there is still a certain amount of theoretical predication going on which can at least plausibly be argued to be understood in the analogical way. Even if virtual particles are sufficiently remote from dust particles in their properties that to speak of them as particles at all is just to mislead, still they do have, perhaps, spatiotemporal location, momentum, energy etc., and perhaps these can be understood on analogy with those features when</page><page sequence="28">272 P. P. ALLPORT predicated of objects of experience. Second, insofar as the features of things cannot be understood on any analogy whatever with features of the elements of experience, we can, at this point, always retreat to an instrumentalistic denial of full meaning to the predicates altogether. Unfortunately I can see no reason why, with sufficient ingenuity, anal ogies cannot be stretched to fit any feature of a theory. Therefore, in practice, it still remains undetermined which conditions should be considered grave enough to precipitate the kind of retreat Sklar contem plates. It has been argued in this section that the rigorous application of calibration techniques does encourage belief in the existence of many hypothetical entities. However, calibration techniques are fundamen tally based on an argument by analogy from the familiar to the unfamil iar as observed by the same instrument. At the limits of current research in particle physics and astrophysics, the step-by-step application of this argument has lead to the postulation of some highly bizarre objects. Lack of belief in these objects must then require a lack of confidence either in some specific application of this reasoning or in its general validity. In the former case, it is incumbent upon the critic to identify the erroneous step. In the latter case, even belief in well-established theoretical entities must seek an alternative justification. If we are to take as the theoretical entities the causal agents postu lated by the theory, then I cannot see how we avoid the inclusion of virtual particles, fields with non-zero vacuum expectation values, and the like in our list of putative physical objects. But then it is no longer possible to distinguish clearly the postulation of entities from that of the laws governing interactions. (Are we in fact to take real/virtual photons, electromagnetic fields, etc., as postulated entities? Or does the theory only postulate laws, such as Coulomb's, which govern the relative motions of the 'observable' charged particles?) It seems, then, that the brand of scientific realism that supports theoretical entities but rejects theoretical laws faces difficulties not only in clearly establishing that the postulated entities necessarily have greater plausibility but even in clearly distinguishing the aspects of the theory which can be taken as postulating entities from those postulating laws. 7. ARE THE LAWS OF PHYSICS OPPRESSIVE? The hydroelectric plant is not built into the Rhine River as was the old wooden bridge that joined bank with bank for hundreds of years. Rather the river is dammed up into</page><page sequence="29">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 273 the power plant. What the river is now, namely, a water power supplier, derives from out of the essence of the power station. In order that we may even remotely consider the monstrousness that reigns here, let us ponder for a moment the contrast that speaks out of the two titles, 'The Rhine' as dammed up into the power works, and 'The Rhine' as uttered out of the art work, in H?lderin's hymn by that name. But it will be replied, the Rhine is still a river in the landscape, is it not? But how? In no other way than as an object on call for inspection by a tour group ordered there by the vacation industry. Martin Heidegger (1977, p. 16) In his essays 'The Question Concerning Technology', 'The Age of The World Picture, Science and Reflection', and 'The End of Philosophy', Heidegger develops a vision of post-Nietzschean Western civilization as dominated by the metaphysics of power; thinking becoming so shackled by a crass instrumentalism, itself the very essence of technol ogy, that the only issue of interest is that of utility, whether an object or idea enhances or diminishes the individual's power. The very 'ob jectivity' of science owes its existence to the radical dualism insisted on by Descartes, without which no such subject/object division between intellect and instrument could ever have been contemplated. The scien tific 'world picture' and even the concept of a 'world picture' are devel opments of the modern Western metaphysics whose growth has ac companied the rise of science. It is useful to consider Heidegger's views as these have not only had important influence in both philosophical and literary circles, but they are also representative of an increasingly popular (and respectable) movement against the tyranny of the Western scientific 'world picture'. Some contemporary philosophers of science, most notably Feyerabend, show sympathy with this movement and treat the accepted 'world pic ture' as just one amongst many whose rivals have been either simply ignored or suppressed by an authoritarian 'scientific establishment'. This inspires a view of the laws of physics as 'holy lies' (Nietzsche 1968, p. 175) or 'Sunday truths' (Russell 1950);9 mysteries to be understood only by experts and not to be revealed to the profane. Most of the scientific enterprise, most of the time, is directed by adherence to accepted dogma to which the young initiate must show unfailing al legiance throughout his training up to the time of his Ph.D. viva, the ceremony that immediately precedes his scientific ordination.10 This view of science, so completely at odds with scientists' own view of themselves as scrupulous liberals in intellectual matters, seems to find expression in Kuhn's highly influential The Structure of Scientific Revo</page><page sequence="30">274 P. P. ALLPORT lutions. Here, science is conceived as progressing by the main through periods of intellectual stagnation, denoted 'normal science', where the task of the scientist is to solve puzzles within the accepted theoretical framework (the paradigm) and where failure in puzzle solving reflects only on the individual researcher, never on the paradigm. Only when puzzle solving within a given tradition becomes so tough that it sparks of a schism in the scientific community is there significant theoretical upheaval. It is unclear from Kuhn's diagnosis whether he considers this state of affairs healthy or otherwise. As Feyerabend writes (1970, p. 198): More than one social scientist has pointed out to me that now at last he has learned how to turn his field into a 'science' - by which of course he meant how to improve it. The recipe, according to these people, is to restrict criticism, to reduce the number of comprehensive theories to one, and to create a normal science that has only one theory as its paradigm. Students must be prevented from speculating along different lines and the more restless colleagues must be made to conform and 'to do serious work'. Is this what Kuhn wants to achieve? In contrast to this, Feyerabend endorses methodological anarchism as stimulating the widest possible proliferation of theories and a corre spondingly liberal attitude to rival 'world pictures'. Then, when a parti cular paradigm begins to collapse, there will be a large supply of rivals able to show up its weaknesses and take up the running. It is not even clear whether what Feyerabend proposes should happen does not indeed happen, and not only during the 'scientific revolutions' but also during the long, dull intervening periods of 'normal science'. Feyer abend reports of Kuhn (1970, p. 206): He has pointed out more than once, in complete agreement with our methodological remarks, that refutations are impossible without the help of alternatives. Moreover, he has described in some detail the magnifying effect which alternatives have upon anomalies and has explained how revolutions are brought about by such a magnification. He has therefore said, in effect, that scientists create revolutions in accordance with our little methodological model and not by relentlessly pursuing one paradigm and suddenly giving up when the problems get too big. Feyerabend goes on to say he suspects that normal or 'mature' science, as described by Kuhn, is not even a historical fact. From the somewhat critical tone of Feyerabend's books Against Method, Science in a Free Society, and most recently Farewell to Reason I think it is fair to assume that he does not believe methodological anarchy to be a historical fact either.</page><page sequence="31">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 275 The charge of illiberalism is best examined through the treatment of heretics. It is unfortunately true that one does encounter successful individuals in science whose treatment of proponents of rival systems is at best patronizing and at worst manifestly uncivil. However, there seems to be some evidence that such ill-mannered behaviour is also found in other academic disciplines, and this is not sufficient to establish the existence of a conspiracy against non-conformists. What is perhaps more worrisome is the way that some pre-eminent scientists seem to be lionized by adoring audiences, as they follow their 'world tour' through the international conferences. It is too often these audiences shift uncomfortably in their seats when a proponent of a highly contro versial theory takes the stand. There seems to be an acute feeling of embarrassment on behalf of the talker, who seems oblivious to the fact recognised by everyone else, that he is making a fool of himself in front of the experts. (Of course, in many cases this may be true!) On the other hand, there is no reason to assume that all conferences or conference audiences are hostile in this fashion, and there is even an increasing trend in physics to drop the taboo on questioning quantum mechanics itself. This was recently illustrated by the rather open discussion on the foundations of quantum mechanics held at the 1987 Schr?dinger Centenary Conference in London. (There has also been a spate of books in the last few years by both physicists and philosophers of science critically examining precisely this issue and re-evaluating the 'pilot-wave' theories of de Broglie and B?hm.) What are we then to make, in practice, out of Feyerabend's call for a sort of 'anything goes' methodology? Clearly there are enormous problems in ensuring equal treatment of all research programmes. Apart from the problem of funding travel budgets to allow equal oppor tunities for all to an international audience (without which any system of funding must fall back on the restrictive practice of peer review), there is also the enormous problem of attracting any skilled theoreti cians to work on a programme which seeks to rebuild the current edifice from near its base. The sheer amount of effort that has been expended in devising and working out the consequences of a theory like the standard model make it very difficult to see how a small team could create any form of rival structure. In a sense Kuhn's picture is justifiable in that some research programmes (paradigms) gather so much inertia that they may continue rolling long after they run out of steam. If the standard model, for example, is in broad details correct (the expected spectrum of particles is confirmed and the gauge couplings</page><page sequence="32">276 P. P. ALLPORT are as predicted), then to have sponsored in anticipation of its failure a number of rival research programmes would be quite a squandering of intellectual and financial resources. Is Feyerabend's liberalism, even if intellectually justified, financially prohibitive? Ackermann writes (1985, p. 28): With modern big science, there may be economic silliness in proposing that every conceiv able alternative tradition should be equally supported and encouraged. Perhaps just what is done is optimal for the rate of growth of science; whether true or not, this is just as unprovable as the advantages of anarchy. He concludes his criticism (1985, p. 29) of Feyerabend's position by conceding that anarchy or dadaism compromises empiricism and rationalism, but in the laziest possible fashion. Although, of course, proving that an argument is easy to defend is not usually considered a sufficient refutation. What I think Feyerabend is probably most critical of is the way many old traditions have been swept away without any attempt to study the relative merits of modern and ancient wisdom. In some areas, due to the complexity of the issues involved, it is now relatively easy to see with hindsight how recklessly 'modern' techniques have been imposed, where in practice the older techniques had unrecognised advantages. It remains, of course, a contentious issue whether these conclusions also apply with regard to scientific methodologies. Feyerabend certainly succeeds in his books in giving many protagonists of science a bad odour, although it is not clear whether this is to be taken, in itself, as sufficient condemnation of the scientific tradition(s) they defend. (I take it that we are not to take discrediting the origins of a belief as sufficient grounds for discrediting the belief itself, the sort of 'genetic fallacy' of which Nietzsche is guilty in The Antichrist.) Whilst it is difficult in today's liberal intellectual circles to assert anything that we can have privileged access to knowledge, to the experi mental scientist, who may well cast himself in the role of an explorer, it seems obvious that the fruits of his labour are discoveries to which initially he and his colleagues alone have access. Would we repudiate such a claim from explorers returning from previously uncharted terri tory as elitist? For this reason, I think it is rather difficult for anyone actively engaged in a field of pure research to endorse a thoroughgoing</page><page sequence="33">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 277 relativism. This is not, I believe, simply a manifestation of 'theory immersion', as van Fraassen would assert (1980, pp. 80-81). The working scientist is totally immersed in the scientific world-picture. And not only he - to varying degrees so are we all. If I call a certain box a VHF receiver, if I call a fork electro-plated, if I so much as decide to turn on the microwave oven to heat my sandwich in the cafeteria, I am immersed in a language which is thoroughly theory infected, living in a world my ancestors of two centuries ago could not enter. Now I don't know whether the last remark is a reference to the, by now, rather overplayed hand of incommensurability, but I do think that there are facts about the world we inhabit which we have acquired empirically, which contradict the views of other civilizations, and which can, nevertheless, be said to be objectively true of the world. That is, I do not believe there is some enormous conspiracy amongst the airline companies to conceal from their customers that the topology of the world is not in fact that of a sphere; I do not see how anyone but a paranoiac could sincerely imagine that the entire American and Russian space programmes have been nothing but orchestrated propaganda designed to brainwash the public with Newtonian mechanics and Coper nican cosmology; and I would be reluctant to accept that the holocaust that was inflicted upon Hiroshima and Nagasaki had nothing to do with the instability of atoms with heavy nuclei. I wish good luck to the sceptic who thinks he can sincerely dissent on all these issues, but I think for the rest it just smacks of hypocrisy to claim any practical doubt concerning these matters. And if this is conceded, I cannot see how a thoroughgoing cultural relativism is to be maintained, as it is no longer possible to assert that our knowledge claims are merely true or false relative to social context. It is conceded that there could well be incommunicable ideas specific to certain communities with alien 'world pictures'; but is it correct to then conclude that since there are such incomprehensible aspects, no judgement concerning the correctness of different 'world pictures' is possible? No doubt to the committed relativist, the assertion that rival cosmologies and cosmogonies are simply mistaken smacks of high arro gance; but given the discoveries made in the light of the 'scientific world picture' it is also simply patronizing to affect merely some kind of 'world picture egalitarianism' in the name of metaphysical liberalism, whilst in practice only taking scientific cosmologies and cosmogonies</page><page sequence="34">278 P. P. ALLPORT seriously. (And how many 'educated' people of any nation can still treat geocentric cosmologies as serious contenders?)11 Of course there is more than a grain of truth in some of the criticisms of the scientific community. It is certainly true that the large power structures that now exist do engender a certain uniformity of approach which can endanger creativity. Nevertheless, I do not see sufficient evidence for the assertion (which I do not attribute to Cartwright or van Fraassen) that the laws of physics are deliberate lies, aspects of a vast propaganda drive sponsored by a scientific elite who wish to set up a world-wide secular religion dedicated to indoctrination with the teachings of materialism. 8. DO AESTHETIC CONSIDERATIONS FOSTER DISHONESTY? The scientific man is the further development of the artistic man. Friedrich Nietzsche (1910, p. 205) There has already been some discussion of the anti-realist assertion that laws are oversimplifications which disguise the richness and diversity of the phenomena.12 One motivation for seeing things as simpler than they are, for which there is firm evidence in the popular writings of theoretical physicists, is aesthetics. It is common to treat Dirac's (1963) notorious remark It is more important to have beauty in one's equations than to have them fit experiment as simply facetious. However, too many other leading theoreticians have made remarks in a similar vein for the issue of aesthetics to be so easily dismissed. It is noteworthy that mathematical elegance and simplicity are regarded as necessary conditions for greatness in a theo retical work. After presenting the opinions of several renowned theore ticians on the subject, I propose to examine the possible grounds for such an aesthetic bias. It is difficult, however, to see how attribution of properties such as beauty or simplicity to the universe can evade the charge of anthropomorphism. The first passage to which I wish to draw attention is from the book Uncommon Sense by Oppenheimer (1984, p. 135), one of the main contributors to the development of quantum field theory (although better known for his role as the wartime director of the Los Alamos Laboratory).</page><page sequence="35">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 279 The things that make us choose one set of equations, one branch of inquiry rather than another are embodied in scientific traditions. In developed science each man has only a limited sense of freedom to shape or alter them; but they are not themselves wholly determined by the findings of science. They are largely aesthetic in character. The words that we use: simplicity, elegance, beauty, indicate that what we grope for is not only more knowledge, but knowledge that has order and harmony in it, and continuity with the past. An example of this aesthetic bias in action dates from the time when the experimental evidence started to show that the electron possessed an additional magnetic moment to that predicted by the Dirac equation. Schwinger (in Brown and Hoddeson 1983, p. 332) cites Breit's hesitant remarks, inspired by discrepancies between the highly accurate mea surements of the Lamb shift and the prediction, as the first published reference to the possibility that the electron had contributions to its magnetic moment other than those given by the Dirac equation. Never theless, Breit wrote (as quoted by Schwinger in Brown and Hoddeson 1983, p. 332): It is not claimed that the electron has an intrinsic moment. Aesthetic objections could be raised against such a view. In fact, Breit turned out to be correct in his disavowal of such an ad hoc manoeuvre as being too inelegant a solution; but his statement does show the unwillingness of the theoretical community at that time to countenance departures from the simplicity of the Dirac treatment. (As mentioned above,13 the agreement between the measured value of the electron magnetic moment and that predicted using higher-order QED calculations is now taken as compelling evidence for the theory. What was once an unpalatable experimental anomaly, developed into a theoretical triumph. Here, though, as with the case of the symmetries C, P, and CP,14 aesthetic prejudices seem to have initially had an adverse influence on the interpretation of the available data.) More recently, on the topic of unification, Freedman and Nieuwen huizen have written (1978, p. 126): [P]erhaps the most ambitious goal of modern physics is to find in the diversity of particles and forces a simpler underlying order. In particular, a more satisfying understanding of nature could be achieved if the four forces could somehow be unified. Weinberg, one of the three who were awarded Nobel prizes for the unification of the weak and electromagnetic theories, summarizes this outlook with his assertion that (1974, p. 50):</page><page sequence="36">280 P. P. ALLPORT One of man's enduring hopes has been to find a few simple general laws that would explain why nature, with all its seeming complexity and variety, is the way it is. It is a common, though seldom articulated, notion amongst both theo retical and experimental particle physicists that simplicity is the hall mark of truth. The quest for a 'grand unified theory' is at least partly motivated by this belief. Weinberg has this to say about the quest for a single comprehensive theory of everything (1988, p. 29). First of all, let me say what I mean by a final underlying theory. Over the last few hundred years scientists have forged chains of explanation leading downward from the scale of ordinary life to the increasingly microscopic. So many of the old questions - Why is the sky blue? Why is water wet? and so on - have been answered in terms of the properties of atoms and of light. In turn, those properties have been explained in terms of the properties of what we call the elementary particles: quarks, leptons, gauge bosons and a few others. At the same time there has been a movement towards greater simplicity. It's not that the mathematics gets easier as time passes, or that the number of supposed elementary particles necessarily decreases every year, but rather that the principles become more logically coherent; they have a greater sense of inevitability about them. My colleague at Texas, John Wheeler, has predicted that, when we eventually know the final laws of physics, it will surprise us that they weren't obvious from the beginning. I don't know if we will ever get there, in fact I am not even sure that there is such a thing as a set of simple, final, underlying laws of physics. Nonetheless I am quite sure that it is good for us to search for them, in the same way that Spanish explorers, when they first pushed northward from the central parts of Mexico, were searching for the seven golden cities of Cibola. They did not find them, but they found other useful things, like Texas. (Professor Weinberg is currently employed by the University of Texas.) It is probably true that the views of science held by scientists are at least as diverse as those held by philosophers of science. Nevertheless it does seem that the popular writings of contemporary theoretical particle physicists abound in such statements and it is notable that in technical presentations the mathematical elegance of a theory is stressed much more than its empirical adequacy. It is widely anticipated that the correct solution will be a 'neat solution'. Some justification for seeking a single, monolithic, all-embracing theory is suggested by the principle of Ockham's razor. Certainly one result of unifications is to reduce the number of independent principles posited; but usually at the expense of enhanced abstraction. Although this provides some methodological justification, it does not explain why in the passages quoted it is not the pragmatic grounds for simplicity</page><page sequence="37">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 281 that are stressed but the opinion that simpler laws 'have a greater sense of inevitability', i.e., they have the ring of truth to them. To the anti-realist this aesthetic bias must seem strongly supportive of the view that the issue of theory choice is a question of choosing between different models for which often only aesthetic grounds can be found for making any decision. This then undermines the realist's claim that the theories themselves mirror the workings of the world, since it is difficult to believe that there can be any grounds for believing that our aesthetic standards reflect any absolute principles operative in the universe. I believe that these problems pose some tricky philosophical ques tions which ought to be addressed. Holton took the opportunity of discussing some of these issues when he presented the 1981 Jefferson Lecture (1986). He draws attention (1986, p. 284) to the fact that much of pure research is motivated by a constant search for unities and simplicities behind nature's spectacle of variety but then what can we say of the presupposition that such 'unities and simplicities' are there to be found? Holton (1986, p. 284) finds himself forced to confess that like all who have struggled with this question, I find it utterly mysterious that such a search should be so successful. One solution (already alluded to in the previous section) is that the scientific enterprise is to be understood as effectively seeking to re-cast the image of the world much as a bronze statue may be melted down to form a new figure. To such a view, the raw materials of perception lack any meaning prior to their being formed into a particular image. It is then this drive to remould the 'apparent' world which betrays the sublimated will to power of not only the poet, artist, and philosopher (Nietzsche 1968) but also that of the scientist. With such a man uvre one certainly can avoid the question 'Why should the world accord with our presuppositions?' by arguing that the world of perception (assertedly, the only world it makes any sense to talk of) is largely or wholly determined by our choice of interpretation (the 'world picture' we adopt). It is thus claimed that, as for great movements in the history of art, science has now managed to provide a way of looking at the</page><page sequence="38">282 P. P. ALLPORT world which is so seductive that its validity has ceased to be questioned (Feyerabend 1987, p. 143). One could perhaps envisage an analogy between the activities of the scientific community and those found in the studio of a painter commissioned to work on an extensive canvas. Whilst the master deter mines the broad outlines and concentrates his talents on those features of most difficulty or importance, many of the less significant details on the canvas can be left to be filled in by apprentices. Similarly within a particular scientific paradigm, a picture of the world is developed which originates in the vision of one (or at most a few) great thinker(s) whilst the details remain to be provided by many other contributors. This view also makes it highly significant that a common theme to be found in the writings of most pre-eminent theoretical physicists is their belief in the one true answer - the small set of equations dreamt up by God prior to His retirement when the temperature of the universe dropped below the Planck scale. It is precisely this 'monotheorism' which now stands in need of justification. It has been argued above that just as art is be seen as imposing an interpretation on its subject so also is science. In both cases we gain ascendancy over the world by recreating it in our image. Holton writes of the 'monotheorist' position (1986, p. 285): There was the Holy Grail: the mastery of the whole world of experience, by subsuming it ultimately under one unified theoretical structure and he proceeds to characterize the aim of science as omniscience concerning the world accessible to positive science. Holton traces this motivation for science back through to the Newtonian tradition. Nietzsche's writings (1974, p. 335) show that there were certainly clear manifestations of such an outlook among nineteenth century physicists. It is no different with the faith with which so many materialist natural scientists rest content nowadays, the faith in a world that is supposed to have its equivalent and measure in human thought and human valuations - a 'world of truth' that can be mastered completely and forever with the aid of our square little reason. What? Do we really want existence to be degraded for us like this - reduced to a mere exercise for a calculator and an indoor diversion for mathematicians? Above all, one should not wish to divest existence of its rich ambiguity: that is a dictate of good taste gentlemen, the taste of reverence for everything that lies beyond your horizon. That the only justifiable interpre tation of the world should be one in which you are justified because one can continue to</page><page sequence="39">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 283 work and do research scientifically in your sense (you really mean, mechanistically?) - an interpretation that permits counting, calculating, weighing, seeing and touching, and nothing more - that is a crudity and naivete, assuming it is not a mental illness, an idiocy. The assertion then is that as scientists we impose on the world an interpretation chosen to accord with our aesthetic prejudices. To some extent I have attempted to answer this charge in the previous section. Here I will attempt to see if a realistic case can in fact be made for 'monotheorism'. For a start, it is possible that a single-minded adher ence to some such principle has had practical advantages. One might argue that, whilst conceding that one cannot tell a priori whether, say, a mechanistic interpretation is adequate (most people would now assume a more subtle approach is needed), one has learnt immeasurably more by pushing a given metaphysical principle until its deficiencies are manifest than by abandoning the venture ab initio. I think, though, that this pragmatic argument is insufficient to do justice to the problem under discussion. Clearly for a realist, the sort of argument we have been outlining, that science is an aesthetic evaluation of the world, must be unaccept able. One can then either deny the evidence that aesthetics plays a role in theory choice altogether, whilst perhaps allowing that such criteria may play some part in theory conception, or one can boldly assert that something corresponding to what we call 'simplicity' or 'neatness' is true of the world. Within the framework of a realist interpretation, the best justification, I believe, for adopting the aesthetic criteria discussed above is that cosmology can only be possible as a science if it is assumed that the development of the entire universe has been governed in the main by a finite set of all-embracing laws. Laws that one can provision ally take as being those found to account adequately for terrestial phenomena. It then becomes the case that observational support for cosmological models derived from accepted laws of physics provides a degree of corroboration for both the principle of universality and that of essential simplicity. If our theories are only those interpretations adequate to account for the available terrestial measurements made in some highly contrived environments, then, since the evidence for them in more commonplace environments is at best indirect for the reasons discussed in Section 4, one might expect even less success in accounting for astronomical phenomena. I would argue that, to the same extent that one considers the laws of physics to have instead proved successful in this last respect (and clearly there are enough anomalies to render</page><page sequence="40">284 P. P. ALLPORT this an open question), one should also consider that God's will, the universe, or the underlying laws have shown themselves to be relatively straightforward in the sense Weinberg intimated above. Of course, if the perceived world is considered to be malleable enough, then the fact that two such diverse fields of enquiry as cos mology and particle physics share the same theoretical foundations need not be a telling argument for the validity of those foundations. However, it is a remarkable fact that using particle physics theories, with the simplest of boundary conditions, has turned out to be very successful in cosmology. This, I think, does support the belief that principles like unity, elegance, and simplicity could reflect genuine features of the world, rather than necessarily just providing further evidence for a 'human, all too human' aesthetic bias in theoretical physics. It is towards establishing the truth (or otherwise) of this as sertion that much of the current efforts in pure physics research are directed. I think Popper represented the aspirations of many scientists working in this, unfortunately rather esoteric, field when he wrote in the Preface (p. 15) of the 1959 English edition of The Logic of Scientific Discovery: I however believe that there is at least one philosophical problem with which all thinking men are interested. It is the problem of cosmology: the problem of understanding the world - including ourselves, and our knowledge, as part of the world. All science is cosmology, I believe, and for me the interest of philosophy no less than science, lies solely in the contributions which it has made to it. For me, at any rate, both philosophy and science would lose all their attraction if they were to give up that pursuit. It remains to be seen how successfully an integrated account of the world can be derived from a single (grand?) unified theory. I think there can be no a priori justification for the metaphysical position of 'monotheorism' (although I think such an ambitious approach may have some practical advantages), but one has already seen some surprising a posteriori support (for example, in successfully accounting for the relative element abundances (Weinberg 1977; Boesgaard and Steigman 1985)). It should also be noted that some of the limits implied by cosmological considerations will be accessible to measurement at future terrestial experiments allowing an independent check on the validity of the cosmological arguments employed (Dolgov and Zeldovich 1981; Schramm and Steigman 1988). The best argument for the adoption of aesthetic criteria in theory choice has to be the past success of this policy. The reason for that success (assuming one concedes that the</page><page sequence="41">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 285 scientific enterprise is successful, at least according to its own criteria) does, however, pose a deep metaphysical question. 9. CONCLUSIONS [T]he physicist's image of reality is rooted in a sort of meta-universe of mathematical objects and relationships that are concrete, eternal and totally dependable, while the Universe is nebulous, shifting and unpredictable. Paul Davies (1988, p. 60) We have heard how the laws of physics stand accused of perjury. The case relies on the purportedly deceitful nature of the claim that it is the laws that explain or predict phenomena and justify belief in phenomenological relationships or theoretical entities. Whilst not dis puting that the 'covering law' mode of explanation is frequently em ployed, it has been argued that this has not been convincingly shown to be misleading. Indeed, in examples taken from high energy physics, the fundamental theories do lead to unambiguous predictions and, moreover, these can often appear highly counter-intuitive if a causal/ mechanistic interpretation is sought. It has also been found that in this field (which has undergone much recent development) the distinctions between fundamental and phenomenological laws or between law and entity postulation are just not well defined. The value of the underlying theories is seen as more than just summarizing less general principles but also as showing their interconnected nature (so that a revision in one part of the structure may well have implications in many other areas) and accounting for their scope and limitations. On the other hand, it does have to be conceded that aspects of these theories may be difficult to interpret literally.15 The case for the prosecution has been largely based on examples drawn from more applied areas of research, where the primary goal is not simply the testing of fundamental principles but the provision of an adequate mathematical basis for developing technological innovations. Here the emphasis is not on isolating the conditions under which direct results of the 'covering laws' are manifest, so it may not be surprising that many assumptions are needed to derive predictions; predictions whose success or failure does not directly reflect on the validity of the fundamental theory. Whilst it is true that the technological achieve ments of science are the most tangible evidence of its success, parti</page><page sequence="42">286 P. P. ALLPORT cularly for those not directly involved in experimental work on the fundamental principles, it does not follow that the underlying laws are useless merely because direct tests require dedicated experiments. Only a myopic pragmatism or an exchequer's epistemology would seem to justify such a view. The assertion that the 'covering laws' are simply lyingly added to the successful phenomenological laws seems to either require belief in a conspiracy within the scientific community or an unconscious prejudice for generality amongst its proponents. The grounds for the former position are found to be completely unconvincing whilst some grounds for the latter position may, in fact, exist. Attention has been drawn to the anti-rationalist and not just anti-realist implications of the existence of an aesthetic bias in our theory selection through several passages taken from the writings of noted antagonists of Rationalism. The prob lem being that if one can develop one set of theories to describe the world in accordance with one set of aesthetic criteria, say 'simplicity', 'neatness', and 'comprehensiveness', then proponents of other criteria may argue that a totally different description awaits to be formulated, consistent with their alternative aesthetic principles. Given that we have already denied the existence of any sharp divide between the fundamental laws and other aspects of our description of the world, if we are to maintain any sort of realist interpretation of physics as a whole, we are driven to asserting that the aesthetic criteria used by the theoretical physics community must be in some sense privileged. The success of cosmology, which depends in part on the validity of these criteria (if the ambitious extrapolations involved are to make sense), lends some credibility to such an assertion. I would then conclude that the case against the laws of physics, which rests on the purported evidence of pervasive dishonesty (with the possible motivation of an aesthetic bias), has not been adequately established. Several grounds for believing that there exists validity in the assertions of fundamental physics have been outlined whilst the evidence for a preference of beauty to truth cannot be deemed incontro vertible. (However, it does seem rather ironic that the 'truth' quark should remain elusive whilst the 'beauty' quark has now become a well-established member of the 'fundamental fermion' family.) The plaintiff's arguments would also seem open to the criticism that they imply all aspects of current basic research must be seen as pointless, if the laws themselves are taken to be lying. Such an extreme position</page><page sequence="43">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 287 seems hardly possible to maintain sincerely. Although it is conceded that, on the odd occasions, the laws of physics may have exercised a certain 'economy with the truth', the accusation of widespread lying has just not been convincingly made. NOTES * I would like to thank my colleagues in high energy physics for their comments on this paper, and I am also particularly indebted to members of the Department of History and Philosophy of Science at Cambridge University for many fruitful discussions on these and related topics. I have benefitted over many years from stimulating philosophical arguments with my friend Alan Birchall, of the NRPB. I would like to dedicate this paper to the memory of John Bell, who had arranged to meet with me at CERN to discuss some of these topics shortly before his tragic death. He was an inspiration to many of us in high energy physics, with interests in the wider issues raised by our subject, and he will be greatly missed. 1 For a discussion of the role of models in physics, see Redhead (1980). 2 One further oddity here is that whilst the Dirac condition (Cheng and Li 1986, p. 455) effectively requires the quantization of charge, no such restrictions exist for the fundamental particle masses. Yet the treatment of both proceeds along similar lines. Charges and masses do naively seem to be different sorts of parameters (Higg's mecha nism notwithstanding). 3 See Section 2. 4 Although quantum theory describes only the evolution of waves and these are notori ously difficult to identify with particles, in high energy physics experiments, which are supposedly investigating the properties of matter at distance scales where quantum effects dominate, it is in fact particles that essentially dictate the subject matter. Indeed, the wave-like properties only manifest themselves, if at all, in rather subtle effects. Maybe something like Bohm's hidden variable theory (B?hm 1952; Vigier et al. 1987, p. 169) provides a sensible compromise in that it allows talk of particle trajectories, etc., but, perhaps surprisingly, his views are hardly treated as orthodoxy. 5 In spite of the freedom to choose different gauges and thus different forms of the potential I think that confining the category of 'fre-ables' to the fields, as Bell does in his introduction to 'The Theory of Local Beables' (1987, p. 53), not only has problems according to Cartwright's analysis but also leads to difficulties in accounting for the Bohm-Ahronov effect (Ahronov and B?hm 1959). 6 Perhaps one could cite the original derivation of the expression for the Baimer series in this context. 7 I believe a very similar (but more involve argument) can also be made for microscopic phenomena. 8 See Section 2. 9 'That cold makes water boil would be a Sunday truth, sacred and mystical, to be professed in awed tones, but not to be acted on in daily life'. 10 Comparison of my own career in physics with that of my brother who is a clergyman in the Church of England does seem to show some such parallels. However, the historical</page><page sequence="44">288 P.P. ALLPORT connections between the universities and the church may be sufficient to explain the similarities in the two traditions. 11 To avoid any misunderstanding it should be stressed that the issue under discussion is not the relevance of the scientific world picture to other cultures. However, it is often unfortunately the case that the issue of technological supremacy does lead to other societies being obliged to adopt some of the Western materialistic values in order to survive. (A fact that raises the important question: How much of our vaunted 'will to knowledge' is merely sublimated 'will to power'?) For example, I would consider it unlikely that, in the event of a nuclear war, those societies whose world picture does not incorporate fissile material would be spared the effects of radioactive fallout. To some extent the global ecological catastrophes with which our technological development now threatens all mankind makes it desirable that as many people as possible become aware of the issues involved so that pressure may be brought to bear on governments to strive to avert such disasters. 12 See Section 5. 13 See Section 2. 14 See Section 4. 15 The conceptual difficulties concerning quantum mechanics itself have only been alluded to in this text. For a much fuller discussion, see Redhead (1987) and Brown and Harr? (1988). REFERENCES Abbott, L.: 1988, 'The Mystery of the Cosmological Constant', Scientific American 258.5, 106-13. Ackermann, R. J.: 1985 Data, Instruments and Theory, Princeton University Press, Princeton. Ahronov, Y. and D. B?hm: 1959, 'Significance of Electromagnetic Potentials in the Quantum Theory', Physics Review 115, 485-91. Aitchison, I. and A. J. G. Hey: 1982, Gauge Theories in Particle Physics, Adam Hilgar, Bristol. Armstrong, D.: 1983, What is a Law of Nature?, Cambridge University Press, Cambridge. Bell, J. S.: 1987, Speakable and Unspeakable in Quantum Mechanics, Cambridge Univer sity Press, Cambridge. Bell, J. S. and H. Nauenberg: 1987, The Moral Aspects of Quantum Mechanics', in Bell (1987), pp. 22-28 (reprint of: 1966, in A. De Shalit, H. Feshbach, and L. Van Hove (eds.), Preludes in Theoretical Physics, North Holland, Amsterdam, pp. 279-86). Boesgaard, A. M. and G. Steigman: 1985, 'Big Bang Nucleosynthesis: Theories and Observations', Annual Review of Astronomy and Astrophysics 23, 319. B?hm, D.: 1952, 'A Suggested Interpretation of Quantum Theory in Terms of Hidden Variables', Physics Review 85, 166-93. Brodsky, S. J., F. E. Close, and J. F. Gunion: 1972, 'Phenomenology of Photon Pro cesses, Vector Dominance, and Crucial Test for Parton Models', Physics Review D6, 117. Brown, H. R. and R. Harr? (eds.): 1988, Philosophical Foundations of Quantum Field Theory, Clarendon Press, Oxford.</page><page sequence="45">ARE THE LAWS OF PHYSICS ECONOMICAL WITH TRUTH? 289 Brown, L. and L. Hoddeson: 1983, The Birth of Particle Physics, Cambridge University Press, Cambridge. Caldwell, D. O. et al.: 1979, 'Measurement of Shadowing in Photon-Nucleus Total Cross Sections from 20 to 185 GeV, Physics Review Letters 42, 553-56. Camus, A.: 1975, The Myth of Sisyphus, trans, by J. O'Brien, Penguin, Middlesex. Cartwright, N.: 1983, How the Laws of Physics Lie, Clarendon Press, Oxford. Cheng, T. and L. Li: 1986, Gauge Theories of Elementary Particle Physics, Clarendon Press, Oxford. Commins, E. D. and P. N. Bucksbaum: 1983, Weak Interactions of Leptons and Quarks, Cambridge University Press, Cambridge. Davies, P.: 1988, 'Law and Order in the Universe', New Scientist 120, 58-60. Democritos: 1966, The Presocratics, trans, by P. Wheelwright, Odyssey, Indianapolis. Dirac, P. A. M.: 1963, 'The Evolution of the Physicist's Picture of Nature', Scientific American 208.5, 45-53. Dirac, P. A. M.: 1978, Directions in Physics, J. Wiley and Sons, New York. Dolgov, A. D. and Y. B. Zeldovich: 1981, 'Cosmology and Elementary Particle Interac tions', Review of Modern Physics 53, 1-41. Feyerabend, P.: 1970, 'Consolations for the Specialist', in Lakatos and Musgrave (1970), pp. 197-230. Feyerabend, P.: 1987, Farewell to Reason, Verso, London. Franklin, A.: 1986, The Neglect of Experiment, Cambridge University Press, Cambridge. Freedman, D. Z. and P. van Niewenhuizen: 1978, 'Supergravity and Unification of the Laws of Physics', Scientific American 282.2, 126-43. Glauber, R. J.: 1969, 'Theory of High Energy Hadron Nucleus Collisions', in S. Devons (ed.), International Conference on HEP and Nuclear Structure, Plenum Press, New York, p. 207. Goodman, M. et al.: 1981, 'Observation of Shadowing in the Virtual Photon Total Hadronic Cross-Section on Nuclei', Physics Review Letters 47, 293-96. Gotsman, E., A. Levy, and U. Maor: 1988, 'A Comprehensive Description of the Photon Structure Function and Photon-Photon Total Cross-Section Data', Zeitschrift f?r Physik C 40, 117. Grammer, G. and J. D. Sullivan: 1978, 'Nuclear Shadowing of Electromagnetic Proces ses', in A. Donnachie and G. Shaw (eds.), Electromagnetic Interactions at High Ener gies, Vol. 2, Plenum Press, New York, pp. 195-348. Hacking, I.: 1983, Representing and Intervening, Cambridge University Press, Cambridge. Halzen, F. and A. D. Martin: 1984, Quarks and Leptons, J. Wiley and Sons, New York. Harr?, R.: 1988, 'Parsing the Amplitudes', in Brown and Harr? (1988), pp. 59-71. Heidegger, M.: 1977, The Question Concerning Technology and Other Essays, trans, by W. Lovitt, Harper and Row, New York. Holton, G.: 1986, The Advancement of Science and its Burdens, Cambridge University Press, Cambridge. Lakatos, I.: 1970, 'Falsification and the Methodology of Scientific Research Programmes', in Lakatos and Musgrave (1970), pp. 91-195. Lakatos, I. and A. Musgrave (eds.): 1970, Criticism and the Growth of Knowledge, Cambridge University Press, Cambridge. Lee, T. D.: 1981, Particle Physics and Introductions to Field Theory, Harwood Academic Publishers, Chur. Medawar, P.: 1984, Pluto's Republic, Oxford University Press, Oxford.</page><page sequence="46">290 P.P. ALLPORT Merzbacher, E.: 1970, Quantum Mechanics, J. Wiley and Sons, New York. Nietzsche, F.: 1910, Human all too Human, trans, by H. Zimmern, Foulis, Edinburgh. Nietzsche, F.: 1968, The Anti-Christ, trans, by R. Hollingdale, Penguin, Middlesex. Nietzsche, F.: 1974, The Gay Science, trans, by W. Kaufmann, Vintage Press, New York. Nietzsche, F.: 1986, The Will to Power, trans, by W. Kaufmann and R. Hollingdale, Vintage, New York. Olsson, J.: 1988, 'Photon-Photon Interactions', International Conference on Lepton-Pho ton Interactions, Nuclear Physics B, Proceedings Suppl 3, 613. Oppenheimer, J. R.: 1984, Uncommon Sense, Birkhauser, Boston. Perkins, D.: 1982, Introduction to High Energy Physics, Addison-Wesley, Reading. Popper, K.: 1976, Conjectures and Refutations, Routledge and Kegan Paul, London. Redhead, M.: 1980, 'Models in Physics', British Journal of the Philosophy of Science 31, 145-63. Redhead, M.: 1987, Incompleteness, Nonlocality and Realism, Clarendon Press, Oxford. Redhead, M.: 1988, 'A Philosopher Looks at Quantum Field Theory', in Brown and Harr? (1988), pp. 9-23. Ruijsenaars, S. N. M.: 1983, 'The Ahronov-Bohm Effect and Scattering Theory', Annals of Physics 146, 1-34. Russell, B.: 1950, Unpopular Essays, Allen and Unwin, London. Russell, B.: 1979, History of Western Philosophy, Allen and Unwin, London. Schopenhauer, A.: 1974, The Fourfold Root of the Principle of Sufficient Reason, trans. by E. F. J. Payne, Open Court, La Salle. Schramm, D. N. and G. Steigman: 1988, 'Particle Accelerators Test Cosmological Theo ry', Scientific American 258.6, 66-72. Sklar, L.: 1985, Philosophy of Spacetime Physics, University of California Press, Berke ley. Stodolsky, L.: 1967, 'Hadronlike Behaviour of y, ^-Nuclear Cross Sections', Physics Review Letters 18, 135-37. Stodolsky, L.: 1970, 'Coherence in High Energy Reactions', Methods in Subnuclear Phys. TV, Gordon and Breach, New York, p. 259. Teller, P.: 1988, 'Three Problems of Renormalization', in Brown and Harr? (1988), pp. 73-89. Vaihinger, H.: 1973, Nietzsche, ed. by R. C. Solomon, Anchor, New York. Van Fraassen, B. C: 1980, The Scientific Image, Clarendon Press, Oxford. Vigier, J.-P., C. Dendney, P. R. Holland, and A. Kyprianidis: 1987, 'Causal Particle Trajectories and the Interpretation of Quantum Mechanics', in B. J. Hiley and F. D. Peat (eds.), Quantum Implications, Routledge and Kegan Paul, London, pp. 169-204. Weinberg, S.: 1974, 'Unified Theories of Elementary Particle Interaction', Scientific American 231.1, 50. Weinberg, S.: 1977, The First 3 Minutes, Fontana, London. Weinberg, S.: 1988, 'In the Dirac Tradition', Cern Courier April, 29-30. The High Energy Physics Group Cavendish Laboratory University of Cambridge Madingley Road Cambridge, CB3 OHE England</page></plain_text>