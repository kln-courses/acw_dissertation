<plain_text><page sequence="1">2007-01-3925 A Software Method for Demonstrating Validation of Computer Dummy Models Used in the Evaluation of Aircraft Seating Systems D. Twisk and P. A. Ritmeijer TNO Automotive Safety Solutions Copyright © 2007 SAE International ABSTRACT This paper describes a software system that automates the process of comparing experimental to simulation data. First, it plots all experimental and the numerical signals for direct comparison. A rating of each numerical signal is calculated by comparing peak amplitude values, timing of peak amplitudes, and signal shape information, and expressing the correlation as a scalar. Weighting factors can be applied to each score, which can be used to increase the importance of some, and decrease the importance of other signals. This allows for the evaluation of the model for a specific purpose or loading condition. A report describing the model validation can be generated automatically. The report contains a description of each test performed, a description of the corresponding simulation model, rating scores, and graphs for each output signal. The report can be used as proof of validation by applicants, either as a Validation and Analysis report as described in Advisory Circular 20- 146, or as an appendix to such a report. INTRODUCTION Advisory Circular AC 20-146 [1] is the result of a methodology developed by a group of representatives from industry, research institutions, academia and government, working in an Advanced General Aviation Transport Experiments (AGATE) program. [2]. Rule 23.562 allows certification of aircraft seating systems either through dynamic testing or by " rational analysis, supported by dynamic tests" [3]. When 23.562 was first written, the tools required to perform this rational analysis were considered to be insufficient. The current situation, more than 20 years after 23.562 was written, is that simulation software and computer models have reached a level in which they are used as tools for design and research only. In the automotive industry tools and methods for certification of restraint systems by software are currently being developed [4], Advisory Circular 20-146 provides guidelines for the use of modeling techniques and documentation of results when using simulation tools in order to demonstrate compliance with the requirements of: • Title 14 Code of Federal Regulations (14 CFR) parts 23, 25, 27, and 29, §§ 23.562, 25.562, 27.562, and 29.562. • The Technical Standard Order (TSO) associated with the above regulations, TSO-C127/C127a. In summary, a baseline dynamic test has to be carried out with a dummy in a certified seating system, after which designs derived from the baseline may be evaluated numerically using a validated computer model. Both the software and the numerical models used in the simulation process must be proven to be suitable for the purpose. For numerical models this means that they must be able to predict dummy response to loading with a defined accuracy. Documentation must be provided to the FAA Aircraft Certification Office (ACO) that allows assessment of the tools used in the certification process as well as the results achieved by the analysis procedure. A procedure that documents the validation process and the results of the process has been in use for developing dummy models at TASS in the past two years [5]. Since this process is very similar to the one described by AC 20-146, both the tools and the reporting method have been adapted to conform to the requirements of AC 20- 146. This paper will describe the key features of the evaluation and reporting tools, and will provide some examples using data from the development of Hybrid-ll and Hybrid-Ill 50th percentile FAA dummy models. DEMONSTRATING QUALITY OF MODELS The set of tools developed for demonstrating the quality of models to be used for numerical certification of aircraft seating systems according to the guidelines of AC 20- 146 has two key functionalities: 1074</page><page sequence="2">• It is able demonstrate that the model is capable of predicting the response of the seating system with sufficient accuracy. This is achieved through the use of an objective (numerical) rating system. • A report can be produced in which the results of the rating process are described, and which provides the information on modeling of seat certification tests required by the FAA. OBJECTIVE RATING OF MODELS The objective rating method is a set of algorithms, implemented in software, that allow the quality of a dummy model, or its fitness for purpose, to be expressed in a single number (between 0 and 1, or 0 and 100%). The higher the number, the better the model is for an application or range of applications. The method for assessing the quality of dummy models described here has been developed for a number of purposes: • To guide simulation model developers in their work to improve dummy models. Improvement is defined as an increase in the objective rating score. Meeting a set of model validation requirements may involve design compromises - finding the correct balance between these requirements will result in a higher score. • The rating method allows different versions of a model or different implementations of a dummy model in software, to be compared objectively. This means that versions developed for a single code as well as models developed for different codes can directly be compared. • The rating method allows the model developers to provide information to the model users on the quality of the model and whether it meets the requirements of the application. Implementation The objective rating method expresses the quality of a dummy model in a single number. In order to derive the rating for a dummy model simulation outputs are compared to experimental data. Each experiment on a dummy, or a part of a dummy, will result in a number of response signals, e.g. head accelerations, neck forces, etc. Corresponding simulation time history data can be generated by modeling the experiment. Criteria that allow a quantative comparison to be made between two signals can be used to derive a score for each signal. For example, the peak value of a signal is almost always used to compare and rate simulation output. More than one comparison can be applied to each signal. Individual scores are then added in a defined way to derive a total score for the model. At the base of the objective rating system are three algorithms that allow a comparison to be made between an experimental time-history and a time history output from a numerical experiment. Each signal can be evaluated on three aspects: 1 . The amplitude of the maximum or minimum absolute peak. This criterion is called PEAK. 2. The point in time, relative to t=0, of the peak amplitude of point 1 , called PEAKTIME. 3. The correlation between the shapes of the two signals over the entire time history interval. The name of this criterion is called WIFac. These criteria and their formulations are described in more detail in Appendix A. These three criteria have been chosen because they are easy to understand, and the results of the calculations can be interpreted using the plots of the signal data. An example application of the rating criteria is shown in Figure 1. This figure shows the result for a head drop certification test of the Hybrid-ll dummy as specified in 49 CFR part 572 [6]. The 3 components of the head acceleration as well as the resultant acceleration are plotted. Figure 1. Part 572 Hybrid-ll dummy head drop results. Black line is experiment, red line is simulation response. Top-left: Head acceleration X-component, Top-right: Y-component. Bottom-left: Z- component. Bottom-right: Resultant acceleration. For each of the 4 signals shown in Figure 1 , a rating can be calculated. A total score for the signal is derived by performing a weighted addition of the three criteria; see Appendix A for details and a detailed discussion. The results for the head resultant acceleration are given in Table 1. 1075</page><page sequence="3">Table 1. Rating scores for Head Acceleration Resultant. Criterion Weight Score Peak 0.333 0.959 PeakTime 0.333 1.000 Wifac 0.333 0.931 Total 0.959 The values of the scores shown in Table 1 Indicate that the correlation between experiment and simulation is very high. Similar values can be calculated for the head acceleration X-, Y-, and Z-components, and for the HIC injury criteria. These can be added, using weight factors, in order to derive a total score for the entire test. Table 2. Rating score for the 572 head drop test. Signal Weight Score Acceleration X 0.200 0.960 Acceleration Y 0.000 0.000 Acceleration Z 0.200 0.949 Acceleration R 0.200 0.959 HIC 15 ms 0.200 0.990 HIC 36 ms 0.200 0.990 Total I I 0.965 Note that in Table 2 the contribution of the Y-component has been assigned a weight factor of 0. This has been done because both the experimental value and the simulation result are both almost equal to zero (Figure 1, top-right) and the component is not relevant for this loading condition. Under these circumstances, the relative difference between each of the corresponding data points of the time histories is large, which means that the correlation is low and consequently rating scores are very low as well. The weighting factor is therefore set to 0, which means that there is no contribution of the head acceleration Y-component to the total score. In the above paragraphs, the calculations of a score of a single signal, and one single test have been described. A total score for a validation set can now be calculated by performing a weighted addition of all test scores. However, this approach does not use the information available in the data very effectively. A better analysis of strong and weak points of a model can be made when data can be organized into smaller units, with each unit covering a certain part or aspect of the model. The rating method therefore allows the data to be arranged in a hierarchy of multiple levels that can be defined by the developer or user. On the lowest level, the smallest entity is just a single test, for example a head drop test performed according to the part 572 protocol. Several similar tests can be grouped together and rated as an entity, in which a weighted average is determined from the individual scores. To continue the example, a series of tests using the part 572 test equipment can be performed with the dummy head at different velocities, and a weighted score can be calculated from the individual scores. On a higher level, the rating scores of groups of dissimilar tests can be combined to derive a total score for a certain part or component. For example, several types of dynamic tests can be combined with static tests on dummy components or sub-assemblies. The purpose of the grouping is to allow the user of the model not only to look at the total dummy rating score, but also to examine certain aspects of the model in greater detail. On the highest level, the MADYMO rating system uses three categories of groups called "components", of which the scores are combined in a single scalar value. These components are: 1. Full Full tests are typically of the complete dummy in a test environment. Examples of "full" tests are sled tests using aircraft seats and a representation of the aircraft environment. In simulations of this type the dummy as well as the environment needs to be accurately modeled. Simulations of the full type are typically used for model validation. 2. Assembly An assembly test is performed not on a complete dummy, but on a subset of the dummy, and is intended to study the interaction between dummy parts under dynamic conditions. These tests are important, because dummy models generally cannot be developed on the basis of certification data only. 3. Part These are tests on isolated parts of the dummy, for example the head, neck or pelvis. The purpose of the test is to determine the dynamic properties of the part. Examples of tests in this category are all dummy certification tests, such as the part 572 head drop test, neck test, or thorax impact test. On each level, the weight factors can be used to either increase or decrease the contribution of a score to the total score. An example for the head drop test has already been given above. Something similar can be applied to full scale tests. If a dummy is loaded symmetrically, for example when it is being restrained by a four- or five-point belt system, the experiment essentially becomes two-dimensional and the weight factors of all signal Y-components can be set to 0 (except moments around the Y-axis). The data hierarchy is stored in a model project definition file in XML format, of which an example is shown below in Figure 2. In this particular example, eight tests have been defined in the "full" component, and five "part" components have been defined, with in total 45 certification tests performed on various parts of a Hybrid- II dummy model. 1076</page><page sequence="4">Figure 2. Sample XML definition file. The definition shown in Figure 2 also contains a reference to the location of the experimental data (EXP_PATH) to which the simulations results are compared. The definition of one of the seven tests on the head of the Hybrid-ll model defined in Figure 2 is shown in Figure 3. The full path of the location in which the data is stored is: /head/certification/BD00010_1. Four signals and two injury criteria are defined for this test. In this case, the required data is extracted from MADYMO simulation output. The XMIN, XMAX, YMIN and YMAX values are used to define the axes of the plots of the data. Figure 3. Single head drop test definition The signal definitions shown in Figure 3 are used by the software to generate a plot of the experimental data and the simulation output data. In this particular case the definitions are for the four plots that are shown in Figure 1. The four signal definitions and the 2 injury criteria definitions were used to calculate the rating scores that are given in Table 2. Types of signals The rating software can perform calculations on any data that is in the form of a time-history. The data does not necessarily have to be of a crash dummy. Any signal that can be measured with a transducer and stored as a time-history can be rated. This includes vehicle or platform data, belt forces, and signals from airbags. Rating trajectories of trajectory markers is possible if the position data is split up into time-histories. The position data can then be rated individually for each X-, Y-, or Z- direction. An example is given in Figure 5, in which time histories for marker positions have been measured up to 175 milliseconds, which corresponds to the point in time in which the largest forward motion of the head relative to the seat reference point is reached. The position of the marker on the head of the Hybrid-ll dummy is shown in Figure 4. Figure 4. Hybrid-ll sled test with rigid seat and 2-point belt system. Figure 5. Hybrid-ll rigid sled with 2 point belt: head trajectory. Black line = experiment. Red line = simulation. Top: XZ trajectory. Bottom- left: X-component time history. Bottom-right: Z-component time history. Rating scores for the X- and Z- component time histories are high, as is shown in Table 3. 1077</page><page sequence="5">Table 3. Rating scores for Head Position. Criterion X Z Peak 0.879 0.990 PeakTime 0.996 0.930 Wifac 0.905 0.956 Total I 0.911 I 0.952 These high rating scores may not seem to be in agreement with the impression created by the cross-plot shown in the upper part of Figure 5, but the two lower plots in Figure 5 show good correlation for the X- component of the head trajectory up to about 130-140 milliseconds, i.e. over the largest part of the interval, and good correlation for Z-component of the head trajectory over the entire interval. These results suggest that using the criteria of the rating method described in this paper may require modifications if they are to be used to for rating marker trajectories or other kinematics data. The problem here is that the data on the abscissa of the cross-plots is not equidistant, as with time-history data. If the value of the abscissa data changes significantly in a short interval, the differences between the experimental and simulation results can dominate the cross-plot. Results for Hybrid- 1 1 and Hybrid-Ill models The objective rating method has been applied to models of the Hybrid-ll and Hybrid-Ill FAA [7, 8, 9] dummies, images of which are shown in Figure 6 and Figure 7. Figure 6. Hybrid-ll model on a rigid sled, and restrained by a 2-point FE belt. Figure 7. Hybrid-ll, rigid seat on sled, oriented for a 60 degree angle test and restrained by a 2-point FE belt. Table 4 and Table 5 show results for component (head, neck, lumbar spine) and complete dummy (thorax, knee) ratings of the simulation models of the Hybrid-ll and Hybrid-Ill dummies shown in Figure 6 and Figure 7. Table 4. Hybrid-II rating scores. Number of Number of Score Conditions Tests Head ī 6 0.945 Neck ī 6 0.688 Thorax 2 12 0.911 Lumbar 14 14 0.378 Knee 1 6 0.832 Total I I 44 I 0.622 Table 5. Hybrid-Ill rating scores. Number of Number of Score Conditions Tests Head 3 3 0.800 Neck 26 26 0.706 Thorax 6 6 0.802 Lumbar 14 14 0.821 Knee 3 3 0.766 Total I I 52 I 0.753 The lower score of the Hybrid-ll model is caused mainly by the quality of the lumbar spine model. The model of the Hybrid-Ill FAA dummy uses a more advanced representation of the straight lumbar spine that is used in both the actual Hybrid-ll and Hybrid-Ill FAA dummies. Note that the tables above do not yet include the results of the rating of a series of actual aerospace applications. The data for the development of models of tests 1078</page><page sequence="6">performed according to FAR regulations is available, but the models based on the data are not yet completed and could not be included in the results presented in this presentation. Suitability for application AC 20-146 specifically states that a model does not have to fulfill all possible requirements in order to demonstrate suitability for a particular application. The dummies used in Aircraft Seat Certification testing are, when fully instrumented, capable of measuring dozens of signals. Not all of these signals are considered to be essential for every application. AC 20-146 literally states that (section 7.1.1): "The computer model is considered validated if reasonable agreement (as discussed in Sections 7.1.1.1 to 7.1.1.6) between analysis and test data can be shown for those parameters critical to the application of the modei'. The example given in section 7 is that of lumbar spine loads, which are considered to be relevant in the vertical test, but not critical in the horizontal test. The use of weight factors in the objective rating method allow the critical responses of a simulation model to be identified and other, non-critical responses to be either included with a low value for the weight factor or removed from the evaluation altogether by using a weight factor of 0. Moreover, if the weight factors are defined a priori by applicant and ACO in the Certification Plan, the rating scores can be used to steer the model development process. The process of model validation then becomes an optimization process, i.e. in which the rating score is maximized. Note that in our view, the use of weight factors to either apply large weight factors to key model responses or to reduce the contribution of less important model response should be used on the validation of the baseline model of the seat test only. It should not be applied to the data that is provided to demonstrate the quality of the components that are used to construct the application, such as the models of the dummy, the seat, and the aircraft environment. Each of these must meet the requirements set by the available test data. AUTOMATED DOCUMENTATION A key element of AC 20-146 is that the applicant must be able to demonstrate the quality of the computer simulations used in the seat model validation procedure to the ACO. In order to do this, the applicant needs to provide documentation that: • describes the seat system to be modeled; • describes the hardware (computer) and software (versions) used in the modeling process; • describes how compliance is shown; • provide a description of material sources; • provides a description of validation methods; • gives an interpretation by the applicant of the results; • includes substantiation documentation of the data submittal package. AC 20-146 refers to this as the Certification Plan Document. This plan needs to be approved by the FAA as early in the certification process as possible [1]. After performing the analysis, the applicant must submit a document, called the Validation and Analysis Report (VAR) that contains the results of the simulations and compares the model response data to experimental results. Implementation A reporting system has been developed that automates the process of generating the VAR report. The key elements of this reporting system are: • Experimental and analytical results are organized in identical data (file) hierarchies, allowing direct comparison of the two sources. This organization of the data allows for an automatic processing of the data for reporting purposes. The definition of the data hierarchy is defined by the "model project file", which used XML elements. The data file hierarchy is the identical to the one used by the objective rating tool. • The data itself is stored in ASCII files, and each file consists of two columns, of which the first is the time information. The ASCII files are created from the experimental results and from the simulation outputs. The advantage of this approach is that results from different codes (e.g. MADYMO, LS- DYNA, PAM- CRASH) can be compared to each other. • The report uses a template to define the contents of the report. Different templates can be made, allowing the same data to be presented in a number of different formats for different purposes or for different audiences. The template for the VAR contains sections with descriptions of the seating systems, the dummy model used, the restraint systems, hardware and software used, and any other topics that are considered to be relevant for the report. • The major part of the report is generated automatically by the software. This includes: o Tables providing an overview of the tests performed and signals measured; o Descriptions of all simulations performed; o Rating scores of all tests, as well as the combined scores; 1079</page><page sequence="7">o Range plots, which provide information on the range of the loading conditions of the tests performed. Examples of range plots are given below. o Signal plots of all signals used in the validation process. The report template is in XML format. A skeleton report can be generated from a model project file. The skeleton report contains tables with overviews of the tests included in the report, tables of data channels measured, rating scores and all signal plots. Not generated automatically are introductory sections, evaluations, information on model CPU performance and detailed descriptions of the tests and the simulations. These have to be written by the model developer and added to the report. Test and model descriptions consist of both text and visual elements such as photographs and simulation output. If an extensive set of validation results is available, the software can generate "range plots". The range plots are used to cross-plot peak values of experimental data and simulation response data. This allows the user to assess the range over which a particular response has been validated. The software allows several model implementations to be compared to the experimental data and to other models. Dummy models can be for the same code, but also for different codes. If a model, either of the dummy or of the seat, is changed and improved, then a report can be generated showing the results of the validation process of the previous and new versions of the model. Some examples of range plots, in which two versions of a particular model are compared, are shown in Figure 8. The figure contains data of certification tests, assembly tests, and full-scale sled and vehicle tests. Figure 8. Range plot for dummy chest displacement. Green color = previous model, red color = current model. The solid line indicates perfect correlation; the dotted lines indicate a rating score of 0.8. Several observations can be made from Figure 8: • A significant number of tests have been performed on the dummy model that provides information on the chest deformation under load. • The loading conditions are over a fairly wide range, from 10 mm up to about 70 mm, which corresponds to very low and to high severity loading. This allows the assessment of the quality of the dummy model under different loading conditions. • The plot can be used to determine whether the model predicts responses correctly or whether it either under- or over predicts the dummy response. If the angle of the regression line through the data points is much less than 45 degrees, the model lacks sensitivity to loading severity. If the angle is much large than 45 degrees, the model is very sensitive to small changes in input parameters. Figure 9. Tibia Z-forces (axial loads). Green color is used to indicate results of previous model, red color marks results of current model. 1080</page><page sequence="8">Figure 9 shows a second example - for tibia load cell axial forces. The observation here is that the model predicts the forces well, but tends to under predict the forces at higher loading levels. This applies to the results of both versions of a model shown in Figure 9, although the more recent version of the model (red color) appears to perform slightly better because the data points are closer to the 45 degree line. Tests included in the report A typical test report used to demonstrate the validation level of a dummy model and a seating system contains: • Results of tests performed on the dummy model only. This includes the certification tests and any additional tests that are considered to be relevant to the performance of the dummy. An example is a series of tests performed on the head, not only at the default certification impact velocity, but also at lower and higher velocities. • Results of rigid seats with the dummy model under different loading conditions, including 2-point, 3- point, and 4-point belt systems, and under horizontal and vertical loading conditions. • The seat model, including validations of the deformable parts such as the (foam) cushions and the load carrying structures. • The baseline seat configuration as required by parts 23, 25, 27, and 29 and described in TCO-127. CONCLUSION In order to assist the use of simulation methods in the certification of aircraft seating systems according to the requirements of FAR 23, 25, 27, and 29, tools have been designed that allow the quality of computer models to be determined in an objective way and to be reported as part of a Validation and Analysis report in an uniform format and using an automated procedure to reduce development effort. The tools and procedures have been based on similar tools developed for use of automotive dummy development. The rating tools can determine a score for any data that is in the form of a time history. The evaluation of kinematics data is possible if marker trajectories are available and if the coordinates of the marker data can be split up into time histories. The criteria used to evaluate the simulation model responses: Peak, Peak-timing, and WIFac have been chosen for simplicity and because the results achieved with these criteria are typically in agreement with subjective comparisons of test and simulation model response data. Three criteria are included now, but it is possible to add criteria in the future by defining new XML elements for the rating definition file and adding the corresponding algorithms to the software. A key feature of the rating software is that weight factors may be applied to rating results to put emphasis on certain key model responses and reduce the contribution of other model response data if it is not considered to be of importance to the quality of the simulation. These weight factors can be defined a-priori in order to define objective requirements for simulation model quality. Weight factors can be applied on an application basis. Although the rating tools in their current form can be used for rating kinematics data, the objective results are not always in agreement with subjective interpretations of the data. This is caused by the non-equidistant placing of data-points when making cross-plots of time-histories. The same file used to define the rating of the simulation response results can be used to generate an automated report of all simulations that were used to validate the computer model(s). This report contains summaries of all tests, descriptions of all experimental set-ups and the corresponding simulation models, plots of all generated experimental and simulation time-histories, and range- plots that allow determination of the range of loading conditions in which the computer model(s) can be used. At this moment, reports for the Hybrid-ll and Hybrid-Ill FAA dummy models are under development. The validation set for these model currently include tests on components, assemblies, and on the complete dummy. Data of tests performed with aircraft seats according to Aerospace Standard 8049 is not yet available. Models of test with rigid seats that are intended to evaluate the quality of the dummy simulation models are being developed, but results of this work were not available in this for this paper. ACKNOWLEDGMENTS TASS would like to thank the National Institute for Aviation Research and Denton ATD for their permission to use the experimental data used in this paper. REFERENCES 1. Advisory Circular AC 20-146. "Methodology for Dynamic Seat Certification by Analysis for Use in Parts 23, 25, 27, and 29 Aircraft and Rotorcraft." March 2003. 2. T. Lim. "Methodology For Seat Design And Certification By Analysis". (Revision A). AGATE- W P3 .4-0340 1 2-079- R E PO RT . August, 2001. 3. FAA Federal Aviation Regulations, Part 23: "Airworthiness Standards: Normal, utility, acrobatic, and commuter category airplanes". Section 562: "Emergency landing dynamic conditions". March 1998. 1081</page><page sequence="9">4. Hoof J. van; Happee R.; Puppini R.; Baldauf H.; Oakley C. (2001). "Extended Occupant Safety Through Virtual Testing: Objectives Of The European Project VITES", International Mechanical Engineering Congress and Exposition of the American Society of Mechanical Engineers (ASME International), New York/USA, 2001 . 5. Hovenga P.E., Spit H.H., Uijldert M. , Dalenoort A.M. "Improved Prediction Of Hybrid-Ill Injury Values Using Advanced Multi-Body Techniques And Objective Rating". SAE paper 05AE-222, SAE World Congress, Detroit, 2005. 6. Federal Code of Regulations. Title 49: "Transportation". Part 572: "Anthropomorphic Test Devices". May 2000. 7. Van Gowdy, et al. "A Lumbar Spine modification to the Hybrid III ATD for Aircraft Seat Test". SAE Technical paper 1999-01-1609. April 1999. 8. Waagmeester, C.D., et al. "Enhanced FAA-Hybrid III Dummy for Aircraft Occupant Safety Assessment". SAFE Symposium 2002, Jacksonville, FL. 9. Boucher, H., Waagmeester, C.D. "Enhanced FAA- Hybrid III Numerical Dummy Model in MADYMO for Aircraft Occupant Safety Assessment". SAE Aerospace Conference &amp; Exhibition 2003. CONTACT For more information about this topic, please contact Mr. Dirk Twisk, email: dirk.twisk@tass-safe.com. DEFINITIONS, ACRONYMS, ABBREVIATIONS ACO: Aircraft Certification Office. CPD: Certification Plan Document. VAR: Validation and Analysis Report. XML: Extended Markup Language. APPENDIX A: SIGNAL RATING Signals can be compared in several ways to derive multiple scores describing the correlation between experimental data and simulation data. This comparison can either use scalars, for example comparing peak signals values or the HIC calculated from the head accelerations, or vectors, for example by calculating a phase shift between two signals or comparing the areas under the curves. How these comparisons are performed is described in the following paragraphs. Additional background information on comparing signals is given at the end of this appendix. Comparing two scalar values To calculate the score of two scalar values, for example peak values or injury values, MADYMO uses the following equation, referred to as the Factor Method (FM): max (O, exp- sim) crit = 7 r-r Equation 1 max (exp ,sim ) Where crit is a derived scalar (score), and exp and sim are two scalars describing an aspect of the experimental and simulation data. The FM calculates the correlation between exp and sim, resulting in a value that lies between 0 and 1. A characteristic of the FM is that it calculates values that do not drop off faster for over-prediction than for under- prediction. This means that using FM to optimize models does not result in models that either systematically under- or over-predict dummy response. Comparing two signals A method is available to calculate the correlation between experimental and simulation data that uses entire time histories. It is called the WIFac Method, which stands for Weighted Integrated Factor method. The formulation is as follows: I 2~ Z max(/M (sf 12 ,g[n' r 12 )f, )■ 1 max(0,/[n]-g[«l)^ max(/M 12 ,g[n' r 12 )■ 1 . _n I max(/[n] ,g[n')J I Çma Ąf[nf,g[nf) Equation 2 Where f[n] and g[n] are two signals that are to be compared over a time interval of N data points. When both f[n] and g[n] are equal to zero, the result of crit will be set to 0. Equation 2 is the combination of Equation 1 for the calculation of a score from a single scalar with the following equation for calculating a weighted average from a series of scalar scores. ^ W[n] - (l - cnřfn])2 score = 1 - - =¡ - ? - ļ Equation 3 v =¡ !&gt;["] ? ļ Where W are weights, and crit are rating scores for each set of data points. In Equation 2, the weight factor W is calculated as the absolute maximum of the two function values at each point, and Equation 1 is used to calculate crit for each 1082</page><page sequence="10">time step n. Effectively this means that every individual score is scaled such that it contributes to the total score just as the function value would contribute to the total area underneath the graph. In other words, low signal values have lower weights than high signal levels. Adding scores The previous paragraphs have described how data from experimental and simulation data can be compared, and the correlation between the two signals expressed with a single scalar. Multiple comparisons can be made between any set of two time history signals resulting in a set of scalars. To obtain a single rating number for the complete model or model component, results from different tests and signals need to be combined. For reasons of comparison, the domain of the combined number is often taken identical to the domain of the individual numbers. Therefore for combined numbers the domain [0, 1] is used. Just as for the individual numbers, a score of 0 means that there is no correlation at all, and a score of 1 (100%) means a perfect match. The equation used to calculate weighted averages is called the Root-Mean-Square Method (RMS): (Y (w-(l-cni)2) comb = 1 - = Equation 4 il 2&gt; = Where comb is a combined score, W are weights for scores, and crit are rating scores. Equation 4 is used on all levels to calculate averaged scores, resulting in a single scalar that expresses the quality of the model. On the lowest level, it is used to calculate the average value of the scores for peak amplitude, timing of the peak amplitude, and the WIFac for a single signal. On the next level, scores for all signals are averaged to calculate a score for a single simulation/test. One level up, an average score is calculated for all similar tests, etc, etc. Additional Information on calculation of dummy rating scores This section will give some background information on the methods used to calculate the scores and then illustrate the differences with an example. Comparing two scalar values To calculate the scores between two scalar values, for example peak values or injury values, the following expression is often used: í i • i' exp- sim' • crit - max 0,1 ¡ ¡ - Equation 5 I ¡ |exp| ¡ J This method will be called the Relative Error Method (REM). The REM is not associative: crit(exp, sim) * crit(sim, exp). The REM score drops off faster with over- prediction than with under-prediction. As a result, applying this formula to optimize a model to a set of tests, tends to result in a model that systematically under-predicts test outcomes. Since this under-prediction is undesirable for the dummy models the following calculation is used instead by MADYMO, which is called the Factor Method: max(0,exp- sim) crit = 7 rr Equation 6 max (exp , sim ) The Factor Method (FM) calculates the correlation between exp and sim, resulting in a value that lies between 0 and 1 . This expression is associative: crit(exp, sim) = crit(sim,exp). For small differences the results are identical or almost identical to the results of the REM. However, it is important to realize that for over- predictions the FM calculates values that do not drop off faster than for under-prediction, as is the case with the REM. For example for an over-prediction of 100%, using FM results in a score of 0.5, whereas the REM score will have dropped to 0. When optimizing a target function using the FM, the solution will not tend to under-predict most tests as it would when the REM had been used. Adding scores To obtain a single rating number for the complete model or model component, results from different tests and signals need to be combined. For reasons of comparison, the domain of the combined number is often taken identical to the domain of the individual numbers. Therefore for combined numbers also the domain [0, 1] (x100%) is used. Just as for the individual numbers, a score of 0 means that there is no correlation at all, and a score of 1 (1 00%) designates a perfect match. Sometimes the following expression is used to achieve a combined score, which is called the Simple Addition Method (SAM): V (W crit) comb = = Equation 7 Where comb is a combined score, W are weights for scores, and crit are rating scores. When a model is optimized using the maximum value of the combined score according to Equation 7, it might very well end in a point where it has a maximum score 1083</page><page sequence="11">for one test, and a low score for all other tests. For this type of optimal solution, the solution is not sensitive to small variations in all but one or some tests in specific. The value of the Simple Addition Method will vary when one of the less important tests varies, but the optimal design point will not be adjusted in all cases. The risk of an optimal solution that lacks sensitivity for the results of many tests, is the main disadvantage of the Simple Addition Method. Therefore within MADYMO dummy development the Root Mean Square (RMS) addition is used (Equation 4). When this formula is used in an optimization process, the optimal design point will always vary when the result of one of the tests varies. Furthermore optimizing the total score calculated with the RMS Addition method is equal to optimizing the problem with the generally known least squares method (Equation 4). Example In this example a model is optimized using two tests. The model is optimized by varying a single parameter K, which represents some parameter of the model. In dummy models, K can for example be stiffness or damping. Most often it is not possible to find a value for the parameter that results in a perfect correlation for all tests. In this example it is assumed that K scales the response of the model in the following (conflicting) way: sim1(K) = 0.5/K, sim2(K) = 1.0/K. The scores of sim1 and sim2 are relative to the goal value of 1 . So there are two values of K for which one of the two tests correlates perfectly : K = 0.5: sim1 = 1 .0-exp, a perfect match for test 1 . sim2 = 2.0-exp, an over-prediction of 100% for test 2. K = 1.0: sim1 = 0.5-exp, so an under-prediction of 50% for test 1 . sim2 = 1 .0-exp, a perfect match for test 2. Rating values calculated using the Relative Error Method (Equation 1 ) are shown in Figure A1 . Figure Al. Rating values calculated using the Relative Error Method. Figure A1 shows a much steeper drop-off for over- prediction than for under-prediction. Rating values calculated using the Factor Method (Equation 2) are shown in Figure A2. Here the drop-off for under- and over-prediction is about equal within a certain range from the peak value. Figure A2. Rating values calculated using the Factor Method. The two scores of the tests can be combined using the Simple Addition Method (Equation 5), and RMS Addition method (Equation 6). The results for the Simple Addition method are shown in Figure A3. 1084</page><page sequence="12">Figure A3. SAM using Relative Error and Factor Method. For the Relative Error Method Figure A3 shows a maximum average score of 0.75 at K = 1.0. For this value of K the performance for test 2 is optimal, but the result of test 1 is under-predicted by 50%. The fact that the optimum value gives a perfect match for one test, and a poor for another is because the Simple Addition method uses linear terms in its calculation, and the REM drop-off is not symmetric for under- and over-prediction. In this example, the optimum is found at that test that requires the highest value of K. Other tests will be under- predicted. Using the Factor Method two maximum values are found, one for K = 0.5 and one for K = 1 .0. Two maxima are found because the Factor method is symmetric for over and under-prediction. Whichever method is used to calculate the single rating scores, using the Simple Addition Method results in an optimum that has a high score for one test and a poor score for the other test. However, in model optimization, a solution is sought that finds a balance between the two tests, i.e. that the results of both tests are as close as possible to the required values. The results for addition of scores with the RMS method are shown in Figure A4. Figure A4. RMS using Relative Error and Factor Method. Because the RMS addition does not use linear terms in its calculations, none of the tests in this example can dominate the calculation of the optimum value. For both the Relative Error Method and the Factor Method, the optimal value for K lies between 0.5 and 1 .0. If used with the Factor Method ( = WIFac), the optimum value is K = 0.707. If used with the Relative Error Method, the optimum value is much closer to K = 1 .0, because the Relative Error method is in favor of under-prediction. The two tables below summarize the results for Simple Addition and Root-Mean-Square. SAM I REM I FM ~ Test 1 0J&gt; 1 2 Test 2 I 1 I 2 I 1 RMS I REM I FM Test 1 0.588 0.707 Test 2 I 1.176 I 1.414 ~ Results for more complex examples The previous example showed that for a model with two tests and one model parameter that scales the test results, the Factor Method combined with RMS addition gives much better results than the combination of the Relative Error Method and Simple Addition. However, the question is if this is still the case for a model with more parameters and more tests. In this section the results are shown for a model with a single parameter K and 6 tests. The optimal values for K are chosen as 0.5, 0.6, 0.7, 0.8, 0.9 and 1 .0. Only the Factor Method criterion has been used during this evaluation. 1085</page><page sequence="13">Figure A5. SAM and RMS for 6 tests, equal weight factors. For this case the Simple Addition curve is much more fluent than in the case for two tests. However there are still peaks at every test. The optimum value for the Simple addition is 0.8, which is exactly at one of the tests (K=0.8). Another peak is at 0.7, but its value is slightly lower. The reason that these values are not equal is that the distribution of the optimal K values between 0.5 and 1 .0 was chosen linear, not harmonic. If one of the tests has a weighting factor such that it counts more in the addition than the other tests, the results for the Simple Addition method are not optimal, as is shown in Figure A6. Figure A6. SAM and RMS for 6 tests. In this example there are 3 tests with optimal K at 0.5, 0.7 and 1 .0. The test with optimum at K = 1 .0 weighs just as much as the other two tests. The optimum with the Simple Addition Method is at K = 1 .0, which more or less ignores the other tests. The RMS addition has its optimum around K = 0.78, which is between the optimal K values for the tests. From this more complicated example it is concluded that the Simple Addition Method can result in sub-optimal solutions. MADYMO therefore uses the RMS Addition. However, It should be noted that the RMS addition leads to a lower score than the simple addition. The exact difference depends on the case, so it can not be compensated for using a scaling factor. 1086</page></plain_text>