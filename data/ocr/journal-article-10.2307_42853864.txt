<plain_text><page sequence="1">SOCIOLOGY Vol. 23 No. 3 August 1989 409-426 ON SPEAKING ABOUT COMPUTING Brian P. Bloomfield Abstract This paper explores the connection between language and our thought and beliefs about computers. In particular it considers how certain features of language - such as verbal habits, or the traces in language due to social interests and power - help to shape particular reports and interpretations of the behaviour of computer programs and thereby sustain or reinforce beliefs about the organisational role of computers and even their status vis-a-vis human beings. It is contended that when computers and computer-related practices are introduced into an organisation users become members of computer cultures where new or reshaped ways of thinking and speaking are acquired in order subsequently to discuss or operate the technology. It is suggested that these cultures and their language be made the focus of sociological scrutiny. Introduction We live in an era where the 'white hea≈• of technology is once again held up as the guarantor of social development and progress. Voices from a variety of quarters proclaim that we are entering a post-industrial society where information and communication technologies driven and controlled by computer systems will provide rich rewards for all. Already we can see that the increasing use of computers in a wide variety of organizational settings - teaching, health care, administration, management - requires more and more people to be trained in the actual use of these machines. Moreover, as the gamut of technical instruments derived from information technology (IT) become ever more pervasive the population in general is exhorted to keep up-to-date, to acquire new skills, and in particular to become computer literate. To accomplish such changes one needs new ways of thinking, even a different orientation to the world: 'Before the computer, the animals, mortal though not sentient, seemed our nearest neighbors... Computers, with their interactivity, their psychology, with whatever fragments of intelligence they have, now bid for this place. . . Where we once were rational animals, now we are feeling computers, emotional machines' (Turkle 1984:326). Against this background, the subject of this paper is concerned with the ways in which thought about the nature of computing and computers is mediated and reinforced through the language used in the context of computing, through the ways in which we commonly speak about computers.1 If we are to seek an insight into the role of language in thought about computing - in understanding and reporting or interpreting the activity of computers and computer programs - we must consider both the shaping of thought by language and, conversely, the shaping of language by social experience or culture: for language, thought, and social relations are complexly intertwined. On the one hand, certain features of language - such as grammar or verbal habits - shape thought (Whorf 1956); while on the other, when viewed from a sociological perspective we</page><page sequence="2">4 1 0 BRIAN P . BLOOMFIELD can see that language and thought are in turn shaped by features of social relations (Bernstein 1971; Gadamer 1975). Whatever their modality, these influences on thought remain largely implicit, setting boundaries on our understanding of the world in ways that we are normally unaware of. The relative importance of language in thought about computers may well be expected to depend on the particular group of people we look at. For example, it is unlikely that the role played by verbal habits in shaping the thought of those people with an established academic, professional, or other similar involvement with computers - e.g. computer scientists - would be sufficient to provide an understanding of their particular reports or descriptions of computing. In cases such as this we must also consider the traditions, beliefs, educational training, and thought style (Mannheim 1936; L. Fleck 1979) of the group in question because it is in relation to these aspects of culture that verbal habits often take form. On the other hand, at the point of transition from relative ignorance of computing and membership of/or socialisation into a computer culture, language might well play a more significant role, acting as a touchstone which helps to make sense of a newly- open area of experience. Moreover, I will argue that even as we begin to assimilate the beliefs and competences of a computer culture, our speech about the operation of computers is mediated by the structure of sentences in the English language in such a way as to invite us to project intelligent agency on to these machines. It is contended that an explication of the interactions between language, thought, and social experience in the context of computing is of relevance to sociologists for a number of reasons. First, it can help to open up a site of investigation in an area which is becoming increasingly important. We do not have to go along with the ideologues of the IT Revolution to see that in contrast to the past predominance of the data processing orientation of computing, computers are coming to play a more important and pervasive role in organisations. In particular, database management systems (DBMS), decision support systems (DSS) and applications of Artificial Intelligence (henceforth AI) such as expert systems - programs which allegedly embody human expertise, can solve problems and can provide chains of reasoning for the solutions reached - and IT more generally, are giving rise to important issues concerning work, management control, knowledge and expertise. It has even been suggested (Collins 1986) that computer programs such as expert systems pose questions concerning the very autonomy of sociology: for if non-social machines could hold, communicate, and exchange knowledge then clearly this would represent a challenge to sociological theories of knowledge and the primacy accorded to the collective within sociological thought. Second, the paper aims to contribute towards the general problem of understanding the introduction of computers into organisations and the associated issues of power and control. It is contended that a central part of the process by which computer-dependent practices become instituted involves subtle changes whereby organisational settings become further instances of computer cultures - that is, social locations with particular beliefs, myths, shared bodies of knowledge, ways of thinking and speaking, and operational procedures corresponding to the use of computer technology. In this regard, and in contrast to many other approaches to computer technology, it is important to stress that computers do not impact on or determine culture.2 On the contrary, built into the network or web of social relations computers can be used to play a variety of diverse roles - e.g. the reinforcement (or</page><page sequence="3">ON SPEAKING ABOUT COMPUTING 41 1 avoidance) of various implicit or explicit power struggles between different organizational factions (Kling 1980). In order to play any role within the social practices of an organization, a computer must be constituted as a particular embodiment of processes, controls, and competences. These ascriptions are not in any way imposed by the sheer technical capacity of the associated hardware/ software but are socially constructed during the introductory phase in the organization. Thus the notion of what are the technical questions and what are the social questions in relation to the introduction of a computer system cannot be taken as a given. Rather, the very dichotomy between the technical and the social in the language of the organisation must be put to the test. The task of understanding computer cultures is just as serious as it is with any other group within society {cf. Woolgar 1985), perhaps even more so because for so long the ascribed dominance and legitimacy of science and technology in Western societies have effectively tended to dull the sociological imagination. Thus it is only in recent years that detailed ethnographies of scientists and scientific knowledge have begun to emerge from within the sociology of science. If the study of science and technology - including scientific knowledge and technological artefacts - requires the considered attention of the sociologists then so too does the application of science and technology (in this case computer science and computers) in organisations outside scientific laboratories and research programmes. Given the nature of the issues addressed in this paper it is inevitable that the argument which it articulates is necessarily speculative. However, its purpose is not to present a final report or conclusion on the issues which are tackled but, rather, to open up a space for enquiry and debate. The structure of the argument is as follows: first I will consider the role of verbal habits in shaping our reporting, description and understanding of computing. Second, there will be a discussion of certain specific beliefs about computers vis-a-vis human beings which will include reference to the concept of animism and its limited role in explaining the projection of agency onto computers. Third, I will turn to the social shaping of the language of computing and examine how this influences the ways in which we think about computers. And, in the fourth section I will consider the language of computing in the context of organisations. The argument here will begin with the assertion that differential organisational power leaves its mark on language and this will then be explored in connection with, first, the process of systems analysis, design, and implementation (which underpins the introduction or modification of computer systems); and second, the constitution of computers as the embodiment of particular competences within the overall pattern of organisational practices. THE EXAMPLE OF COMPUTING Language and Thought To begin we can consider some of the commonplace verbal habits used in connection with computers. The importance of verbal habits in shaping thought was a line of enquiry predominantly pursued by Whorf and his mentor Sapir who opined that the ' real world is to a large extent unconsciously built up on the language habits of the group' (Sapir 1933). Perhaps one of the simplest instances of interesting</page><page sequence="4">412 BRIAN P. BLOOMFIELD verbal habits that most readily springs to mind, and which has an almost universal currency at all levels of computing (including its associated application areas wherever computers are installed) concerns the description of the process of output from a computer. Typically, we talk of a computer producing output, or a program computing something. Certainly this appears to be no different than when we talk of a car driving down the road. However, in the case of a car we know that (usually) there is a human agent sitting behind the wheel, but what subject or agency exists in relation to a computer? When we think or talk of the computer producing output or computing something we tend to ignore, forget, or even be unaware of the implicit role of the person who wrote the particular program which is running on the computer; similarly we ignore our own interpretative procedures which are employed when we look at the output from a computing machine and draw our various conclusions. To put the matter in a different perspective we can say that a computer is an inscription device (Latour 1987), it merely provides a means for transforming one set of symbols, or marks on a sheet of paper, into another set. In the simplest situation, no agency is involved here other than the original programmer and the user who must make sense of the transformation. Yet, in commonplace reports or descriptions about computers the process of computing - which involves these two human agents and a technical tool, the computer - is reified and reduced to a static situation in which the tool is constituted as a third and usually primary agent which does the computing and produces output.3 The relationship (however impersonal) between programmer and user becomes reified and displaced by the attention paid to the technical tool which thus comes to stand not as an electro-mechanical intermediary in the relation but as an agent in its own right {cf. Fields 1987). 4 The particular verbal habits under consideration here are in fact tied up with the grammatical form of English sentences: that is, English (except when we are speaking in the imperative) requires a statement or sentence to consist of a verb (pertaining to some action) prefixed by a substantive - in other words, a subject or agent and an action or predicate. This is so even when the agent in question and the action referred to are in fact one and the same or essentially indistinguishable. For example, the following statements are telling instances of this feature: 'the river flows' and 'lightning flashed across the sky' Clearly, a 'river' and 'flowing' are highly interrelated concepts; just as 'lightning' and 'flashing' are. How can we think of a river without flowing, or lightning without flashing? Yet we talk about these phenomena as if there was an agent 'river' which performed the action of 'flowing'; or similarly, a thing called 'lightning' which performed the action of 'flashing'. Thus we are compelled in many cases to read into nature fictitious acting-entities simply because our sentence patterns require our verbs, when not imperative, to have substantives before them. . . (Whorf 1956:262-63) In effect we fall prey to reification and reduce a process to a static subject performing some given predicate. Thus, at the very outset of exposure to computers our language can be seen to help sow the seed of the notion that these particular machines are purposive; English grammar sets conditions which make possible a series of related verbal habits (e.g. 'the computer works out the answer') that imply agency on the part of computing devices. Of course, should we be forced to reflect on our speech acts, grammar does not prevent us from stating clearly that we do not</page><page sequence="5">ON SPEAKING ABOUT COMPUTING 4 1 3 wish to imply any intentionality or purposiveness on the part of such machines, that in this respect our verbal formulations are merely convenient; we might even claim to be speaking metaphorically. In instances such as statements about cars, rivers, or lightning etc., this may be so but when it comes to computers the subject-predicate format underpinning our verbal habits becomes altogether more problematic because the notion that these devices exhibit agency already has a wide currency.5 Indeed, while the number of people actively promoting the notion that computers have intentions is largely restricted to the supporters, both academic and popular, of AI - a movement or thought collective which though still relatively small is growing in number - many other groups (e.g. in education and in the media, including popular science/-technology programs) don't exactly go out of their way to counter this belief or set it in perspective. In fact it is no great exaggeration to say that we are increasingly surrounded by reports of computer predictions and computer forecasts as well as other findings; or depictions of contexts in which computers are implicitly or explicitly attributed with purposive agency. AI is seen by many groups as a central plank in the onward march of the IT revolution; and it is of particular relevance to the argument here because it brings with it not just items of computer software but, perhaps more fundamentally, an attempt to re-orient our view of ourselves as human beings. In short, our view or understanding of computers is intimately bound up with our conceptions of what it is to be a human being. The notion of human nature and our understanding of thought and reason have been the subjects of struggle and debate for centuries; in this regard the cohorts of AI (both academic and popular) represent the latest contenders in the battle to claim legitimacy on these matters. Thus, it isn't so much innocent verbal habits which are at issue here so much as the debatable assumptions which often sit alongside them and which are mediated and reinforced by such habits. Moreover, when it comes to debates about the computer programs which have allegedly discovered theorems in mathematics (Lenat 1977; Michie and Johnston 1985) the linguistic 'conveniences' become even more contentious; not only because such notions mystify or aggrandise computers but because they also impoverish the nature of mathematical discovery by ignoring its peculiarly human characteristics - namely the creative, informal, and intuitive aspects of mathematics which elude encapsulation in the formal logic of computer programs (cf. Lakatos 1976; Bloor 1984). Further remarks about intentionality may be made in relation to computer games: if I play chess with a computer do I either win, draw, or lose with the machine, or with the person who programmed it? On the basis of the preceding argument we should conclude that the machine does not win , draw, or lose. However, that said we can also answer the question in a different way: by reflecting on the meaning of the concept of play as applied to a game between two people and a game between a computer and a person. As Shanker (1987) warns us, we must not confuse purposive concepts such as a programmer's goal with electrical operations: 'it is unintelligible to describe the operations of a mechanical device with normative or intentional concepts.' Thus on philosophical grounds too we can say that we do not play chess with the computer. It is of course easier or more convenient to say that I lost to the computer and the objection here might seem pedantic: ultimately, English grammar doesn't stop us from adopting alternative verbal habits and saying that we produced output via a computer, or 'played' chess with a programmer via a computer (perhaps</page><page sequence="6">414 BRIAN P. BLOOMFIELD similar to the way we could play chess with another person through the post) but such forms can seem almost counterintuitive. Though we could choose to speak otherwise it is clear that in general this has not happened; and if there were not so many misconceptions concerning the nature of computers and computing (consider the recent flurry of advertising claims for microwave cookers that think for themselves, or intelligent video cassette recorders) it would not be necessary. The point to be emphasised again here is that speech does not occur in a social vacuum; the verbal habits under consideration here - underpinned as they are by the subject- predicate form of English sentences - do not determine thought about computers but are instead part-and-parcel of the spread of a form of culture involving the use of these machines. That culture favours a particular reading of our verbal habits; it helps to sustain the interpretation that computers exhibit agency. Thus, verbal habits mediate and reinforce particular ways of thinking about computers and form an important means by which we enculture others into thinking along the same lines. Apart from the rather esoteric area of theorem-discovering programs, an increasingly commonplace example of AI is to be found in the technology of expert systems. In contrast to conventional programs, an expert systems program is said to have a knowledge base rather than a data base; to have the capacity to make inferences from this knowledge (in conjunction with user-supplier data) and to be able to provide reasons for its chain of inference . But if we say that a program can explain its own reasoning are we speaking metaphorically, or are we granting agency and intentionality to machines? And what is the public perception of the way in which such programs are described? If it was only a matter of metaphor then presumably there would not have been the quasi-legal debate about who should be sued - the programmer or the program - in the event of someone coming to grief after taking erroneous advice from an expert system (Yazdani and Narayanan 1984). Another commonplace problematic term we can consider is 'behaviour'; talking about the behaviour of a computer or a computer program conjures up many resonances in relation to human beings and again it tends to attribute purposive concepts such as goals and intentions to machines. Moreover, 'behave' is not exactly an unproblematic term in the case of humans either. Rather, it has several connotations - e.g. the common-sense description of a person's action; or, following Skinner, that of behaviouristic responses in relation to a stimulus. While it might appear difficult to sustain the notion that computers behave in the sense implied by everyday usage, it is possible to imagine a computer behaving in accordance with the insect-like responses implied by behaviourism - after all, this is the essence of programming. However, our beliefs about the behaviour of computers are not just underpinned by our observations of computers qua computers but also by our own theoretical preferences concerning the nature of human motivations and behaviour; again, considering the status of the behaviour of a computer requires us to reflect on our own understanding of ourselves. If any doubt should remain about the importance of the way we think about computers the history of computing indicates the pressing nature of the need to be careful in our interpretations of computer programs. Infamous episodes such as the Limits to Growth debate (Bloomfield 1986) or the reported reactions to Weizenbaum's (1976) ELIZA program, illustrate the seeming willingness with which people attribute authority to computer programs.6 The position taken here is that whatever the ultimate reasons for our readiness to project authority onto computer</page><page sequence="7">ON SPEAKING ABOUT COMPUTING 4 1 5 programs the nature of speech acts that govern the reporting of computing can play a not inconsiderable role in helping to mediate and reinforce the context of legitimacy within which computers are construed. As a final example of the influence of verbal habits on thought we may consider the concept of a program bug - this is, an error or defect in a piece of computer software. As well as meaning a defect, 'bug* is also synonymous with the idea of something small or unimportant. The problem here is that while many program bugs may well be insignificant there may exist others which, depending on the particular application, may be more or less catastrophic. Further, a bug can be thought of as an intrusion or corruption in an otherwise flawless piece of program logic - bugs in the shape of bacteria invade an otherwise healthy body - whereas in reality a program bug might well be inherent in the problem-solving philosophy or conceptualisation which underpins the whole program. Hence, just as Whorf identified workers who had considered it safe to smoke near gasoline barrels because they thought that they were empty (when in fact they were dangerously filled with gasoline vapour), so too there could be a problem with programmers or users thinking that software was to be trusted because it contained only 'a few bugs'. Beliefs about Computers The reification problem that arises because of the role of verbal habits in thought is compounded by the fact that computers are popularly thought of in a qualitatively different way to other everyday objects such as cars. More specifically, computers are surrounded by a particular web of preconceptions and myths - e.g. notions of robots, automata and the long-standing fascination with the project of creating a creature in our own image (J. Fleck 1984) - which in various ways essentially differentiate them from other machines. Put another way, thought about computers takes place within the compass of already existing ideas, hopes, and fears, which are manifest within society more generally. Thus, it isn't that computers are essentially different from other machines such as cars so much as the fact that they are embedded in different cultural practices - controlling rather than being controlled. A catalogue of the resultant phantasmagoria of notions would provide a rich vein of interesting material for semiological analysis. Indeed, our verbal habits in relation to computers might lead a proverbial anthropologist from Mars to infer that the objects which we call computers really did constitute a form of agency almost akin to us; not because of the inherent capacities of the machines but because of the ways in which we talk about, fear, or stand in awe of them. Some of the more extreme forms of anthropomorphism in computing can be found in recent serious discussions concerning questions such as whether computers have legal rights; are alive; or intelligent (Fields 1987; Simons 1983, 1985). Moreover, it must be stressed that the constitution of computers as significant agents is not merely to be observed in the media but is a hallmark of much discussion in academic computing. Indeed, the attainment of an intelligent computing machine is the raison d'etre of AI. One of the particularly potent beliefs or myths which serves to reify the discussion or interpretation of computing - such that computers are attributed with concrete agency - centres on the way in which computers can be seen to blur the boundary between people and machines, between thought and computation. Indeed, even in</page><page sequence="8">416 BRIAN P. BLOOMFIELD the esoteric area of academic computing one of the questions put by AI researchers is not whether a machine will ever think, but that of how a brain computes. Obviously, this blurring or transgression of categories is not imposed by computers but is a feature of specific settings in which the description of computing is transmitted or imposed - as in a classroom; or negotiated - as in the case of an organization, a paradigm, or a research programme. Objects or animals which appear to straddle classificatory boundaries often evoke fear and/or reverence (Douglas 1966) and in this regard evidence of both forms of reaction are commonplace when it comes to the confrontation between people and computers. On a different tack, Frude (1983) argues that human beings have a psychological predisposition toward animism - that is, the projection of human characteristics onto animals and physical objects in the natural world.7 Moreover, he further contends that though this tendency has become rather dormant in modern societies - in comparison with other times and more primitive cultures - current developments in computing are reawakening the animism of our species. 'Technology now has the power to ensure that we are confused by physical objects, confused into thinking that they have feelings and emotions. Animism will ensure that computers are regarded as having personality' (Frude 1983:91). Hence, just as we are fascinated by images of the human form so too we are drawn to computers that appear to behave as we do or which, more specifically, can seem to engage us in activities, such as games, which usually take place in the context of human interaction. While it would be difficult to sustain the idea that a psychological explanation can in general account for anthropomorphic reports and descriptions of computers, by the same measure we should not flirt with the idea that singularly social explanation will suffice instead. Rather, there is scope for both approaches: but while the psychological level highlights a commonalty among people the sociological level brings some of the differences into relief; for it is in specific social contexts that animism might be retarded or advanced. In other words, we cannot reduce the problem of reports about computing to the psychology of individuals. Moreover, it is perhaps worth stating that Tur kle' s (1984) interviews with home computer enthusiasts indicate the possible existence of other psychological factors in people's reaction to computers. In particular, she identified a desire to achieve and to sustain total control over one domain of life (computational problems) in compensation for a perceived lack of control or low esteem elsewhere. Though it must be said that compensation theories or accounts of human behaviour may be ultimately as inadequate as a psychological view of animism, Turkle's example has a limited use here in so far as it indicates that even within a psychological framework Frude' s argument has limited explanatory power. Beliefs are not the property of individuals; rather, cognition can be regarded as our 'most socially-conditioned activity' with knowledge ≈•as the paramount social creation' (L. Fleck 1979:42). Different groups bring or assimilate different sets of assumptions and beliefs in relation to the various contexts within which computing is discussed. In everyday situations computers are talked about within the compass of specific interpretative procedures as part of the prevailing social setting. As such, beliefs about computers are to be considered as artefacts of such procedures and situations. Thus, with a sociological rather than psychological view of beliefs I will now turn to some of the social aspects of language and computing.</page><page sequence="9">ON SPEAKING ABOUT COMPUTING 417 Language and Social Context One of the obvious features of computer cultures which is most readily observed by outsiders is the apparent high degree of technical jargon which permeates the speech acts of the insiders within the culture. Talk of bits, bytes, CPUs, interfaces, disk drives, programs, compilation, execution, PCs, IBM 370s, LISP, PROLOG, COBOL etc., etc. renders the subject quite impenetrable to those not in the know. Whether computing is actually more interspersed with jargon than other fields is an empirical question but it is interesting to speculate on the role of this jargon in the establishment of computer cultures. Classification systems mediate and reinforce social order and in this regard jargon should be seen as an expression of a form of social bonding: communication is terse, brief, and impenetrable to outsiders because it rests on a wealth of tacit knowledge which is shared by insiders. Thus, the common feeling that computer people are rather too technical - hence the profusion of technical jargon - and in danger of becoming asocial or like robots may be a misconception. Computer jargon may well be evidence of social bonding not a lack of it. Moreover, the growth in jargon provides a window on the changed social practices in an organization as computer-related tasks displace old practices; at the same time it may be a pointer to issues concerning moves toward increased professionalisation and technical expertise on the part of those involved.8 Another area where language and social context interact can be illustrated by considering how the terms employed in the field of computing can change with use. To develop this point it is first necessary to make a further observation about the nature of classification. While structures of thought can be shaped by language, it can also be argued that the language habits of a group, or a language game to coin Wittgenstein's related term, are shaped by the social experience of the group. In other words, players in a particular language game are not passive prisoners of their language, but through various social processes of bargaining, negotiation, competition and conflict, actively, but not necessarily consciously shape it. Thus, while classification systems are fundamental to the structure of language and the imposition of order on the chaos of reality they are far from being given or immutable. For example, Kress and Hodge (1979:63-4) argue that: 'the contingencies of the situations in which individuals meet, and their, at times, divergent interests, place strains on these classification systems. In this way classification becomes the site of tension and struggle'. Similarly, in anthropology Douglas (1975) represents one school of thought derived from Durkheim and Mauss which contends that classification systems are embroiled in the micro-politics of everyday social interactions; that there exists a perpetual social interest in the management and control of human relations and that this impinges on the way in which we rank, order, and classify the world around us. (For an opposing view see: Horton 1971.) Thus, according to this line of reasoning, classifications do not reflect the structure of the world as it is, but are social constructions. A recent example of the mutuability of classifications in computing comes from the field of AI and the arguments about the proper definition of expert systems. When expert systems rose to prominence in the early 1980s it was originally thought by many people that a proper expert systems program must be based on one of the principal (high status) languages of AI such as LISP or PROLOG and would have to run on a specialist piece of powerful hardware (known in the field as a workstation).</page><page sequence="10">418 BRIAN P. BLOOMFIELD However, while some people in the field still continue to argue about what constitutes the real essence of an expert system, developments in both the application areas of expert systems technology and the market place have brought about a new situation. In particular, the past few years have witnessed a proliferation of cheap off-the-shelf expert systems packages based on a variety of computer languages (including the scientifc, numerically-oriented and low status language of FORTRAN) which run on relatively low power (and hence low status) personal computers (Bloomfield 1988). Thus, whatever particular people might think in the higher echelons of AI a different use of the term 'expert system' has emerged; to understand the term we must look, not to the various abstract definitions, but to how it is used in practice. In this case we could view the definition or classification of expert systems as the site of struggle between different interested parties - for example, on the one hand certain AI academics (for whom legitimate work in the area requires impressive and sophisticated processing power) and suppliers of workstations; and on the other hand, the particular commercial software companies who wish to market their own cheap off-the-shelf AI products. What is at stake here is the meaning/status of work in AI: for if every Tom, Dick, or Harry can construct an expert system then the mystique surrounding the field of AI no longer has potency. Hence the need to demarcate the real AI work taking place in research laboratories and differentiate it from the work centred on cheap personal computers which is undertaken by people who have not undergone a formal training in AI. Also, within our culture at large we find that different groups explicitly or implicitly lay claim to being the legitimate voice concerning the concepts of intelligence and artificial intelligence; each group is taking part in a struggle to influence or dominate the means of orientation that we adopt on these matters. The outcome cannot be the final discovery of what an expert system really is but, rather, a convention laid down in language.9 This example of social processes within computing is of course only the tip of the iceberg. The sheer size and influence (social, economic, and political) of the multinational computer companies indicates the importance of the issues arising from our understanding of computing and its role in society. To take but one simple example, let us consider the humble personal computer or PC. To be sure, here we have a piece of technology and a new term in our language; but in modern societies we also have an increasingly individualised conception of ourselves which is mediated by language and constituted in part by new forms of practice which use the technology. Personal computers meet our personalised computing needs: from home budgeting (in these days of increased individual financial sophistication) to various diagnostic tools, to games for individual entertainment. Yet another important actor on the world stage of computing is the military. Military establishments in various countries provide huge contracts to suppliers of computer hardware and software and this has import for the way that computing . develops (Edwards 1986). Computing was born against the backdrop of global warfare and is now quite firmly entrenched in the military domain. How this connection ultimately relates to our culture as a whole is complex and unclear but it seems unproblematic to assert that the military involvement in computing as well as the prospects for increased militarism opened up by advanced computing technologies will continue to play an important role. In terms of the language of computing, the importance of the military lies in its role in helping to shape the</page><page sequence="11">ON SPEAKING ABOUT COMPUTING 419 agenda in the development of computing - for example, by putting AI and expert systems under the spotlight. Whatever opponents of AI might say to the contrary, it seems that the notion of intelligent computers - and hence the speech that goes with it - will be with us for some time: if only because the military have chosen to follow a path which leads them to cede more and more battle management decisions to computer programs. A different example of the social shaping of the language of computing is to be found in the received wisdom of the history of the subject. Here we find that the development of computing is invariably described as a sequence of generations - either of hardware (characterised, successively, by the use of valves, transistors, large scale integration etc.) or software (low level languages, high level languages, non-procedural languages etc.). Aside from the fact that the classification of the generations depends on the classifier - with no unanimity in the area - and the added complication that the boundaries between the categories are usually far from concrete, the generational view of computing is evidence of a wider social phenomenon. For any group, such as an academic discipline, can be expected to present a rational account of its past; on this lies the group's intellectual coherence and future purpose. Further, the generational view also confirms the belief in the forthcoming discovery/emergence of new generations - e.g. the much discussed 5th Generation (Bloomfield 1987) - and the technological determinism which seems to underscore the whole project of computing. Computing and Organisations Though statements about computers or computer programs are open to interpretation by different parties, we must acknowledge that in practice computers are never introduced into situations where different actors/interest groups have equal power. Take the case of expert systems again; the rhetoric to be found here claims that these programs enable rare skills to be codified and archived; make expensive expertise more widely available; or support management decision making. Whether management believe these claims is a moot point but the other side of the coin reveals the possible extension of managerial control, increased regimentation and rationalisation of organisational practices (cf. Dawson 1983). Thus, in terms of public knowledge at least, power differentials can be expected to reveal themselves through the language and beliefs articulated in connection with computing. This raises a whole host of issues about the penetration and adaptation of computer ideology within different social settings. For example, we might consider the question as to how beliefs about computers impinge on people's awareness in a world increasingly centred around computational activities. Another approach would be to ask how language might play a role in mediating and reinforcing existing power relations in an organisation; here we have an issue which is central to the problem of understanding the control of the labour process in the context of computing. One example of this role stems from a consideration of systems analysis, design, and implementation (the system development process). The very conceptualisation and public articulation of this process may be a crucial factor in framing the organisational understanding and receptivity of a new computer system. Put simply, on the one hand there is the idea that organisational practices must be</page><page sequence="12">420 BRIAN P. BLOOMFIELD adapted to technology, to the logic of technical instruments, and accordingly users tend to be excluded from the design process or merely allowed a token involvement. On the other hand, where technology is seen as enhancing the potential of human expertise a participative approach to design may be favoured (Mumford and Weir 1979). The precise ways in which analysis, design, and implementation are articulated by management and the ways in which systems analysts (whether these be in-house or external consultants) are constituted vis-a-vis computing and the organisation will play a subtle role in mediating the introduction or modification of computer based organisational practices. A second example arises from the idea that the constitution of computers as significant agents might serve to facilitate the avoidance of potential conflicts in organizations. To envisage this possibility one can imagine a situation in which those responsible for the instigation of a computer system seek to apportion certain socially or politically sensitive aspects of control to the machine and thereby obfuscate their own vested interests and greater leverage in the operation of those controls. In short, social relations within the organisation become reified and construed as the properties of an object - the computer. In order successfully to attribute control to a computer system while still retaining ultimate - though now masked - control, it is necessary that the computer be seen as a particular form of agent with various competences and capacities to control and process information. One only has to think of the term 'user-friendly' to appreciate how computer systems can be categorised and thus constituted as being either more or less like us as humans. This example is particularly interesting because some would argue that the development of user-friendly computer programs masks a subtle deskilling of the user: In practice, 'user-friendly' means that the programmer has adopted a deficiency model of the user's competence and has encoded another layer of instructions to present a quasi- human appearance to the language which confronts the program user. In practice 'user- friendliness' makes a program easy to use at the cost of understanding how the program actually achieves its effects. Thus ease of use is related to powerlessness, rather than to control (Linn 1985:87). Another interesting instance of the problem is to be found in a study of the introduction of a computer system into particular aspee of nursing practice concerned with the description and recording of information on patients (Mathiassen and Andersen 1983). Before computerisation, nurses at the hospital under study (as elsewhere) would keep a manual record written on cards (the Kardex system) of patients' conditions, prescriptions issued, tasks to be performed etc.. After computerisation, which incidentally involved nursing representatives in the design process, the nurses were required to classify patients in accordance with the categories built into the software. The introduction of the computer system led to a subtle change in the semiotic system of the ward: 'the system invites a special concept of nursing and disease: from being based upon the speech acts of description and interpretation, it shifted towards classification and determination of species.' (Mathiassen and Andersen 1983:255) Further, not only did the professional language of the nurses appear to change, but the form of the power relations between nursing and management also underwent a metamorphosis. More</page><page sequence="13">ON SPEAKING ABOUT COMPUTING 421 specifically, the computer system issued directives on patient care (e.g. drugs to be given, temperatures to be taken, drains to be checked) and at the same time relayed information to management on staff levels and resource requirements. One of management's concerns was to reduce costs and increase control over nursing and to this end their control was articulated through the classification scheme worked out by the system development group which included systems analysts, managers and nursing representatives. The computer system issued directives, as if it was an agent, but did so only within the boundaries established by that group. A more recent and perhaps most extreme form of example concerning the sort of competences that are considered appropriate for computer systems is to be found in Fields (1987; see also Chapanis 1984). Referring to the issue of computer politics in the workplace and the prevalence of the fear of computers, Fields writes 'Computers may be feared precisely because they are not socialized; they are not understood, and are therefore feared. . . Socializing computers, in the sense of providing them with the behavioural capacities that make them easy to work with, may reduce computer anxiety simply by making computers behave more like people' (Fields 1987: 20). He then goes on to suggest that regarding computers as social entities might also reduce computer anxiety because they would then be seen as subject to the laws and conventions that regulate the workplace. The upshot of this idea is that industrial relations legislation must not pay attention only to the relationships between management and workers but also those between human workers on the one hand, and on the other, their co-workers in the form of computer systems! In this context the arguments articulated earlier about the shaping of classification systems is again relevant. Describing a computer system as a socialized entity can be viewed not so much as a reflection of the inherent competences of the machine so much as an exercise in reshaping the boundaries of our classifications in order to admit computers into the category of social beings. Thus, arguments about the status of computer programs can be read not as reports of technical advancement but as attempts to exert control over classifications in order to further the interpretations of particular groups. Summary To recap, the argument developed in this paper is that the understanding and reporting of the behaviour of computer programs is shaped in two important ways by language.10 First, certain features of language favour certain habitual modes of thought. For instance, because of the structure of our sentences we speak and therefore think of subjects or agents performing predicates or actions; in the context of computing devices this lays the foundation for various verbal habits such as 'the computer produces output' or 'the computer beat me at chess'. One implication to be drawn from this could be that our language forces us to project agency and intentionality onto computing devices; but a more sociologically oriented view would be that as so many people already seem to ascribe agency to computing devices, language tends to confirm the myths, beliefs and intuitions of our social experience. The position taken here is that so far as computing is concerned, language and social experience reinforce each other in their effects; language is always part of a social context.</page><page sequence="14">422 BRIAN P. BLOOMFIELD Our understanding of computing is also shaped by various other verbal habits we have adopted and which now characterise the subject - such as the concept of a program bug. Ultimately, these habits are social conventions but none the less have linguistic properties which are significant too. We can choose the speech used in the context of computing but whatever our choices might be, the words (such as 'program bug') will have referents and interpretative connections which draw upon the rest of our language experience. Secondly, social experience - including power, interests, bargaining, conflict, styles of thought - influences the language we use to describe and understand computing; whether this is on the global scale of the multinational computer companies and their role in the development of computing, the military use of computers, or the particular organisational emplacement of computers within social/professional practices. To illustrate this social shaping of language I have argued that the classifications of computing - e.g. between conventional programs and expert systems, or between different generations of computing hardware/ software - are the site of tension and struggle between parties with different interests and interpretations; while the evolving jargon of the subject is indicative of the collective bonds within the growing community of computer users. In terms of organisations and the labour process, the language involved in the introduction of computing involves issues of power and control. The understanding of the capacities and role of a computer system is not just a reflection of inherent technical factors but is potently coloured by the language games which are employed in its constitution. Beneath the language games of the organisation are to be found social tensions or divisions; conversely, the consolidation of a particular language game marking the institution of computer dependent practices - the crystallisation of a computer culture - is an exercise of power (conscious or unconscious) on the part of those who have most influence in its shaping. Aside from its important role in thought, the language of computing leads us to address certain types of question but not others: for instance, fixing our gaze on the computing machinery we then pose questions about its intelligence; whether it is alive; or has legal rights. Questions such as these are often portrayed as the central issues of computing, but while they may be ill-formulated they also displace what are arguably the more important issues - namely, who has access to computing and who does not; who uses computers, for what purpose, and on whose behalf. Conversely, tackling issues such as the latter would require that the role of the computer, how it is thought about and understood, and therefore spoken of, be reassessed in quite fundamental ways. Questioning the nature of computing, artificial intelligence, or what it means to talk about an expert system must involve us in a debate which reflects on our own self-conception, our understanding of intelligence and expertise, and forces us to take seriously the ways in which social relations become reified by the language and social practices which circumscribe the use of computers. Each form of influence on our understanding of computers has a tendency to be self-reinforcing because by their very nature particular verbal habits, whether these are predicated on the grammatical features of language, or the ways of speaking rooted in a particular style of thought or language game, appear natural, predetermined and immutable. While the explication of these influences cannot in any way provide a final answer to the question of the status of the operations of a</page><page sequence="15">ON SPEAKING ABOUT COMPUTING 423 computer program - an issue which falls to philosophy (see Shanker 1987) - it does contribute to our understanding of the currency of particular interpretations and casts light on the role that we (actively or passively) allow computers to play in our culture. On the negative side it must be admitted that one of the important things which is missing from this paper is the question of gender and computing; or more specifically, the role of language in helping to shape computing as a predominantly male preserve. This is a subject worthy of a paper in its own right and its omission is not an indication of its unimportance to the argument articulated here. Notes 1 . This approach to language and computing is therefore distinct from the question of how different theories of language can inform the design of better computer programs. For an example of such a discussion in connection with the design of management information systems see Lyytinen 1985. 2. Though the myth of technological determinism was challenged quite some time ago, particularly by the Frankfurt School in the 1940s, it seems perpetually to be reasserted in one guise or another. Thus, as Woolgar [1985] has suggested, there has been a tendency for sociological treatments of computing to be restricted to the social impact of computers. 3. Here the term agency is being used solely in connection with human subjects - that is, agents are conceptualised in terms of purposive human beings. It is of course possible to think of non-individual agents such as classes, joint stock companies, or football teams but these entities do not have intentional or purposive consciousness [cf. Cutler et al.]. 4. Of course, given the ways of thinking and speaking characteristic of the contemporary ideology of individualism there is a wider cultural preponderance to ignore the relationship between programmer and user and concentrate on individual brains - whether human or machine. Further, there is another sense in which computing can be said to manifest reification - namely, a marxist view to the effect that at the same time as we acquiesce on the contentious matter of agency on the part of computers we help to sever the link between the programmer and the computer program which is the product of his or her labour [Lukacs 1971]. In other words, attributing agency to computers may be seen as the outcome of the general reification of labour power within the capitalist mode of production. 5. In fact Whorf did not consider his ideas about the role of language in thought to be relevant only to everyday speech acts; he went so far as to surmise that the properties of European languages might play an inhibiting role in the conceptualization of certain problems in areas of modern physics. For no matter how abstract and mathematical a given physical theory might be, physicists often employ verbal formulations relating to their sense of physical intuition both to provide new input to theory; to arrive at interpretations of it; and also to win allies. The import of Whorf's ideas for the development of other disciplines has not gone unnoticed. In particular, Elias [1978] has discussed the implication of the Whorfian hypothesis (sometimes referred to as the Whorf-Sapir hypothesis) for the ways of thinking and speaking manifest in sociology. One example explored by Elias concerns the use of the personal pronouns (I, you, he, she, we, they) and the way in which social relationships can be reduced to static objects when thinking about and discussing the nature of individuals and societies: The pronoun T is normally used to communicate to others that a certain statement refers to the person speaking. But in scientific usage it is abruptly turned into a substantive and, given prevailing</page><page sequence="16">424 BRIAN P. BLOOMFIELD habits of speech, appears to refer to some independent, isolated person. The concept of ego as used by Freud or Parsons is a good example of how ... [a] concept of relationship can be transformed into a concept of substance, a concept of a thing. Parsons's use of the term 'ego' demonstrates the strength of the individual-centred way of thinking. It was quite characteristic that a sociologist like Parsons should remove the lone T from the series of pronouns and contrast it with all other people, although in reality we experience them as 'you' (singular), 'he', 'she', 'we', 'you' (plural) and 'they', not as 'alter' or 'the other' . . . [Olne cannot imagine an T without a 'he' or a 'she', a 'we', 'you' (singular and plural) or 'they'. [Elias 1978:122-123] Elias 's point here is that within Parsonsian functional sociology a particular structural feature of our language (in this case the personal pronouns available to us in English) is selectively moulded by a particular way of thinking and speaking about people and society which places stress on individuals. This has helped to shape the view whereby individual persons have become thought of and perceived as somehow separate and distinctly isolated entities (egos); while at the same time the networks of relationships of which they are but part, and which constitute the entities we refer to as societies, are ignored or played-down. The mode of sociological analysis mediated by this way of thinking and speaking does not reflect an a priori gulf between individual persons but, rather, expresses a belief which presumes them to be thus separate. 6. In an experiment conducted by Garfinkel [1967] a group of students were asked to present questions to a counsellor in such a form that a 'yes' or 'no' answer could be given. Although the students did not realise it, the replies of the counsellor were actually determined randomly; yet, they each reported that the advice they received was understandable. Oldman and Drucker [1985] argue that the counsellor could be thought of in terms of a computer program: attributing authority to a computer program we can use the interpretative flexibility of our intellects to construe significant implications from its output. 7. This psychological view of animism is of course different from the treatment of the subject in anthropology. 8. During a recent appointment at the local building society in connection with a mortgage application the branch manager used an on-line computer service to calculate and present repayment figures for a particular loan. When asked whether he resorted to a calculator or standard mortgage table if the system was out of action for any reason, the manager stated that he would resort to his calculator. He added that using a calculator was in fact his preferred mode of working - it was actually quicker and not prone to disruption - but the society's policy was that customers would be more impressed and have a greater degree of confidence in computer-generated repayment figures. 9. In fact elsewhere [Bloomfield 1988] I have argued that conventional programming and programming in the field of expert systems are constituted by different practices which are mediated and reinforced by two different language games. In this connection the debate about the definition of expert systems is only the tip of the iceberg. 10. The connection between language and thought is not restricted to the domain ol natural languages. For even though computer languages are in quintessence different from natural languages, they too can play a constraining role in thought: it is evident that different computer languages facilitate different modes of conceptualising and solving problems. For example, LISP allows a greater variety of individual freedom on the part of the programmer while the newer structured languages such as PASCAL or ADA (much used for military work) place greater constraints on the programmer to produce code which manifests a larger degree of inter-subjective explicitness. References Bernstein, B. 1971. Class, Codes and Control: Theoretical Studies Towards a Sociology of Language. London: Routledge and Kegan Paul.</page><page sequence="17">ON SPEAKING ABOUT COMPUTING 425 Bloomfield, B.P. 1986. Modelling the World: the social constructions of systems analysts. Oxford: Basil Blackwell. Bloomfield, B.P. 1987. ' The Culture of Artificial Intelligence ' in Bloomfield (ed.) The Question of Artificial Intelligence: philosophical and sociological perspectives . London: Croom Helm. Bloomfield, B.P. 1988. 'Expert Systems and Human Knowledge: a view from the sociology of science'. AI and Society 2(1): 17 - 29. Bloor, D. 1984. Wittgenstein - a social theory of knowledge. London: Macmillan. Chapanis, A. 1984. 'Taming and civilizing computers'. Annals of the New York Academy of sciences 426:202-219. Collins, H. 1986. The Turing Test: Sociological Approaches'. Paper presented to the Society for Social Studies of Science, Pittsburgh, 25 - 28 October. Cutler, A. et al. 1977. Marx's Capital and Capitalism Today. Volume 1. London: Routledge and Kegan Paul. Dawson, P. 1988. 'Intelligent Knowledge Based Systems (IKBS): Organisational Implications'. New Technology , Work and Employment 3(1):56 - 65. Douglas, M. 1966. Purity and Danger. London: Routledge and Kegan Paul. Douglas, M. 1975, Implicit Meanings. London: Routledge and Kegan Paul. Edwards, P.N. 1986. Artificial Intelligence and High Technology War , Silicon Valley Research Group, Working Paper No. 6. Elias, N. 1978. What is Sociology? London: Hutchinson. Fields, C. 1987. 'The computer as tool: a critique of a common view of the role of intelligent artifacts in society'. Social Epistemology 1(1):5 - 26. Fleck, J. 1984. 'Artificial Intelligence and Industrial Robots: An Automatic End for Utopian Thought?', in Mendelsohn, E. and Nowotny, H. (eds.) Nineteen Eighty-Four: Science Between Utopia and Dystopia. Sociology of Sciences Vol. VIII. Dordrecht: Reidel, 189-231. Fleck, L. 1979 (orig. 1935). Genesis and Development of a Scientific Fact. Chicago: University of Chicago Press. Frude, N. 1983. The Intimate Machine. London: Century Publishing. Gadamer, H.G. 1975. Truth and Method. London: Sheed and Ward. Garfinkel, H. 1967. Studies in Ethnomethodology . Englewood Cliffs, N.J.: Prentice Hall. Horton, R. 1971. 'African Traditional Thought and Western Science', in Young, M.F.D. (ed.) Knowledge and Control: New Directions for the Sociology of Education. London: Collier Macmillan, 208-66. Kling, R. 1980. 'Social Analyses of Computing: Theoretical Perspectives in Recent Empirical Research'. Computing Surveys 12(1):61 - 1 10. Kress, G. and Hodge, R. 1979. Language as Ideology. London: Routledge and Kegan Paul. Kuhn, T.S. 1962. The Structure of Scientific Revolutions. Chicago: University of Chicago Press. Lakatos, E. 1976. Proofs and Refutations. Cambridge: Cambridge University Press. Latour, B. 1987. Science in Action. Milton Keynes: Open University Press. Lenat, D. 1977. 'The Ubiquity of Discovery'. Artificial Intelligence 9(3):257-85. Linn, P. 1985. 'Microcomputers in Education: living and dead labour', in Solomonides, T. and Levidow, L. (eds.) Compulsive Technology. London: Free Association Books. Lukacs, G. 1971. 'Reification and the Consciousness of the Proletariat', in History and Class Consciousness. London: Merlin. Lyytinen, K.L. 1985. 'Implications of Theories of Language for Information Systems'. MIS Quarterly March:61 -74. Mannheim, K. 1936. Ideology and Utopia. London: Routledge and Kegan Paul. Mathiassen, L. and Andersen, P.B. 1983. 'Nurses and Semiotics: the Impact of EDP-Based Systems upon Professional Languages', The Sixth Scandinavian Research Seminar on Systemeering', 227-260.</page><page sequence="18">426 BRIAN P. BLOOMFIELD Michie, D. and Johnston, R. 1985. The Creative Computer. Harmondsworth: Penguin. Mumford, E. and Weir, M. 1979. Computer Systems in Work Design: the ETHICS Method. New York: Wiley. Oldman, D. and Drucker, G. 1985. The non-reducibility of ethno-methods: can people and computers form a society?', in Gilbert and Heath (eds.) Social Action and Artificial Intelligence. Aldershot: Go wer. Sapir, E. 1933. Encyclopaedia of the Social Sciences 9:155-169. Shanker, S.G. 1987. The Decline and Fall of the Mechanist Metaphor', in Born, R. The Case Against AI. London: Croom Helm. Simons, G. 1983. Are Computers Alive?. Boston: Birkhauser. Simons, G. 1985. The Biology of Computer Life. London: Harvester. Turkle, S. 1984. The Second Self. London: Granada. Weizenbaum, J. 1976. Computer Power and Human Reason. San Francisco: W.H. Freeman. Whorf, B.L. 1956. Language , Thought and Reality. Cambridge, Mass.: MIT Press. Woolgar, S. 1985. 4 Why Not a Sociology of Machines? The Case of Sociology and Artificial Intelligence'. Sociology 19:557-572. Yazdani, M. and Narayanan, A. (eds.) 1984. Artificial Intelligence: Human Effects. Chichester: Ellis Horwood. A cknowledgement: I would like to express my thanks to David Cooper, David Knights, Stephen Parsons, and Keith Robson for their useful comments on an earlier draft of this paper. Thanks are also due to the editors and anonymous referees for their help and patience in bringing the paper to light. I also wish to acknowledge the support of the ESRC PICT initiative through its funding of the Centre for Research on Organisations Management and Technical Change (CROMTEC) at the School of Management, UMIST. Biographical note : Brian Bloomfield is a lecturer in the School of Management, UMIST. His interests include the sociology of science and technology and his current research is focused on issues of knowledge, control, and culture in connection with the introduction of IT in the NHS. Address: Brian P. Bloomfield, Centre for Research on Organisations Management and Technical Change, Manchester School of Management, UMIST, Manchester, M60 1QD.</page></plain_text>