<plain_text><page sequence="1">THE JOURNAL OF THE LEARNING SCIENCES, 4(3), 281-319 Copyright ? 1995, Lawrence Erlbaum Associates, Inc. Learning in the Right Places Susan L. Epstein Department of Computer Science Hunter College and The Graduate School of The City University of New York A learner's experience is in large measure determined by the situations it encounters in the problem space, and, for a challenging task, only a small fraction of that space can ever be visited. People's ability to learn to perform well on such tasks is therefore a clear indication that not all situations are equally relevant to learning. The primary contributions of this article are the notion of key nodes, situations particularly important to the development of expertise, the presentation of empirical evidence for their existence, and the design of a training method intended to capitalize on them. This article suggests why key nodes may be clustered in the search space and describes how a learner may arrive at a key node by chance, be drawn there by a choice the learner makes, or be driven there by the actions of another agent. This article offers empirical evidence of substantial improvement in the quality of performance when a game-learning program is deliberately directed to clusters of key nodes, and considers several ways to do so. It also discusses the extension of these results to other domains and speculates on the significance of these results for human learners. This article addresses the acquisition of strategic knowledge that supports expert behavior. The thesis of this work is that expert reasoners can learn to perform difficult tasks well because they identify relatively few decision- making points as key and then extract crucial knowledge from them. It is hypothesized that the existence of key nodes, as these information-rich points are called, permits a learner to extract enough knowledge to perform like an expert without experiencing all possible decision-making situations. Key nodes enable a reasoner to parlay a learning experience that is small in comparison to all possible situations into a robust expertise that can succeed under any circumstances. If this premise is correct, then the quality of learned performance depends, in part, on the learner's ability to locate key Requests for reprints should be sent to Susan L. Epstein, Department of Computer Science, Hunter College of The City University of New York, 695 Park Avenue, New York, NY 10021.</page><page sequence="2">282 EPSTEIN nodes. In other words, a good learner knows not only what and how to learn, but also which situations offer important learning experiences. Informally, key nodes offer more of the information required to perform expertly than other problem situations do. They are also situations in which the learner's decision would have been uninformed, all the alternatives may have seemed poor, the correct choice may have seemed among the weakest, or the learner may have had no clear preference. Insufficient knowledge to make a well-founded decision can motivate a reasoner to learn. Thus, the same situations that trigger this need to learn also act as key nodes because examination of them offers knowledge pertinent to expert performance. Consider, for example, a driver with a formula that directs route selection in an unmapped town of many one-way streets and dead ends. That driver will be considered an expert once its formula is better than most people's (D'Andrade, 1991). The development of that formula, however, may require steering the car to key nodes-places where the driver discovers information that improves the formula. A dead end is an example of a key node. The driver tries a route that includes the dead end and learns that it offers no ready access to most locations in town. The motivation for visiting the dead end the first time was not to learn about every street in town (knowledge about the task), but to find an efficient and effective route (knowledge about how to behave there). A second example of a key node is a traffic circle that offers along its short circumference a convenient interchange among eight major thoroughfares. Note that key nodes may (the traffic circle) or may not (the dead end) be places worth revisiting. The terminology of state spaces clarifies the substantial difference be- tween knowledge about a task and knowledge about how to perform it. A state is a description of some situation in which a decision must be made such as a location where a driver may turn left, turn right, or continue straight ahead. A state space is the set of all possible states that can arise in a particular task such as all possible locations in a town that require a driving decision. The states in a state space are linked together by the permissible decisions there. In the driving example, a decision at one place will lead to some subsequent location where a choice is once again required. Thus, a location with three options (left, right, straight) is linked to each of the three subsequent states encountered when one of those options is chosen. In the driving example, knowledge about the task is knowledge about the states that exist and how they are linked together, whereas knowledge about how to perform the task is knowledge about how to drive from one point to another. In this state space representation, problem solving is navigation through a sequence of states in the space, and expertise is correct and efficient navigation. A set of related problem spaces, like driving through unmapped towns or playing board games, is called a domain. Well-guided search of a state space thus becomes an appropriate metaphor for well-directed learning. Well-guided search describes a learning experi- ence in which the learner arrives at the key nodes-the places where knowl-</page><page sequence="3">LEARNING IN THE RIGHT PLACES 283 edge to support behavioral expertise can be acquired. The central issue in well-guided search is the balance among learning speed, learning accuracy, and the development of self-reliance in the learner. In the context of key nodes, this becomes proof that key nodes exist, methods to reach them, and their role in the learning process. Does how to encounter key nodes vary with the problem class, for example, with the town in the driving example? Does how to encounter key nodes vary with the problem domain, for example, with driving as opposed to flying? How does one learn to encounter key nodes efficiently? To what extent does knowledge about how to perform expertly overlap with knowledge about how to encounter key nodes? To what extent does the learning method for one inform the learning method for the other? The premise that key nodes exist arose during experiments with Hoyle, a program that learns to play two-person, perfect information, finite-board games. Hoyle begins with general game-playing knowledge plus the rules for one specific game and then learns during competition to play that partic- ular game better. Competition may be against an external contestant (a person or another program) or Hoyle itself. As Hoyle analyzes its experience and identifies key decision-making situations, the program extracts game- specific knowledge it can apply in later competition at the same game. To date, Hoyle has learned to play 18 different games as well as human experts. This article details aspects of Hoyle's behavior for which only the exis- tence of key nodes provides an explanation. It shows that guidance to key nodes results from both internal direction (from the learner's reasoning principles) and external direction (from the learner's environment). The results argue for carefully designed teaching environments that value both instruction and practice; they also recognize certain aspects of an individual that make learning possible. The empirical results described here are from the domain of board games, but broader general applicability is discussed as well. The contributions of this article are the identification of key nodes and well-guided search as important concepts, the demonstration of their power, and the formulation of an initial theory for them. The next section consists of some simple definitions to establish common terminology. Subsequent sections explain the role of key nodes in experi- ence, provide background on Hoyle, and document how a learner can arrive at key nodes through both external and internal direction. The article goes on to discuss these results in the broader context of cognitive science and related work. The final section outlines the beginnings of a theory for well-guided search. Descriptions and representations for all referenced games appear in the Appendix. SOME FUNDAMENTAL DEFINITIONS Game playing is a good domain in which to study key nodes. Each state can be completely described by relatively few values. Games are noise free; that</page><page sequence="4">284 EPSTEIN is, descriptions are always correct without intervening instrumental or human error. It is easy to replicate a situation in which a decision must be made, or even a sequence of such situations. For simple games, and even for many states in more difficult games, the correct decision is readily comput- able. Finally, it is easy to measure different facets of expertise as perfor- mance in competition against a variety of opposition. Although human experts for games like chess certainly exist, little is known about how people learn to become experts. Psychologists have stud- ied expert human game players, particularly chess players, for a century, but it is the nature of their skill rather than its acquisition that has been the primary focus of attention (Binet, 1894; Charness, 1981; Djakow, Petrowski, &amp; Rudik, 1927; Holding, 1985). When the study of expertise has been extended to other domains, the focus has been primarily on experts' memo- ries and their organization (Allard, Graham, &amp; Paarsalu, 1980; Chase &amp; Simon, 1973; Egan &amp; Schwartz, 1979; Eisenstadt &amp; Kareev, 1975; Engle &amp; Bukstel, 1978; Goldin, 1978; Shneiderman, 1976; Watkins, Schwartz, &amp; Lane, 1984). Work on the transition from novice to expert has focused primarily on the reorganization of knowledge as in Chi, Feltovich, and Glaser (1981). An exception to this is recent work on the acquisition of tic-tac-toe (or naughts and crosses) skill by 6- to 9-year-old children who are shown to acquire strategies such as Win, Fork, and Block with age and, presumably, with more experience at the game (Crowley &amp; Siegler, 1993). The importance of well-guided search is supported by the fact that "most of the recent world champions in chess were at one time tutored by chess masters" (Ericsson &amp; Charness, 1994, p. 739). Indeed, extensive studies across many domains support the hypothesis that deliberate practice (activi- ties found to improve performance most effectively) is fundamental to the development of expertise (Ericsson, Krampe, &amp; Tesch-R6mer, 1993). This article addresses the nature of the situations that deliberate practice exposes. There is only one program, to the best of my knowledge, that has strong connections with human game playing and game learning. Hoyle, described further in the Hoyle section, learns to play 18 different games expertly and uses multiple learning methods to do so. The program employs common- sense strategies like those Crowley and Siegler (1993) detected in people and does not insist that its knowledge be absolutely correct. Also like people, Hoyle's learning algorithms are selective enough to control the amount of knowledge it acquires, and it makes use of visual symmetries. Many of the program's learning methods are familiar even to a novice human game player. Hoyle can express the reasons for its move selection in terms that people can understand. Using learning time as a measure of difficulty, Hoyle's performance orders three games identically to the way seven human subjects' performance does (Ratterman &amp; Epstein, in press). Hoyle is a discovery program that, like a person learning to play, shapes much of its own learning experience by its behavior. Hoyle is the program whose empir- ical data instigated the theory of key nodes described here.</page><page sequence="5">LEARNING IN THE RIGHT PLACES 285 The following definitions facilitate the discussion of game playing in the remainder of this article. Some of the ideas are illustrated in Figure 1. * A game is an activity for two agents called contestants. Each contestant is assumed to have perfect information and access to all the relevant knowl- edge about the current state of the world, so, for example, there are no concealed cards. A game is defined by a finite board, playing pieces, and a set of rules determining play. For example, the rules of tic-tac-toe stipulate that the grid is initially empty, that the contestants alternate placing an owned playing piece in any empty position, and that play halts if the grid is filled or if one contestant (the winner) achieves three owned pieces along a row, column, or diagonal. * A position is a location on the game board where a playing piece may rest in accordance with the rules of the game. For example, there are nine positions on the tic-tac-toe board. a contest a move X to move a state O to move O to move O to move X to move _I" Sto move I I X to move a leaf 0 to move FIGURE 1 A portion of the tic- tac-toe game tree.</page><page sequence="6">286 EPSTEIN * A state is a configuration of playing pieces on the board and the identity of the mover, the contestant whose turn it is to play. A reachable state is one that can actually occur during competition. For example, an empty grid with X to move is a reachable tic-tac-toe state, but with O to move it is not. Interesting board games usually have a great many reachable states. * A move is an action that is permitted by the rules of the game and transforms one state into another. For example, from the empty grid state with X to move in tic-tac-toe, there are nine legal moves, each of which results in a different grid with a single X, eight empty positions, and O to move. * A game tree organizes the set of all reachable states for a game so that each state points to all possible next states after a single legal move. For example, in the tic-tac-toe game tree, the initial empty-grid state points to its nine next states. A game tree is the search space (set of possible problem situations) for a problem-solving program that plays a game. The average number of states pointed to by any single state in a game tree is called its average branching factor. This is a metric that reflects the static nature of the game tree and is unrelated to any particular search strategy. * A contest is one complete experience at a game from some initial state specified by the rules to some final state in which the rules designate a winner or declare a draw. A contest may be envisioned as a path through a game tree from the initial state to a leaf, a state that points to no others. The first moves of a contest are called the opening, the last moves are called the endgame, and the rest are referred to as the middlegame. * A tournament is a sequence of contests between two contestants in which they alternate moving first. One may, for example, play a tournament of 20 contests at the game of tic-tac-toe. * Perfect play is when, for every state in the game tree, one always moves to secure the best possible outcome despite subsequent error-free play by the opposition. With perfect play, one always wins from a winnable position and draws from a drawable one. * A draw game is one in which every contest between two perfect contes- tants must, by the nature of the game graph, end in a draw. Tic-tac-toe is an example of a draw game. * A contestant who loses a contest at a draw game has made an error; one who wins or draws is said to have achieved an expert outcome. KEY NODES Given a learning program with competence in more than one problem space, key nodes by definition exist in every problem space where that program learns from experience to perform better. A formal definition of key nodes follows. For any search space S, the key nodes K for a learner L with foreknowl- edge F are a minimal subset of nodes from S such that when L begins with</page><page sequence="7">LEARNING IN THE RIGHT PLACES 287 F learns while it visits K, and then turns learning off, L performs expertly throughout S. The remainder of this section first looks more carefully at the role of the foreknowledge F and the minimality restriction and then offers evidence of the existence of key nodes and discusses their relation to learning. Several examples of key nodes in game playing are included. The Role of Foreknowledge in Key Nodes The property of being a key node is a function of the learner's foreknowl- edge; that is, what one knows in advance necessarily delimits what a key node is. Consider first the situation in Figure 2(a). Most people who consider themselves expert at tic-tac-toe believe that O's prospects look grim here. Not coincidentally, this is a key node (an element of K) in the game of tic-tac-toe (S) for Hoyle (L) whose general game-playing foreknowledge (F) is detailed in the Hoyle section of this article. Tic-tac-toe is so easy that if Hoyle encounters this situation once in play against an external expert, it will acquire all the additional knowledge beyond F that it needs to play tic-tac-toe perfectly. If the program does not encounter Figure 2(a) or its symmetric equivalent and learning is turned off, Hoyle will never play tic-tac-toe perfectly because it will always make the wrong move (into a corner) here. As another example, consider Figure 2(b), a key node (an element of K) in the game of lose tic-tac-toe (S) for Hoyle (L) with the same general game-playing foreknowledge (F). Like tic-tac-toe, lose tic-tac-toe is played on a 3 x 3 grid between X and 0. Whoever achieves three owned playing pieces in a row vertically, horizontally, or diagonally, however, loses. Error- free lose tic-tac-toe contests are always draws. This game is harder for Hoyle to learn than tic-tac-toe. Most people who have never played this game before are certain that X has made a serious error in Figure 2(b). Hoyle, too, will persistently try every possible way to play as X from the other openings; its foreknowledge incorrectly makes Figure 2(b) unattractive. If Hoyle is X -O X (x (a) (b) FIGURE 2 Key nodes for Hoyle with O to move against an expert as X in the search space (a) for tic-tac-toe and (b) for lose tic-tac-toe.</page><page sequence="8">288 EPSTEIN playing X the first time it experiences Figure 2(b), its foreknowledge and whatever it has learned thus far are likely to be so inadequate that it still loses, despite happening upon the correct opening. The program's learning from that failure could further discourage use of the correct opening in the future. If Hoyle is playing O against an external expert when it encounters the situation in Figure 2(b), however, it will learn the opening and go on to use it in subsequent contests. The Minimality of Key Nodes Because they are a minimal subset of S, key nodes offer an efficient path to important knowledge. In Figure 2(b), after all possible error-ridden contests with the other openings are experienced, the program can deduce the knowl- edge that would have been supplied immediately by the key node. In the course of this experience, Hoyle would acquire extensive, detailed knowl- edge about each of the alternative openings, much of it irrelevant to exper- tise at lose tic-tac-toe. Unfortunately, when the other contestant realistically and deliberately varies its play, happening upon every subsequent way to lose is likely to require hundreds or even thousands of contests. Even if the other contestant were to deliberately instruct Hoyle by playing in turn each possible lose tic-tac-toe contest with the wrong opening, learning would require more than a hundred contests. How or why one loses with the other openings is unimportant to behavioral expertise in a game that always ends in a draw with error-free play on both sides. The efficient summary available from Figure 2(b) is "open in the center." Evidence of Key Nodes' Existence Tic-tac-toe offers a simple, well-understood example of the contrast between the total number of situations an expert may theoretically confront and the number of situations a developing expert experiences during learning. Al- though initially the combinatorics of tic-tac-toe are a bit daunting, Table 1 shows that the rules of the game reduce the number of situations that actually arise during play to about 5,478. People do not need to experience all 5,478 situations to develop expertise; in several games of this size, human subjects often achieve expertise after exposure to at most 7% of the possible situa- TABLE 1 Reducing Combinatoric Complexity in Tic-Tac-Toe Representation Count States based on average branching factor and contest length 94." = 19,683 States calculated from combinatoric formulae 6,046 States that actually occur during rule-abiding play 5,478 Contests that sequence symmetrically distinct occurring states 170</page><page sequence="9">LEARNING IN THE RIGHT PLACES 289 tions (Ratterman &amp; Epstein, in press). There are also some basic symmetries in tic-tac-toe; for example, there is no difference between beginning with the first X in the upper left corner or in the upper right. These symmetries reduce the possible situations even further (e.g., nine possible first moves become three). Even after reduction for symmetry, however, the 7% encountered during learning do not encompass all possible situations. Perhaps experts learn sequences of responses rather than what to do in every possible situa- tion. There are, after all, only 170 different ways to play an entire tic-tac-toe contest if one recognizes symmetrically equivalent situations, applies them to narrow the possibilities, and stores sequences of up to nine moves (calcu- lated from Berlekamp, Conway, &amp; Guy, 1982, pp. 670-671). Few human experts, however, experience all 170 scenarios, let alone remember them. Key nodes appear in much larger spaces as well. Data indicate striking similarities between the way ordinary folk cope with the somewhat over- sized tic-tac-toe search space and the way grandmasters at chess cope with an enormous one. The literature on expert chess players confirms that they neither search nor remember very much of such a space. With an average branching factor of about 20 and contests that average 80 moves, the chess game graph could have as many as 2080 - 10120 nodes. Although there may be as many as 40,000 distinct chess openings, most tournament players encoun- ter no more than 400 of them in the course of a year of play and prepare only 3 or 4 for any given tournament (Charness, 1991). If one restricts the graph to these 40,000 openings, assumes a memorized endgame, and restricts moves to those that masters may select, there would still be 4 x 1020 reason- able contests with exhaustive search (Charness, 1983). An expert chess player is estimated to experience 1,200,000 nodes per year (Charness, 1991). During 20 years of learning, that amounts to only about 6 x 10-14 of such a reasonable space and 2.4 x 10-"1 of the entire space. The famous chess-playing computers, like HiTech and Deep Thought, experience more nodes in 1 min than any grandmaster does in a year (An- antharaman, Campbell, &amp; Hsu, 1990; Berliner &amp; Ebeling, 1989). Although the programs outplay most skilled human chess players, there are still hun- dreds of grandmasters who consistently outplay the machines. Those people learned to play that well, better than any high-speed, deep-searching chess program, in only a tiny fraction of the chess space. This suggests that the chess game tree, like the tic-tac-toe game tree, does not uniformly distribute the knowledge necessary for people to learn to play well across all its states, but that some of the visited nodes trigger important learning experiences and that learners regularly encounter those nodes during play. Key Nodes and Training The central theme of this article is that a learner's ultimate proficiency depends on its exposure to key nodes. Because human learners acquire</page><page sequence="10">290 EPSTEIN knowledge that supports expertise from key nodes, direction to as many key nodes as possible should accelerate and strengthen the development of ex- pertise. This opportunity occurs during training. In this article, training is the learner's experience in the search space. In much of machine learning, a program is given a training set of experiences from which to learn. This set may be selected at random from a problem space, or it may be deliberately chosen, even sequenced, by an instructor. One way to evaluate a training experience would be to have an exhaustive list of key nodes with respect to a program's input knowledge and the search space and check them off as they are encountered. This assumes that the space is extremely well understood by some person who can devise either a description for those key nodes or a procedure to enumerate them. For any challenging game, however, people cannot enumerate the key nodes and the obvious exhaustive algorithm for their identification (consider every possi- ble set of nodes) is exponential in the number of nodes in the search space. A more pragmatic approach would be to indirectly assess the percentage of key nodes experienced during learning by testing the program's playing skill after learning. A game-learning program's opposition is, to some extent, its instructor, called a trainer here. The trainer has proclivities in its play that effectively shape the game learner's experience in the search space. The ideal trainer would be aware of the key nodes appropriate to a game-learning program, its foreknowledge, and the game; that is, it could compute the function f such that K = f(L, F, S). Such a trainer would play so that the learner would encounter every key node as quickly as possible. Once again, for a challeng- ing game, this approach is intractable. In any domain, how does a human or machine learner encounter key nodes? Here are some possibilities. * Key nodes could arise by chance. Given the apparent relative scarcity of key nodes and the strength of expertise people develop, this seems un- likely. Among all 5,478 possible tic-tac-toe nodes, only 2 (Figure 2(a) and its symmetric equivalent) are key nodes for Hoyle. * Key nodes could be presented by a teacher as a sequence of training examples with explanations or solutions. For instruction, a series of puzzle problems like that in Figure 2(a) would require that the teacher have a clear test for key nodes and a road map to reach them. Unfortunately, neither is available for difficult tasks; the only evidence that one has visited key nodes is in the improved quality of one's performance. * Key nodes could be encountered in a carefully structured environment. * The learner could seek key nodes out by itself. The last two possibilities, a structured environment and internal direction, have been explored with Hoyle, a program named for the 18th-century chess player Edmond Hoyle who wrote a book declaring official rules for popular</page><page sequence="11">LEARNING IN THE RIGHT PLACES 291 games of the period. It was empirical work with Hoyle that initially led to a theory about key nodes. HOYLE Hoyle is a program that learns to play games during competition. Given the rules, Hoyle can play any two-person, perfect information, finite-board game correctly, and then learn to play it better based on its experience. Hoyle learns to play each game much the way the reader would learn to play a strange game. You may not know anything at all about the game, but you do have a set of skills and expectations that would enable you to play according to the rules and guide your efforts to learn to play well. Hoyle is based on a learning and problem-solving architecture called FOr the Right Reasons (FORR), predicated on multiple rationales for decision making (Epstein, 1994a). FORR supports multiple learning strategies (the ability to learn an item of knowledge more than one way, store alternative values, and formu- late recommendations in a uniform manner based on alternative values), multiple decision strategies (the ability to make recommendations from more than one viewpoint), and multiple knowledge application (the ability to use an item of knowledge more than one way). A FORR-based program is expected to learn both by imitating expert behavior and by trying (and perhaps failing) to solve problems on its own. This section describes what Hoyle knows before it learns, what it is directed to learn, how it learns, and how Hoyle's learning impacts on its behavior. Further details on Hoyle are available in Epstein (1992). What Hoyle Knows Before Learning Hoyle has been carefully designed to facilitate learning about games and learning about learning. Like many people, Hoyle comes equipped with a store of knowledge about game playing in general; it knows, for example, to try to make winning moves and to stop playing when one contestant has won. Unlike people, Hoyle can begin as a novice at any particular game as often as we like and make explicit exactly what it knows and what it learns. Hoyle simulates an expert game player who in general already knows how to play games (take turns, follow the rules), the good reasons for selecting a move (the Advisors described in the next section), what to learn about a new game (the useful knowledge described in the What Hoyle Learns section), and how to learn it (the learning algorithms described in the How Hoyle Learns section). Given the rules of a new game (like those in the Appendix), Hoyle becomes an expert at the game by playing it. As it plays, Hoyle acquires specific useful knowledge about the new game until it gradually makes better move choices based on that useful knowledge. When Hoyle</page><page sequence="12">292 EPSTEIN learns to play a game, it is easy to distinguish between what the program knows in advance and what it learns, and to isolate knowledge about one game from knowledge about another. How Hoyle Plays Hoyle learns about a game by playing contests at it. A contest appears to Hoyle as a sequence of game states (nodes in the game tree) in which the mover makes a decision. A game state is described to the program as the location of the playing pieces on the board, which side is about to select a move, and whether Hoyle or its opposition is playing that side. A sample tic-tac-toe contest may begin with the three game states in Figure 3 where the empty positions are labeled with numbers. Hoyle is a limitedly rational program; that is, it implements methods known by its programmers to be reasonable but not necessarily perfect. The purportedly rational approach to game playing is to search down through the game tree from the current state to the leaves and then reason back up to pick the best move from the current state. In tic-tac-toe, it is relatively easy to calculate the best next move that way; in the enormous game tree for chess, it is usually impossible. Hoyle's Advisors are the core of its limited rational- ity, the common-sense strategies that help it select a move. An Advisor is a resource-limited, game-independent, automated proce- dure that epitomizes a reasonable basis for selecting a move like "it captures a piece" or "it wins the contest." When it is Hoyle's turn to move, the program consults its Advisors; they recommend for and advise against the current choices available. A full list of the Advisors appears in Table 2. The input to every Advisor is the same: the current game state, the current legal moves, and the current useful knowledge accrued for the game from experi- ence. An Advisor's output is one or more comments, ordered triples naming the Advisor, a legal move, and an integer that indicates an opinion some- where between strong aversion (0) and enthusiastic support (10). For exam- ple, in the right-most state in Figure 3, input to each Advisor would be that state, the seven empty locations (1, 2, 3, 4, 6, 8, 9) on the grid that constitute legal moves, and whatever useful knowledge Hoyle has about tic-tac-toe. Comments the Advisors may make in this state include: Hoyle moves X 4 56 7 8 9 Opposition moves O 1 2 3 4 X 6 7 1819 Hoyle moves X 1 2 3 4 X 6 00 FIGURE 3 The beginning of a tic-tac-toe contest.</page><page sequence="13">TABLE 2 Hoyle's Advisors Useful Learning Advisor Tier Description Knowledge Strategy Wiser 1 Makes the correct move if the Significant states Deduction current state is remembered as a certain win. Sadder 1 Resigns if the current state is Significant states Deduction remembered as a certain loss. Victory 1 Makes the winning move from None the current state if there is one. Don't Lose 1 Eliminates any move that results Significant states Deduction in an immediate loss. Panic 1 Blocks a winning move the Significant states Deduction nonmover would have if it were its turn now. Shortsight 1 Advises for or against moves Significant states Deduction based on a two-ply lookahead. Enough Rope 1 Avoids blocking a losing move None the nonmover would have if it were its turn now. Anthropomorph 2 Moves as a winning or drawing Expert moves Abduction non-Hoyle expert did. Candide 2 Formulates and advances naive None offensive plans. Challenge 2 Moves to maximize its number of None winning lines or minimize the nonmover's. Coverage 2 Maximizes the mover's markers' None influence on predrawn game board lines or minimizes the nonmover's. Cyber 2 Moves as a winning or drawing Important contests Abduction Hoyle did. Greedy 2 Moves to advance more than one None winning line. Leery 2 Avoids moves to a state from Play failure and Abduction which a loss occurred but proof failure where limited search proved no certain failure. Material 2 Moves to increase the number of None its pieces or decrease those of the nonmover. Freedom 2 Moves to maximize the number None of its subsequent immediate moves or minimize those of the nonmover. Not Again 2 Avoids moving as a losing Hoyle Important contests Abduction did. (Continued) 293</page><page sequence="14">294 EPSTEIN TABLE 2 (Continued) Useful Learning Advisor Tier Description Knowledge Strategy Open 2 Recommends previously observed Opening data base; Induction expert openings. average contest length Patsy 2 Re-creates visual patterns credited Visual patterns Associative for positive outcomes in play; pattern avoids those blamed for classifier negative ones. Pitchfork 2 Advances offensive forks or Forks EBL destroys defensive ones. Shortcut 2 Bisects the shortest paths between None pairs of markers of the same contestant on predrawn lines. Vulnerable 2 Reduces the nonmover's capture None moves on two-ply lookahead. Worried 2 Observes and destroys naive None offensive plans of the nonmover. Note. These procedures are general rationales that reference one or more items of useful knowledge, each supported by its own learning strategy. EBL = explanation-based learning. (Worried, move to 1, strength 8) (Greedy, move to 1, strength 9) (Patsy, move to 3, strength 2) The algorithm for each Advisor is intended to capture the particular perspective it adopts, and the strength of a comment is determined differ- ently by each one. Victory, for example, is fairly simple; it tests each legal move to find one that creates a winning state for the current mover. Pitch- fork, on the other hand, is quite elaborate; it constructs a complex, insightful representation of the current state and then relates it to a useful knowledge item called forks (Epstein, 1990). Each Advisor is implemented as a proce- dure with a time limit. To construct their comments, some Advisors are permitted to search exhaustively at most two moves ahead; that is, they look at all the possible states that could occur if Hoyle made every legal move and then the other contestant did the same. From the last state in Figure 3, for example, without symmetry there would be 42 (7 x 6) such possible states. Although Advisors are intended to be generally applicable, some Advisors may never comment for a particular game or may be irrelevant. For example, Material focuses on piece capture; it is irrelevant in games like tic-tac-toe in which the number of pieces on the board monotonically increases with every move. The role of the Advisors is to provide opinions upon which move selection is based. Not all 23 Advisors are equally reliable or equally important, so, as in the schematic of Figure 4, they are organized into two tiers. The 7</page><page sequence="15">LEARNING IN THE RIGHT PLACES 295 Advisors in the first tier have a priority ranking based on common-sense knowledge such as "try memory before computation." They sequentially attempt to compute a decision based on correct knowledge, shallow search, and simple inference such as Victory's "make a move that wins the contest immediately." If no decision is forthcoming, then all 16 Advisors in the second tier collectively make their many, and often less reliable, comments based on their narrow viewpoints like Material's "maximize the number of your playing pieces and minimize the number of your opponent's." The move with the most support is chosen, and ties are broken by random selection. What Hoyle Learns What Hoyle learns is useful knowledge, data about a game that are possibly relevant to its future play and probably correct. A useful knowledge item is a good question to ask about a new game; its value is an answer to that question. The questions are the same for every game, but their answers typically differ from one game to another. Thus, useful knowledge in Hoyle acquired useful knowledge current state legal moves [Victory Tier 1: Shallow search and inference based on Panic perfect knowledge Enough Rope Absolute yes make Tier 2: decision? move Heuristic opinions no Material ]Shortcut -..Coverage Pt Blackboard Voting FIGURE 4 Hoyle makes decisions based on comments from two tiers of Advisors.</page><page sequence="16">296 EPSTEIN is game-dependent data computed and stored in a game-independent manner. There is one learning procedure for every item of useful knowledge. Openings are an example of useful knowledge. (Recall that an opening is a sequence of moves with which two competitors begin a contest.) The Advisor Open recommends moves that forward previously successful open- ings and advises against moves that forward previously unsuccessful ones. Before the lose tic-tac-toe contest that began with Figure 2(b), Hoyle's useful knowledge slot for openings may have been empty. After the contest, one impact of learning may be that the lose tic-tac-toe useful knowledge slot for openings would now contain a representation for "open in the center, get a draw." Another example of useful knowledge is dangerous states. The Advisor Leery recommends against, but does not forbid, moves that lead to danger- ous states. For a few seconds after a nondraw contest, Hoyle searches exhaustively from the last unforced move of the loser looking for a nonlos- ing alternative. Because the allotted time is severely limited, such a search may prove inconclusive; if so, the state from which it began is learned as "dangerous." Before the tic-tac-toe contest that included Figure 2(a), Hoyle's useful knowledge slot for dangerous states may have been empty. Unless Hoyle has enough time after the contest to prove that a corner response is a certain loss, one impact of learning may be that the tic-tac-toe useful knowledge slot for dangerous states would now contain a representa- tion for Figure 2(a) with an additional O in some corner and X as the mover. Hoyle does not create an explicit representation of any game tree. Instead, Hoyle's useful knowledge is a compendium of ways to behave, that is, a set of openings with some history about their success, a set of dangerous states with warnings to avoid them, and so on. Such information may be repre- sented in many ways, for example, as explicitly formulated rules or as procedures with knowledge embedded in them. Because it is a FORR-based program, however, Hoyle represents this information as a set of game-spe- cific knowledge caches (e.g., lists or hash tables) plus a set of game-indepen- dent procedures (the Advisors) that exploit the caches. Each kind of useful knowledge is represented so that the Advisors that reference it can use it most efficiently. How Hoyle Learns Hoyle learns useful knowledge after both contests and tournaments. Each item of useful knowledge is associated with at least one heuristic, game-in- dependent learning procedure. Each learning algorithm has its own trigger, a condition whose truth initiates execution. Triggers can be tested after decisions, contests, or tournaments. The trigger for openings, for example, is Hoyle's failure to win a contest, and it is tested after every contest. When- ever a useful knowledge item's trigger is tested and found true, the learning</page><page sequence="17">LEARNING IN THE RIGHT PLACES 297 algorithm for it is executed. Thus, useful knowledge items are like perpetu- ally recurring questions about experience, and their associated algorithms are procedures that attempt to construct answers. The learning strategy varies from one procedure to the next and may include explanation-based learning, induction, and deduction. For example, openings are learned by rote, just the way chess masters learn them. Hoyle simply records every opening along with whether the contestant using it won, lost, or drew. The selection of a learning strategy for a particular item of useful knowledge is purely pragmatic and often modeled on what has been observed in expert humans. Deduction amounts to proof in the game tree and is relatively expensive (i.e., takes much time and space). It does, however, provide very strong support to the Advisors in the first tier and is well worth some limited expenditure of resources. Induction bootstraps offer a limited sampling and can be quite effective in certain situations. Although its results may be less trustworthy, they are often good enough. Abduction (p achieves q and q happened, so p must also have happened) is a flawed learning strategy that people often apply with surprisingly good results. The "proba- bly correct" nature of useful knowledge is directly attributable to the preva- lence of nondeductive learning strategies. The learning algorithms are highly selective about what they retain, may generalize or abstract, and may choose to discard previously acquired knowledge. How Learning Affects Hoyle's Behavior Hoyle plays better when its Advisors have more complete and more correct knowledge to guide its decision making. Because many of its individual Advisors apply current useful knowledge to construct their comments, Hoyle learns from its experience to make better decisions from acquired useful knowledge. Consider, for example, significant states. These are states in the game tree where, although the contest is not yet over, perfect play by one contestant will result in a win for that side no matter how well the other contestant plays. With experience, an expert player should learn to recognize signifi- cant states and learn how to exploit them when they offer his or her side an advantage. An example of a significant win state (one that must result in a win for the mover) for tic-tac-toe appears in Figure 5(a) with X to move. An example of a significant loss state (one that must result in a loss for the mover) appears in Figure 5(b) with O to move. By definition, all the children of a significant loss state are themselves significant win states for the other contestant, and at least one of the children of a significant win state is a significant loss state for the other contestant. Once a significant win state is learned, the Advisor Wiser recommends moves to it, and when Shortsight looks ahead two-ply (one more for each contestant), it opposes choices that will in turn afford a move by the opposition to a significant win state of its own.</page><page sequence="18">298 EPSTEIN x 0 (a) X 0 0 x x (b) X 0 X (c) FIGURE 5 (a) A significant win state from tic-tac-toe, with X to move. This is not a leaf, but with perfect play X will win. (b) A significant loss state from tic-tac-toe with O to move. O cannot prevent a perfect-playing X from a win. (c) A key node for Hoyle from tic-tac-toe with O to move. This is not a significant state; with perfect play on both sides a draw will result. The relation between key nodes and significant states is nontrivial. Con- sider, for example, a state S with a dozen children. After playing one or more contests including each of S's children, a learner may first deduce that each of S's children is a significant win state for the same contestant, and thereby deduce that S is a significant loss state for the other contestant. This does not mean that S and all its children are key nodes; perhaps visiting S alone would dissuade the learner from visiting it again. Thus, a significant state is not necessarily a key node. It may also happen that important learning occurs at a state that is not itself a significant state. Figure 5(c) is a replica of Figure 2(a), a key node for Hoyle at tic-tac-toe. The program will always make the wrong choice the first time it encounters this state. After the contest that includes Figure 5(c) or its symmetric equivalent, Hoyle learns that Figure 5(b) or its symmetric equivalent is a significant loss state for 0. Thereafter, two-ply lookahead Hoyle will always refuse to play a corner in Figure 5(c). Clearly, significant states and key nodes are in some sense intertwined and lie in each other's vicinity, but they are by no means equivalent. EXPERIMENTS WITH HOYLE: VISITING KEY NODES VIA EXTERNAL DIRECTION Without an explicit road map to key nodes, the most obvious way to expose a learner to them is to have the opposition play so that the learner must confront them. The data reported in this section are taken from extensive experiments in which Hoyle was required to learn to play three quite differ- ent draw games with small search spaces. Full details on that work are available in Epstein (1994b); here, data are cited to demonstrate how diffi- cult it is to reach key nodes.</page><page sequence="19">LEARNING IN THE RIGHT PLACES 299 In each experiment, Hoyle was expected to learn to play a game in competition against a hand-crafted, external program called a trainer. This learning tournament went on until the program had learned to play well enough to draw 10 consecutive contests. Then learning was turned off and Hoyle played a 20-contest testing tournament against each of four pro- grammed challengers for each game: a perfect contestant and routines that simulated a (slightly imperfect) expert, a novice, and a random contestant. Although the results were consistent for all three games, only those for lose tic-tac-toe are recounted here, the most difficult of the three for the program to learn, and therefore the one that made the impact of external direction clearest. (Lose tic-tac-toe is difficult for Hoyle to learn because perfect play involves a concept it cannot easily represent-symmetric movement re- flected through the center-and some fairly elaborate algorithms for achiev- ing a win as O if the opening move is not the single correct one.) There are a variety of competitors in this experiment: Hoyle, a perfect contestant, and some imperfect contestants. The perfect contestant is one that always makes the best possible move. If there is more than one such move, a perfect contestant chooses one at random to offer a broad variety of high-quality opposition. (A perfect contestant does not, however, set out to teach lessons about the nature of the game tree in any organized manner.) A single routine models all of the imperfect contestants. It accepts an error parameter that determines what percentage of the time to make a randomly chosen, legal, possibly perfect move instead of a perfect one. For testing, this error parameter simulated the expert, novice, and random challengers with 10%, 70%, and 100%, respectively. For training, there was a spectrum of fallible trainers with error parameters of 10%, 20% ... 100%. In each of these experiments, there is one learner (always Hoyle), one trainer, and the same four challengers: the perfect contestant and the expert, novice, and random challengers. The trainer is Hoyle's opposition during learning; the challengers are Hoyle's opposition during testing after learning is turned off. The trainer varies from one experiment to the next; it may be the perfect contestant, a fallible trainer, or even Hoyle itself. Table 3 highlights representative data; each value represents an aver- age over five runs. Power in this experiment is the ability to exploit the other contestant's errors; it is measured here for a draw game by the percentage of contests won and is reported against the expert, novice, and random challengers, in that order. (In a draw game, power against a perfect contestant must be zero.) Reliability in this experiment is the consistent achievement of an expert outcome; it is measured here for a draw game by the percentage of contests won or drawn and is reported first against a perfect contestant and then against the other challengers in the same order. In a draw game, ideal reliability is 100%. For lose tic-tac-toe, maximal power was computed in a 10,000-contest tournament between the perfect contestant and each of the other challengers. Maxi- mal power is 16% against the expert, 66% against the novice, and 74%</page><page sequence="20">300 EPSTEIN TABLE 3 A Comparison of the Power and Reliability Achieved After Different Instruction in Lose Tic-Tac-Toe Learning With Power Reliability Spacea Timeb 1 Perfect contestant 27-65-73 100-88-82-81 89.2 35.6 2 10% fallible 16-70-61 57-91-84-78 135.6 45.6 3 20% fallible 15-54-68 60-96-86-79 224.2 68.8 4 30% fallible 31-59-68 57-78-78-82 137.6 40.6 5 40% fallible 24-65-78 58-89-81-86 325.6 95.2 6 50% fallible 28-61-74 49-71-84-82 306.8 87.6 7 60% fallible 35-61-78 52-72-81-84 273.8 77.0 8 70% fallible 29-69-77 41-63-83-82 301.6 85.6 9 80% fallible 41-72-71 45-68-80-85 154.8 42.8 10 90% fallible 32-60-76 42-72-78-88 209.2 62.0 11 100% fallible 41-64-75 57-66-80-87 185.4 57.8 12 Self-training 14-66-69 39-54-77-78 186.2 71.4 13 Lesson and practice 17-63-77 95-92-88-90 323.0 122.7 Longer training 14 Perfect contestant 12-59-80 100-93-80-88 101.0 49.3 15 Lesson and practice 18-63-85 100-98-97-100 299.0 127.7 Note. The last two columns detail the memory and experience requirements for such learning. The first 13 lines used a termination condition of 10 consecutive wins or draws; the last 2 lines used 20. aMeasured in items of useful knowledge. bMeasured in contests played. against the random challenger. Space is long-term memory allocation for useful knowledge; time is the number of contests the learning tournament ran until Hoyle was able to win or draw 10 consecutive contests. A perfect contestant is an inadequate trainer because it provides no experi- ence with key nodes beyond those that two perfect contestants would proffer to each other. The first line of Table 3 demonstrates that there are key nodes beyond those regularly visited during competition against a perfect contestant. Even though Hoyle learns to play perfectly reliably (100 as the first entry in Column 3) against this trainer, its achievement is fragile: When confronted during testing with the imperfect moves of the other challengers, it has difficulty with situations that arise from their errors. Although an expert should play better against a weaker contestant, Hoyle is less reliable against the other challengers than it is against the perfect contestant (88%, 82%, and 81% in Column 3). Instruction with a perfect contestant shows the learner ideal play, but only in an extremely narrow context, as evidenced by the relatively small long-term mem- ory (in Column 4) allocated for useful knowledge. For example, a key node like Figure 5(c) may never arise in competition against a perfect contestant for which equally good moves are equally likely. Quite reasonably, such con- strained experience is also brief; the learning time against the perfect contestant is significantly shorter than that with other trainers.</page><page sequence="21">LEARNING IN THE RIGHT PLACES 301 Noise in the trainer does not lead the learner to more key nodes; that is, key nodes are not randomly distributed. A series of fallible trainers was tested, each a perfect contestant with a stochastic opportunity to err. The random move selection rates tested were multiples of 10% from 10% to 100%, inclusive. Lines 2 through 11 of Table 3 show representative data for the fallible trainers. Although some fallibility in the trainer provides Hoyle with expanded experience, too much fallibility makes the trainer so easy to defeat that Hoyle wins or draws 10 contests in a row without learning enough to do well in testing. This is why learning times peak midway through the last column of Table 3. Along with an increase in learning time and a randomized component go an increase in the number of nodes the program is likely to encounter and an increased demand on long-term memory. If those additional nodes were equally likely to be key nodes, then the program should have learned to play better with more fallible instruction. In fact, with a highly fallible trainer, Hoyle generally played worse-further evidence that key nodes are or- ganized some way within the search space, a way that a perfect contestant somehow reflects. (Another factor in learning against a fallible trainer is imitation. One of Hoyle's second-tier Advisors imitates the other contes- tant as if it were a model of good play. When the other contestant makes a mistake, Hoyle can eventually unlearn it- learn to prevent its replica- tion-but that takes time and slows the development of expertise. Be- cause imitation is one among many factors, a fallible trainer is not an insurmountable obstacle, but it certainly adds to the learner's confusion.) Left on its own, a program is unlikely to encounter key nodes. When the trainer is eliminated, its lack of guidance to key nodes proves costly. In self-training, Hoyle is expected to learn while playing against itself. Al- though self-training is often presumed to be a natural environment in which to improve expertise gradually, the data in Line 12 of Table 3 indicate otherwise. With self-training, Hoyle was the least reliable player against every challenger. Although Hoyle had achieved 10 consecutive draws during self-training, the nodes it chose to visit did not include enough of the knowledge learnable at key nodes that would later be required in an encoun- ter against the challengers. Self-training is not particularly fast, but it ap- pears fairly repetitive. Although self-training takes about twice as long as learning against a perfect contestant, it retains far less useful knowledge than with other training that requires about the same number of contests. A new training paradigm called lesson and practice training provides good guidance to key nodes. Lesson and practice training advocates inter- leaving relatively few learning contests against the best available expert (the lesson) with relatively many self-training contests (the practice). The appli- cation cited here gave Hoyle experience in cycles of two lessons followed by seven practice contests. Lesson and practice training in Line 13 of Table 3 shows some improvement over training against a perfect contestant in Line 1; its increased reliability against the weaker challengers indicates a less</page><page sequence="22">302 EPSTEIN fragile expert. Performance against the stronger challengers in Line 13, however, is less satisfactory. The problem may have been that training against a perfect contestant offers repeated opportunities to observe wisdom in action, whereas this particular version of lesson and practice training offers a lesson only two ninths of the time. To determine if longer training would solve this problem by offering more exposure to high-quality play, lesson and practice training was rerun with a termination condition of 20 consecutive wins or draws instead of 10 and found an even more dramatic improvement as shown in Line 15. Of course, this necessitated a suite of additional experiments to offer this potential advantage to the other trainers as well and to test whether 50 or 75, or even 100 consecutive wins or draws was a better termination condition than 20. In most cases, performance did not simply improve or continue to improve; there was no statistically significant improvement with later termination or only a single instance of slightly improved performance that never approached Line 1. The single exception, as reflected in Line 15, was lesson and practice training with Termination Condition 20. (Line 14 is provided only for comparison.) Lesson and practice training proved more reliable and powerful against all the challengers than most other kinds of instruction; it was never less reliable or less powerful at the 95% confidence level. One explanation for this improved performance is that Hoyle was able to derive useful knowl- edge because it was forced by lesson and practice training to visit more key nodes during learning. Increasing the termination condition to 20 gives Hoyle (on average) one additional lesson, enough to help it simulate a robust expert. Lesson and practice training is really an experiential style. It pre- scribes relatively few lessons (experience with an expert) followed by rela- tively extensive practice (exploration on one's own). This treats the lesson giver (in this case, the perfect contestant) as a scarce resource-an attitude appropriate in many real-world situations. It also drives the learner's experi- ence to places in the search space it would not experience during lessons. During practice, the learner will attempt to apply what it knows to a variety of related problems. As long as the learner's knowledge does not support perfect play, it is likely to make mistakes, particularly during practice, that take it to key nodes where it can learn to play better. Practice probably works because the learner has good reasons (Hoyle's Advisors) and fairly reliable information (useful knowledge) upon which to base its (perhaps misguided) decisions, and because it can learn from its mistakes so that its experience refines its knowl- edge. Support for this theory appears in the next section. EXPERIMENTS WITH HOYLE: VISITING KEY NODES VIA INTERNAL DIRECTION Hoyle is currently learning nine men's morris, an ancient African game played with 18 pieces on a 24-position board. Although it is a draw game,</page><page sequence="23">LEARNING IN THE RIGHT PLACES 303 most people find the search space of 7,673,759,269 nodes quite challenging (Gasser, in press). Hoyle's contests against an external expert program aver- age 60 moves in two stages-a placing stage with an average branching factor of 15.5 and a sliding stage with an average branching factor of 7.5. Only in a game this difficult has the impact of internal direction on Hoyle become evident. Originally, Hoyle was not very good at nine men's morris; it lost every contest against its trainer-a hand-crafted, very strong, expert program. Recently, however, two new Advisors were added that capitalize on the visual cues provided by predrawn lines on the game board (Epstein &amp; Gelfand, in press). With these Advisors, the program initially had some ability to draw and then began to learn to win. Table 4 shows the outcome of a 50-contest tournament with both of the new Advisors in place. During the 50 contests represented in Table 4, Hoyle lost 24 times, drew 17 times, and won 9 times. (Because nine men's morris is a draw game, the wins mean that the hand-crafted program made errors, which are now being corrected. There is, however, no program that plays this game perfectly. The proof that the game is a draw consists of a single, computer-generated path through the game tree that relies on a 2-gigabyte data base generated by full retrograde analysis; Gasser, in press.) The first win was not until the 27th contest, and 5 of the wins were in the last 8 contests, suggesting that Hoyle was learning to play better as it acquired more experience. (The likelihood that when 10 wins are distributed at random among 50 contests, none would appear before the 27th contest is .02% and that 5 would appear in the last 8 is 0.4%.) Comparison against what is believed to be a perfect contestant for the placing stage indicates that the program made no identifiable errors in the placing stage of any of the last 10 contests. There are several possible explanations for this clear improvement: TABLE 4 The Outcome of a 50-Contest Tournament at Nine Men's Morris Between Hoyle and a Handcrafted Expert Program No. Outcome No. Outcome No. Outcome No. Outcome No. Outcome 1 draw 11 loss 21 loss 31 loss 41 draw 2 loss 12 loss 22 draw 32 loss 42 draw 3 draw 13 loss 23 loss 33 win 43 win 4 loss 14 draw 24 draw 34 draw 44 draw 5 loss 15 loss 25 loss 35 win 45 win 6 draw 16 draw 26 loss 36 loss 46 win 7 loss 17 draw 27 win 37 loss 47 win 8 draw 18 draw 28 loss 38 loss 48 win 9 loss 19 loss 29 loss 39 win 49 loss 10 draw 20 loss 30 draw 40 loss 50 draw Note. "Win" indicates that Hoyle defeated the trainer; "loss" indicates that Hoyle lost to the trainer.</page><page sequence="24">304 EPSTEIN * New application of useful knowledge: One possibility is that the new Advisors were associated with a new category of useful knowledge, but these two Advisors do not reference any useful knowledge at all and so could not benefit from learning. * Unusually accurate new Advisors: Another possibility is that the new Advisors were so clever that they quickly dominated the decision-making process. This is also not the case; Hoyle gathers statistics on the performance of its Advisors and, although the new ones were active, they by no means overwhelmed the others. Although there were now some early draws, the ability to win was not immediate as it should have been if these Advisors "knew" the right moves from the start; the program's improvement was clearly gradual. * Direction without learning: Yet another possibility is that learning was already drawing the program to places where all Hoyle needed was some good advice from these new Advisors to play well. If that were so, then the useful knowledge store would be roughly the same size without the new Advisors as with them. This is not the case; the useful knowledge store increased with the new Advisors. * Visiting the key nodes: The final possible explanation and the one advocated here, is that the program performed better because of the new nodes it chose to visit during learning. With different useful knowledge as input, the same Advisors in the same states are likely to make different comments, and based on those different comments, Hoyle is likely to select a different move. The new Advisors drew Hoyle to places in the search space where its learning algorithms extracted additional, more powerful, useful knowledge upon which the other Advisors were able to capitalize. Hoyle definitely plays smarter with these new Advisors, smart play directs it to the key nodes, and Hoyle learns enough there to make even better decisions than it did without the new Advisors or with the new Advisors but without learning. DISCUSSION The results described here support hypotheses about the nature of a learner's task and teaching toward it, and they offer a perspective on learning pro- grams. Exploiting Key Nodes: The Learner's Task This article addresses the ability of a human learner to bootstrap from limited experience in a very large problem space to robust expertise through- out it. The previous section offered evidence that some problem instances</page><page sequence="25">LEARNING IN THE RIGHT PLACES 305 (key nodes) can provide important developmental experience and that these instances are not randomly distributed in the search space. The section before it offered evidence that knowing how to find those key nodes is extremely important. Indeed, a sudden rather than a gradual improvement in performance during learning may be explainable as the crystallization of developing internal direction as if the learner has finalized some good decision-making technique. The impact of new Advisors on Hoyle's ability to learn nine men's morris supports this hypothesis. Current research is successfully exploring ways to have the program learn new Advisors on its own that appear at this writing to have a similarly beneficial effect (Epstein &amp; Gelfand, in press). Another example of sudden improvement appears in TD-gammon, the neural net program that learns to play backgammon as well as the best human experts (Tesauro, 1992). Work on TD-gammon supports the principles of well-guided search. The program originally learned in about a month, play- ing 200,000 contests against itself. Contests averaged 60 moves, so the program encountered no more than 2 x 108 nodes, a tiny fraction of its search space. During the first week, however, the program played "long, looping contests that seemed to go nowhere" (G. Tesauro, personal communication, 1991). Then the program improved rapidly, suggesting that first the network learned to find key nodes and then learned to make good decisions. A less accomplished precursor, Neurogammon, had a trainer, but repeatedly expe- rienced the same 400 contests without any ability to encounter additional key nodes. When TD-gammon learned against Neurogammon instead of against itself, it was able to progress away from random moves much more quickly (G. Tesauro, personal communication, 1991). The ability of people like grandmasters to function at a high level when they have only experienced a tiny fraction of a search space suggests, as noted in the Key Nodes section, that key nodes are not evenly distributed. A person studying chess will often deliberately explore multiple alterna- tives from a state that arose during play and find that such search clarifies some difficulty by driving the examination back to the problem's origin in earlier play or by driving it forward to some resolution. If one envi- sions those small searches as paths in a restricted neighborhood or clus- ter, the hypothesis is that such clusters are particularly rich in key nodes and that human game learners deliberately traverse them because such clusters offer a particularly effective learning experience. The evidence offered for this theory is Hoyle's improved performance at the same games under lesson and practice training. Lesson and practice training is driven by Hoyle's Advisors and its game-specific useful knowledge. In the laboratory, Hoyle regularly uses practice contests to explore in the vicinity of nodes it had experienced during lessons. Hoyle's Advisors rely on useful knowledge while they encourage cluster exploration by devices like the repetition of learned openings (Open) and the imitation of expert (Anthropomorph) and successful (Cyber) play.</page><page sequence="26">306 EPSTEIN Well-Guided Search: The Teacher's Task There are clear lessons in this work for those who teach learners like Hoyle and, to the extent that Hoyle is human-like, for those who teach people. In a difficult problem space, there is far too much to remember by rote, and not all experience is equally worth retaining. Given that random experience is unlikely to take the learner to enough key nodes in an otherwise intractable problem space, the role of a teacher is to provide guidance there, and this supervision is nontrivial. * A lack of instructional breadth, like training with a perfect contestant, may overlook nodes that the nascent expert should visit for robustness. A correct model of behavior is insufficient because it is too rigid in its paths through the search space. * A lack of instructional reliability, like training with fallible contestants, may fragment experience and distract the learner from causal associations. In game playing, for example, the problem state representation is always complete in the sense that no information is excluded, that is, that the identity of the mover and the location of every playing piece is specified. Thus, the randomness introduced by a fallible trainer can vary any aspect of the learning situation, whether or not it is relevant to the problem of learning to play well. Whether the learner can manage to extract some useful knowl- edge from such random variation depends, in part, on the distribution of key nodes in the search space. If key nodes are relatively sparse in the space, the randomness of a fallible trainer is unlikely to drive the learner to a key node. * Left to its own devices, a learner is also unlikely to encounter key nodes. Self-training is likely to combine a lack of expertise with a lack of inherent variation. Instead, the learner needs both to be led and to explore the problem space alone, preferably by perturbing its behavior based on knowl- edge. High-quality instruction requires variation that addresses the vicinity of the key nodes, which is what lesson and practice training appear to do. The lessons direct the learner to the nodes an expert would be likely to visit, whereas the practice permits the learner to explore variations on that guid- ance. (Although lesson and practice training was modeled on skill develop- ment in chess players, it has strong similarities with classroom instruction and homework.) Unlike fallible training, the nonrandom variation of lesson and practice training, motivated in Hoyle by Advisors and useful knowledge, apparently leads to key nodes and to more successful learning. Making It Work: The Programmer's Task There are also more general lessons here for the development of expert programs that learn. An evaluation function in a learning program should be constructed not only to make good decisions but also to route exploration to guide learning. The typical, competitively successful game-playing program</page><page sequence="27">LEARNING IN THE RIGHT PLACES 307 has incorporated only some of the work from cognitive science-an exten- sive opening book (early move sequences favored by human experts) and a store of endgame knowledge that describes the best way to finish a contest. It is in the middlegame where such a program makes most of its mistakes, when it selects a move with a heuristic approximation of exhaustive search. The program relies on a prespecified, game-dependent set of features, prop- erties of a state like "king in check" or "control of the center." The program's evaluation function is a game-dependent metric for the goodness of a state computed as some combination of the values of the feature set at that state. Middlegame decisions are dependent on the accuracy of a feature-based evaluation function and, to some extent, on the search depth. This article shows that, unless those features direct the program's attention to key nodes and unless the program can learn there, middlegame play will be both inefficient and inaccurate. A careful distinction between the expert's successful behavior and the learner's is constructive. For most challenging games, one must presume an imperfect trainer. If a program takes an imperfect trainer as a model of expert behavior, however, it will eventually learn something that is incorrect. One reason lesson and practice training succeeds is probably that Hoyle dis- tinguishes carefully between its own moves and those of its trainer. A trainer's move is more highly regarded, and trainer error is less likely to be suspected. Thus, nodes are in some sense labeled by the reason that they were visited, and they are treated accordingly. If the trainer makes errors unidentified as such, the program may choose to imitate them (with the Advisor Anthropomorph) and thereby slow (but not disable) its own devel- opment of expertise. The learning program is expected to make errors, but it tolerates them well and is expected to make fewer as it has more experience. A theory of well-guided search is applicable to domains beyond game playing. Recent work in cognitive science indicates that studies of chess experts are also representative of expertise in other fields. Regardless of the domain, experts are distinguished from novices in several ways: They rely heavily on mental models and special-purpose knowledge representations; they have larger long-term memories and expert procedural techniques; and they have extensive, readily accessible knowledge (Chase &amp; Simon, 1973; Ericsson &amp; Smith, 1991; Ericsson &amp; Staszewski, 1989). Their search, how- ever, is distinguished by its limited focus, not its breadth or speed (Charness, 1991). This information suggests that key nodes are also present in other domains. Like these experts, Hoyle has special-purpose knowledge represen- tations (e.g., half a dozen ways to represent the board), large long-term memory (dynamically allocated caches of thus-far unlimited size), and ex- pert procedural techniques (the Advisors). To date, however, Hoyle's search is more severely restricted and less focused, and its memory organization is less conducive to fast retrieval than human experts'. Finally, the work described here highlights the dangers inherent in learn- ing with a reactive program-a goal-free program that simply senses its</page><page sequence="28">308 EPSTEIN environment and responds to it (Brooks, 1991). A reactive program is in some sense the victim of its environment; it is expected only to experience, generalize, correct, and keep reacting. It has no control over the nature of its next experience and can take no initiative. Because the environment is its trainer, a reactive learning program can only succeed to the extent that its environment shapes its experience appropriately. Thus, a reactive learner must rely on its environment to deliver it to key nodes in far greater propor- tion than they appear in its search space. Additional Issues There are many interesting issues associated with the theory of well-guided search as proposed here. The identification of key nodes is nontrivial; al- though Hoyle has some successful, domain-dependent detectors, "import- ant" or "interesting" remains an elusive property. The distribution of key nodes, in particular their tendency to cluster, may vary with the domain. Perhaps combinations of key nodes, rather than individual ones, are what drives learning. (Indeed, some individual runs observed in the laboratory fortuitously encountered sequences of key nodes in early training and pro- duced exceptionally rapid learning.) Different learning strategies may de- mand different key nodes. Explicit labeling of key nodes may even suggest a new class of learning strategies that relies on their significance as exem- plars or counterexamples, or requires clusters of key nodes as input. All of these are topics for future work. A program needs to learn knowledge that will correct its expectations about each state the next time it encounters it. Rather than begin each contest from the initial state, a game-learning program could begin a practice session from such a key node. This would still not be exhaustive search, but a series of carefully organized "what ifs" that addressed problematic positions. A surprised learner could, quite appropriately, query its trainer or begin a line of inquiry along with or in competition against its trainer. A game-learning program may deliberately construct a state (or a set of states) from which to begin a practice session. For example, if a program encounters a new open- ing and loses to it, why should it have to wait until it encounters that opening again to seek an appropriate defense? The program could instead undertake a deliberate investigation of the opening, alternately playing each side, preferably against the same expert who introduced it. Here the key node (or key nodes) may expose important strategic strengths or weaknesses. Hoyle already saves contests that it considers significant because they violate its expectations. Key nodes arise in the context of good teaching cases. Thus, a natural extension of a skilled learner to a skilled teacher would be to focus on these paradigmatic experiences. Where the program has learned, so may its students, particularly if the instructor and the students have similar learning algorithms.</page><page sequence="29">LEARNING IN THE RIGHT PLACES 309 RELATED WORK Computers and Game Learning Often, the computer simulation of expertise is simply a representation by the programmer of a formula like the one to direct the driver; that is, the program is created as an expert with knowledge about the entire space. To impart that expertise, the programmer must first calculate it or extract it from a human expert. Most expert game-playing programs are like this. The best of them usually achieve their prowess with an opening book or a search engine that drives the program to explore and estimate the winning potential of millions of possible future situations per second and, in some cases, a store of endgame knowledge (Anantharaman et al., 1990; Berliner &amp; Ebeling, 1989; De Jong &amp; Schultz, 1988; Schaeffer et al., 1992; Schultz &amp; De Jong, 1988). For complex problem areas like a large unmapped country or a difficult game, however, exhaustive representation of knowledge about a space may be infeasible because people may not have the knowledge, or it would require too much time and computer memory to calculate and store it. A more realistic approach is to design a program that learns to become an expert, like a driving program whose route selection formula evolves with its experience. Among the few game-playing programs that learn to play ex- pertly, most are limited to a single game and a single learning method. The outstanding success among them is TD-gammon, which learns weights for a neural network to select the best next move (Tesauro, 1992). This technique has met with disappointing results in other games, however (Boyan, 1992). Most game-learning programs also use a single learning method. Although a person may learn openings by rote and learn the value of almost-completed contest states by deduction, game-learning programs generally use a neural net only, temporal difference learning only, or explanation-based learning only. Aside from TD-gammon, game-learning programs tend to be over- whelmed by the highly detailed, possibly inaccurate knowledge they ac- quire. As a result, their performance is either unacceptably slow or not at the level of a human expert (Fawcett &amp; Utgoff, 1991; Freed, 1991; Levinson &amp; Snyder, 1991; Morales, 1991; Wilkins, 1980). Consider, for example, T2, which learned 45 predicate calculus expressions for tic-tac-toe with 52 exception clauses after 800 contests (Yee, Saxena, Utgoff, &amp; Barto, 1990). There is no natural organization for this knowledge, and binding during search is costly. Clauses and exceptions may subsume each other and are oriented toward optimal (shortest contest length) play. Although these de- duced truths support correct play, they afford too much information to provide real-time decision-making guidance. Such programs are acquiring exhaustive knowledge of the problem space, not just knowledge about how to play expertly there.</page><page sequence="30">310 EPSTEIN Driving Discovery The distinctive property of discovery learning is that the learner is expected to formulate and direct its own tasks rather than to follow the examples or the tasks set by an instructor. Discovery learning is notoriously difficult because the program must focus its own attention. Early work on mathemat- ical discovery, for example, was found to be unintentionally biased by LISP, the language in which the concept definitions were formulated (Lenat, 1976; Lenat &amp; Brown, 1984; Ritchie &amp; Hanna, 1984). Focus of attention requires metaknowledge: a measure of self-awareness and a metric for interesting- ness. A discovery program must therefore either begin with a bias as to what is interesting or it must learn it. If interestingness were domain independent, or if the program were re- stricted to a set of related domains, interestingness could be defined in advance. Three fundamental elements of the definition are surprise (i.e., expectation failure), curiosity (i.e., inconclusive knowledge), and ignorance (i.e., computation failure). Each of these is an important indication of inade- quate knowledge and is often cited by people as the reason they formulate a task. Surprise occurs when a learner expects something different from what actually occurs; for example, a robot believes it has picked up a wrench and then determines that the wrench is still on the table. Surprising states are a kind of key node. Indeed, the position in Figure 5(c) is interesting because it violates the expectation that X will win. Langley's BACON program (Lang- ley, Simon, Bradshaw, &amp; Zytkow, 1987) determined its explorations by surprise when data did not fit curves as it expected to do. Curiosity is spurred by inconclusive data. Pell (1991) considered incom- plete experiential data about the efficacy of a move in Go on a 9 x 9 board. His program gathered statistics on the wins and losses associated with individual moves. It pursued moves that had a true mean of winning most likely to be better than a fickleness parameter. This amounts to a forced exploration of situations that are not clearly categorized by the feature set and the program's playing experience. Against an external, hand-coded ex- pert, Pell's program with fickleness parameter of .5 performed better than a program that simply chose a move based on the historical data. In the context of key nodes, the primary difficulty with this approach is that it is directed toward move preference rather than toward knowledge acquisition for expert behavior. Thus, the learning it results in is reactive (what to do) and not particularly transparent. Moore (1991) developed a program for a learning control system that attempts to concentrate experience in portions of the control space relevant to the current task. His program introduces some randomness into an other- wise carefully learned controller. He showed that random perturbations of a decision are better than random decisions and that predictive analysis of random perturbation is even more effective. This last class of methods</page><page sequence="31">LEARNING IN THE RIGHT PLACES 311 attempts to harness some regularity in the space to identify good decisions. The tacit assumption in his approach is that the value of an action is to some extent continuous in the space. Although that may be true for direction in his toy car domain, it is not necessarily true in many other domains, including game playing. Ignorance was the impetus to plan to learn, in the context of biological research, for Hunter's (1989) IVY. This opportunistic planner generated knowledge goals and saved them until appropriate knowledge arrived in the system. An IVY-inspired example of ignorance-driven search would be to seek a situation in which one decision-making principle (Advisor) would be inapplicable or always take precedent over another one. Rather than attempt to construct such states, the program could save their partial descriptions and alert itself for analysis when they arose. The brevity of this survey of related work shows that driving discovery is a little-addressed task, although its principles (surprise, curiosity, and igno- rance) are now clearly identified. The successes of some of the work cited here bode well for giving a learning program (and perhaps a learning person) more control of its experience. Key Nodes, Cases, and Exemplars Two other ways to address the acquisition of expertise in a large search space from limited experience in it are case-based reasoning and exemplars. Case- based reasoning (CBR) attempts to draw an analogy between a case (an old, previously solved and retained problem such as a state from which a move must be chosen) and a new, unsolved one. If an analogy is found, the case-based reasoner tries to adapt the old solution to serve as a solution to the new problem (see, e.g., Ram, 1993; Veloso &amp; Carbonell, 1993.) Cases are abstracted and then indexed by relevant features to speed retrieval. Given the right cases in a domain and effective algorithms to match cases and to adapt solutions, a case-based reasoner should function expertly in a domain where it has limited experience. There are several differences between CBR and the theory of key nodes as proposed here. Usually, every experienced problem distinct after abstrac- tion becomes a case and is therefore retained. Retained cases serve much like Hoyle's useful knowledge; they are possibly relevant to future performance, and solutions constructed from them are probably correct. In ordinary CBR, however, all cases are equally important; the strength of their role in future problem solving does not distinguish them. Key nodes, on the other hand, are not necessarily retained, but are distinguished precisely because they make a greater than average contribution. An exemplar is a case that serves as prototype for some members of its class (Porter, Bareiss, &amp; Holte, 1990). Like a key node, an exemplar is distinguished from other experiences in the search space. An exemplar can</page><page sequence="32">312 EPSTEIN provide so much information that other members of its class need not be consulted at all; a key node can lead to the acquisition of such powerful information that other nodes need not be visited at all. Unlike a key node, however, an exemplar is always retained and used as a primary source for the construction of a solution, whereas useful knowl- edge acquired at key nodes is in no way distinguished as special. Knowledge from key nodes can therefore be intermingled and applied in a variety of unanticipated ways, whereas the combination of exemplars is more complex. Unlike key nodes, it is easy to show that cases and exemplars are used by people to reason and to teach. CBR is, however, generally intended as a response to experience in the problem space rather than a constructor of its own new learning experiences. Thus, guidance to new cases is not an issue as it is for key nodes; one simply relies on the trainer to present the relevant cases or hopes for the best. The role of cases in learning is also different; they offer paradigms for solution construction and are expected to provide a general-purpose plan. Key nodes, in contrast, are not expected to be near-an- swers, only worthwhile learning experiences. A domain like game playing is, in any case, a poor candidate for CBR. Game playing lacks a powerful general feature language for indexing and must contend with a second uncooperative and unpredictable contestant. In addition, game trees have two structural differences from the typical CBR domains: discontinuity and goal irregularities. Game trees lack continuity; often even the relocation of a single playing piece to an immediately adja- cent position destroys any deep similarity between a state and its perturba- tion. In addition, although "with all case-based systems, the assumption is that the domain itself is regular with regard to the goals that will tend to be conjoined," there is no guarantee of that in game playing (Hammond, Con- verse, Marks, &amp; Seifert, 1993, p. 301). The goal in game playing is to achieve the best possible outcome permitted by the structure of the game tree. Beyond that, there are many game-dependent subgoals like queening a pawn in chess or making a mill in nine men's morris. A problem state may be generalized as an interaction among subgoals if they could all be identified, noticed, and remembered. The current state of the art in game-playing programs, in my estimation, focuses on overly specific generalizations that overlook the themes that underlie play. CONCLUSIONS If the knowledge intrinsic to expert performance can only be extracted after all or even most of the nodes in an intractably large search space are visited, then prospects for a person to learn expertise there would be dim. The existence of human experts in spaces intractable for them argues that most of the important knowledge is located at a limited number of key nodes. Thus, visiting the key nodes should become a priority during learning in a very large space regardless of the learning strategy used after arrival there.</page><page sequence="33">LEARNING IN THE RIGHT PLACES 313 Hoyle is predicated on the idea that general expertise in a broad domain can be efficiently instantiated for a subdomain to develop specific expertise there as when general game-playing knowledge is applied to a particular game. For Hoyle, this is discovery learning triggered by its experience during play. Although Hoyle exploits nonexperiential knowledge (like the rules, the Advisors, and various knowledge representations), the program will learn nothing unless it plays. For human experts, too, experience is the catalyst for learning. Thus, the focus on key nodes is appropriate. Deliberate direction to key nodes is well-guided search. Given the goal of learning to perform expertly in a large space, a theory for well-guided search begins with the following principles: * There are key nodes where important knowledge may be extracted by the learner. * Key nodes appear in clusters. * Guidance to those key nodes is both appropriate and necessary. * Internal direction to key nodes is available from high-quality decisions made by the learner. * External direction to key nodes can be managed by the trainer. * A mixture of external direction to key nodes and exploration in their vicinity is a productive way to exploit the clusters of key nodes in a large space and to compensate for trainer error. ACKNOWLEDGMENTS The development of Hoyle was supported, in part, by National Science Foundation Grant No. 9001936. This work has benefitted from conversations with Anders Ericsson, Jack Gelfand, Tom Mitchell, and Stan Matwin. I take full responsibility for any remaining obscurities. Pascal Abadie and Joanna Lesniak provided expert programming support. REFERENCES Allard, F., Graham, S., &amp; Paarsalu, M. E. (1980). Perception in sport: Basketball. Journal of Sport Psychology, 2, 14-21. Anantharaman, T., Campbell, M. S., &amp; Hsu, F-H. (1990). Singular extensions: Adding selectiv- ity to brute-force searching. Artificial Intelligence, 43, 99-110. Berlekamp, E. R., Conway, J. H., &amp; Guy, R. K. (1982). Winning ways for your mathematical plays. London: Academic. Berliner, H., &amp; Ebeling, C. (1989). Pattern knowledge and search: The SUPREM architecture. Artificial Intelligence, 38, 161-198. Binet, A. (1894). Psychologie des grands calculateurs etjoueurs d'echecs [Psychology of great mathematicians and chess players]. Paris: Hachette. Boyan, J. A. (1992). Modular neural networks for learning context-dependent game strategies. Unpublished master's thesis, University of Cambridge, Cambridge, England.</page><page sequence="34">314 EPSTEIN Brooks, R. A. (1991). Intelligence without representation. Artificial Intelligence, 47, 139-160. Charness, N. (1981). Search in chess: Age and skill differences. Journal of Experimental Psychology: Human Perception and Performance, 7, 467-476. Charness, N. (1983). Human chess skill. In P. W. Frey (Ed.), Chess skill in man and machine (2nd ed., pp. 34-53). New York: Springer-Verlag. Charness, N. (1991). Expertise in chess: The balance between knowledge and search. In K. A. Ericsson &amp; J. Smith (Eds.), Toward a general theory of expertise: Prospects and limits (pp. 39-63). Cambridge, England: Cambridge University Press. Chase, W. G., &amp; Simon, H. A. (1973). The mind's eye in chess. In W. G. Chase (Ed.), Visual information processing (pp. 215-281). New York: Academic. Chi, M. T. H., Feltovich, P., &amp; Glaser, R. (1981). Categorization and representation of physics problems by experts and novices. Cognitive Science, 5, 121-152. Crowley, K., &amp; Siegler, R. S. (1993). Flexible strategy use in young children's tic-tac-toe. Cognitive Science, 17, 531-561. D'Andrade, R. G. (1991). Culturally based reasoning. In A. Gellatly &amp; D. Rogers (Eds.), Cognition and social worlds. Oxford: Clarendon. De Jong, K. A., &amp; Schultz, A. C. (1988). Using experience-based learning in game playing. In J. Laird (Ed.), Proceedings of the Fifth International Machine Learning Conference (pp. 284-290). San Mateo, CA: Morgan Kaufmann. Djakow, I. N., Petrowski, N. W., &amp; Rudik, P. A. (1927). Psychologie des schachspiels [The psychology of chess playing]. Berlin: de Gruyter. Egan, D. E., &amp; Schwartz, B. J. (1979). Chunking in recall of symbolic drawings. Memory and Cognition, 7(2), 149-158. Eisenstadt, M., &amp; Kareev, Y (1975). Aspects of human problem solving: The use of internal representations. In D. A. Norman &amp; D. E. Rumelhart (Eds.), Explorations in cognition (pp. 308-346). San Francisco: Freeman. Engle, R. W., &amp; Bukstel, L. (1978). Memory processes among bridge players of differing expertise. American Journal of Psychology, 91, 673-689. Epstein, S. L. (1990). Learning plans for competitive domains. In B. Porter &amp; R. Mooney (Eds.), Proceedings of the Seventh International Conference on Machine Learning (pp. 190-197). San Mateo, CA: Morgan Kaufmann. Epstein, S. L. (1992). Prior knowledge strengthens learning to control search in weak theory domains. International Journal of Intelligent Systems, 7, 547-586. Epstein, S. L. (1994a). For the right reasons: The FORR architecture for learning in a skill domain. Cognitive Science, 18, 479-511. Epstein, S. L. (1994b). Toward an ideal trainer. Machine Learning, 15, 251-277. Epstein, S. L., &amp; Gelfand, J. (in press). Learning new spatially-oriented game-playing agents through experience. In Proceedings of the Seventeenth Annual Cognitive Society Confer- ence. Hillsdale, NJ: Lawrence Erlbaum Associates, Inc. Ericsson, K. A., &amp; Charness, N. (1994). Expert performance: Its structure and acquisition. American Psychologist, 49, 725-747. Ericsson, K. A., Krampe, R. T., &amp; Tesch-R6mer, C. (1993). The role of deliberate practice in the acquisition of expert performance. Psychological Review, 100, 363-406. Ericsson, K. A., &amp; Smith, J. (1991). Prospects and limits of the empirical study of expertise: An introduction. In K. A. Ericsson &amp; J. Smith (Eds.), Toward a general theory of expertise: Prospects and limits (pp. 1-38). Cambridge, England: Cambridge University Press. Ericsson, K. A., &amp; Staszewski, J. J. (1989). Skilled memory and expertise: Mechanisms of excep- tional performance. In D. Klahr &amp; K. Kotovsky (Eds.), Complex information processing: The impact of Herbert A. Simon (pp. 235-267). Hillsdale, NJ: Lawrence Erlbaum Associates, Inc. Fawcett, T. E., &amp; Utgoff, P. E. (1991). A hybrid method for feature generation. In L. A. Birnbaum &amp; G. C. Collins (Eds.), Proceedings of the Eighth International Workshop on Machine Learning (pp. 137-141). San Mateo, CA: Morgan Kaufmann.</page><page sequence="35">LEARNING IN THE RIGHT PLACES 315 Freed, M. (1991). Learning strategic concepts from experience: A seven-stage process. In Proceedings of the 13th Annual Conference of the Cognitive Science Society (pp. 132-136). Hillsdale, NJ: Lawrence Erlbaum Associates, Inc. Gasser, R. (in press). Solving nine men's morris. Computational Intelligence. Goldin, S. E. (1978). Memory for the ordinary: Typicality effects in chess memory: Human learning and memory. Journal of Experimental Psychology: Human Learning and Memory, 4, 605-611. Hammond, K., Converse, T., Marks, M., &amp; Seifert, C. (1993). Opportunism and learning. Machine Learning, 10, 279-309. Holding, D. H. (1985). The psychology of chess skill. Hillsdale, NJ: Lawrence Erlbaum Associ- ates, Inc. Hunter, L. (1989). Knowledge acquisition planning: Results and prospects. In A. M. Segre (Ed.), Proceedings of the Sixth International Workshop on Machine Learning (pp. 61-65). San Mateo, CA: Morgan Kaufmann. Langley, P., Simon, H. A., Bradshaw, G. L., &amp; Zytkow, J. M. (1987). Scientific discovery: Computational explorations of the creative processes. Cambridge, MA: MIT Press. Lenat, D. B. (1976). AM: An artificial intelligence approach to discovery in mathematics. Unpublished doctoral dissertation, Department of Computer Science, Stanford University. Lenat, D. B., &amp; Brown, J. S. (1984). Why AM and EURISKO appear to work. Artificial Intelligence, 23, 249-268. Levinson, R., &amp; Snyder, R. (1991). Adaptive pattern-oriented chess. In L. A. Birnbaum &amp; G. C. Collins (Eds.), Proceedings of the Eighth International Machine Learning Workshop (pp. 85-89). San Mateo, CA: Morgan Kaufmann. Moore, A. (1991). Knowledge of knowledge and intelligent experimentation for learning con- trol. In Proceedings of the IJCNN-91 (pp. 683-688). Morales, E. (1991). Learning features by experimentation in chess. In Proceedings of the European Workshop on Learning '91 (pp. 494-511). Vienna: Springer-Verlag. Pell, B. (1991). Exploratory learning in the game of GO: Initial results. In D. N. L. Levy &amp; D. F. Beal (Eds.), Heuristic programming in artificial intelligence 2: The second computer olympiad (pp. 137-152). Chichester, England: Ellis Horwood. Porter, B. W., Bareiss, R., &amp; Holte, R. C. (1990). Concept learning and heuristic classification in weak-theory domains. Artificial Intelligence, 45, 229-263. Ram, A. (1993). Indexing, elaboration and refinement: Incremental learning of explanatory cases. Machine Learning, 10, 201-248. Ratterman, M. J., &amp; Epstein, S. L. (in press). Skilled like a person: A comparison of human and computer game playing. In Proceedings of the Seventeenth Annual Cognitive Society Confer- ence. Hillsdale, NJ: Lawrence Erlbaum Associates, Inc. Ritchie, G. D., &amp; Hanna, F. K. (1984). AM: A case study in AI methodology. Artificial Intelligence, 23, 269-294. Schaeffer, J., Culberson, J., Treloar, N., Knight, B., Lu, P., &amp; Szafron, D. (1992). A world championship caliber checkers program. Artificial Intelligence, 53, 273-289. Schultz, A. C., &amp; De Jong, K. A. (1988, March). An adaptive othello player: Experience-based learning applied to game playing. In H. Berliner (Chair), Proceedings of the AAAI Spring Symposium on Game Playing. Palo Alto, CA. Shneiderman, B. (1976). Exploratory experiments in programmer behavior. International Jour- nal of Computer and Information Sciences, 5, 123-143. Tesauro, G. (1992). Practical issues in temporal difference learning. Machine Learning, 8, 257-277. Veloso, M., &amp; Carbonell, J. (1993). Derivational analogy in PRODIGY: Automating case acquisition, storage, and utilization. Machine Learning, 10, 249-278. Watkins, M. J., Schwartz, D. R., &amp; Lane, D. M. (1984). Does part-set cueing test for memory organization? Evidence from reconstruction of chess positions. Canadian Journal of Psychology, 38, 498-503.</page><page sequence="36">316 EPSTEIN Wilkins, D. (1980). Using patterns and plans in chess. Artificial Intelligence, 14, 165-203. Yee, R. C., Saxena, S., Utgoff, P E., &amp; Barto, A. G. (1990). Explaining temporal differences to create useful concepts for evaluating states. In Proceedings of the Eighth National Conference on Artificial Intelligence (pp. 882-888). Boston: AAAI Press. APPENDIX Game-Dependent Knowledge HOYLE's game-dependent prior knowledge is kept to a minimum. This section provides the rules for two games, tic-tac-toe and nine men's morris, first as a human might define them and then as Hoyle is given them in its game frame. This is the only kind of game-dependent informa- tion Hoyle is given before it learns to play. Each game frame consists of nine variables and eight functions. (There are a few additional descriptors such as the number of boards that are displayed on the screen during play before it is scrolled. They support implementation but contain no relevant playing knowledge.) The functions in the game frame appear to Hoyle as "black boxes"; that is, Hoyle passes arguments to them and receives answers but cannot inspect them. For example, there is no way to determine if captures can happen in a game except by playing it. Directions for the user are only displayed on the screen to inform a human contestant. The move input reader communicates with a human contestant at the keyboard. The move filter tests that an input or calculated move obeys the rules. The display function draws the current state on the screen. The move effector applies a move to a state to produce the next state, that is, it makes a move. The legal move generator calculates the list of all rule-abiding moves from a given state. The endp, winp, and lossp predicates compute whether a given state is the last in a contest and, if so, whether it is a win or a loss. The visualize function transforms a list-like board into an array represen- tation; the devisualize function reverses that transformation. Unless the display graphics are elaborate, the functions referenced in the game frame typically require only about 70 lines of code in all. The code is object oriented and contains no game-specific features or evaluation function. Tic-Tac-Toe and Lose Tic-Tac-Toe Tic-tac-toe is played on a 3 x 3 grid. The contestant that moves first has five Xs; the other contestant has four Os. Initially the board is empty. A turn consists of placing one of the playing pieces in any empty square. The first</page><page sequence="37">LEARNING IN THE RIGHT PLACES 317 one to place three owned playing pieces in a row vertically, horizontally, or diagonally, wins. There are eight such winning lines. Play ends in a draw when there are no more empty squares. In the game definition for tic-tac-toe in Table Al, the squares in the grid are numbered from left to right, one row at a time beginning with 1. The primary internal representation of every two-dimensional game board is a list. (Note that there is therefore no explicit representation of corner, center, or edge, although one may be computed.) Different Advi- sors use different representations, but the list-like one predominates. The only changes required to transform Hoyle's tic-tac-toe rules into lose tic-tac-toe rules would be to reword the directions for a human contestant and to interchange the way the winner and loser are computed. Nine Men's Morris Nine men's morris is played on the game board in Figure Al. The contes- tant that moves first has nine black playing pieces; the other contestant has nine white ones. A playing piece may only rest on the intersection of two or more lines; there are 24 such positions. The game has two consec- TABLE Al Hoyle's Rules for Tic-Tac-Toe Name: tic-tac-toe Token for Player: X Token for Opponent: O Initial board: (NIL NIL NIL NIL NIL NIL NIL NIL NIL) Adjacency graph: NIL Visible predrawn straight lines: NIL If square grid, dimension: 3 Piece may change location once played: no Winning lines: ((1 2 3) (4 5 6) (7 8 9) (1 4 7) (2 5 8) (3 6 9) (1 5 9) (3 5 7)) Directions for the user: directions-tic-tac-toe Move input reader: reader-tic-tac-toe Move filter: legalp-tic-tac-toe Display function for current state: display-tic-tac-toe Move effector: effector-tic-tac-toe Exhaustive legal move generator: generator-tic-tac-toe Predicate to detect end of contest: endp-tic-tac-toe Predicate to calculate winner: winp-tic-tac-toe Predicate to calculate loser: lossp-tic-tac-toe Visualize: visualize-tic-tac-toe Devisualize: devisualize-tic-tac-toe</page><page sequence="38">318 EPSTEIN utive stages-a placing stage and a sliding stage. In the placing stage, the initial board is empty, and a turn consists of placing one's previously unused playing piece on an empty position. Once all 18 playing pieces are on the board, the sliding stage begins, and a turn consists of sliding one's playing piece along a line to the next empty position. No playing piece may jump over another or be lifted from the board during a slide. Three of the same color playing pieces in a straight line along any side of any square (e.g., 1-2-3 or 7-12-16) is called a mill. Each time a contestant achieves a mill, it immediately removes a playing piece of the opposite color that is not in a mill. If all markers of the color to be removed are in mills, any such playing piece may be removed. Removed playing pieces never return to the board. The first one reduced to two playing pieces or unable to slide, loses. Hoyle's game definition for nine men's morris appears in Table A2. Note that there is explicit representation of neither mill nor square, corner, center, or edge. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 FIGURE Al The game board for nine men's morris.</page><page sequence="39">TABLE A2 Hoyle's Rules for Nine Men's Morris Name: nine-men's-morris Token for Player: B Token for Opponent: W Initial board: (NIL NIL ... NIL) Adjacency graph: ((1 2) (1 10) (2 1) (2 3) (2 5) ... (24 15) (24 23)) Visible predrawn straight lines: ((1 2) (1 10) (2 1) (2 3) (2 5) ... (24 15) (24 23)) If square grid, dimension: NIL Piece may change location once played: yes Winning lines: NIL Directions for the user: directions-nine-men's-morris Move input reader: reader-nine-men's-morris Move filter: legalp-nine-men's-morris Display function for current state: display-nine-men's-morris Move effector: effector-nine-men's-morris Exhaustive legal move generator: generator-nine-men's-morris Predicate to detect end of contest: endp-nine-men's-morris Predicate to calculate winner: winp-nine-men's-morris Predicate to calculate loser: lossp-nine-men's-morris Visualize: visualize-nine-men's-morris Devisualize: devisualize-nine-men's-morris 319</page></plain_text>