<plain_text> <page sequence="1"> Desiring Fakes  AI, Avatars, and the Body of   Fake Information in Digital Art Daniel Becker (LMU Munich) On 3 November 1948, the Chicago Tribune led with the premature headline  “Dewey Defeats Truman”. Thomas E. Dewey was expected to win the presidential  election. Instead, the actual winner Harry S. Truman ironically held up a copy of  the newspaper after his victory was announced (fig. 1). This historical false re- port reached unimaginable relevance in 2016: According to most polls of the 45th  US presidential election, Hillary Clinton would surely become the next president.  Donald J. Trump sʼ shock victory was also attributed to widespread posting and  sharing of so-called ‘fake news’ via social media. In light of this, even Facebook  felt compelled to make a statement.1 The political dimension of fake news says a lot about forgeries in general: forg- eries are not copies. Perpetrators of forgery fake evidence, obscure their sources  and rewrite history. Famous art forgers in the 20th century — such as Tom Keating,  Eric Hebborn and Edgar Mrugalla — have not copied pictures to compete with the  originals, but to imitate the ‘style’ of other artists. What these painters falsified  were not objects, but (art) history itself. In this essay I will propose an understand- ing of forgeries much more as a formal process in the terms of information theory,  rather than focusing on the process of their manifestation. In most instances, the word “forgery” is used with negative connotations. How- ever, forgers have often completed (art) history more than actual experts, who, in a  way, have written this history with gaps: Forgers have created the missing pieces of  the historical puzzle, even if they are false ones. In this context, the German art critic  Niklas Maak writes about the spectacular case of the forger Wolfgang Beltracchi: 1 | See the statement by Mark Zuckerberg at https://www.facebook. com/zuck/posts/10103269806149061 (last accessed on 14 June 2017).  For an analysis of fake news in the 2016 presidential election see All- cott / Gentzkow 2017. </page> <page sequence="2"> 200 Daniel Becker Figure 1: Harry S. Truman holding the issue of the Chicago Daily Tribune at  St. Louis, Missouri on 3 November 1948. What Beltracchi painted are not classical forgeries but his own works  of art, which reveal the mechanism of the art market — and which,  because they are put so precisely in art-historical niches, in terms of  market needs, painted in desideratum, give a precise portrait of the  epoch. They say much about the present time, the image of art history,  and about the economic preconditions of ‘masterpieces’. (2011, my  translation)2 Summarising Maak, then, forgers fulfil the expectations and desires of the art  market by analysing the art system and integrating themselves into its immanent  mechanisms. 2 | “Was Beltracchi gemalt hat, sind keine klassischen Fälschungen,  sondern eigene Kunstwerke, die den Mechanismus des Kunstmarkts  offenlegen — und die, weil sie so präzise in kunsthistorische Nischen,  in Marktbedürfnisse, in Desiderate hineingemalt sind, ein präzises  Epochenporträt abgeben. Sie sagen viel über die Gegenwart, ihr Bild  von Kunstgeschichte, und über die ökonomischen Bedingungen von  ‘Meisterwerken’”. </page> <page sequence="3"> Desiring Fakes 201 Accordingly, a key feature of forging is the supposed context. Antique forgers  have always resorted to fabricated stories to make their alleged ‘discoveries’ seem  plausible. Even in antiquity, ‘original’ writings were claimed to have been found  “‘under the feet of Anubis’ or ‘in the night, fallen into the court of the temple  in Koptos, as a mystery of this goddess [Isis]’” (Grafton 1990: 8). Such forged  provenances were intended not only to make the discovery plausible through ap- parent eye-witness accounts, but to add even more (false) credibility. Likewise,  forgeries cannot be thought of without false collection labels or stories of adven- turous find ings or invented provenances.3 Therefore forgeries are not so much an  expression of craftsmanship as they are vehicles for the creation of a narrative  with similarities to circular reporting, in which the original source is hidden be- hind multiple other sources. Following these considerations, here forgeries are understood as an aesthetic  practice that reflects exactly this mechanism of desire and empty promises, where  the ‘false’ serves the purpose of corroborating pre-existing expectations in order  to make them ‘true’. Due to the information age, I will focus more on the false  information which counterfeits an object as original or authentic than on the object  itself. The focus of this essay will be on new media artworks which utilise fake  identities, and reflect on and question the benevolent art system and its gaps. As  a result, a connection to AI (artificial intelligence) and its history is fundamental:  AI, similar to forgeries, is intended to fit and to satisfy the demands of an ana lysed  environment. These programs work in a defined system, a system, like the art mar- ket, that has been previously analysed.4 Today, AI has various uses, for instance  in the financial sector as well as an instrument to spread ‘fake news’ in social net- works. Google, for example, even managed to defeat the world class GO player Lee  Sedol with its AI AlphaGO. The works discussed here do not reach such technical  competence, but they do address a central aspect of the discussion of AI: decision-  making.5 The mastery of GO, a game so complex that experience and intuition play  3 | The importance of provenance shows the case of John Drewe. Drewe  commissioned art forgeries and forged documents by way of their pro- venance and partly smuggled them into museums and archives in order  to sell the forgeries as originals. 4 | On this see for example an article by Bloomberg News from 22  April 2015 about the Flash Crash, at http://www.bloomberg.com/news/ articles/2015-04-22/mystery-trader-armed-with-algorithms-rewrites-  flash-crash-story (last accessed on 14 June 2017). 5 | Daniel Dennett discusses this aspect in the early 1970s not as a  category of AI itself but a stance in reception. The point in his essay is  not his description of different kinds of stances — design, physical and  intentional stance (1971: 88-91) — but that he reverses for his assessment  of AI the machinable criterion to a human one. In this approach, an AI  </page> <page sequence="4"> 202 Daniel Becker a decisive role, was considered impossible for AI until 2016. For a certain degree  of complexity in fact, decisions cannot be predicted by an algorithm (Turing 1937).  The works discussed here deal with this problem, this gap of information: simi- larly to forgeries, AI artworks stimulate expectations and desires, which are more  reveal ing of the frame of communication in which they operate, than the conceal- ment of their technical deficits. true or falSe — a DeCISIon of botS The question as to whether something is a fake can simply be answered with ‘Yes’  or ‘No’; its ascription is either true or false. The simplest method is to compare  two objects, one which is identified as genuine and another. Here, criminalistics  also argue in the case of identification by means of dactyloscopic and biometric  methods with the words “nature does not repeat itself” (Vec 2006: 209). Classical  reproduc tion in the sense of a perfect duplicate is therefore characterised by its  (twofold) identity, a sample and a match. To identify something as an original or a  forgery is to determine its essence. This means that it is identical to an object, but  not that object itself. In contrast, forgeries deal much more with the different fields  of consistency, the authorship, or the purity of a work. An essential characteristic  of forgeries is therefore the deception by imitation, not only the material imita- tion, but, above all, a simulation, through faked information, of the production and  origin. Nonetheless, in the digital age exact imitations are already obsolete since  everything can be copied without loss. Thus, an imitation also must embrace the  context of creation, produce a reality of production constituted by mimetic premis- es, because the alleged object is not original anyway. Forgeries are linked to technical conditions, and in the course of history more  techniques have become accessible to forgers. Digitalisation, however, has re- leased technical production from its material boundaries, because its essential  property is its basic reducibility to information. The ascription as original or forg- ery is no longer so simple as the degree of complexity involved increases. It is ul- timately independent of hardware, which makes the question of original, copy and  counterfeit become redundant in the context of the digital. In particular, there are  hardly any tangible mediums anymore, since data is mirrored and outsourced in  can only take a (intentional) decision if it fits human behaviour, because  “the goals of a goal-directed computer must be specified intentionally,  just like desires” (91). In this regard the current AI reasearch by Google Director of Engineering  Raymond Kurzweil or Swedish Philosoph Nick Bostrom, for example, is  not of further interest. In fact, the fundament characteristic of AIʼs agency  is in this context relevant. </page> <page sequence="5"> Desiring Fakes 203 clouds, there are no more master-copies and thus no hierarchy of information: no- thing is copied and forged, all data is multiple at its inception. Nevertheless, there  is an important similarity between forgery and digital practices, referring not to  the generation of material but to the performative dimension of the inter action:  forgeries in the digital context do not aim at technical perfection, but rather they  are shared as if they were original. In this sense, they are simply reduced to their  informative content for an existing communicative framework; that is, they can  be linked to expectations. Such an atmosphere of sensationalism and desire for information provides the  perfect breeding ground for fake news. The common greed for information ena bles  fake news to propagate and spread. This was the case, for example, a few days  after the attack at the Boston Marathon in April 2013 and the subsequent manhunt  for two suspects, in which authorities, the public, and the news media participat- ed equally. In a race for the latest and most spectacular news, rumours and false  reports were published without being checked, even by renowned news stations  and thus gained a wide audience. The incentive of such attention prompted some  Twitter users to create false profiles and spread false information. So, the tweet “I  want to kill all of you, you killed my brother” from a profile that pretended to be  one of the wanted assassins, was adopted by social media and news as an actual  statement (European Media Art Festival 2014: 153). Although it was clear after a  few minutes that it was a fake profile and the tweet was just a (cruel) joke, this news  went viral over several hours. In his work Fake Account the artist Alexander Repp  visualises the network of tweets, which are related to the report of the fake profile,  by analysing a five-minute live recording of Twitter: all messages with the word  ‘killed’, the users who wrote them, the attached links and the hashtags form a point  in the network. This artwork not only shows how false messages are spread easily online, but  also how forgeries generally work. The imitative fake profile is not so important,  since the profile and the messages were clumsy inventions whose absurdity was  easy to uncover by deeper consideration: why would a suspect, for whom the whole  country was searching, be sending tweets? What really matters is the fabricated  pre­mitation6 of something that will cause a predictable effect. In this case, some- thing that triggers the desire of (media) reality for sensational news. Through the  numerous participants, the half-life and haste of the news cycle and anticipation  of a spectacle, a network, as Repp presents it, is created. The actual forgery fades  away under the quantity of factors. Favoured by this complexity, the decidability of  6 | This term refers to the German philosopher Hans Blumenberg and  his concept of “Vorahmung”. The English translation as “anticipation” is  misleading as Blumenberg understands this term as a function of the  concept of “imitation” (“Nachahmung”) which would mean, in an overly  literal translation, “post-imitation” (2000: 48). </page> <page sequence="6"> 204 Daniel Becker whether an object — here the tweet — is true or false is irrelevant as long as it fits  expectations. Only an evaluation, which contradicts the hasty machinery of sensa- tionalism, allows an accurate conclusion. Leaving Twitter aside and focussing instead on another social network, Face- book, Sarah Waterfeld describes the practice of self-representation as mimesis 2.0  (2012). Here, she refers to René Girard sʼ model of the “mimetic desire”. In his book  Deceit, Desire and the Novel: Self and Other in Literary Structure, Girard develops  this concept based on literature of the 19th and 20th centuries. In a nutshell, this is a  triangular model of a subject, the “mediator” and an object of desire (Girard 1969: 2).  Instead of desiring the object for its own qualities, the subject or the protagonist  wants it, because it is valuable to the mediator, e.g. his antagonist, whose desire he  imitates by doing so. Therefore, the mimetic process is a mediation which can be  “external”, if it refers to spiritual type like the imagination, or “internal”, if it refers  to a physical type like a person (9). Girard sʼ analysis goes deeper, he is inter ested in  the character sʼ jealousy, envy or rivalry and its consequences for the relations in the  novel, but the literary and even the general anthropological implications of Girard sʼ  model have little relevance for this essay. More important is to point out Waterfeld sʼ  understanding of this model of triangular desire as a valid pattern for interactions  in social networks. Waterfeld sees self-expression on Facebook as a mimetic process  in the sense of Girard: the actual user does not desire some object itself, but the  reactions shown on one sʼ timeline and therefore a desire for something somebody  else wants or likes. Following this reasoning, the profile or the account in social net- works is an expression of the Other, because it represents an ideal not of one sʼ self  but of an image, that would most likely be ‘liked’ and commented. What Waterfeld  de scribes by updating Girard is an imitation of an imagination or, well-known since  the emergence of psychoanalysis, an image of the Other. Although Waterfeld trans- fers the triangular model of Girard and notes rightly that there is no “dislike-button”  (2012: 234) she avoids naming the components in this relation ship. That is maybe  because every component is exchangeable with the others. But moreover, she ignores  that a profile is a representation of a user and not the actual one. Instead, I would  like to understand such profiles as an imitation of a type of self-approval. The point  is, that the user does not follow real references but virtual idols or (role) models that  embody what is ‘liked’ and therefore desirable. In this sense the triangular model  consists of ‘likes’ or attention (object), the (distorted) self-expression in the profile  (mediator) and the user (subject). The user mimics an image in his profile, a desirable  ideal he understands as self-expression, that should be solely ‘likeable’. He therefore  has no genuine desire for ‘likes’; his urge is only based on his understanding of the  popularity of other users who are successful in the system of social networks. This  accompanies virality, or the phenomenon of memes, and this is what Girard calls,  with reference to Gregory Bateson, a “double bind”, because the primary impulse  of imitation to get an object of desire is necessarily reciprocal. Girard understands  that this an instinctive threat to is created by being imitated, so a rivalry between the  </page> <page sequence="7"> Desiring Fakes 205 subject and the “mediator” occurs, for which reason “mimetic desire is simply a term  more comprehensive than violence” (2005: 156-58). In the case of fake accounts, the  object of desire is the alleged news, the, in fact, fake news, that triggers sensational- ism. The fake account becomes the “mediator”, while the subject, that imitates and  copies this “mediator”, is something like the news media or profile that shares the  false information. But this relation only works if one is beware of the triangular  relation, it is a mimesis of mimetic desire. The fake (profile) imitates, but does not  be come, the object of desire with the intent that the other user’s profiles imitate  this fake profile. The object here is to gain attention, ‘likes’ in the context of social  networks. So by imitation I mean, more accurately, the pre­mitation, because the  creator of a fake account anticipates and counterfeits the desire of the Other; he pre- sumes how his audience will react if he triggers their desire.7 Aware of the “double  bind”, the profile deals with this by counterfeiting the “mediators’” qualities, thus  satisfying their desires or expectations. Fake profiles, as showcased by Repp, are not a rarity. These take not only in the  form of false profiles managed by real people to remain anonymous, but also in the  form of chat bots. Such bots may be helpful, just like the assistant AI I have men- tioned, but they can also increase the number of followers of real profiles, increas- ing the popularity level, and they can thus mislead a user to interact with an only  al legedly real person, as happens, for example, on some dating websites. Such bots only work in a calculable system. They themselves cannot make  any decisions, so they must be programmed into desiderates. Following a proce-  dure — and that is the purpose of robots — they can then carry out an action in  relative autonomy. Such an autonomous action which is only possible in a certain,  defined framework, was the topic of the artist group !Mediengruppe Bitnik and  their project Random Darknet Shopper (RDS) at the Kunst Halle St. Gallen in 2014  7 | This triangular model can similarly be found in the Internet practice of  ‘trolling’, because here the ‘troll’ tries to trigger a response that is itself  worse than his original insult. He hides behind a fake identity and aims to  involve a third party by staging this argument for an audience. Therefore  ‘trolling’ is more about faking or imitating identities, as Judith Donath  states: “Trolling is a game about identity deception, albeit one that is  played without the consent of most of the players. The troll attempts  to pass as a legitimate participant, sharing the groupʼs common inter- ests and concerns; the newsgroups members, if they are cognizant of  trolls and other identity deceptions, attempt to both distinguish real from  trolling post ings, and upon judging a poster a troll, make the offending  poster leave the group. Their success at the former depends on how well  they — and the troll — understand identity cues; their success at the lat- ter depends on whether the trollʼs enjoyment is sufficiently diminished or  outweighed by the costs imposed by the group” (1999: 45). </page> <page sequence="8"> 206 Daniel Becker (fig. 2). The centre of this work was a bot, which had a weekly amount of $ 100 in  Bitcoins to buy goods and deliver them to the exhibition. The bot does not shop in  any online shop, but in the so-called ‘agora’, which is offline by now, in the darknet,  a marketplace similar to the well-known ‘silk road.’ The darknet is an overlay net- work, it uses the Internet infrastructure, but without public access. To become a part  of this network, one must be invited, but subsequently a high level of anonymity is  guaranteed, especially at these darknet-markets. Basic for the RDS is to experi- ment, to explore, and to document how such a relationship works, when it is  based only on information and quasi confidence in a system. Week for week the  Figure 2: !Mediengruppe Bitnik, “Random Darknet Shopper”, 2014 / 15,  installation shots at Kunst Halle St. Gallen, Switzerland. </page> <page sequence="9"> Desiring Fakes 207 bot bought items such as counterfeit sneakers or jeans, high-quality passport scans,  a copy of a UK Fire Brigade Master Key Set or drugs like ecstasy. At the end the  Swiss police confiscated this ‘evidence’ of this artwork, but, interestingly, without  charging the human artists. An aesthetic dimension of the RDS is its title-giving contingency. With this  incalculability, the work stands in the tradition of Digital Art, because the “seren- dipity” (Cybernetic Serendipity 1968) or the “aesthetic gap” (Becker 2017: 172) is  a fundamental and genuine characteristic of this art form. For the RDS, this gap  is its relative autonomy. At the same time, however, the shopper works only by  the command-execute-demand-structure of the darknet shopping platform, the ran- domness is therefore given in the selection of the products and thus only the bone of  contention. The communication and trafficking between the bot and the traders was  ‘successful’ in two ways: Firstly, this scheme realised the artistsʼ intention to get  such scandalising items and therefore attention, otherwise this performance could  have taken place on eBay or any other shopping-platform; secondly, it is exclusively  based on a rational system of ratings. Given the special community of the darknet,  the sellers are as interested in a redundant but working identity like a rating as is  the bot, which uses these ratings for judging and deciding for from whom to buy.  As on Amazon, the credibility of a seller is decided by his ratings. The RDS is  therefore a type of ‘programmed scandal’, which, crucially, is based only on digital  information, on the exchange between bot and sellers, whether they are controlled  manually or programmed. The success of the communication between the RDS and the human sellers  depends on the expectations of the sellers. They do not expect anything except  payment for their goods. As long as this adheres to market mechanisms, or “market  needs” (Maak 2011), everything else does not matter. On the other hand, there are  also procedures to prevent such communication. Websites try to protect themselves  from such artificial users by using so-called CAPTCHAs (Completely Automated  Public Turing test to tell Computers and Humans Apart), which are installed before  the content can be accessed. The idea here is that a bot cannot easily solve visual  tasks that a human user can, because a computer programme cannot recognise that  these graphics include letters and characters and cannot serve the required input.  Of course, this remains a constant race: When bots solve the CAPTCHAs, these  must in turn be improved. But as in the work of the South Korean artistsʼ group  Shinseungback Kimyonghun, the principle can also be reversed in order to exclude  people: a so-called FADTCHA (Face Detection Turing test to tell Computers and  Humans Apart) (fig. 3). Face detection is based on an algorithm of the open source  library “OpenCV”. The computer detects faces in its camera vision and marks them  with a red square. The actual work, however, is a book with nine round, colour  patches, which act on the human eye like a diffuse collection of monotonous circles.  In this collection, the computer recognises a face, but the human eye does not. For  a dichotomous categorisation — true or false — here only the system-immanent, i.e.  </page> <page sequence="10"> 208 Daniel Becker programmed, factors matter. So in the case of FADTCHA as well as of the RDS the  actual object — purchased object or image — plays no semantic role, because their  judgment is only based on the calculated work steps. But are they forgeries or do  they deal both with fake identities in a proper sense? From the perspective of the  seller, the RDS is a false identity, because it orders using the name of and to the real  address of a legal person, i.e. the Kunsthalle St. Gallen. From the point of view of a  human being the images in FADTCHA are false faces, because they do not concur  with our image of faces. Forgeries are therefore not false facts but false, created  situations. In the digital age, forgeries rather fake a construct of identity, object, and  reception, of artist, work, and expectation; they create a situation in which an object  becomes adequate, they fake a triangular relation of desire. Figure 3: Shinseungback Kimyonghun, FADTCHA, computer sees the face in the  test image of the book and the human user, 2013. </page> <page sequence="11"> Desiring Fakes 209 the IMItatIon gaMe This triangular relation is central for AI research, because what is important is not  the form of the AI, but its deception of being humanoid. The fundamental issue of  AI sʼ interplay and autonomy in this relation marks the beginning of AI research,  and leads the British computer scientist Alan Turing, to open his famous essay  “Computing Machinery and Intelligence” with: “I propose to consider the question,  ‘Can machines think?’” (1950: 433). But Turing himself relativises this approach  by replacing it with the question of whether machines can be realised as thinking  humans. He illustrates this, the later so-called ‘Turing Test’ to which CAPTCHAs  refer, in a mind experiment which he calls the “Imitation Game”. In this respect,  Turing was not concerned with the extent to which machines or computers can  think in any form, but how far — and this shows the behaviouristic approach of his  thinking — they can behave as if they were thinking beings (435, 438). This game consists of three elements: a machine or computer, a person and  separate from these two an interrogator, who ideally communicates only via tele- communication with the other participants. The task of the interrogator is to distin- guish the two others from each other; the task of the machine and of the human is to  answer the questions so that they are perceived in each case as a human being (434).  One must keep in mind that in 1950, when Turing described this game, the available  skills and range of computing were very limited, apart from the fact that digital  computers were not beyond an initial phase of development. Nevertheless, Turing  already speaks of machines or computers that could imitate humans as “human  computers” or, in today sʼ words, as robots (438). With the increasing development  of digital computers that can store and process an unimaginable amount of infor- mation, Turing was visionary in his foresight that it is just a matter of programming  and commands that enable machines to ‘mimic’ human behaviour (438). Never- theless, it was not his intention to equalise people and computers or to put them on  some ontological level, he wanted to point out and raise awareness of the potential  of these machines. One has to understand Turing sʼ reflections on the “imitation game” in the  context of his article “On Computable Numbers, with an Application to the Ent- scheidungsproblem”, written several years earlier (1937). Here, Turing describes  his solution to the Entscheidungsproblem (“decision-problem”) according to Da- vid Hilbert, namely, that it is undecidable for each possible mathematical formula  whether it is provable or not. This Entscheidungsproblem cannot be transferred  directly to the question of whether something is actually a forgery, since Turing  was primarily concerned with mathematical and formal problems, not with se- mantic ones. It is, however, important for understanding the ‘Imitation Game’,  because here Turing has already substituted the vague concept of predictability  with being computable by a machine: “According to my definition, a number is  computable if its decimal can be written down by a machine” (116). In this sense  </page> <page sequence="12"> 210 Daniel Becker the Turing machine is a universal machine, a simulation machine, since its opera- tions can be described as “‘rule of thumb’” or “‘purely mechanical’” (1948: 4); all  it does depends only on the information on a tape. Before mentioning machines, Turing describes the “Imitation Game” in a dif- ferent constellation namely: “a man (A), a woman (B), and an interrogator (C) who  may be of either sex” (1950: 433). Assumed to be “B”, to convince the interrogator  of one sʼ sex Turing suggests that “the best strategy for her is probably to give truth- ful answers. She can add such things as ‘I am the woman, donʼt listen to him!’ to  her answer, but it will avail nothing as the man can make similar remarks” (434). To  cause an incorrect identification with this strategy, one has to mimic the other sex.  Despite whether this really is the best strategy, Turing sʼ mind experiment is very  similar to Girard sʼ model: both assume a triangular constellation and both suppose  that one has to imitate or mimic their rival to succeed. With regard to this ‘foreplay’ of the “Imitation Game” it is also interesting that  a successful imitation in reverse means that the original (person) cannot present  itself as such. Juliane Rebentisch understands this part of the “Imitation Game” as  a gender construction, with the male imitating the female. Here, Rebentisch makes  a reference to Judith Butler: the sexual construction by Turing is based, like social  interaction in general, on normative rules (Rebentisch 1997: 28). Actually, Turing sʼ  idea postulates an original which will be imitated by a machine. But as soon as  he transforms this assumption into a game situation the concept of originality is  necessarily questioned, because in this framework the original appears as an imi- tation of an unattainable ideal, induced by cultural, social, institutional and political  practices (29). This raises the issue of whether imitation is not a question of the  reference itself, but a means of navigating a system. Turing, similarly to Rebentisch and Butler, also presupposes social norms:  “The book of rules which we have described our human computer as using is  of course a convenient fiction. Actual human computers really remember what  they have got to do. If one wants to make a machine mimic the behaviour of  the human computer in some complex operation one has to ask him how it is  done, and then translate the answer into the form of an instruction table” (Tu- ring 1950: 438). As in his article about the Entscheidungsproblem, Turing defi- nes the problem of calculability as mechanical. In this regard, he presumes two  things without mentioning: enough information can purport or simulate a com- mon-sense knowledge and there must be some kind of benevolent interrogator  or observer. Here Turing follows a mathematical-information-theoretical logic:  We know the information that is transmitted, the receiver is defined normatively,  so the sender (the imitator) results as a variable which can either be successfully  deceived or not. In other words, if one has an interrogator who knows how the  programme works, asks the right questions, for example logical contradictions or  detects that the computer reacts in unclear situations with counter-questions, then  the “Imitation Game” does not work. </page> <page sequence="13"> Desiring Fakes 211 The (human) reaction based on feelings, emotions or instinct in unforeseen  situations is a well-known argument against AI, because calculation means that  there is no room for consciousness. Turing himself mentions this argument but  rejects it, because in “this view the only way by which one could be sure that a  machine [as well as a man] thinks is to be the machine [or the man] and to feel  oneself thinking” (445). So, as in any conversation, the success of the communi- cation is based on how the codes, knowledge, or expectation of the participants  concur. This applies to both human and artificial counterparts. Through Turing sʼ work, one realises that computers are no longer just pure  computing machines, but symbol-processing machines. Though he asks the pro- vocative question “Can machines think?” in his essay, he is not concerned with  the intention of proving that machines can be intelligent, but how they can be  perceived as intelligent. However, this ontological question of the autonomy of AI  can be understood within the tradition of the philosophical ‘body-soul problem’  and plays a strong role in contemporary discussions of AI. The RDS also raises  the question of who takes responsibility for its (illegal) actions, and consequently  AI researchers warn of the consequences in regard to the progress of AI sʼ auto- nomy.8 Turing, however, defines intellect in a purely linguistic, information-technical  sense. This way, he can dissociate his concept of intelligence from a material and  physical body:  The new problem has the advantage of drawing a fairly sharp line be- tween the physical and the intellectual capacities of a man. No engineer  or chemist claims to be able to produce a material which is indistinguish- able from the human skin. It is possible that at some time this might be  done, but even supposing this invention available we should feel there  was little point in trying to make a ‘thinking machine’ more human by  dressing it up in such artificial flesh. (434) In this detachment from the physical, which is supposed to strengthen the argu- ment of machine intelligence, there is, however, still a recognition of the phy- sical. For the “Imitation Game” “the ideal arrangement is to have a teleprinter  communicating between the two rooms”, because any physical perception would  immediately make the imitation impossible, for qualities such as the sound of  a voice are rooted too strongly in the human perception apparatus (434). The  telecommunicative situation and the obscuring of physical conditions support  the indistinguishability in Turing sʼ experiment, because bodily features are so  compelling. But exactly because they are so compelling, the imitation of these  8 | See https://futureoflife.org/open-letter-autonomous-weapons/ (last  accessed on 31 May 2017). </page> <page sequence="14"> 212 Daniel Becker Figure 4: Joseph Weizenbaum acts out “Eliza” at a computer with printing  output, photograph, 1966. characteristics can support the deception. Turing, owing to the technical condi- tions of his time, ignores that, but today a machine, i.e. a computer, that imitates  such bodily features can forge an identity and even belie its deficit in (artificial)  behaviour. aVatarS One of the first programs that can be seen as the implementation of Turing sʼ “Imi- tation Game”, the Turing test, and that is still a milestone in AI research, is Joseph  Weizenbaum sʼ ELIZA (fig. 4). This language analysis programme consisted of two  parts, the language analyser and the script composed by a set of rules. This could  include rules for a conversation about cooking, insurance, banking, etc., depending  on which conversation was intended by the programmer. For the first experiment,  Weizenbaum used a therapy session whose script is based on the “Rogerian psycho- therapy” and is known under the name DOCTOR (1976: 3-4).9 Weizenbaum himself saw the overwhelming response to his programme cri- tically. In fact, he was surprised that a machine which used a regular procedure  9 | “Rogerian psychotherapy” or “person-centred therapy” is a form of  talking therapy. It is characteristic of this form of therapy that the client  is focused on and the therapist avoids intervention as much as possible.  It tends to let the client reflect and become aware of his own emotions  and cognition. </page> <page sequence="15"> Desiring Fakes 213 was seen by laymen as well as by experts as an equivalent to human intelligence  (5-8). Basically, its utilisation of the regular communication situations of “Roge- rian psychotherapy”, which were highly structured, made calculated behaviour by  the computer possible. A situation, in other words, for which one usually accepts  that it follows clear rules, is less associative, and allows only a small range of be- haviour. For example, the programme responded to the statement “Perhaps I could  learn to get along with my mother” with “Tell me more about your family” (4, see  also 189). The supposed semantic component is based on a simple classification  by means of a thesaurus. Therefore the script is based on lexical database. The  programme itself, however, provides a mere syntax, the actual semantics originate  from the users, because DOCTOR does not provide any information (Weizen- baum 1966: 42). It simulates a dialogue by means of contentless counter questions,  which are based on the — in this case lexical — user sʼ expectations. Therefore,  Weizenbaum also writes: “It is important to note that this assumption is one made  by the speaker” (42). As Claude Shannon, founder of information Theory, describes, the content of  information is dependent on the recipient (Weizenbaum 1976: 209). What Weizen- baum after Shannon hereby actually means is that the same information can be  understood differently in different contexts. Therefore, in the example of ELIZA, it  is remarkable how much autonomy and identity can be seen in simple answers and  counter-questions which in reality do nothing more than reassure the questioner.  The communication situation of a therapy discussion, in which the role of the thera- pist actually denies a personal relationship, is surely conducive. However, this could  be transferred to all sorts of professionals, since a certain degree of professionalism  always prevails over personal interests. As I argued, the identity of the therapist in  ELIZA is ultimately based on a database in form of a thesaurus. Even in the early  days of (criminal) identification, analogue databases of photographies or Bertillona- ges were important (Vec 2006: 185-86). Such discussions on data retention, data en- cryption and data monitoring are still current. And when the artists KairUs (Linda  Kronman and Andreas Zingerle) evaluated hard drives they found at an African  dump, in their work Forensic Fantasies Trilogy (2016), creating in the third part of  this work anonymous but also intimate and personal photo albums from the found  pictures (fig. 5), they showed that the relationship between data and identity, today,  is even more basic.  Because AIs are based on neural networks, they only learn on the basis  of their accessible data. Therefore, they reproduce systemic stereotypes in fa- cial recogni tion if they occasionally classify faces of Asians as having ‘closed  eyes’ because they were trained with Caucasian models.10 On the other hand,  10 | The accuracy of face recognition software depends on its training  parameters. In this way, these programmes can reproduce mistakes  which are caused by its programmers, in this case, because they are  </page> <page sequence="16"> 214 Daniel Becker Figure 5: KairUs (Linda Kronman and Andreas Zingerle), “Not a Blackmail”,   Part one of “Forensic Fantasies Trilogy”, 2016, installation shot at Ars   Electronica 2016. this facial recogni tion would not work so well in Europe if other parameters  were broader. Accord ingly, databases are designed with regard to their crea- tor’s claim.11 Weizenbaum sʼ ELIZA was a primitive forerunner of today sʼ common chat- bots, whose database structures are much more complex. Even though today sʼ  chatbots are at least equipped with a profile image, Weizenbaum, like Turing,  ignores the visual dimension in his programme. This is mainly due to the fact that  early AI research focused on the production of natural language (Weizenbaum  1966; 1976: 182-201). Therefore, he also named his programme after the character  only fed with one biometrical data. This led to unintentional racist cate- gorisations by the AI. The biometric identification by AI is therefore different to a general phy- siognomic or the FACS (Facial Action Coding System), because first of all  it develops parameter to recognise a face and not produces categories  to analyse it. 11 | For an overview of databases in art see Deep storage 1998. </page> <page sequence="17"> Desiring Fakes 215 “Eliza Doolittle” in George Bernard Shaw sʼ play Pygmalion. It is interesting to  mention this point because of two aspects — apart from the clear reference to a  female muse and divine creator in Pygmalion: First, although this character learns  to speak more eloquently, Eliza Doolittle arguably does not become more intelli- gent, and still uses inappropriate language. Second, the play focuses on linguistic  imitation of other people. With regard to the false therapist in ELIZA, one has to differentiate between  two aspects of forgeries. One, which is linguistic, plays a form of the “Imitation  Game”. Here, imitating is indeed deceiving, but not deceiving in the technical  sense. In this respect, forgeries work only if they are reduced to pure informa- tion. The second strategy of forgeries function upon whether a form of desire is  awakened by the forgery, which obscures the technical character. Such a form  is an ‘avatar’, which emerges in an artificial world instead of the protagonist to  imitate and in the end, to substitute for them. The concept of the avatar is closely related to control elements that connect  the user with the software. However, two restrictions can be made so that not  every cursor or status bar can be seen as an avatar: an avatar must first have a  certain bonding and continuity in the virtual world, otherwise a button could  also be considered as an avatar. Secondly, it must have a certain degree of an- thropomorphic features, so that it has a potential for identification. Accordingly,  there is always a degree of visuality in the concept of the avatar. In game studies  the aspect of the avatar-player-binding and thus the function of control elements  is emphasised. To use the avatar in this context goes a step further. Instead  of analysing the representation and the perspectivation and other immersive  elements of the avatar I would like to focus on the consequences of this bond.  Assuming the avatar is an immersive representation of a user, others (human)  users have to interact with this unknown player like a real counterpart. The  concept of the avatar appears here to be appropriate, because of the unspoken  understanding that an avatar is a representation of an actual user. Its artificial  elements substitute for a real person. In the case of ELIZA for example, this  would be the protocolary language. In general, these are mostly visual elements  which in the form of anthropomorphic elements, like profile pictures, simulate  that a real user is behind this avatar. Even chatbots usually provide profile pic- tures in order to be taken as a real person by an actual user sitting in front of  the computer. The visuality of a kind of mug shot is therefore to obscure their  actual identity, as Jean Baudrillard writes: “In the last analysis, robots are al- ways slaves. They may be endowed with any of the qualities that define human  sovereignty except one, and that is sex” (1996: 120). This not only points out  the distinction between man and machine, but vice versa, also suggests that by  gendering the machine, the sovereignty of the human individual would become  brittle. This is an interesting parallel between the representation of the machine  and the art-historical concept of personification where the gendering of abstract  </page> <page sequence="18"> 216 Daniel Becker concepts also has an intentional function.12 The gender-specific representation  of the avatar thus allows an alleged conclusion about the actual user, insofar  that a user represents his “true” identity through their avatar. But this is not a  deficit of the machine, as Baudrillard writes, on the contrary, it is a potential,  because without sex it can better construct any sex and satisfy any sexual desire.  In context of this gender construction, Judith Butler writes more generally that  “gender is a kind of imitation for which there is no original” (1991: 21). What  Butler understands specifically in relation to the performance of gender identity,  in Baudrillard sʼ observation acquires a completely new dimension. By separa- ting gender and body, the machine gains sovereignty, because its embodiment is  exchangeable and can adopt and occupy every form. Therefore, just as one can  perform their gender, a personification has a gender role, AI can also adopt a  role according to its (programmed) aims. “There is no original”, a central factor  in cases of forged identities or identity theft — and that is nothing less than what  Turing describes in his “Imitation Game” — that the desire of the human is the  key to a successful imitation. Therefore he emphasises the role of sexual appeal  (Hodges 1994: 620). 12 | In general, one can observe an anthropomorphisation with respect  to a gendering of AIʼs humanlike qualities, for instance, with the use of  mostly female voices. In literary or cinematic works the anthropomorphi- sation of AI follows basic gender roles. Characters like Hadaly (Auguste  Villiers de lʼIsle-Adam: Lʼ Ève future, 1886), Samantha (Spike Jonze: Her,  2013) or Maria (Fritz Lang: Metropolis, 1927) are female representations  of an ideal. Characters like HAL (Stanley Kubrick: 2001. A Space Odys- sey, 1968) or Terminator T-800 (James Cameron: Terminator, 1984), in  contrast, are male representations of threat and destruction. Silke Wenk  writes about art-historical personifications: “The female allegories repre- sent the opposite of the feminine; they represent not the women, but the  sovereignty, which even the ‘great men’ lack and point beyond them.  The male-patriarchal order demands more from the men than what they  are and do. There has to be another image for the cohesion of order,  especially of the ‘nation’, which is ‘invented’ as a political community of  equals (of ‘brothers’). Male images are not suited to represent the ima- ginary community, through which the state can be analysed through a  bourgeois society — as a community beyond the debate about particu- lar interests, through which the national state constitutes itself” (Wenk  1996: 101, my translation). In this regard, the anthropomorphisation of AI  is similar to the personification, because it uses the same methods when  it comes to in gendering.  For an overview of anthropomorphic machines in literature and film, see  also Bukatman 1993. </page> <page sequence="19"> Desiring Fakes 217 Turing sʼ approach is a semiotic and not a visual one, but to be clear, the point  is that it is not the machine which becomes more human-like — although it can be  perceived as such on a visual level — but the human becomes more machine-like,  or, as Harry M. Collins states, “Wherever we choose to mimic a thing, a thing can  mimic us” (1990: 216). This human follows a command-structure while they are  blinded by the visual elements of an avatar, an object. A successful deception there- fore depends not on the forgery itself, but on a gamesmanship, a narrative that causes  credulity by the user, so “just when humans engage in behaviour-specific acts they  can be mimicked by machines” (41) or forgeries, because then they are predictable. Thus, spam or clickbaits use sexual content to attract the user. In the early time  of the Internet the net artist Alexej Shulgin launched the project FuckU­fuckme  (fufme.com, offline, 1999) to discuss the new possibilities of cybersex. This web site,  which offered “dildonics” (Rheingold 1991: 345-77) for each sex, received a wide  audience. In fact, it was a fake; the offered sex toys never existed and were only  illustrations. But this example shows that desires, imitated or assumed, especial ly  when they are sexual, can get an attention that ignores, overlooks or disregards the  real state of an (artificial) framework. fakeD IDentItIeS Before Computer (b.C.)   anD afTer DigiTal (a.D.) In her film Teknolust (2002) the artist Lynn Hershman Leeson explicitly dis- cusses the relationship between sexual desire and AI. I conclude this text by  focusing on her, because she deals with the relationship of false identity, desire,  and technology in her entire work from the early 1970s onwards — “a panoply  of identities” (Weibel 2016: 44) — and has adapted herself over and over again to  changing conditions.13 The headstone in this context is her creation Roberta Breitmore (1973). In this  nearly five-year performance, Hershman Leeson lived under a fictional and fake  identity as Roberta Breitmore. She documented this performance with false — not  forged — documents, which were made in the name of Roberta Breitmore, like an  apartment contract, a bank account, a credit card, a driverʼs license, and even a  notebook about meetings with psychologists. Similar to the work Forensic Fanta­ sies, mentioned above, the documental artefacts play an essential role for the false  identity (Weibel 2016: 45-52). Unlike a double life or a real fake, where persons  take over another identity to protect themselves or to act out themselves, Roberta  Breitmore was more of an artistic experiment. Therefore Hershman Leeson was  interested in observing the construction of identity and desire of voyeuristic looks  13 | For an overview of the œuvre of Hershmann Leeson see Civic Radar  2016. </page> <page sequence="20"> 218 Daniel Becker Figure 6: Lynn Hershman Leeson, “Lorna”, 1984, interactive installation,  installation shot at Hansen Fuller Goldeen Gallery, San Fransisco (left) and  screenshot (right). above all; she even emphasised the effect of this role-playing-game for other real  people: “Even with four different characters assuming her identity, the patterns  of her interactions remained constant and negative. After zipping themselves into  Robertaʼs clothing, each multiple began to also have Roberta-like experiences”  (Hershman Leeson 1994: 4). Although Roberta Breitmore was created simultaneously to the discourse  around AI exemplified in other artworks of the period, such as those by Lynda  Benglis, Valie Export, Cindy Sherman or Martha Rosler, which dealt with the  problems of gender and identity, there is no direct connection between these two  dimensions.14 However, for Hershman Leesonʼs work the turning point of the era  “Before Computers” (B.C.) to “After Digital” (A.D.) (1994: 3) is marked by the in- teractive work of Lorna (1979-84): Here, a video disc is used as an artistic medium  for the first time (fig. 6). Lorna deals with the story of a lonely girl in a room, who  only communicates via TV and telephone with the outside world. In this mixed  media installation the user sits in a copy of Lornaʼs room and can follow her life via  the monitor in a hypertextual narrative. Based on Lorna, in 1984 Hershman Leeson  developed the work Deep Contact which attracts the attention of passing visitors  by a motion sensor. A woman in a mini-skirt on a red couch invites them to inter- act and to touch one of her body parts on the touchscreen. Both works allow the  interaction with the virtual character: One can watch Lorna taking a bath or follow  her to a date at a motel, or see the sexual and voyeuristic fantasy of Marion in Deep  Contact, and follow her into a secret garden. These works are actually not forged identities or identity theft nor frauds, they  do not refer to a real existing person. Lorna, for example, works — like many sub- sequent digital artworks — with the strategy of hypertext to convey a feeling by  14 | Accordingly, as with AI research, Peter Weibel points out the linguis- tic dimension of Hershman Leesonʼs work and therefore the reference as  a central category (2016: 48). </page> <page sequence="21"> Desiring Fakes 219 Figure 7: Lynn Hershman Leeson, “Agent Ruby”, 2002, screenshot. connecting to the user. But they show how affined digital and telematic artworks  are to questions about the construction of facts. They reflect role traces and docu- ments for the construction of faked identities and their authority in this process, as  Hershman Leeson also states: “The new technology will be extremely subversive of  all Forms of Traditional Authority — political, social, and religious. That is, when  one encourages active participation by individual citizens and worshipers in public  life, the standing of Authorities to issue commands is greatly retarded” (1985: 1). Teknolust is another turning point in her work, because here she focuses on the  role of cloning and bio-art. Yet, she combines this discourse with the dimension of  AI, because simultaneously to Teknolust, Hershman Leeson developed Agent Ruby  from 1998 to 2002; this is an online chatbot, which is similar to ELIZA (fig. 7). There  are about 35 years between Agent Ruby and ELIZA, so of course, Ruby is more elo- quent but it is based on the same concept, it is not pre-programmed and its reaction  depends on the questions of the interrogator. But in contrast to ELIZA it does not  imitate a person like a psychiatrist anymore, it is some kind of a new person, because  Ruby incorporated their identity as artificial intelligence into the chats. In conclusion I wanted to show, that such strategies of forging, counterfeiting, imi- tating or deceiving are deeply rooted in the electronic or digital arts, even if one  cannot speak of actual fakes in the works. Today, there are even more possibilities:  Computers are much faster than in Turingʼs times, countless amounts of informa- tion from networks and big data are easily accessible and machines are able to  learn. There was even a Roberta Breitmore avatar created by Hershman Leeson for  Second Life. But besides this, ELIZA, Agent Ruby or the RDS can work if they have  the correct work environment. To create that, they use strategies similar to forging  by being oriented towards the usersʼ expectations: they are narrative, immersive or  </page> <page sequence="22"> 220 Daniel Becker interactive as is necessary to seem credible, and therefore real. In order to convince  the users, they use a narrative that disguises their own deficits. Thus, the acquired  pieces of the RDS were exhibited and the identities in Hershman Leesonʼs work  were displayed through documents and pictures of her alter ego. That is why the  German forger Wolfgang Beltracchi also staged his forgeries in a supposedly histo- rical photograph: to suggest, argue and narrate their authenticity.15 Forgeries are the expression of a formal rationalisation of reception — which  resonates with the rationalisation of digital programmes like AI. Where gaps could  be filled, they could be filled with forgeries. Artists use these gaps productively  and reflect them critically, whilst forgeries just adapt themselves. The new media  artworks I have discussed disclose the schematics of the forging process, because  these machines in general and also AI follow a programmed command structure.  Forgeries, in the classical sense, on the other hand, aim to disguise this process.  Therefore, the description of something is a forgery, or not, as well as whether AI is  considered to be real or deceptive, always depends on the conditions and desires of  its reference system. Forgeries have no final state, or as Agent Ruby says when you  ask her several times “Are you a forgery?” — Am I a forgery? It is useful for me to have several identities user. Am I a forgery? Of course I am. Am I a forgery? Only when it matters. Am I a forgery? I don’t know.16 Eventually forgeries are ambivalent and are situated in a reciprocal framework of  themselves, the forger as well as the recipient, and depend on the desires seen in  them. So, maybe like Eliza, I would answer the question for Agent Ruby with: “If  you canʼt tell, does it matter?” WorkS CIteD Baudrillard, Jean (1996): The System of Objects, translated by James Benedict,  London / New York: Verso. Becker, Daniel (2017): “Water — Liquidity as Digital Form”, in: State of Flux:  Aesthetics of Fluid Materials, eds. Marcel Finke / Friedrich Weltzien, Berlin:  Reimer, pp. 157-73. Blumenberg, Hans (2000): “‘Imitation of Nature .ʼ Toward a Prehistory of the Idea  of the Creative Being”, translated by Anna Wertz, in: Qui Parle 12.1, pp. 17-54. 15 | On the case of Wolfgang Beltracchi see also the article of Henry  Keazor in this volume, especially page 36 16 | Interaction with Agent Ruby on 31 May 2017 at http://agentruby.sf- moma.org/ </page> <page sequence="23"> Desiring Fakes 221 Bukatman, Scott (1993): Terminal Identity. The Virtual Subject in Postmodern  Science Fiction, Durham / London: Duke University Press. Butler, Judith (1991): “Imitation and Gender Insubordination”, in: Inside/Out:  Lesbian Theories, Gay Theories, ed. Diana Fuss, London / New York: Rout- ledge, pp. 13-31. Civic Radar (2016): Lynn Hershman Leeson, exh. cat., ed. Peter Weibel, ZKM  Karlsruhe et al., Ostfildern: Hatje Cantz. Collins, Harry M. (1990): Artificial Experts. Social Knowledge and Intelligent  Machines, Cambridge, MA / London: MIT Press. Cybernetic Serendipity (1968): The Computer and the Arts, exh. cat., ed. Jasia  Reinhardt, Institute of Contemporary Arts London, London / New York: Studio  International. Deep Storage (1997): Arsenale der Erinnerung, exh. cat., eds. Ingrid Schaffner / Matthias Winzen, Haus der Kunst München et al., Munich: Prestel. Dennett, Daniel C. (1971): “Intentional Systems”, in: The Journal of Philosophy  68.4, pp. 87-106.  Donath, Judith S. (1999): “Identity and deception in the virtual community”, in:  Communities in Cyberspace, eds. Marc A. Smith / Peter Kollock, London / New  York: Routledge, pp. 29-59. European Media Art Festival (2014): We, the enemy — Leben unter Verdacht, exh.  cat., Osnabrück. Girard, René (1969): Deceit, Desire, and the Novel: Self and Other in Literary  Structure, translated by Yvonne Freccero, Baltimore: Hopkins Press. — (2005): Violence and the Sacred, translated by Patrick Gregory, London /  NewYork: Continuum. Grafton, Anthony (1990): Forgers and Critics. Creativity and Duplicity in Western  Scholarship, London: Collins &amp; Brown. Hodges, Andrew (1994): Alan Turing. Enigma, Vienna et al.: Springer. Rebentisch, Juliane (1997): “Sex, Crime &amp; Computers. Alan Turing und die Logik  der Imitation”, in: Ästhetik &amp; Kommunikation 26.96, pp. 27-30. Rheingold, Howard (1991): Virtual Reality, New York: Summit Books. Turing, Alan (1937): “On Computable Numbers, with an Application to the Ent- scheidungsproblem”, in: Proceedings of the London Mathematical Society  2.42, pp. 230-65. — (1950): “Computing Machinery and Intelligence”, in: Mind, A Quarterly Review  of Psychology and Philosophy 59.236, pp. 433-60. Vec, Miloš (2006): “Defraudistisches Fieber. Identität und Abbild der Person in der  Kriminalistik”, in: Fälschungen. Zu Autorschaft und Beweis in Wissenschaft  und Künsten, ed. Anne-Kathrin Reulecke, Frankfurt a.M.: Suhrkamp 2006,  pp. 180-215. </page> <page sequence="24"> 222 Daniel Becker Waterfeld, Sarah (2012): “Mimesis 2.0”, in: Mimesis (=Archiv für Mediengeschichte,  vol. 12), eds. Friedrich Balke / Bernhard Siegert / Joseph Vogl, München: Fink,  pp. 229-39. Weibel, Peter (2016): “The Work of Lynn Hershman Leeson. A Panoply of Identi- ties”, in: Civic Radar, pp. 44-55. Weizenbaum, Joseph (1966): “ELIZA — A Computer Program For the Study of  Natural Language Communication Between Man and Machine”, in: Communi­ cation of the ACM 1.9, pp. 36-45. — (1976): Computer Power and Human Reason. From Judgment to Calculation,  San Francisco: Freeman. Wenk, Silke (1996): Versteinerte Weiblichkeit. Allegorien in der Moderne, Co- logne / Weimar / Vienna: Böhlau. onlIne SourCeS Allcott, Hunt / Gentzkow, Matthew (2017): “Social Media and Fake News in the  2016 Election”, Journal of Economic Perspectives 31.2, https://web.stanford. edu/~gentzkow/research/fakenews.pdf (last accessed on 14 June 2017). Hershman Leeson, Lynn (1985): “Politics and Interactive Media (An excerpt)”,  http://www.lynnhershman.com/wp-content/uploads/2015/03/Politics-and-  Interactive-Media-Art-Excerpt-Lynn-Hershman.pdf, first published in Journal  of Contemporary Studies (last accessed on 31 May 2017). — (1994): “Romancing the Anti-body: Lust and Longing in (Cyber)space”, http:// www.lynnhershman.com/wp-content/uploads/2016/06/Romancing-the-  Anti-Body.pdf (last accessed on 31 May 2017). Maak, Niklas (2011): “Kunstfälscher-Prozess. Das Schicksal korrigieren”, Frank­ furter Allgemeine Zeitung, Oct. 27, http://www.faz.net/aktuell/kunstfael- scher-prozess-das-schicksal-korrigieren-11508538-p4.html?printPagedAr- ticle=true#pageIndex_4 (last accessed on 31 May 2017). Turing, Alan (1948): “Intelligent Machinery. National Physical Laboratory  Report”, http://www.alanturing.net/turing_archive/archive/l/l32/L32-001.html  (last accessed on 31 May 2017). </page> </plain_text> 