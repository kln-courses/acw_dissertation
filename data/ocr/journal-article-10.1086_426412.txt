<body xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:mml="http://www.w3.org/1998/Math/MathML"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
      <p>Thanks to theoretical advances in the natural sciences and the decreased cost of computer technology, computational modeling is becoming an increasingly popular tool in the social sciences. Due to its relative novelty and somewhat marginal position in most disciplines, however, research of this kind has primarily focused on methodological challenges posed by applications to social phenomena. By contrast, the method’s theoretical foundations are still relatively poorly understood and many theoretical possibilities remain unexplored by computational scholars. At the same time, social theorists, following in the footsteps of Georg Simmel’s pioneering contributions a century ago, have developed a process‐based research tradition that anticipates the scientific practices of today’s computer‐based research. In short, if the sociological process theorists have been computational modelers <italic>avant la lettre</italic>, the latter can be seen as process theorists “<italic>après la lettre</italic>.”</p>
      <p>In view of this apparent conceptual convergence, it would seem useful to bring the two intellectual traditions together. Agreeing with recent efforts to bridge the two fields (e.g., Hanneman, Collins, and Mordt 1995; Sawyer 1998, 2003; Müller, Malsch, and Schulz‐Schaeffer 1998; Sallach 2000; Macy and Willer 2002), I argue that both stand to benefit from such cross‐fertilization. Scientists relying on computational techniques would gain a better understanding of what they are actually doing by anchoring their research more firmly in social theory, both in terms of epistemology and ontology. This research strategy would enable them to respond more effectively to criticism that assumes alternative philosophical positions. Even more important, such a widening of the ontological spectrum would help them recognize opportunities that a focus on a more narrow, method‐driven agenda may obscure.</p>
      <p>Conversely, I assert that such an intellectual bridge‐building exercise could assist process theorists working in the Simmelian mold as well. Customarily relying on rather loose metaphors and analogies, these theorists have struggled to grasp the consequences of their uncompromising assumptions. Although some of them have devised simple models, their analysis has generally remained qualitative and intuitive. Due to their compatibility with the basic principles of the process perspective, computer‐based techniques provide unique conceptual resources to capture the evolution of complex social forms in time and space.</p>
      <p>In this article, I limit the scope to macrotheoretic problems. It is in this domain that formal modeling is the most needed, but also where its application causes the most severe difficulties due to the overwhelming complexity of social systems (Hanneman et al. 1995; Cederman 1997). Computer‐based research promises to overcome some of the conceptual hurdles that have provoked a powerful trend away from systemic thinking to microlevel theorizing, especially in the rational‐choice tradition (Collins 1999).</p>
      <p>I proceed by highlighting the main epistemological and ontological principles characterizing sociological process theory before turning to the corresponding dimensions in contemporary computational research. Then, a section illustrating different types of social forms in agent‐based modeling follows. A concluding section assesses what each literature has to offer and what theoretical gaps remain to be filled.</p>
      <sec id="sc1">
         <title>Sociological Process Theory</title>
         <p>Reacting to positivist currents in the late 19th century, Georg Simmel pioneered a distinctive tradition of process‐driven theorizing in sociology. Rather than assuming that social reality consists of fixed and given entities, whether individual or collective, Simmel conceived of it as an ongoing interactive process giving rise to social forms in a spatiotemporal continuum: “The large systems and the super‐individual organizations that customarily come to mind when we think of society, are nothing but immediate interactions that occur among men constantly every minute, but that have become crystallized as permanent fields, as autonomous phenomena” (Simmel quoted in Wolff 1950, p. 10).</p>
         <p>Stressing the emergent quality of such social forms, Simmel refers to them under the heading of sociation (“<italic>Vergesellschaftung</italic>,” see Wolff 1950, p. lxiii). Viewed in sociational terms, society is thus not a “‘substance,’ nothing concrete, but an <italic>event</italic>” (Simmel quoted in Wolff 1950, p. 11; original emphasis). Languages, social structures, norms, and conventions are created through “societal production, according to which all these phenomena emerge in interactions among men” (p. 13). According to Simmel, the primary goal of sociology is to analyze social forms: “Sociologists are directed to identify and classify different forms of social interaction: to analyze their subtypes; to study the conditions under which they emerge, develop, flourish, and dissolve; and to investigate their structural properties” (Levine 1971, pp. xxvii–xxviii).</p>
         <p>This “process worldview,” which parallels the strictures of Whitehead’s philosophy (Fararo 1989), has also given rise to a rich tradition of scholarship that explicitly, or sometimes implicitly, follows Simmel’s theoretical lead. In the United States, primarily thanks to the efforts of Albion Small, Robert Park, and George Herbert Mead, the Chicago school became an influential bastion of sociological process theory. Its main insight tells us that “every social fact is situated, surrounded by other contextual facts, and brought into being by a process relating it to past contexts” (Abbott 1997, p. 1152).</p>
         <p>Despite the influence of Simmel’s work and that of his American successors during the first decades of the 20th century, mainstream sociology went off in a very different direction, largely under the influence of Parsons’s relatively static theorizing (Levine 1991) and the trend favoring decontextualized quantitative research methods (Abbott 1997). However, the process perspective continued to thrive in other scholarly contexts. Outside the mainstream, for example, Norbert Elias (1978, 1982) developed his own somewhat idiosyncratic macrohistorical process theory based on the notion of “figurations.” In anthropology, Fredrik Barth (1981) stands out as one of the greatest process theorists. Like Simmel’s original research agenda, Barth’s theoretical approach accounts for social forms by showing how they are generated by social processes.</p>
         <p>More recently, social theorists have attempted to revive and synthesize these and related streams of thinking by promoting “relationism” (Emirbayer 1997), “structuration” (Giddens 1979), a “morphogenetic” perspective (Archer 1995), or the continued relevance of the Chicago school (Abbott 1997). It goes without saying that this is a somewhat sprawling literature with considerable internal tensions.2 Nevertheless, it is possible to discern a common metatheoretical pattern of inquiry: all these process approaches advance generative explanations of social forms conceptualized as configurations. To articulate these principles more clearly, let us now consider the epistemological notion of generative theory (How do we explain?) before turning to the ontological issue of social forms (What do we explain?).</p>
         <fn-group>
            <fn id="fn2">
               <label>2</label>
               <p>For example, Archer (1995) rejects Giddens’s “inseparability claim linking individuals with social structure. Likewise, there exist differences as to how radical these authors’ rendering of process ontology is” (see Sawyer 2002<italic>b</italic>). It should also be noted that symbolic interactionism in the Meadian tradition has come to focus mostly on microsociological problems, thus partly deviating from the macrofocus of this article.</p>
            </fn>
         </fn-group>
      </sec>
      <sec id="sc2">
         <title>From Nomothetic to Generative Epistemology</title>
         <p>Generative theory differs radically from the positivist focus on explanation in terms of laws and regularities. Most of today’s social science subscribes at least loosely to a Humean regularity notion of causation, requiring a constant conjunction of factors, typically across a large number of cases. Such a conception of what it means to explain has been reinforced by the wider use of statistics across the disciplines. For example, the political scientists King, Keohane, and Verba (1994) argue that such a logic of social inquiry applies regardless of whether the analysis proceeds qualitatively or quantitatively. These, and many other authors (e.g., Goldthorpe 1997), emphasize the search for, and explanation in terms of, causal regularities through comparative analysis or statistical control as the primary goal of social science. Within the philosophy of science, Hempel’s (1965) idea of covering laws provides the clearest and most sophisticated expression of this nomological epistemology.</p>
         <p>How then, can explanation be achieved, if not by reference to regularities? The sociological process approach starts with an observed social phenomenon, whether unique or ubiquitous, and then postulates a process constituted by the operation of mechanisms that together generate the phenomenon in question. Ultimately, explanatory value resides in the specification of (often unobservable) mechanisms and the reconstruction of a process within which they are embedded.3</p>
         <p>The process theorists’ preference for generative explanations stems from the difficulties of finding and isolating regularities in complex social systems. Although Simmel ([1890] 1989) was especially pessimistic on this point, he did not exclude the possibility of identifying ideal‐typical forms and mechanisms under a diverse set of conditions. Ultimately, however, the question of whether there are macrolevel regularities is not essential to the generative approach, because even in those cases where they can be said to exist, process theorists would regard them as insufficient and superficial substitutes for the deeper understanding yielded by a generative explanation. Simmel believed that it was impossible to account for social forms without describing how they are generated through a “genetic method.” Instead of resorting to divine intervention or individual will, such entities should be explained “by the notion of societal production, according to which all these phenomena emerge in interaction among men, or sometimes, indeed, <italic>are</italic> such interactions” (Wolff 1950, p. 13; original emphasis).</p>
         <p>Using various labels, such as “abductive” or “retroductive” inference or “inference to the best explanation,” philosophers since Peirce distinguish this pattern of inquiry from both induction and deduction (Lipton 1991). Intimately linked to scientific realism, this notion of causal reasoning has gradually gained terrain at the expense of Hempel’s nomothetic ideal. Examples abound in the natural sciences, especially in historically oriented fields such as geology and evolutionary biology (McMullin 1964). As we have seen, sociology is no exception from this trend. Although Simmel did not explicitly use these terms, his method has been characterized as “abductive” (Krähnke 1999).4 The same goes for much of social theory, including Giddens’s structuration theory (e.g., Wendt 1987, 1999).</p>
         <p>The construction of generative explanations based on abductive inference is an inherently theoretical endeavor (McMullin 1964). Instead of subsuming observations under laws, the main explanatory goal is to make a puzzling phenomenon less puzzling, something that inevitably requires the introduction of new knowledge through theoretical innovation. Simmel’s response to this challenge was to rely heavily on metaphors and analogies, thus shedding new and unexpected light on his subject of study (Simmel 1989; see also Krähnke 1999, p. 94). Likewise, Elias (1978) resorts to metaphoric constructs, such as dance and the use of personal pronouns, to clarify the meaning of his social figurations.</p>
         <p>Though they regard metaphors and analogies as integrated parts of scientific practice, realist philosophers of science also stress the pivotal role of models in the construction of generative theories. Like metaphors and analogies, models mediate knowledge from a known “source domain” to a “target domain,” but they do so in a more consistent and precise fashion, especially in complex settings (Harré 1970). The main function of models, then, derives from their capacity to trace and reconstruct causal processes in a coherent way.</p>
         <p>It thus comes as no surprise that many efforts to spell out a process‐theoretic vision often involves explicit model building. Fredrik Barth offers an especially clear justification of such a research program. In his view, generative models<disp-quote>
               <p>are not designed to be homologous with observed social regularities; instead they are designed so that they by specified operations, can <italic>generate</italic> such regularities or forms. They should be constituted of a limited number of clearly abstracted parts, the magnitude or constellation of which can be varied, so that one model can be made to produce a number of different forms. Thus by a series of logical operations, forms can be generated; these forms may be compared to empirical forms of social systems, and where there is correspondence in formal features between the two, the empirical form may then be characterized as a particular constellation of the variables in the model. (Barth 1981, p. 32)</p>
            </disp-quote>
         </p>
         <p>Note that Barth distinguishes generative explanations from the mere discovery of emergent social forms. Clearly, it is not sufficient to identify the associations. Rather, what is needed is a deeper explanatory reconstruction of how the social forms in question were generated: “Explanation is not achieved by a description of the patterns of regularity, no matter how meticulous and adequate, nor by replacing this description by other abstractions congruent with it, but by exhibiting what <italic>makes</italic> the pattern, i.e., certain processes. To study social forms, it is certainly necessary but hardly sufficient to be able to describe them. To give an explanation of social forms, it is sufficient to describe the processes that generate the form” (Barth 1981, pp. 35–36).</p>
         <p>Reasoning along similar lines, Fararo (1989) elaborates a general conceptual framework for generative theory. In agreement with other modern process theorists, Fararo subscribes to realist philosophy of science. To explain an empirical macrophenomenon, he argues, a social scientist should “construct a <italic>generating process</italic>” that produces it (p. 43). Processes are constituted by recursively iterated operations of generative mechanisms and rules, very much like a computer program. At the same time, Fararo’s realist approach to explanation raises the difficult question of how to model process:<disp-quote>
               <p>In the context of present‐day empirical research, a great deal of stress is placed on the specification of variables and “explaining” variation in one variable by appeal to variation in a battery of other variables. This conception of explanation is based on a legitimate <italic>aspect</italic> of the dynamic or process‐oriented frame of reference of general theoretical sociology. Namely … as the generative mechanism produces changes of state, it does so under parametric conditions. Variations in parametric conditions, either dynamically or comparatively, thereby produce variations in states <italic>via</italic> the generative mechanism. What is omitted in the usual accounts of “causal models” based on “explaining variation” is the explicit formal representation of process. (Fararo 1989, p. 42)</p>
            </disp-quote>This representational challenge makes it impossible to further postpone a discussion of ontology. As argued in the next section, sociological process theory differs from mainstream approaches in this respect as well.</p>
         <fn-group>
            <fn id="fn3">
               <label>3</label>
               <p>In its generative logic, the process‐theoretic approach to mechanisms is similar to that of some rationalistic analysts who emphasize mechanism‐based explanations (e.g., Hedström and Swedberg 1998). Note, however, that truly relationist process sociology rejects the reification of actors assumed by rational‐choice theory, as well as its tendency to regard mechanisms as the “poor cousin” of nomological explanations (see Abbott 1996).</p>
            </fn>
            <fn id="fn4">
               <label>4</label>
               <p>Some scholars argue that Simmel did not engage in causal theorizing at all (see Sylvan and Glassner 1985).</p>
            </fn>
         </fn-group>
      </sec>
      <sec id="sc3">
         <title>From Variable‐Based to Configurative Ontology</title>
         <p>True to its scientific realist principles, sociological process theory requires explanations to specify theoretical entities, relations, and mechanisms that together generate the social forms to be explained. Often these components have to be postulated because they are unobserved, or in some cases, even unobservable (Miller 1987). By contrast, positivist research tends to express causation in terms of variables that indicate properties of social entities, relations, and mechanisms without specifying these components directly. To the extent that social forms and their components are postulated at all, this is done primarily for instrumental reasons, without attributing actual existence to the theoretical building blocks. After all, prediction of actual outcomes is often more important than explanation in these nonrealist perspectives (see Friedman 1953).</p>
         <p>The difference between variable‐based ontologies and process‐theoretic alternatives can be traced back to Simmel’s general distinction between “form” and “content.” In his formal sociology, Simmel stressed the former at the expense of the latter (Levine 1971).5 By virtue of its ability to capture form, Simmelian sociology thus differs from conventional approaches that “do not describe social processes, but merely correlations between elements. Simmel studies the concrete sociations between persons and groups, not indicators of their outcomes (number killed, riot frequencies, etc.), and explains social structures as the synthesis of these interactions” (Sylvan and Glassner 1985, p. 45; see also p. 92).</p>
         <p>Despite the importance of social form in Simmel’s theory, however, it is hard to find a clear definition of this key concept in his work (Wolff 1950, p. xxxix). Levine defines social forms as “the synthesizing principles which select elements from the raw stuff of experience and shape them into determinate units” (Levine 1971, p. xv), but that definition seems too broad for the purposes of this discussion. On the other hand, Barth’s (1981, p. 32) equation of social forms with “series of regularities in a large body of individual items of behaviour” is too narrowly behaviorist. For the present purposes, then, it seems reasonable to define <italic>social forms</italic> as configurations of social interactions and actors that together constitute the structures in which they are embedded.6 Furthermore, <italic>configurative theories</italic>, as opposed to variable‐based ones, contain an explicit representation of configurations, whether exogenous or endogenous.</p>
         <p>From the assumptions of generative theory, it follows that all social forms are dynamically produced through social processes. Or to put it even more starkly: social forms <italic>are</italic> by definition social processes. The extent to which actors and structures vary over time depends on the particular case, however. Some processes generate configurations of behavior or attitudes without any change in their component entities or their interaction structures. Other processes are more complex in that they feature change in actors’ boundaries and their structural arrangement. Configurative theories that endogenize such phenomena can be characterized as <italic>sociational </italic>(<italic>Vergesellschaftungstheorien</italic>), to use Simmel’s terminology. The longer the time span, and the more profound the societal transformation, the more likely it is that sociational processes are operating. Considering primarily macroprocesses in the Simmelian tradition, I will focus on such complex configurative transformations of actors and structures.</p>
         <p>It is in these cases that the weaknesses of the variable‐based paradigm become most evident, for, as has already been stated, variables merely measure dimensions of social forms; they cannot represent the forms themselves except in very simple cases. Such variable‐based analysis “detaches elements (substances with variable attributes) from their spatiotemporal contexts analyzing them apart from their relations with other elements within fields of mutual determination and flux” (Emirbayer 1997, p. 288). By contrast, social forms always possess a <italic>duration</italic> in time and an <italic>extension</italic> in physical or some abstract, conceptual space. Thus they can be said to have their own “bodies” or “corporate identities” in a literal or figurative sense. The latter case applies to collections of individuals, such as social groups or organizations (Wendt 1999).</p>
         <p>Theories that “hardwire” their actors and structures into their ontologies can be characterized as “essentialist” or “substantialist.” In these cases, “analysis is to begin with these self‐subsistent entities, which come ‘preformed,’ and only then consider the dynamic flows in which they subsequently involve themselves” (Emirbayer 1997, p. 282). In contrast, sociational theories, which Emirbayer labels “relationist,” “reject the notion that one can posit discrete, pregiven units such as the individual or society as ultimate starting points of sociological analysis” (p. 287). Such a perspective “sees relations between terms or units as preeminently dynamic in nature, as unfolding, ongoing processes rather than as static ties among inert substances” (p. 289).</p>
         <p>The overwhelming majority of all social science theories falls into the variable‐based category. In a lucid article on explanations of macropolitical processes such as democratization and state formation, Charles Tilly provides examples from historical sociology showing how almost all analysts in that field “(1) assume a coherent, durable, self‐propelling social unit; (2) attribute a general condition or process to that unit; (3) invoke or invent an invariant model of that condition or process; (4) explain the behavior of the unit on the basis of its conformity to that invariant model” (Tilly 1995, p. 1595).</p>
         <p>Yet this practice makes very little sense, because “coherent, durable, self‐propelling social units—monads—occupy a great deal of political theory but none of political reality” (Tilly 1995, p. 1596).</p>
         <p>Likewise, suggesting that mainstream sociology postulates “that the social world consists of fixed entities (the units of analysis) that have attributes (the variables),” Andrew Abbott (1988) criticizes the substantialist mainstream position for making strong methodological assumptions about social reality that unduly limit or distort the underlying ontology. Most seriously, the conventional, variable‐based paradigm “ignores entity change through birth, death, amalgamation, and division” (pp. 171–72). These are all themes that have recurred in the process‐theoretic literature since Simmel ([1908] 1992) asked the fundamental question about the intersubjectivity, duration, and spatial extension of social forms (Cederman and Daase 2003).</p>
         <fn-group>
            <fn id="fn5">
               <label>5</label>
               <p>Simmel’s separation of form from content has been criticized by those who think that these two dimensions of social reality are intricately intertwined (see e.g., Durkheim 1960, p. 357). This criticism resembles objections to Barth’s formal anthropology or to Chomsky’s attempt to develop a syntactic theory of linguistics without reference to semantics.</p>
            </fn>
            <fn id="fn6">
               <label>6</label>
               <p>See also Elias’s (1978) notion of figuration.</p>
            </fn>
         </fn-group>
      </sec>
      <sec id="sc4">
         <title>Agent‐Based Modeling</title>
         <p>Sociological process theory offers suggestive insights into the emergence of social forms. Yet its potential is partly unrealized, because, as we have seen in the previous section, it remains a somewhat underspecified research program in terms of modeling. Most process theorists have had to content themselves with employing metaphors and analogies, although some have developed simple models (e.g., Elias 1978; Barth 1981). In this section, I therefore turn to formal modeling as a way to improve conceptual precision and theoretical consistency. Given the complexity of macrosociological research problems, I focus here on computational modeling rather than on rational‐choice models and, within the computational category, more specifically on agent‐based modeling.</p>
         <p>Agent‐based modeling is a computational methodology that allows scientists to create, analyze, and experiment with artificial worlds populated by agents that interact in nontrivial ways and that constitute their own environment (e.g., Axelrod 1997; Casti 1997; Epstein and Axtell 1996; Epstein 1999; Macy and Willer 2002). In these “complex adaptive systems,” computation is used to simulate agents’ cognitive processes and behavior in order to explore emergent macrophenomena, for example, structural patterns that are not reducible to, or even understandable in terms of, properties of the microlevel agents (Holland 1995; Cederman 1997, chap. 3).</p>
         <p>Since Herbert Simon’s (1981) brilliant essay on the “architecture of complexity” and Friedrich von Hayek’s (1967) pioneering writings on social complexity, researchers in the social sciences have increasingly come to the conclusion that complex systems call for different theories and methods from those used to study simple ones. Growing out of the tradition of general systems theory, complexity theory typically puts more weight on understanding the operation of microlevel mechanisms than its systems‐theoretic predecessors did (Rosser 1999, p. 184).</p>
         <p>In recent years, complexity theory has acquired a new momentum across the sciences thanks to developments in modern statistical physics together with interdisciplinary research, much of which has been conducted at the Santa Fe Institute in New Mexico (Waldrop 1992). This kind of theorizing should not be confused with related concepts such as catastrophe theory or chaos theory (Rosser 1999). While sharing the assumption of nonlinear interactions with much of today’s complexity theory, both these notions are usually applied to situations of low dimensionality. More central to the development of complexity theory are the nonequilibrium approaches developed by European physicists, especially that of the controversial Nobel laureate Ilya Prigogine and his associates, who specialized in the physics of dissipative systems. Aided by computational techniques, the current generation of complexity research has started to make inroads into the social sciences (e.g., Axelrod and Cohen 1999; Rihani 2002; Urry 2003).</p>
         <p>Agent‐based models of complex adaptive systems typically feature local and dispersed interaction rather than centralized control (Resnick 1994). Moreover, as opposed to conventional rational‐choice models that assume either a small number of dissimilar or numerous identical actors, agent‐based models normally include large numbers of heterogeneous agents (Epstein and Axtell 1996). Rather than studying equilibrium behavior, the focus is often on dynamics and transient trajectories far from equilibrium. Finally, instead of assuming the environment to be fixed, many agent‐based models let the agents constitute their own endogenous environment.</p>
         <p>Agent‐based approaches should also be contrasted to earlier uses of simulation in the social sciences, including the tradition of global modeling (see, e.g., Gilbert and Troitzsch 1999; Taber and Timpone 1996, pp. 48–49).7 Such macromodels, which were especially popular in the 1970s following the pessimistic report <italic>Limits to Growth</italic>, typically attempt to predict population and economic trends many years, sometimes even decades, into the future (e.g., Forrester 1971; Meadows, Randers, and Meadows 1972). By the early 1990s, the method had fallen in disrepute, mostly in response to overly ambitious predictive claims. Still, it continues to inspire theory building among some sociologists (e.g., Hanneman, Collins, and Mordt 1995; see also Sawyer 2003).</p>
         <p>To further distinguish agent‐based modeling from the dynamic systems tradition, and to underline the congruence between this methodology and sociological process theory, let us now return to the two metatheoretica1 themes of the previous section. By doing so, it will become clear that agent‐based tools enable the social theorist to pursue generative and configurative theorizing without abandoning the support of formal models.</p>
         <fn-group>
            <fn id="fn7">
               <label>7</label>
               <p>There are also other types of computational approaches to social science, such as rule‐based models and natural language processing derived from artificial intelligence, but these fall outside the purview of this review (see Bainbridge et al. 1994; Taber and Timpone 1996).</p>
            </fn>
         </fn-group>
      </sec>
      <sec id="sc5">
         <title>From Nomothetic to Generative Models</title>
         <p>Whereas most traditional uses of simulation modeling center on prediction, agent‐based modelers usually rely on generative explanation. This epistemological orientation has been evident since Schelling’s (1978) classical study <italic>Micromotives and Macrobehavior</italic>. Using an example of aggregate behavior, such as people choosing seats in an auditorium, Schelling (1978, p. 13) invites the reader “to try to figure out what intentions, or modes of behavior, of separate individuals could lead to the pattern we observed.” Obviously, it may be quite easy to figure out what the social pattern is going to be, but often the aggregate behavior surprises us. Schelling points out that the latter typically applies when there are strategic connections among the individuals. Such settings do not lend themselves to easy summation of the individual motives, which implies that<disp-quote>
               <p>we usually have to look at the <italic>system of interaction</italic> between individuals and other individuals or between individuals and the collectivity. And sometimes the results are surprising. Sometimes they are not easily guessed. Sometimes the analysis is difficult. Sometimes it is inconclusive. But even inconclusive analysis can warn against jumping to conclusions about individual intentions from observations of aggregates, or jumping to conclusions about the behavior of aggregates from what one knows or can guess about individual intentions. (Schelling 1978, p. 14)</p>
            </disp-quote>
         </p>
         <p>It is evident that this pattern of explanation conforms closely to the abductive logic discussed above. Adopting a realist as opposed to an instrumentalist position, Schelling (1978) is primarily interested in uncovering the causal mechanisms that generate surprising macrolevel patterns (p. 18). This explanatory strategy is most clearly exemplified in his famous segregation model, which is one of the world’s first agent‐based models. Puzzled by the stark segregation patterns that appear, for example, in American cities, Schelling postulates a simple and quite tolerant microlevel rule that produces two perfectly cultural clusters: it is sufficient to assume that the residents of each group are unwilling to live in neighborhoods almost totally dominated by the other group.</p>
         <p>In another influential statement, Axelrod (1997) articulates similar principles for agent‐based research. Distinct from both induction and deduction, the agent‐based method “is a third way of doing science” (p. 3). Rather than creating accurate predictions, “agent‐based modeling is a way of doing thought experiments. Although the assumptions may be simple, the consequences may not be at all obvious” (p. 4; see also Gilbert 2000). Ultimately, Axelrod’s primary aim is to advance a better theoretical understanding by uncovering the mechanisms that generate “emergent properties” (see also Holland 1995, p. 156). Like Schelling, Axelrod therefore stresses simplicity rather than realism.</p>
         <p>Explicitly using the label “generative social science,” Epstein (1999) further articulates the link to the theme of generative explanation in sociological process theory. In his view, agent‐based modelers working in this mold have to answer a general question: “How could the decentralized local interactions of heterogeneous autonomous agents generate the given regularity” (p. 41)? The preliminary answer to the question is to generate a process that generates the phenomenon under scrutiny. It should be stressed that this approach is necessarily tentative, because abductive inference can only produce “candidate explanations.” Or to put it more tersely: “If you can’t grow it, you haven’t explained it.” This formula obviously begs the question of how we are to select among the candidate explanations. One way to do this is to test the microlevel implications, as suggested by Epstein (p. 43).</p>
         <p>Here the scientific‐realist foundations of the computational paradigm become particularly evident, for in their search for causal mechanisms, agent‐based modelers such as Axelrod, Schelling, and Epstein reject instrumentalist reasoning.8 In summary, Gilbert and Chattoe put it well: “The realist epistemology is also a natural progenitor of computational simulations. It is a short step from developing <italic>theoretical</italic> models of mechanisms to developing <italic>computational</italic> models of mechanisms. It is also the case that simulations provide an appealing way of both formalising and exploring realist theoretical models” (2001, p. 114).</p>
         <p>Frequently, computational researchers introduce generative explanations under the label of “emergence,” although the latter is really a subset of the former because not all generative explanations exhibit emergence. This term is notoriously tricky to define in light of its varying uses in both the computational (Holland 1998) and the sociological and philosophical literatures (Sawyer 2002<italic>a</italic>), but it could be said that emergence usually refers to the fundamental irreducibility of complex systems to their constitutive parts. The failure of reductionism in such settings often relates to the nonlinearity of the systemic interconnections, something that implies that the whole is literally more (or less) than the sum of the parts (Simon 1981; Smith and Stevens 1996).</p>
         <p>Simple forms of emergence in agent‐based models rely on a straightforward ascending logic featuring self‐organized patterns that are compatible with methodological individualism (Macy and Willer 2002), which is why Epstein and Axtell (1996) refer to agent‐based modeling as a “bottom‐up” approach. Schelling’s model of residential segregation exemplifies such an ascending notion of emergence. But, as illustrated by Durkheim’s nonreductionist social theory, there is also a deeper sense of emergence that relies on “downward causation” (Sawyer 2002<italic>a</italic>). In this case, emergent social phenomena acquire a causal impact and cannot even in principle be reduced to laws and mechanisms operating at the microlevel. In a careful conceptual analysis, Crutchfield (1994) calls this type of emergence “intrinsic” because it presupposes that novelty be detected and acted upon by the systems’ agents: “The observer in this view is a subprocess of the entire system. In particular, it is one that has the requisite information processing capability with which to take advantage of the emergent patterns” (pp. 3–4).</p>
         <p>Given that most existing agent‐based models assume that agents are equipped with very simple rules and possess extremely primitive cognitive capacity (cf. Axelrod 1997; Schelling 1978), it is not surprising that Sawyer (1998, 2002<italic>a</italic>, 2003) argues that such frameworks fail to embrace intrinsic emergence. Limiting their schemes to self‐organization, computational modelers conceive of emergent properties as epiphenomenal configurations that emerge from dynamic webs of interactions, but that do not feed back down to the microlevel through the agents’ internal models. Yet as Sawyer (2003) admits, the macrolevel may affect the microlevel through ecological effects in agent‐based models, and to that extent, they are less reductionist than rational‐choice models that restrict the systemic view. Nevertheless, by limiting top‐down emergence to such an external logic, current computational modeling still does not fully capture the sociological process tradition, which subscribes to a nonindividualist ontology featuring downward causation in both the intrinsic and external sense. I will return to this important issue in the context of viewing collective actors as social forms below.</p>
         <fn-group>
            <fn id="fn8">
               <label>8</label>
               <p>Obviously, the agent‐based method is not the only type of formal modeling that is compatible with a generative epistemology. Although rational‐choice models are usually associated with a nomothetic approach to explanation (e.g., Green and Shapiro 1994), several theorists stress how rationalistic modeling can be used as a way to explain phenomena in terms of their generating mechanisms (Elster 1989; Scharpf 1997). Furthermore, it has already been noted that variable‐based macrosimulations can also be employed dynamically with the goal of formalizing theories.</p>
            </fn>
         </fn-group>
      </sec>
      <sec id="sc6">
         <title>From Variable‐Based to Agent‐Based Modeling</title>
         <p>By now, it should be clear that sociological process theory distinguishes itself from conventional “variable‐based” theory building by featuring explicit and endogeneous representations of social forms. Again, the classical process perspective in sociology anticipates more recent developments in computational methodology. In fact, agent‐based modeling has often been contrasted to the variable‐based approach to simulation. In the latter form, computer‐based modeling usually amounts to the construction of dynamic models linking variables together in systems of discretized differential equations (i.e., difference equations), as exemplified by global modeling. In ecology and population biology, this contrast pits “individual‐based” against “equation‐based” modeling (Parunak et al. 1998; see also McCauley et al. 1993; Fahse et al. 1998).</p>
         <p>According to Parunak et al. (1998), there is a fundamental difference between these two types of approaches. Whereas equation‐based modeling attempts to express causal relations among variables, individual‐based modeling represents interactions among the agents directly. This does not mean that the individual‐based paradigm is incapable of tracing the evolution of macrolevel variables. In fact, “the modeler pays close attention to the observables as the model runs, and may value a parsimonious account of the relations among those observables, but such an account is the result of the modeling and simulation activity, not its starting point” (p. 10). The opposite does not hold, however, because equation‐based modeling makes it difficult to represent mechanisms operating at the agent level, especially if they do not add up in a linear fashion. In systems featuring heterogeneous and spatially distributed agent populations, agent‐based modeling has an edge over equation‐based techniques thanks to its superior tractability and increased opportunities of microlevel validation and calibration.</p>
         <p>Modern software technology has paved the road for agent‐based model architectures. In particular, object‐oriented programming (OOP) facilitates the representation of agents as autonomous and discrete entities in simulation models (Hill 1996, chap. 2). In contrast to traditional procedural approaches which separate data from algorithms, the principle of <italic>encapsulation</italic> introduces objects as self‐contained bundles holding both data and algorithms. Objects communicate with each other through message passing that respects their boundaries. Furthermore, as opposed to information held in conventional programming variables, each such entity has its own inherent <italic>identity</italic>, implying that “two objects are distinct even if all their attribute values … are identical” (Rumbaugh et al. 1991, p. 2; see also Khoshaflan and Copeland 1986). Thus, an object has its own dynamic existence in that it can be created and destroyed. This is an aspect of identity that is sometimes referred to as <italic>persistence</italic> (Booch 1991). It is hardly a coincidence that the world’s first OOP language, Simula, was developed to conceptualize the semantics of simulation models in a natural way (Hill 1996, p. 25).</p>
         <p>More recently, multiagent systems (MAS) have added the notion of an agent to the repertoire of computer science. As the term is used in distributed artificial intelligence (DAI), agents are more autonomous than objects by virtue of their ability to refuse performing messages and to effect flexible behavior without central program control (i.e., multithreading; see Wooldridge 1999; Ferber 1999, p. 57). Depending on the complexity of their internal models, MAS feature either reactive or cognitive agents. So far, agent‐based applications in the social sciences have tended to be of the former type (Sawyer 2003).</p>
         <p>Although OOP and MAS may appear to be merely technical details, these advances reflect an important semantic shift away from procedural software engineering to more decentralized paradigms—a move that facilitates the parallel trend pointing from equation‐based to agent‐based modeling. After all, social agents are self‐propelled, autonomous entities with distinct, time‐dependent properties. In complex systems, it is especially useful that encapsulation forces the analyst to declare explicitly what information that agent has access to, and which of their own properties are visible to other agents. Identity is important because each agent can be represented as an independent and unique object <italic>instance</italic> of a more general template, usually referred to as a <italic>class</italic> in OOP languages. As opposed to conventional variables, which in principle have infinite life spans, agents implemented as dynamically allocated objects possess an explicit existence in time and can therefore both be born and die. Beyond object orientation, MAS techniques emanating from DAI are likely to inspire conceptual breakthroughs in theory building and further strengthen the trend toward agent‐based perspectives in social science modeling.</p>
      </sec>
      <sec id="sc7">
         <title>Modeling the Emergence of Social Forms</title>
         <p>The previous discussion has shown that computational modelers tend to follow the research practices of sociological process theorists in that they employ generative and configurative explanations. Less has been said about the types of social forms that figure in existing computational research designs. In this section, I therefore provide a taxonomy of such configurations together with illustrations of existing research. The main criterion for my classification relates to what aspects of a social form are actually treated as endogenous features. Whereas some models are limited to the generation of behavioral patterns, others feature a much more profound “rewiring” of social reality, including the context and constitution of the main actors. Based on these principles, four levels of endogenization can be identified in ascending order of ontological depth. According to this scheme, social forms can be modeled as<list>
               <list-item>
                  <label>1.</label>
                  <p>
                     <italic>Emergent interaction patterns</italic> constituting structures of behavioral choices of microlevel agents, usually in space.</p>
               </list-item>
               <list-item>
                  <label>2.</label>
                  <p>
                     <italic>Emergent property configurations</italic> constituting arrangements of agents’ microlevel properties, such as their culture or attitudes.</p>
               </list-item>
               <list-item>
                  <label>3.</label>
                  <p>
                     <italic>Emergent networks</italic> constituting dynamic configurations of relations that modify agents’ ability or inclination to interact with other actors.</p>
               </list-item>
               <list-item>
                  <label>4.</label>
                  <p>
                     <italic>Emergent actors</italic> constituting both individual and collective agency, including actors’ outer boundaries and their internal organization.</p>
               </list-item>
            </list>
         </p>
         <p>Rather than being mutually exclusive, these types of social forms can be combined, and they often appear recursively. For example, models that generate property configurations frequently endogenize behavioral interaction configurations as well. By the same token, models featuring dynamic boundaries often require endogenization of the relevant interaction networks.</p>
         <p>As shown by Macy and Willer’s (2002) comprehensive review of recent sociological applications, most existing agent‐based models treat social forms as interaction patterns or property configurations, while keeping the interaction topology and the actors’ corporate identities fixed. These two first categories correspond to what Macy and Willer refer to as models of “emergent order” and “emergent structure,” respectively. Studies explaining behavioral aspects of social systems remain the most active research area in agent‐based modeling. Following the path‐breaking work by Axelrod (1984), the literature on behavioral interaction patterns has centered on explaining the emergence of cooperation in anarchic settings. As is well known, Axelrod’s main result indicates that cooperative outcomes can result from egoistic adaptation even in the absence of central enforcement. Subsequently, this work has spawned an extensive computational literature on the dynamic conditions of cooperation (for reviews, see Axelrod and Dion 1988; Macy 1998; Hoffmann 2000; Axelrod 2000; Macy and Willer 2002).</p>
         <p>It should be noted that these cooperation‐theoretic models primarily serve other purposes than generating social forms. Instead, the outcome dimension is typically limited to a one‐dimensional statistic measuring the level of cooperation in the system (see Macy and Willer 2002). To the extent that social forms do emerge, however, they are usually seen as side effects of this primary goal. Illustrating this point, Axelrod (1984, chap. 8) devoted an often‐overlooked chapter of his celebrated book to the “structure of cooperation.” Noting that cooperative strategies stand a better chance of invading noncooperative populations if they appear in clusters, Axelrod studied the geographic distribution of collaborating agents (see also Cohen, Riolo, and Axelrod 2001). Although Axelrod’s book primarily focuses on growing cooperation in general, the spatial configuration of cooperative strategies exemplifies social forms seen as emergent interaction patterns.9</p>
         <p>The generation of social forms appears to be a more central goal in studies that explore explicitly emergent property configurations, such as cultural traits and attitudinal dispositions. Schelling’s (1978) classic model of segregation remains the best‐known example. As we have already seen, this model generates a spatial configuration of actors possessing dichotomous “ethnic” traits. By providing numerous references to similar models, Macy and Willer (2002) demonstrate that this is still a very lively field of research, especially in sociology. Many models omit Schelling’s focus on movement in favor of permanently located social actors that influence each other locally (e.g., Carley 1991; Mark 1998). As is the case with emergent interaction models, researchers typically focus on clustering. Axelrod’s (1997, chap. 7) well‐known culture model, for example, generates distinct spatial clusters of agents with identical traits despite the presence of a homogenizing microlevel mechanism. More generally speaking, the main sociological insight that can be gained from these and similar studies is that macrolevel heterogeneity can result from homophilic microlevel rules (Macy and Willer 2002, p. 13).</p>
         <p>Although these two categories of social forms dominate the computational literature, it would be a mistake to believe that they exhaust it. Based on the process‐theoretic critique of “substantialism,” it is clear that configurative explanations often require a more flexible ontology than that evidenced by the two first categories of emergent social forms. In this sense, one could fault Macy and Willer’s (2002) otherwise excellent review for adopting too shallow a notion of “social structure.” On the other hand, the limited scope of their review is understandable given the scarcity of attempts to generate deeper social forms. Fortunately, however, computational modeling in no way precludes endogenization of such sociational configurations, although the costs in terms of complexity may discourage some researchers from venturing into this domain. Indeed, the Simmelian categories of emergent networks and actors remain comparatively understudied by computational researchers, but as we will see, they are far from empty.</p>
         <fn-group>
            <fn id="fn9">
               <label>9</label>
               <p>If one generalizes the spatial logic from geographic locations to strategic space, Lomborg’s (1996) study of strategic configurations involving strategy mixes serving as “nuclei” and “shields” could also be classified as one that generates behavioral interaction configurations. See also Cederman (2001) for a model that shows that cooperative configurations can emerge among democratic states.</p>
            </fn>
         </fn-group>
      </sec>
      <sec id="sc8">
         <title>Emergent Networks</title>
         <p>Whereas networks figure in the two previous types of studies, research that treats networks as emergent social forms requires that interaction topologies be not only explicitly represented, but also endogenized. The distinction hinges on the endogenization of interaction opportunities, or what game theorists refer to as the “game form.” There is a fundamental difference between studying dynamics <italic>on</italic> networks and dynamics <italic>of</italic> networks (Watts 2003). In this section, I focus exclusively on the latter.</p>
         <p>Because their heritage goes back to cellular automata, traditional agent‐based models typically rely on grids and thus satisfy the requirement of explicit representation (Troitzsch 1997), but they seldom live up to the requirement of endogenized interaction configurations.10 In fact, square lattices can be seen as a special case of networks, which is a much more general class of social forms and thus more suitable for model building in the process‐theoretic tradition.</p>
         <p>In mathematical sociology, there is a very rich literature on social networks that dates back several decades (see e.g., Wellman 1983). Still, partly because of the specific analytical tools employed, much of this scholarly activity has focused almost entirely on static characteristics of network structures (Watts 2003, p. 50). To the extent that dynamics are explored at all, tractability constraints force analysts working with models to assume stationarity and homogeneity (Zeggelink 1994; see also Sawyer 2003). Relying on his own object‐oriented code, Zeggelink (1994) was the first network theorist to apply agent‐based modeling to the analysis of dynamic friendship networks (see also Conte et al. 1998; Skvoretz and Fararo 1996). Today, standard computational packages, such as Repast and Swarm, offer built‐in software for this purpose.</p>
         <p>While these developments bring modeling into closer harmony with process theory, the main focus in the sociological literature remains on the dynamics of small, interpersonal networks. In contrast, Simmel’s formal sociology stresses large‐scale forms as much as smaller ones. Fortunately, in the last few years, statistical physicists have brought their analytical skills to bear on large social networks (Albert and Barabási 2002). Even though the first steps in this direction concerned dynamics <italic>on</italic> networks—as evidenced by Watts and Strogatz’s (1998) pioneering work on the small world problem—more recently, physicists have turned their attention to dynamic configurations. Such settings may converge to an equilibrium, but they may also feature nonequilibrium networks, to which vertices and edges are added or from which they are removed (Dorogovtsev and Mendes 2002, p. 1086).</p>
         <p>The large scale of the physicists’ models enables the researchers to explore emergent macroeffects that cannot be detected in smaller systems. This is crucial, because complex networks exhibit general patterns and regularities that allow us to talk of “universality classes.” Under such conditions, macrolevel effects occur irrespectively of the particular microlevel mechanisms involved: “This is a tremendously hopeful message for anyone interested in understanding the emergent behavior of complex social and economic systems like friendship networks, firms, financial markets, and even societies” (Watts 2003, p. 65).</p>
         <p>It is not hard to see the parallel to Simmel’s aforementioned distinction between form and content. Anticipating the principles of modern statistical physics, Simmel believed that social forms vary more or less independently of the contents of social interactions (Levine 1991, p. 1104). This assumption makes it possible to study the formal “geometry” of social interactions while ignoring many of their microlevel details. In this sense, computational network models are ideally suited to operationalize Simmel’s formal sociology.</p>
         <p>In fact, the computational tools allow for a much more precise connection of the micro‐ and macrolevels than expected by Simmel. Complexity theory suggests that many social forms are associated with distinct statistical footprints in terms of their degree distributions, transitivity, or other formal properties. Such patterns can be used as the starting point for generative theorizing that serves to uncover the mechanisms that are responsible for their emergence. For example, researchers have been puzzled by the highly skewed distribution of connectedness among Web sites on the Internet. To put it more exactly, the degree distribution follows a power law (for a popular introduction, see Buchanan [2002]). To account for this asymmetry, Albert and Barabási (2002, p. 71) ask: “What is the mechanism responsible for the emergence of scale‐free networks?” It turns out that a very simple model can generate power‐law‐distributed configurations. Barabási and Albert (1999) created such a network by continuously adding nodes and connecting the new nodes to the previous ones such that the new links exhibit “preferential attachment.” This term means that new links are added in proportion to each node’s popularity as measured by its current number of links.</p>
         <p>Obviously, not all social networks exhibit scale‐free connectivity. This property appears to follow from topologies that impose little or no cost on the addition of new nodes. Where there are constraints applied by geography or other factors, the dynamic process of network formation and evolution may well generate emergent social forms characterized by other skew distributions, such as log‐normal or exponential distributions (Jin, Girvan, and Newman 2001). The most recent studies in the physics literature have attempted to identify what distinguishes social networks from other types of networks. Newman and Park (2003) postulate that it is the subdivision into groups and communities that produces the key macrolevel properties of social networks (see also Watts 2003, pp. 118–29). In particular, network configurations could emerge thanks to “assortative mixing,” not unlike the homophilic property configurations discussed above (Newman 2003). A related postulate attributes scale freedom and clustering to the hierarchical organization of complex social networks (Ravasz and Barabási 2003).</p>
         <p>Despite this impressive progress, considerable distance remains between statistical physics and Simmelian process theory. This is so partly because the models fail to endogenize the actors’ identities, even in cases where they are explicitly represented. Yet in real‐world social systems, individuals coevolve with network structures to such an extent that it is hard to separate individual “plasticity” from network “elasticity” (Lazer 2001). Few, if any, models combine emergent property configurations with dynamic networks.11 Moreover, endogenizing identities entails more than accounting for actors’ properties, cultural or otherwise, because Simmel’s notion of individuality requires a more profound notion of identification. It is to this challenge that I turn in the next section.</p>
         <fn-group>
            <fn id="fn10">
               <label>10</label>
               <p>In an indirect sense, it could be argued that Schelling’s (1978) segregation model features these types of entities, for in his framework, the actors’ mobility implies that their interaction topology evolves over time. Likewise, some behaviorist interaction models feature exit and other aspects of voluntary partner selection that implicitly produce network structures (e.g., Majeski 1999; for references to other examples, see Macy 1998; Hoffman 2000; Axelrod 2000).</p>
            </fn>
            <fn id="fn11">
               <label>11</label>
               <p>Though see also Padgett (1997), who proposes a model in which network skills emerge dynamically, and Macy et al. (2002), who take a step in this direction by devising a computational model that features self‐organizing groups in an Hopfield attractor network.</p>
            </fn>
         </fn-group>
      </sec>
      <sec id="sc9">
         <title>Emergent Actors</title>
         <p>The models associated with the three previous categories of social forms subscribe to individualism in the sense that individual actors are assumed to exist at the outset of the analysis and to persist until its conclusion. To be sure, such a substantialist assumption simplifies model construction, yet it contradicts Simmel’s ontological principles that require not only structures, but also actors, to be socially constructed. To follow the principles of sociological process theory, then, it is necessary to generate actors as emergent social forms. This in turn requires that the actors be problematized in terms of their “corporate identities” (Cederman and Daase 2003; cf. Wendt 1999).</p>
         <p>How could agent‐based modeling circumvent the need to postulate a set of reified actors at the outset of the modeling process? Drawing explicitly on sociological process theory, Abbott (1995) shows how to avoid the trap of anthropomorphic extrapolation from biological individuals. The trick is to focus on potential boundary elements before assuming the presence of the actors to be generated. Typically, such “sites of difference” are formed in a “soup” of microlevel actors: “Previously‐constituted actors enter interaction but have no ability to traverse the interaction inviolable. They ford it with difficulty and in it many disappear. What comes out are new actors, new entities, new relations among old parts” (Abbott 1995, p. 863).</p>
         <p>Abbott’s scenario dovetails with Simmel’s theory of how collective actors emerge in reaction to external threats. Simmel distinguishes between cases in which the cohesion of an already existing group increases as it enters into an antagonistic relationship with another group, and those cases where there was no preexisting group consciousness before conflictual interaction: “Conflict may not only heighten the concentration of an existing unit, radically eliminating all elements which might blur the distinctness of its boundaries against the enemy; it may also bring persons and groups together which have otherwise nothing to do with each other” (Simmel [1908] 1955, pp. 98–99).12</p>
         <p>Fortunately, at least some aspects of these ideas can be translated readily into computational language. In a generic model of ecological morphogenesis called Echo, John Holland (1995) lets “primitive agents” amalgamate into “multiagents” through a process of boundary formation where lower‐level agents are able to merge into composite entities, thus assuming the status of “agent‐compartments” (see also Cederman 2002). Such an organizational transformation requires that explicit rules of action scope and resource transfer be specified. This is why some type of internal organizational structure is needed to hold together and coordinate the activities of the agent‐compartments.</p>
         <p>Axtell’s firm‐size model provides a particularly concise and instructive example of this approach to the formation of actors. Drawing on work by physicists, Axtell (1999) notes that the size of real‐world companies is power‐law distributed. Treating this phenomenon as an emergent statistic reflecting a specific social form, his generative approach accounts for the mechanisms producing it. By postulating straightforward rules of joining and leaving firms, Axtell is able to generate scale‐free aggregate behavior.13</p>
         <p>Featuring a similar, though inherently more complicated, logic of organizational genesis, another line of research focuses on properties of state systems in world politics. Already in 1977, Bremer and Mihalka (1977) introduced a model featuring conquest in a hexagonal grid, later extended and further explored by Cusack and Stoll (1990). Cederman (1997) introduced a new generation of models in the same tradition. These models share a common architecture that starts with a territorial grid of fixed and indivisible primitive agents that can be thought of as villages or counties. The states that survive grow, and their boundaries expand endogenously through a repeated process of conquest. The resulting states become hierarchical organizations linking capitals to their respective provinces through direct, asymmetric relations of domination. It should be noted that although the agents reside in a grid‐based space, the underlying organizing principles presuppose that the actors be organized as a dynamic network, for conquest inevitably changes the list of neighbors. Models of this type can be used to explore dynamic features of competitive geopolitical systems, such as the duration of balance of power (Cusack and Stoll 1990) and war‐size distributions (Cederman 2003).</p>
         <p>The examples covered so far include only two‐level organizations. However, computational organization theorists have gone beyond this limitation by analyzing multilevel networks explicitly. Whereas most of the literature investigates the properties of fixed social forms, some studies set out to grow organizational structures (Carley 2002). In a prominent example, Carley and Svoboda (1996) apply simulated annealing as a way to represent organizational adaptation in terms of restructuring and learning. Recent computational organization theory fits especially well into the generative research program because it increasingly attempts to solve “backward” rather than “forward” problems (Lomi and Larsen 2001).</p>
         <p>Do actor configurations generated in these illustrative models exhibit emergence? The answer to this question hinges on which class of emergence is aspired to. If the goal is to generate emergent patterns in the “bottom‐up” sense, the question can be answered in the affirmative, at least when it comes to specific instances of the social forms. For example, in Bremer and Mihalka’s (1977) model, it is impossible to predict what the states’ boundaries are going to look like based on inspection of the microlevel rules. As regards entirely new organizational forms, however, it is less obvious that existing research generates even individualist emergence. Axelrod (1997, chap. 6) proposes a “tribute” model of new political actors that may be at least a partial exception to this observation. According to Axelrod’s algorithm, collective actors emerge if there is a pattern of interactions that confirms a number of properties seen to be constitutive of agency. These include effective control over subordinates, collective action, and recognition by third parties that an actor has been formed.</p>
         <p>Nevertheless, intrinsic emergence calls for explicit representations of organizational forms inside the actors’ “internal models,” that is, their cognitive maps of themselves and their environments (Holland 1995). Because of their relative simplicity, the agents in all of the aforementioned cases fail to live up to this standard. Whereas there are models that exhibit downward causation, in these cases, the causal pattern is not emergent in itself. To date, the computational literature lacks emergent models that “are both micro‐to‐macro and macro‐to‐micro modeled simultaneously” (Sawyer 2003, p. 347).</p>
         <p>Thus, it has to be concluded that a considerable gap remains between the state of the art of agent‐based modeling and sociological process theory. To close this gap, model building must confront the challenge of developing agents with internal models that recognize emergent effects while at the same time predicating their actions on them. All this has to be done without resorting to reification of the emergent patterns themselves. As explained by Sawyer (2002<italic>a</italic>, p. 554), philosophers refer to such phenomena as “supervenient” in that these “higher‐level entities and properties [are] grounded in and determined by the more basic properties of physical matter” without being reducible to the latter. We are thus quite far from realizing this ideal in contemporary agent‐based frameworks, but richer cognitive models employing agent communication languages and some developments in Artificial Life should give us hope that such a project is indeed feasible.14</p>
         <fn-group>
            <fn id="fn12">
               <label>12</label>
               <p>Coser (1964) offers an influential, functionalist interpretation of Simmel’s conflict hypothesis that reduces it to a behavioral phenomenon applying to relations among exogenous actors. For a critique of this perspective, see Sylvan and Glassner (1985).</p>
            </fn>
            <fn id="fn13">
               <label>13</label>
               <p>Beyond the specific issue of firm sizes, statistical mechanics continue to inspire generative model building that endogenizes actor boundaries (see e.g., Dittrich et al. 2000).</p>
            </fn>
            <fn id="fn14">
               <label>14</label>
               <p>For example, Fontana and Buss (1994) synthesize the emergence of life as self‐replicating LISP functions that are capable of acting on themselves.</p>
            </fn>
         </fn-group>
      </sec>
      <sec id="sc10">
         <title>Conclusion</title>
         <p>What can be concluded from this attempt to connect sociological process theory with agent‐based modeling? First of all, my comparison has illustrated that the two research traditions exhibit striking similarities. At the epistemological level, both shun nomothetic notions of causation in favor of a generative scheme of explanation. In terms of ontology, both bodies of theory put processes of social forms at the center of the research agenda rather than skipping directly to analysis in terms of causal relations among variables.</p>
         <p>What can process theorists learn from agent‐based modeling? First and foremost, this survey should convince them that formal modeling does not necessarily entail abandoning generative and configurative explanations. Indeed, the agent‐based paradigm offers a viable alternative that is fundamentally compatible with process‐theoretic foundations, serving as an important corrective not only to contemporary rational‐choice theory and statistical methods, but also to variable‐based reinterpretations of Simmel, such as Coser (1964) and Levine (1991). Second, given the massive complexity of macrohistorical processes, there is an acute need for an “accounting mechanism” that assists the theorist in performing thought experiments and counterfactual scenarios involving interacting subprocesses (Cederman 1997, chap. 3). The formalism also helps theory builders sharpen abstract concepts and articulate otherwise hard‐to‐grasp mechanisms. Finally, thanks to their formal precision, computational models sometimes generate characteristic statistical footprints that can be used as generative targets in the search for candidate mechanisms.</p>
         <p>What can computational modelers learn from the process tradition in sociology? Although the sociological literature often appears obtuse, this article has highlighted the value of consulting the classics and keeping abreast of some of the most recent developments in social theory. By virtue of their “head start,” sociological theorists have already had ample time to confront some of the most tricky issues in social theory. Their insights, especially those relating to generativity and social forms, could counteract the strong influence of positivist and individualist perspectives on modelers, particularly in economics. Though often manifested abstractly, the macrosociological literature on process also contains a wealth of empirical and historical examples which are bound to inspire computer‐assisted theorizing. Even more crucially, however, comparison with the process‐theoretic literature highlights theoretical challenges that have only been partially dealt with by computational methodologists. Without such conceptual guidance, it would be tempting to content oneself with models that generate social forms as emergent interaction patterns and property configurations. Instead, a careful reading of sociational theory tells us that model building could also endogenize networks and actors. While much work remains to be done until the computational repertoire of models finally generates intrinsic emergence in accordance with Simmel’s classic strictures, the agenda has been set for future conceptual challenges.</p>
         <p>Ultimately, theoretical innovation advances through analogy‐driven transfers of knowledge and insights from one conceptual domain to another. The well‐established marriage of evolutionary game theory and computational modeling has already proven extraordinarily fertile. Now it is time to explore what the new union between sociological process theory and computational modeling can offer.</p>
      </sec>
   </body>