<plain_text><page sequence="1">The chip, the sovereign, and his crown. International Business Machines Corporation (IBM). 4381 CPU Multi Chip Module. Model: IBM 22 A02 293. Computer History Museum, Inv. Nr. 102645256. 90</page><page sequence="2">Coomnuter Juuridisms CORN ELIA VISMANN AND MARKUS KRAJEWSKI 1. Symbolic Machines Law relies on transfer media and yet is itself a transfer system; it transfers rights.1 Like other transfer systems, not the least of which is language, law tends to underestimate media of transfer as mere tools, with the deceptive out come that media appear as matters that can be regulated. Yet a law that claims to be able to regulate a technological medium merely produces a paradox, because it aims to regulate nothing other than its own grounds of existence. Until the emergence and popularization of the computer and its net works, the law dealt with this paradox rather successfully. The underlying rivalry between law and media, when it came to processes of transfer, was managed by a series of assimilations and mutual affirmations that took place between two discursive regimes, that of a law, on the one hand, and that of a media, such as broadcasting, on the other. Once the computer entered the scene, however, the tactical alliance between law and media could no longer be maintained. The law's traditional hegemony over dis course found itself fundamentally questioned by the medium that-accord ing to Alan Turing's famous dictum-can become all other media. From then on, the computer as universal machine imposed its logic on the law. At the same time, however, the medium that is the computer behaves in a lawlike manner. When granting users access rights to its resources, the computer adopted something like a law of access and restriction. The mutual mimesis between law and the computer indicates that the fundamental paradox posed by a media law can no longer be either avoided or denied. In actuality, law and computer prove similar in structure. The fact that both legal routines and digital media exert discourse regimes that is, both use control mechanisms of inclusion and exclusion, access and nonaccess-qualifies the law and the computer alike as symbolic machines. They both operate in the medium of a text that generates texts, and do so equally in a binary mode, the mode of decision-making. This is why systems theory can so successfully describe legal operations in the cybernetic language developed for describing the computer. While media law aims to regulate the computer, the aforementioned structural similarity between the computer and the law necessarily leads to redundancies that cannot but render the latter ineffective. The law's medial Or R0oo 29 W|nt 2.O 8. p. 9 &gt;109 'C 007 ry Room nc. and Msachusett nstitute of Tecnology 91</page><page sequence="3">Digital Equipment Corporation (DEC). Alpha chip. Model: DEC 290A, Dec. 1991. Computer History Museum, Inv. Nr. 102646999. reinforcement turns it into a tautological repetition. No wonder, then, that the law laments its vanishing power in cyberspace. Instead of analyzing the causes of its impotence, however, the law rather naively continues to per ceive the Internet as a matter of law, which poses certain problems to the legal order. The law is thus blind to its own dependency on the computer as medium as well as to its structural homologies with it. Our analysis begins with the isomorphism between legal and digital machines. Here, the computer opens up a competition with that other great symbolic machine, the law. The competition revolves around nothing other than the power to define reality. Traditionally, the law has dominated the reality of word and image to a degree unequaled by any other performative system. Now, however, with the advent of the computer legal fictions must compete with digital virtuality. The virtual is a mode of reality that evades the space-time categories of the law; it arose once the history of computing passed through the technological phases of ASCII art and command-line interfaces and achieved graphical user interfaces (GUI). From that moment on, reality was computed by transistor cells and silicon chips. That which appears on a monitor as a so-called user interface results from mathematical relations and calculations and is thus not the represen tation of something that preexists in the coordinates of actual space and time (as would be the case in a photograph). Such a computed reality ulti mately proves unbearable for a legal system that operates according to the logic of symbolic representation. The legal system is thus fundamentally imperiled by virtual reality; it follows a spatiotemporal logic threatened by incorporeal procedures, physical infinitude, and the anonymity of commu nicative acts within the digital realm. Simply put, virtuality challenges the law's core concepts: corporeality, finitude, and authentication,2 concepts that are fundamental to any claim of territorial sovereignty as well as to imputations and rules of evidence. The law, of course, does not simply give in to such a challenge. Rather, one can locate three strategies by which the law attempts to maintain the spatiotemporal conditions of legal effectiveness within cyberspace: fiction, simulation, and technology. In the first instance, the law, that classic mas ter of fiction, simply fakes the reality of the physical world. This happens when, for example, certain contracts that require a scriptural form and sig nature are simply declared by legal order to be equal to their physical coun terparts when they are closed on an electronic basis. The second strategy, that of simulation, is applied to questions that concern the legal protection of property. Such protections, however, are primarily based on physical rules of scarcity and singularity. Copyright rules belong to the set of norm simulations that compensate for the condition of scarcity, which is absent 92 Grey Room 29</page><page sequence="4">ft] i*in cyberspace. Where neither the S | ^ J fiction of legal rules nor the simu L4a;_ 1IE ilation of physical ones help fortify the legal ground in cyberspace, the law has recourse to the third strategy, that of media technologies. In civil law, electronic authentication procedures are designed to overcome the technological fact that Internet communications are communications between machines (and their IP-addresses as postal interfaces) and not per sons. Cryptography software is installed to bind natural persons to their computers in the same way that human beings are bound to their own hand writing. Here, once again, the law appears to master media technologies. Although the battle to define reality and hence to maintain sovereignty in cyberspace may seem to be won by governmental legal systems, as opposed to the cybersystem, the contest is in fact futile from the start. The quasi-sovereign power of the computer engineer's code stems from the ease by which posing, implementing, and applying a norm are achieved in technology compared with the cumbersome procedures that a legal code must pass through. The swift effectiveness of a technological code, which cannot, when seen through legal eyes, appear as anything other than uncanny, renders any possible competition between law and com puter pointless. Yet the effectiveness of the computer code is no miracle. The computer itself has undergone a process of what one could call "juridification"-a pecu liar adaptation of the computer to the legal framework. Before the law could even think of superimposing its structure on the computer, the computer, that hedgehog, was already there, fortified within an impregnable burrow. Rabbit law, even when promulgating the first of the computer's legal regulations, arrived too late; the computer already functioned according to a juridical logic. By incorporating the basic elements of the legal system into its own administrative structure the computer had become as sovereign as the law. 2. The Silence of the Legislators One of the rather long list of astonishing facts about the history of comput ing is that during the first three decades of its existence the medium escaped any regulation whatsoever. In her study of the regulation of cyber space, Vanessa Geczy-Sparwasser dates the first legislative action in the United States with regard to the Internet to the year 1989.3 At the time, leg islators were alarmed by an attack upon a government computer network which gave them a hint of how vulnerable the state's information infra structure was. Additional legislative efforts, besides those meant to prevent further info-infrastructural sabotage, were only made once the economic Vismann and Krajewski I Computer Juridisms 93</page><page sequence="5">use of the Internet could no longer be avoided. The U.S. had initially proven reluctant to allow the popular use and popularization of its computer net works. It gave up on this policy of prevention only in 1995-1996 once the Internet had expanded from a solely nationwide connection between acad emic and military sites into the World Wide Web. Eventually, in 2001, the Communications Decency Act came to be enforced, aimed above all at the regulation of electronic commerce. The European situation proved basically the same: no legislative activ ity arose before it became a matter of commerce. In 1994, without explicitly citing the Internet, a report for the European Union (EU) Commission ("The Bangemann Report") demanded the liberalization of telecommunications and an adjustment of the legal standards of copyright and data protection. Two years later, out of the diffuse phenomena of new media arose the cru cial technology that was finally given its proper name within the legal world: the Internet.4 The focus adopted by the EU, as in the U.S., was also guided mainly by commercial interests. In Germany, legislative oversight of the Internet proceeded slightly dif ferently. Lawmakers had already arrived on the scene when the "new" or "multi-" media were on the verge of becoming mass media. The German legislature drew analogies between new media and broadcasting, which is afforded special governmental protection according to Article 5 of the German Basic Law (Freedom of Communication). The aim of the judicature of Article 5 is to regulate the broadcast content, to prevent monopolization, and to guarantee the technical transmission of data. The Internet did not become subject to explicit German legislation until 2001, and just as in the U.S. and the EU, the net of nets turned into a legal matter only once it was perceived as the means of electronic commerce. Such a remarkable period of legislative abstinence-from the onset of the era of computer communication to the turn of the century-corroborates those legendary tales about the un- or self-regulatory forces within the dig ital realm, stories that evoke contradictory effects depending on one's atti tude toward rules, order, and law in general. Whereas those whose profession is to put things in order, not the least of whom are lawyers, pan icked when they learned about the powerlessness of state-enforced law in cyberspace, others took the Internet's alleged "unrule-ability" as a triumph, a counterproof to the thesis that a world without law is unimaginable. Yet, the distinction between a threatening vacuum of legal norms (rechtsfreier Raum) and a welcome space of self-regulation is not as clear as it might at first seem. The absence of any manifest law does not amount to absolutely no legality whatsoever; it can mean, rather, that a legal structure has already somehow been internalized. 94 Grey Room 29</page><page sequence="6">In the case of the computer, the latter case proves to be true, for it adopted a legal structure early on. As long as a computer can be used by multiple users it has to institute a hierarchy or create at least a couple of rules by which to organize and control the execution of different processes. Since the age of mainframes, at least, every computer has had its own col lection of rights by which to grant or deny access. These rights regulate access to (calculation) time-the power of governance-as well as to files the power to dispose resources. a. Personifying the Computer One can speak of the computer's "juridical structure" from the moment that its operational levels become as inaccessible as the law, a moment which has the effect of inciting a permanent desire for access, as shown by the inces sant interpretations of Kafka's short story, "Before the Law" ("Vor dem Gesetz"). This lawlike closure occurred within a specific generation of computers. Indeed, the mutation from an engineer's appliance to a legal machine can be dated precisely: to 1971, the year Xerox developed the com puter as a desktop. Such a personified computer, as J6rg Pflulger's account of the three major changes in computer design has shown, turned an elec tronic device into an entity compatible with a law that computes and imputes things to persons.5 Xerox designed a machine that would empower each individual in front of his or her personal computer, making him or her into an "individual making use of a sophisticated tool," as one of the inven tors put it. The ideal was a direct manipulation of visible objects at the inter face between users and computers. Personifying the computer led to an incisive division between people and lawgivers, or, in computer terms, between users and programmers. Those using the machine were not to be disturbed by the machine status of the operations they performed. They would also have no need to bother with whether their hardware system was capable of doing more (or other) things than a typewriter with an enormous storage capacity. Users didn't have to be programmers anymore; they were spared any translation work from normal language into assembler code. Standard languages cover over the machine language (which ultimately consists of Os and is) or, slightly better, the assembler language, which exists in a less abstract "mnemo code" (three terms per line: one command plus two registers). Once the user is no longer confronted with the entire machine and its outlandish code, the machine appears to "speak" the user's language; the personal computer seems merely to execute the user's will. To get a program running, the user needs only to call it up. Programs appear on the com puter's GUI as icons; they have names taken from the hardware world of Vismann and Krajews;ki Computer Juridsms 95</page><page sequence="7">desks. They dress themselves in the metaphors of offices so the users feel comfortably at work when employing their new writing devices. "Files" can be opened and closed conveniently by mouse clicks. The advent of the user, under the auspices of a personified computer, brings forth the end of the era of the engineer. What was an insult to all engi neers-" userfriendliness"-enabled nonengineers to use the computer instead of computing and programming. From then on, the computer showed only its anthropomorphic face; that is, its interface. That peculiar type of subject that is the user, therefore, is born as one who is capable of neither any insight beyond the surface nor any programmer's knowledge whatsoever. Machine operations are hidden or opaque; they run in back of the fine but still humble GUI. Interaction is welcome, but real action by the user is restricted-and strictly controlled. The computer, then, is a system that consists of a technical base and an ideologized superstructure, a switching core on the one side and a visible surface on the other.6 To the extent that the operational basis becomes highly technical, the medium sets conditions for communication that its users cannot even perceive: The higher and more effortless the programming languages, the more insurmountable the gap between those languages and a hardware that still continues to do all of the work.... While on the one hand it remains possible in principle to write user-software or cryptograms with a knowledge of codes or algorithms, on the other and user friendly concealed hand it is by now impossible to decipher the prod uct specifications of the finished product or even to change these specifications.7 b. The Chip, the Sovereign The split between users and programmers that happened in the name of the computer's personification was a major step into the legal logic of the com puter. As defined so elegantly and briefly by the Prussian domestic law of 1794, "persons" are those who bear rights; they bear rights because the law grants them to the bearers. Accordingly, personal computers, as the combi nation of men and machine, are those who bear rights-not, however, rights granted by a state-enforced law but rights granted by another central alloca tion entity, the so-called central processing unit (CPU). When installed in a chip, for instance, the 80286 developed by the Intel Corporation, the CPU operates in a hierarchic mode. Within the processor (or, rather, within its reg isters, stacks, and abilities to address memory) are switches, implemented to allocate rights. The chip allows access graded according to four protective rings, which are concentrically layered. These separate systems programmers, with 96 Grey Room 29</page><page sequence="8">access to the inner circle, from users, whose privileges do not allow them to go beyond the first protective ring. Commands are activated on the outermost level; their execution takes place within the processing unit itself. The chip is sovereign: it establishes prohibitions, protects privileges, and implements laws of inalterability. Media theorist Friedrich Kittler deci phered Intel's chips as a governmental bureaucracy in miniature. As he writes in his influential essay, "Protected Mode," "Although there may no longer be any written prohibitory signs that guarantee a power gap, the binary system itself encodes the distinction between commands and data, what the system permits and what, conversely, is prohibited to user pro grams."8 Such a legal characteristic results from the strict division between command and data. After all, since the days of Sicilian Emperor Frederic II, no administration can allow any confusion between the two orders of com mand and data. The Intel-legislator acts accordingly and produces proces sors that take measures against the possibility of free alteration by distinguishing between data and command; it prevents the mutual coupling of both and allocates immutable rights of access. By contrast, before the age of the computer's personification, that is, during the era of the engineer, modifications on an operational level were possible at any time for the sim ple reason that commands and data were not differentiated. "Stored pro gramming" meant that data and commands were treated the same, as addresses at a particular memory position in the machine.9 When addresses of data and addresses of commands are kept in the same register, as is the case in so-called von Neumann architecture, they may be altered by a single program. They can even modify themselves by nonhierarchical reentries of commands and data-with unforeseeable and sometimes unwanted consequences. Although data and commands are still kept together, neatly and arbitrar ily, in random access memory (RAM), today's user has no opportunity to intervene within those highly restricted areas where addresses are managed and accounted. Some higher power always intervenes. Even the program developer who uses concurrent standard languages like JAVA has no access to RAM in order, for example, to intervene and manipulate pointers. Although the programmer can issue almost any kind of command to make the machine act as he or she desires, control of RAM is no longer the pro grammer's responsibility. The memory, and thus the sovereignty over addresses and modes of their manipulation, is administered and controlled by the authority between the source code and the binary code of the CPU, the so-called Java Virtual Machine (JVM). This may be understood as the level of control exercised over the programmers, who are treated as users by the programmers of the programmers. Such a delegation of work-in fact Vsnnard Krajewski j omputerJurid2ms 97</page><page sequence="9">International Business Machines Corporation (IBM). IBM 64K FET Memory chip, IBM 4331 Processor, 1964. Computer History Museum, Inv. Nr. 102640496. a restriction of power-produced by the JVM is officially termed "error pre vention" in order to warn everyday programmers away from typical sources of bugs (e.g., pointer arithmetic). In fact, this measure of "error prevention," which the programmers' programmers offer their clientele, means-aside from the habitual claims of convenience and error prevention-a decisive reduction in the machine's potential. c. The Hierarchy of Operating Systems In addition to the split between data and command, the scarcity of time is another reason for the legal design of the computer's interior. The time available to those who might receive the attention of the emperor is not the same for everyone; similarly, a chamberlain is needed to regulate time allocations within the computer. From the moment of the first attempts to create multi-user operating systems in the 1960s, the need to handle many different users, each with a different set of privileges, has only grown. One well-known solution, which has served as the model for all its successors, began as an experiment called Multics (Multiplexed Information and Computing Service). It is based on two main principles: first, the scarce time of the CPU's calculation power could be shared among multiple users with different demands by deploying one user's idle time to serve other active users; second, the computer could be made capable of running processes by several different users at once. Both concepts require strict rules in order to regulate time allocation and decide whether or not to grant admission to desired processes and resources. In 1970, this experimental system was baptized Unix (for Uniplexed Information and Computing Service) and came initially with a strictly hier archical file system that enabled communication between the machine and many users. Interaction was primarily by means of textual commands entered by the user into the terminal. Those commands were then exam ined according to a number of criteria: Who is typing? What does he or she want? What are his or her rights? and, finally, What will he or she conse quently be allowed to access? This examination judged the user's right to access resources, determining whether or not he or she would be allocated a slice of calculation time or access to specific files, processes, and data. The operation of assigning user's rights as granted by the file system is based on an Access Control List (ACL).10 The agent who passes judgment is neither the chamberlain nor the gate keeper, as the operating system's legal modality might lead one to conclude. There is a gatekeeper, but it serves only as the first instance, the first step into a longer series of examinations. The one who rules the list of rules, who tells right from right, who resides at the top of the hierarchy in order to govern 98 Grey Roomn 29</page><page sequence="10">the communication channels, who incorporates the right to give access or deny privileges: this most supe rior user is simply called the "root." Derivatives of Unix (e.g., Zeta) and other operating systems organized in similarly hierarchical file-system structures, call this super-user, slightly readjusted to the hierarchical structure in its noble rank, the "baron."" Possessing all of the rights of inclusion and exclusion, this paralegal authority, with its thinly veiled label, possesses the unlimited power to (re-) arrange and define (new) groups of special users who have higher (or lower) privileges. This super-user must maintain a whole court of ministe rial posts, services, and duties in order to manage the system and keep it running. The super-user hands over special duties to these subgroups in order to assume the welfare of the entire system while, at the same time, limiting the privileges of those it classes as "normal" users to the default use of system resources. The super-user has sole command over this com plex system of differing privileges. In a word, the whole concept of the super-user occupies a solitary and unique position: it serves as the system's sovereign. It has complete power of access at its disposal, because any lib eration from the strictures of access control would endanger the entire sys tem. The "sovereign" must therefore avoid juridical approaches that demand "open access," as, for instance, in the case of laws that would require computer systems to have "back door" access for legal authorities, new types of software architecture based on nonhierarchical approaches to organization, or requests for "root" access from users whose lack of knowl edge means they could do tremendous damage (unintentional or not) if they were granted such access. The super-user's first tenet is thus to resist claims for unrestricted access; it has internalized the tenth of the ten lesser tenets of Mike Gancarz's Unix philosophy: "Think hierarchically."'12 d. The Law of Media under the Terms of the Computer The distinction between commands and data indicates the juridical logic under which Intel's chips already function. Multi-user file systems follow the logic of bureaucratization that accompanies the logic of law. The multi user file system operates on the strict and hierarchically structured distinc tion between admission and denial of access. It is precisely within the chips' legal design and the file systems' hierar chization of access, that the ultimate explanation for the remarkably long absence of legislative action with regard to the computer lies. Chip design follows juridical structures just as the file system that is part of the operat ing system complies with the regulations of a legal design. The juridical administrational structures within the chip render any legal regulation from Vismann and Krajewski Computer Juridisms 99</page><page sequence="11">the outside superfluous as long as outside interests, above all commercial ones, do not demand it. Only when the computer finds itself coupled with socioeconomic systems does the law take on the role of mediator between conflicting interests, such as those of users in the name of free speech or open access or those of the market in the name of freedom of action. The law that assigns itself the role of conflict arbiter remains blind to the com puter's intrinsic juridicality. Thus, the silence of the lawmakers with regard to the profound split between bases and surfaces has a reason: only surfaces fall under explicit juridical regulations, not the transfer technologies between computers. That means the person who is communicating and not the communication tech nology becomes the object of legislation. The law remains unable to scruti nize the lawlike CPU, that clandestine rival of institutionalized government. This situation has given rise to the deplorable consequence that a hand ful of companies have established the conditions for using the computer as a communications medium. While such a circumstance would normally bring about antitrust regulations, it falls below the perceptual threshold of the governments and parliaments of a computer-connected world. From the perspective of the user, this means that the user's so-called autonomy comes at a price: that of an operating system fixed at a certain, contingent moment in the computer's development and not individually alterable. Beyond the interface, users have no access whatsoever. Where the influence of the user ends, the power of the programmer begins. Once again, however, the programmers are themselves classified in a highly hierarchical manner. The programmer of the programmer, design ing the tools and methods of a coding language (such as the compiler, code syntax, abstract data types, and so on) maintains the ultimate power because he or she, as the constructor of the programming language itself, defines what the "normal" programmer, as a user, will be able to do. Both types of programmers establish the conditions for using the computer, and, as such, they behave like lawmakers or, rather, code-makers. Implemented within the CPU and the hierarchy of the file system is the law governing communication with and through the computer. In this respect, code and law maintain a relationship of more than structural homology. The code is a law-as Lawrence Lessig pointed out when he described "code" metaphor ically as a synonym for the conditions under which the computer runs.13 The personification of the computer, its design for the benefit of a private person who is not an engineer, is the user-friendly side of a software pro ducers' market strategy without which the computer would undoubtedly never have gained the popularity it has. Yet it is precisely this populari zation, almost democratization, of a formerly exclusive device under the 100 Grey Room 29</page><page sequence="12">control of a priests' caste of engineers that laid the groundwork for the juridism of the chip's architecture. Privileges, protection zones, and access rights regulate what is accessible for common use. An insurmountable bar rier between users and system programmers safeguards the computer's inal terable functions. Beyond this barrier, as in Kafka's story, a new barrier appears between the programmer and the programmer of the programming language who decides how the basic set of elements is to be designed, which rights and properties will be granted to whom, and which will be denied. What follows, almost as an involuntary reflex, is the claim for access. At all levels of the computer's architecture, access-the keyword of the twenty-first century (one does not need a Jeremy Rifkin to tell us this) is demanded in reaction to these barriers. Although the law has reinforced such a claim by establishing a right to discrimination-free access, it has thus far done so without addressing the uncanny mimesis in effect between the computer and the law. Rather than being alarmed by the functional homology between the law and the com puter, legislators in the Western world continue business as usual. They fail to realize that the universal machine's own inner juridisms render law inef fective. If the law fails to acknowledge the computer's operational mimesis of a legal logic, it will, sooner or later, become outmoded. Contemporary laments about the diminishingly binding force of the law in cyberspace are only the initial manifestation of the trend toward self-marginalization on the part of any law that does not reflect on its own dependence on media; that is, on its own mediality. The long-held understanding of media merely as tools and, accordingly, as matters of law, ultimately reveals the paradox at the core of any media law. How can the law address and regulate what it is itself subjected to? Although legend holds that the state has turned, in regard to its media, from a strong sovereign to the supervisor of networks, the state-enforced media law has been defective from the start, long suffering from an overes timation of its ability to regulate alphabetic media in the same manner as legal matters such as labor, gene technology, or waste. Alphabetic media neither are nor have ever been purely means for legally defined ends; they have never simply served to realize rights. Media technologies have to decide about the conditions under which all systems, including the legal system, think and speak. Media law must therefore reformulate itself under the same conditions established by the universal machine for so-called users. For what is the law with respect to the computer if not a user itself? More than anything, the computer calls for a change in how one views the relationship between media and law. The concept of the law as a medi ator of Internet conflicts is completely inadequate to the medium, since it Vismann and Krajewski Computer Juridisms 101</page><page sequence="13">does not reflect the fact that the law itself is subjected to media. Media are not mere tools for free speech. They are nothing less than the conditions of possibility for communication. Thus, one can no longer maintain a utili tarian conception of media. Needed is a media theory that liberates media from its teleological confines. It is no longer sufficient to focus on the end of the communication process where the so-called user is supposedly found. Such a perspective, habitually adopted by the law, succeeds only in walling users off from the programming level. Required is a shift in focus from the ends of communication to the processes of transfer itself. Such a shift challenges the legal traditions of applying rights, for neither the bearer nor the guarantor of rights, neither the subject nor the state, can any longer be regarded as preestablished, stable entities. They are instead already in transition, part of a transfer process. 3. Codes, Commentaries, and Codifications The paralegality of code does not only dominate the computer's inner core and operating system; it also governs the codes (i.e., the protocols) that reg ulate the network of electronic communication, most notoriously the Internet. In this case, the inherent juridism is more difficult to detect, although the word protocol, like code, points both toward bureaucracy and toward the juridical realm. The term code is in fact deeply rooted within the history of law (e.g., the Code Napoleon). Code has a nicer sound than, say, statute (in German, das Gesetz), and codes in fact lay claim to being some thing less than state-enforced law. 14 In the semantics of contemporary legal discourse, a legal norm is called a code whenever one wants to indicate self emergent forms of law as opposed to norms which are established by a nation-state.15 Under conditions of globalization, legal norms inscribe them selves within the antithesis between self-emergent codes and state-bound laws. Whenever a legal norm is called a code it indicates that it lacks every thing that characterizes a formal parliamentary law statute. Not only do codes lack a pre-established lawmaker, they are also created by practices and habits among colleagues and can be changed at any time. They further more appear not to need any institution of enforcement. "Voice" and "exit," the subject's expression of a personal opinion and disengagement from a network, are the operative functions in deciding about codes in force. Codes, in other words, emerge from the practice of a group. At the same time, the group is made cohesive by those codes and the decisions made about them. Codes thus address everyone who wants to be addressed by them; they function as rules of inclusion. Because there exists no legislative or executive body beyond the partici pants in the code-constituting procedure, codes demand a higher degree of 1 02 GreyRsoom 29</page><page sequence="14">compliance than state-enforced laws. And it is for this reason that they are so attractive to a post-national legal system. "Codes of conduct" did not become a fashionable catchphrase accidentally. The phrase alludes to the fields of etiquette, baroque ceremonies, behavior, and fashion. Computer codes share the traits of self-codification with all such social norms: they are intrinsic, autonomous, alterable, and thus permanently optimizable norms. At the outset, computer codes emerged from an open process, one that consisted of mutual comments on the codes themselves. Codes are thus not set by a single authority but are instead developed by a collaborative process. For their own better understanding, programmers are in the habit of commenting, on their own algorithms particularly, in order to define the current state or to add personal notes (such as "@TODO: improve this sort ing algorithm . . ."). Such commentaries may be necessary to understand the programmer's own code, since after a time a code fragment's function or manner of operation may no longer be self-explanatory. Communication between programmer and computer usually proceeds in a mediated fash ion. Except for a few chip-level programmers, almost no developer speaks the (binary) language of the machine itself. They use instead a set of com mands provided by so-called higher computer languages, which remain more or less abstract. The developer arranges the commands into new struc tures and algorithms within a readable text file called source code. This code, however, is anything but understandable to computers; it is not yet executable by the machine and requires an intermediate step to transform the components into machine-readable binary code. This transformation is usually the result of an interpretation process performed by the so-called compiler program. Source code constitutes a wholly new genre of textual code (or coded text): a hybrid, it can be regarded either as a collection of sequential commands (which the machine will perform during runtime) or, in conjunction with the developer's comments, as a complete documenta tion of the task the machine is supposed to do once the process is initial ized. The source code-literally as source and code-contains the source of its own documentation as well as the preform of the binary code. What the code does depends on the way it's handled: it acts either as a text for docu menting its achievements (i.e., as in a manual for the programmer or for other developers) or as a working code to be executed by the machine. The coincidence or correlation of code and commentary in this peculiar state results in a code's highest level of information density, which can be processed further with the help of specific utility programs (preprocessors) in one of two ways: conversion into an executable file for machine use or conversion into a comprehensible documentation of the entire code includ ing commentaries and algorithms, that is, a commentary on the codification Vsanad Xr jewsk? computer Jurid~sms 103</page><page sequence="15">itself. The dualism of the source code was recognized during the early years of the personal computer age when Donald E. Knuth-famous for his multivolume book, The Art of Computer Programming, and even more famous for the creation of the typesetting language TEX-laid claim to writing computer programs that human beings could read like works of literature. Knuth orig inated a new paradigm of coding, so-called literate programming, which demands a dense fusion of commentary and algorithmic structure.16 Algorithms and commentaries both evolve deeply interwoven within the same file, distinguished only by tags that identify their status. Knuth devel oped this notion further in a language named WEB, where algorithmic structures are embedded in descriptive text rather than vice versa as is usu ally the case. The WEB source code can then be treated in two ways: it can be "tangled," which will then produce compilable Pascal code (today, C code), or it can be "weaved," which renders a neatly formatted, printable documentation (for TEX). (Pre-)processors (like "tangle" or "weave" or the compilers of the Pascal/C language or TEX) act to close codes, that is, as codification. The process of closing a code entails the insertion of a juridical structure at the level of pro tocol. While compiling input files, the (pre-)processors freeze the code development at a certain stage in order to yield new files for the perfor mance of new tasks (like giving hermeneutic aids to the human reader of code fragments or making the string "Hello World" appear on screen). These compilers or (pre-)processors function like filters, or like editors complet ing a book. They ignore the material that serves other purposes and select those particular items necessary for further processes. They act like story tellers, which in this case means that they tell the desired story from the executable code (or vice versa), in a manner, however, this is somewhat like telling right from wrong. The culture of commenting eventually leads to codes and codifications on the network level. There, developers communicate with colleagues through comments, at least since the time that the legendary "requests for comments" became ubiquitous under the abbreviation RFC.17 Stephen Crocker of UCLA provided a personal back-story to the RFC by recounting that he had the idea of using such a philological form of commentary for the collaborative invention of a program that would connect different computer systems to one another. The leader of the interoperability group that devel oped the RFC process explained his choice of the word comment, in retro spect, as intending to express "that anyone could say anything and that nothing was official."'l8 104 Grey Room 29</page><page sequence="16">International Business Machines Corporation (IBM). IBM 512K CMOS SRAM, Static Random Access Memory chip, 1991. Code name: LIGHTNING. Computer History Museum, Inv. Nr. 102640493. In the ideal case, the comments on Crocker's protocol yielded a code as they were transcribed in programs. According to this idealized account of its creation, the RFC eventually produced the network program that was commented on, the so-called TCP/IP protocol, which has become the stan dard for all data-transfer processes between networks. "I never dreamed," wrote Crocker, "these notes would be distributed through the very medium we were discussing in these notes."19 That "very medium," the ARPANET, which was created in 1969 by comments among colleagues, forms nothing less than the transformation of a permanently self-commenting commen tary into a code-a commentary set within a feedback loop in order to opti mize the codes that set the bit-scope and determine the transmission process of contemporary telecommunications. The fact that protocols result from a collaborative developmental proce dure should not distract from the juridism inherent in such procedures of commenting. Analysis of the juridic structure of this interoperation under mines the story of self-regulation in cyberspace. The notion that an acade mic network's use of comments to communicate led to the creation of a communications protocol based on some of those comments implies an organic way of making code. It is a story that still inspires theories that emphasize the Internet's autonomy. Yet, the general picture of collaborative creation obscures the fact that an agent (either human or human-built) must filter all of the comments and decide which are in force and which become code. Much the same process occurs in this instance as in that of the com munication between programmer and computer. Like the transformation between source and code, an agency filters out all comments that do not meet the formal requirements that are set up by certain persons. Not all com mentaries have the potential to become a code. Some are simply humorous, hypothetical, historical, or self-referential. Even among the formal com mentaries (i.e., those that could become code), not all will actually become binding codes.20 Comments may contradict one another or be redundant. Some have to be selected, and someone has to select them. In this case, unlike that of the automatic selection process performed by the com piler/(pre-)processor, the editor is a person: at a certain point in the process, a person reads all the (meta-)tagged, presorted files (in XML manner) to sort workable from inappropriate, thereby enforcing certain comments and can celing others. In the case of the RFC Standards, the editor in charge did not remain in the shadows of any supposedly collective process of norm cre ation through commenting. He was a person with a civil name: Jon Postel. Postel provides evidence of the code's intrinsic juridism. Juridism is characterized by a central agency that allocates rights or defines norms. Thus, computer codes are juridical, even if they stem from an open culture Visma and Kraj~wsk Cr putr Jurdisms 1 05</page><page sequence="17">of commenting. The history of law teaches that a culture of commenting is no impediment to codification and thus to certain juridical traits. Indeed, legal codes often derive from none other than comments. The famous Digesta that laid the groundwork for Western law drew their material from the Roman jurists who had communicated among themselves by comment ing on cases. The mass of their commentaries were eventually "digested," compiled in a book, in Latin a codex, from which the word "code" origi nally derives. The Byzantine Emperor Justinian appointed an editor, a Jon Postel of Byzantium, with the name Tribonian, who guided the process of con verting comments into a single code by selecting particular data from out of the masses of texts. Following this procedure of formatting a book out of abundant commentaries, the Digesta established, on the one hand, an inal terable law text, and on the other, an ever-changing commentary by which it is accompanied. What finds its way into a codex, between the two covers of a book, is from then on literally closed; it cannot be altered and thus brings to an end the nonhierarchical form of text generation via unceasing chains of commentary without ulterior reference. The closure of codes-that is, what is later called codification-ends the practice of codes that appear and disappear according to their use. In the aftermath of this closure, a certain melancholy enters the legal discourse; what is mourned is the end of law in the making. Codification means for lawyers, such as Friedrich Carl von Savigny, the monopolization of legal texts. Savigny described the moment of closure, the codification, as abolishing the ideal of a provisional law in a constant process of making (Werden).22 A similarly melancholic undertone can be observed in some accounts of the history of the Internet when it comes to the monopolization of communication standards. Today, the Digesta of the digital, the RFC, have found their Savigny who praises the time before the Internet's institutions of standardization assumed the status of a central agency that decides about the codes in force-a time when com ments still had the power to create, alter, and overthrow codes.23 Rather than adopting a Savignyian tone of anticodification, Alexander R. Galloway has defended the necessity of centralization, even for code making that stems from comments, acknowledging this centralization as a precondition for more freedom on the side of the user and finding a com forting formula in what he calls a "generative contradiction" between pro tocol and its institutionalization: "in order for a protocol to enable radically distributed communications between autonomous entities, it must employ a strategy of universalization, and of homogeneity."24 The paradox of the intrinsic juridism of the code that claims to be other than law is interpreted here as the dialectic of freedom. Autonomy requires an autonomy-granting institution. 106 Grey Rom 29</page><page sequence="18">The notion that freedom arises from, or is granted by, a central agency sounds familiar and begs the question as to whether computer juridism is unavoidable. The centralized power that emerges at a certain point in the development of codes is, after all, a manifestation of the latent juridism of the code itself. Yet, when it comes to the possibility of avoiding the codes' juridification, it is not helpful to presuppose a generative contradiction (which merely repeats the division into an operational base and a super structure) between strict institutions in the inner arcanum of the computer and freedom on the surface. Such a perspective starts the moment computer culture ha~s already and irretrievably stepped into a juridical framework. The turn from comment to codification, from a culture of permanent com menting to a legal logic of closure, is thus presented as unavoidable, pre supposing an intrinsic desire for order. Such a perspective employs a well-established argument: the codifica tion of the Digesta as the primordial, centralizing legal event legitimized the foreclosure of a culture of commenting in favor of a panoptic survey. The profound effects of this closure for Western culture cannot be over estimated. Commentaries, which provided the material for codifications, consist in a self-observing, self-constituting, and yet at the same time self deconstructing and permanently self-optimizing practice. In the eyes of a centralized law, such a practice produces nothing but disorder and confu sion and thus demands homogenization and the establishment of networks of rules. This was the case with Roman law in the sixth century, and it is the case with transnational law in the twenty-first. The current trend toward codification in transnational law operates according to the logic of codes. The trend is even called creeping codifica tion-as if no codifying agency were at work. The denial of a selecting agent gives rise to stories of self-regulation. As "creeping" as codifications may be, they ultimately create their own agencies, which operate within a juridi cal mode of allocating rights: agencies such as editors and programmers who design the pseudo-autonomy of the surface at the price of centralizing the operational base. Is juridism, specifically the move toward codification in the digital, thus unavoidable? One is tempted to say yes: as soon as codes are at work-even as an alternative to imposed law-they always already have one foot in the juridical realm. And yet, the other foot still belongs in the world of the engineer. What is needed is a perspective on computer developments that is as equally well-informed in media as in law in order to sort out the latent juridisms that contradict the predominant tales of self regulation, in legal as well as in virtual-digital contexts. V~smann and Krajewskg j Computer Juridsms 1 07</page><page sequence="19">Notes 1. For a more detailed explanation, see Cornelia Vismann, "Jurisprudence: A Transfer Science," Law and Critique 10 (1999): 279-286. 2. Christoph Engemann, Electronic Government?Vom User zum B?rger: Zur kritischen Theorie des Internet (Bielefeld, Germany: Transcript, 2003). 3. Vanessa Geczy-Sparwasser, Die Gesetzgebungsgeschichte des Internet: Die Reaktion des Gesetzgebers auf das Internet unter Ber?cksichtigung der Entwicklung in den U.S.A. und unter Einbeziehung gemeinschaftlicher Vorgaben, Beitr?ge zum Informationsrecht Band 3 (Berlin: Ferdinand Sch?ningh Verlag, 2003). The following paragraphs draw upon G?czy-Sparwasser's account. 4. Although the infrastructure known as the Internet has a history which begins at the latest with the ARPANET in 1969, the term "Internet" was used by the EU report metonymically to describe the new phenomenon and its effects which came along with the introduction of the World Wide Web in 1991. 5. J?rg Pfl?ger, "Konversation, Manipulation, Delegation: Zur Geistesgeschichte der Interaktivit?t," in Geschichten der Informatik: Visionen, Paradigmen, Leitmotive, ed. Hans Dieter Hellige (Berlin: Springer, 2004), 367-408. The following quotations are taken from this essay. 6. See Wendy Hui Kyong Chun, "On Software, or the Persistence of Visual Knowledge," Grey Room 18 (Winter 2004): 26-51. 7. Friedrich A. Kittler, "Protected Mode," in Literature, Media, Information Systems, ed. John Johnston (Amsterdam: G+B Arts International, 1997), 158. 8. Kittler, "Protected Mode," 160-161. 9. John von Neumann, "On the Principles of Large Scale Computing Machines," in John von Neumann, Collected Works, vol. 5 (New York: Pergamon Press, 1961), 1-33. 10. The data structure of Access Control Lists are organized like most databases: with tables. Not coincidentally, the table, in addition to being the basic medium of economics, is one of the primary media by which states wield power. For the table as a method of govern ment, administration, and accounting, see Cornelia Vismann, Akten: Medientechnik und Recht (Frankfurt: Fischer Taschenbuch Verlag, 2000), 205-212 (English translation forth coming from Stanford University Press, 2008); Bernhard Siegert, Passage des Digitalen: Zeichenpraktiken der neuzeitlichen Wissenschaften 1500-1900 (Berlin: Brinkmann und B?se, 2003), 166-171; Joseph Vogl, Kalk?l und Leidenschaft: Poetik des ?konomischen Menschen (Munich: Sequenzia, 2002), 59-63; R?diger Campe, "Vor Augen Stellen: ?ber den Rahmen rhetorischer Bildgebung," in Poststrukturalismus: Herausforderung an die Literaturwissenschaft, ed. Gerhard Neumann, Germanistische Symposien, Berichtsb?nde 18 (Stuttgart: J.B. Metzler, 1997), 208-225; and Markus Krajewski, "In Formation: Aufstieg und Fall der Tabelle als Paradigma der Datenverarbeitung," in Nach Feierabend: Z?rcher Jahrbuch f?r Wissensgeschichte, vol. 3, ed. David Gugerli, Michael Hagner et al. (Z?rich, Berlin: Diaphanes Verlag, 2007). 11. Strictly speaking it should be called "king." The developers of these Unix dialects, BeOS, for instance, or Zeta, however, disregarded the formal hierarchy of noble ranks and called the "king" a baron. Barons, both in the aristocratic system and in hierarchical file-system based operating systems like Unix, enjoy a higher reputation than do non-nobles (i.e., regular users). 12. Mike Gancarz, The UNIX Philosophy (Boston: Digital Press, 1995). 108 Grey Room 29</page><page sequence="20">13. Lawrence Lessig, Code and Other Laws of Cyberspace (New York: Basic Books, 1999). 14. On the difference between code and law, see Vaios Kravas and G?nther Teubner, "http://www.CompanyNameSucks.com: The Horizontal Effect of Fundamental Rights on 'Private Parties' within Autonomous Internet Law," German Law Journal 4, no. 12 (2003): 25-27. 15. Klaus G?nther and Shalini Randeria, Recht, Kultur und Gesellschaft im Proze? der Globalisierung, Werner Reimers Stiftung, Schriftenreihe Suchprozesse f?r innovative Fragestellungen in der Wissenschaft, Heft 4 (Bad Homburg, Germany: Programmbeirat der Werner Reimers Konferenzen, 2001). 16. Donald E. Knuth, "Literate Programming," The Computer Journal 27 (1984): 97-111. Knuth's idea has been adopted in a somewhat reduced and less sophisticated manner in actual programming paradigms like JAVA with tools such as Javadoc, which rearranges the commentary of the code as the explanation of how to use it. For details, see http://java. sun.com/j2se/javadoc/writingdoccomments/. 17. For good analytical accounts, see Bernhard Siegert, "Die Trinit?t des Gastgebers/The Host's Trinity," in ONLINE, ed. Helga Konrad. (Graz: REMAprint, 1993), 130; Alexander R. Galloway, "Protocol vs. Institutionalisation," in New Media, Old Media: A History and Theory Reader, ed. Wendy Hui Kyong Chun and Thomas Keenan (New York: Routledge, 2006), 193; and Engemann, 27. 18. Stephen Crocker, The Request for Comments Reference Guide, RFC 1000, August 1987, http://www.rfc-archive.org/getrfc.php?rfc=1000. 19. Crocker. 20. RFC 1111 is titled "Request for Comments on Request for Comments." See Galloway, 193. 21. Siegert, "Trinit?t," 130; and Galloway, 187. 22. Friedrich Carl von Savigny, "Vom Beruf unsrer Zeit f?r Gesetzgebung und Rechts wissenschaft" (1814), in Thibaut und Savigny: Ihre programmatischen Schriften, 2nd exp. ed., ed. Hans Hattenhauer (Munich: Verlag Franz Vahlen, 2002), 115. 23. For a survey of the historical roots of this conduct, see Chris Woodford, The Internet: A Historical Encyclopedia: Issues (Santa Barbara, CA: ABC Clio, 2005). 24. Galloway, 196. Vismann and Krajewski I Computer Juridisms 109</page></plain_text>