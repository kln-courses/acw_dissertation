<plain_text><page sequence="1">Biometrics 64, 807-815 DOI: 10.1111/j.l541-0420.2007.00939.x September 2008 Modeling Time Series of Animal Behavior by Means of a Latent-State Model with Feedback Walter Zucchini,1 * David Raubenheimer,2 and Iain L. MacDonald3 ^nstitut fiir Statistik und Okonometrie, Georg-August-Universitat, 37073 Gottingen, Germany 2Liggins Institute and School of Biological Sciences, University of Auckland, Private Bag 92019 Auckland, New Zealand 3Actuarial Science, University of Cape Town, 7701 Rondebosch, South Africa * email: wzucchi@uni-goettingen.de Summary. We describe a family of models developed for time series of animal feeding behavior. The models incorporate both an unobserved state, which can be interpreted as the motivational state of the animal, and a mechanism for feedback to this state from the observed behavior. We discuss methods for evaluating and maximizing the likelihood of an observed series of behaviors, and thereby estimating parameters, and for inferring the most likely sequence of underlying states. We indicate several extensions of the models, including the incorporation of random effects. We apply these methods in an analysis of the feeding behavior of the caterpillar Helicoverpa armigera, and thereby demonstrate the potential of this family of models as a tool in the investigation of behavior. Key WORDS: Animal behavior; Feedback; Feeding behavior; Hidden Markov models; Latent state; Mixed model; Motivational state. 1. Introduction Animal behaviorists are interested in the causal factors that determine behavioral sequences?i.e., when animals per form particular activities, and under which circumstances they switch to alternative activities. It is accepted that observed behavior results from the nervous system inte grating information regarding the physiological state of the animal, e.g., the levels of nutrients in the blood, with sen sory inputs, e.g., concerning the levels of nutrients in a food (Barton Browne, 1993). The combined physiological and perceptual state of the animal is termed the "motiva tional state" (McFarland, 1999). MacDonald and Rauben heimer (1995) modeled behavior sequences using a hidden Markov model (HMM) whose unobserved underlying states were interpreted as motivational states. Their model captures an important aspect of the causal structure of behavior, be cause an animal in a given motivational state (e.g., hungry) might perform not only the most likely behavior for that state (feed) but also other behaviors (groom, drink, walk, etc.). There is not a one-to-one correspondence between mo tivational state and behavior. And it is the runlength dis tributions of the motivational states that are of interest, rather than those of the observed behaviors. HMMs do not, however, take into account the fact that, in many cases, be havior also influences motivational state; feeding, for example, leads to satiation. We propose here a model that incorporates such feedbacks and apply it in order to model observed feeding patterns of caterpillars. We define a "nutrient level," which is determined by the animal's recent feeding behavior and which, in turn, influences the probability of transition to a different motiva tional state. Latent-state models, including HMMs, provide a means of grouping two or more behaviors, such as feeding and grooming, into "activities." Here the activities of interest are meal-taking, an activity characterized by feeding interspersed by brief pauses, and inter meal intervals, in which the animal mainly rests but might also feed for brief periods. In Section 9, we demonstrate the application of the model to data collected in an experiment in which eight caterpillars were observed at 1-minute intervals for almost 19 hours, and classified as feeding or not feeding. The data are displayed in Figure 1. 2. The Model Suppose an animal is observed at integer times t = 1, 2,... ,T, and classified as feeding at time t (Xt = 1) or not (Xt = 0). We propose the following model. There are two possible (unobserved) motivational states, labeled "hungry" (state 1) and "sated" (state 2). The state process {Ct} is a process in which the transition probabili ties are driven by a process {Nt} termed the "nutrient level," which takes values in [0, 1]. We assume that Nt is some func tion of Nt~\ and Xt. The development that follows is applica ble more generally, but we will restrict our attention to the exponential filter: Nt = \Xt + (1 - A)JVt_! (t = l,2,...,T). (1) ? 2008, The International Biometric Society 807</page><page sequence="2">808 Biometrics, September 2008 &lt;- 6.2 hours -&gt; 8 -w-1-1-h?i? ? -h- -m- ? -n? 95 7 i ii i ii ?ii i im i 11 hi?n-i? ?i?i i ?i? ii? ii i i ii ii i?i-n? ?i? i 235 6 ?I-1-W-1-1-H-H-1?I?l-H-!-+ -1?H-II I II? ?I-H-IHH- -l-l- ? - 207 ? 2" CD CD .a -* ? 5 ?Hi_l?h? hi m i?11 i i i?i-i-i?i?m?i?i i 11 i i i n 11?m?i?i i n n?i n i i 11 n- *|81 S? c CD *- CD O Q. .92, 4 ++?H I I I H- + - -H-1?II I I I I-1 I II I I I I?I-KH?I?H-1-1 llll II-II-1-1-H "| 49 3 _Q CO 3 -H-1 ?I-1-1?I-1-H-1-M-H-H-HH-1?H-H-HI-H?I- "| 26 ^ 2 -?- I I II-II I II I I II I -1-Hi-M-1- ?Hi-m-HK-i-.-lllll.I-. 200 "j -H-l- - - ? - - -1?I- -HH1-M- ?t-H-MH?- I III-1 I II Mil-?- 316 I-1-1-1-1-1-1 0 200 400 600 800 1000 1200 time (minutes) Figure 1. Feeding behavior of eight Helicoverpa armigera caterpillars observed at 1-minute intervals. In biological terms, Nt would correspond approximately with the levels of nutrients in the blood (Simpson and Raubenheimer, 1993). We take the transition probabilities ll3(nt) = Pr(Ct+i =j\Ct = i,Nt = nt), (t = 1,2,... ,T - 1) to be determined as follows by the nutrient level: logit711K) = a0 + axnt, logitj22(nt) = p0 + ftnt; (2) and by the row-sum constraints: 7n( t) +7i2(nt) = 1, 72i (nt) +i22(nt) = 1. In equation (2) the logit could be replaced by some other monotonic function g : (0,1) ?&gt; R. In state 1, the probability of feeding is always tt\, regardless of earlier motivational state, behavior, or nutrient level; sim ilarly 7r2 in state 2. The behavior Xt influences the nutrient level Nt, which in turn determines the transition probabilities of the state process and so influences the state occupied at the next time point, t + 1. With the notation C&amp; = (Cu C2,..., Ct) for the "history" from time 1 to time t of {Ct}, and similarly JV^ and XSl\ our two fundamental assumptions are as follows, for t = 2, 3,..., T and t = 1, 2,..., T respectively: Pr (Ct | C**-1', N0, X""1)) = Pr(Ct | Ct-UNt-i); (3) and Pr (Xt = 11 C? JV0, XC"1)) = Pr(Xt = 11 Ct) _ Jtt! if Ct = 1 \tt2 if Ct = 2. (4) We use the convention that X^^ is an empty set of random variables, and similarly for N^?K Because?given the parameter A?A^_1) is completely de termined by No and X^t_1\ the above two assumptions can equivalently be written as: Pr (Ct | C(t-1}, iV0, N^, X^) = Pr(Ct | Ct_i, JVt-i); and Ttt! if Ct = 1 Pr(lt = lCW,4iV(wUM) = &lt; V ' [tt2 ifCt = 2. We expect 7Ti to be close to 1 and tt2 to be close to 0. It is convenient to introduce here the notation Pi(x) = Pr(Xt =X\Ct=i)= TTf (1 - TTi)1"1 (x = 0,l;t=l,2); in this notation, assumption (4) becomes: Pr(Xt=i|C&lt;t&gt;&gt;JV0,X&lt;t-1&gt;) =Pct{x). The model is conveniently represented by the directed graph in Figure 2. Notice that there is a path from C\ to C3 ( N0 )- ( Ni )- ( iV2 )- ( N3 ). \ Cl J-*V ?2 J- V ?3 J.*" Figure 2. Directed graph representing model.</page><page sequence="3">Latent-State Model for Animal Behavior 809 that does not pass through C2. the state process is therefore not in general a Markov process. The following are treated as parameters of the model: a0, &amp;i, Po, Pi, 7Ti5 7T2, and A. In addition, we take no G [0, 1] to be a parameter, albeit not one of any intrinsic interest. One can also treat as a parameter the distribution of C\ given No (which we denote by 6), or more precisely, the prob ability &lt;5i = Pr(Ci = 11 iVo). It can, however, be shown that on maximizing the resulting likelihood one will simply have either S\ = 1 or 6\ = 0. We follow this approach, but in the application described in Section 9 it turns out in any case that it is biologically reasonable to assume that 6\ = 0, i.e., that the subject always starts in state 2. 3. Likelihood Evaluation Given a sequence of observations {x^T^} assumed to arise from such a model, and given values of the parameters listed above, we need to be able to evaluate the likelihood, both to estimate parameters and to find the marginal and conditional distribu tions we shall use in this analysis. First we write the likelihood as a T-fold multiple sum, then we show that this sum can be efficiently computed via a recursion. 3.1 The Likelihood as a Multiple Sum We therefore seek LT = Pr (X&lt;T&gt; | No) = YI Pr(C(T\*(T)|^o). Ci,...,CT The summand Pr(C(T), X^T) \ JV0) may be decomposed as fol lows: Pr(C^T\X^\No) = Pr(Cl\No)Pr(X1\C1,N0) T xY[{PT(Ct\C^1\No,Xit-^)Pr(Xt\C{t\No,X^t-^)} t=2 T = Pr(C71 | N0) Pr(Xx | C71)J^[{Pr(Ct | Ct.x, Nt^)Pr(Xt | Ct)}. t=2 The first equality follows by repeated application of the defi nition of conditional probability, and the second from assump tions (3) and (4). We therefore conclude that: T Lt= YI 6ciPc1(xi)Y[{^ct^,ct(nt-i)pCt(xt)} cT,...,cT L *=2 3.2 Recursive Evaluation The likelihood is therefore a sum of the form: 2 2 2 ( T \ C]=l c2 = l cT = \ \ t=2 ) that is, if here we define ?i0') = 6jPj(xi) and, for t = 2, 3,..., T: ft(i,j) =li3(nt-i)Pj(xt). Multiple sums of the form (5) can be evaluated recursively. Indeed, this is the key property of hidden Markov likelihoods that makes them computationally feasible, and here makes it unnecessary to sum explicitly over 2T terms. See Lange (2002, p. 120) for a very useful discussion of such recursive summation as applied in the computation of likelihoods of pedigrees, a more complex problem than the one we need to consider. For r = 1, 2,..., T - 1 and ir+1 = 1, 2, define: Or+1(V+l) = 2^ar(v)/r+l(?r,?r+l) ir That is, define and compute the row vector ar+i as ar+i = orFr+i, where the 2x2 matrix Ft has (i, j) element equal to ft(i, j)', ct\ is the row vector with jth element cti(j). It then follows by induction that aT(ir) is precisely the sum over all but ir, i.e.: aT(iT) = ^^...^|/1(i1)fI/t(tt_1,it)l. The quantity L is therefore available as L = ]T\ ar(ir) = oTl, where 1 denotes a column vector of ones. Writing this result in terms of ot\ we get: L = OiF2F3 ... FtI In the present context, ot\ = 8P(x\) and (e.g.) F2 = T(n\)P(x2). Hence the likelihood of the model under discus sion can be written as the matrix product LT = 5P(x1)r(n1)P(x2)r(n2)... r(nr-i)P(xT)l, (6) where: the row vector 8 is the distribution of C\ given Nq, P(xt) is the diagonal matrix with zth diagonal element p%(xt), and T(nt) is the matrix with (i, j) element jy(nt). Apart from the fact that precautions have to be taken against numerical underflow, the matrix product (6) can be used as it stands to evaluate the likelihood. The computa tional effort is linear in T, the length of the series, in spite of the fact that there are 2T terms in the sum. A convenient way to guard against underflow is this. When calculating the row vectors ar, scale them so that (for example) the sum of the elements is 1. Keep track of the logs of the scale factors thus applied, and adjust the resulting log-likelihood by the sum of the logs of the scale factors. For details, see the Web Appendix. 4. Parameter Estimation by Maximum Likelihood Estimation may be carried out by direct numerical maximiza tion of the log-likelihood. Because the parameters iz\, 7r2, X, no, and 8\ are constrained to lie between 0 and 1, it is con venient to reparametrize the model in order to avoid these constraints. A variety of methods were used in this work to carry out the optimization and checking: the Nelder-Mead simplex algorithm, simulated annealing and methods of New ton type, as implemented by the R routines optim and nlm (R Development Core Team, 2007). An alternative to direct numerical maximization would be to use the expectation-maximization (EM) algorithm. In this particular model, however, there are no closed-form expres sions for the parameter estimates given the complete data, i.e., given the observations plus the states occupied at all times.</page><page sequence="4">810 Biometrics, September 2008 The M-step of the EM algorithm would therefore involve nu merical optimization, and it seems circuitous to apply an algo rithm that requires numerical optimization in each iteration, instead of only once. For further discussion of the relative mer its of the two approaches, see Bulla and Berzel (2007). Altman and Petkau (2005) report that, in their work, direct numeri cal maximization was far more efficient than EM. Whatever method is used, however, one has to bear in mind that there may well be multiple local optima in the likelihood. 5. Model Checking When we have fitted a model to the observed behavior of an animal, we need to examine the model to assess its suitability. One way of doing so is as follows. It is a routine calculation to find the forecast distributions under the fitted model, i.e., the distribution of each Xt given the history X^_1). The probabilities are the ratios of two like lihood values: Pr (Xt | X^) = Pr (X?)/Pr (X^) = Lt/Lt-u We denote by pt the probability Pr(Xt = 11 A"(t_1)) com puted thus under the model. Because the joint probability function of X^T^ factorizes as follows: Pr(X^) = Pr(Xi)Pr(X21 Xi)Pr(X3 | X(2)) ... Pr(XT | X&lt;?-V), we have a problem of the following form. There are T binary observations xt, assumed to be drawn independently, with E(xt) =pf We wish to test the null hypothesis E(xt) = pt (for all t), or equivalently Ro:g{E(xt)}=g(pt), where g is the logit transform. The alternative hypothesis is of the form: HA : g{-E(xt)} = f{g(pt)}, where /is a smoothing spline (see, e.g., Hastie and Tibshirani, 1990). Departure of / from the identity function constitutes evidence against the null hypothesis, and a plot of / against the identity function will reveal the nature of the departure. 6. Inferring the Underlying State: Use of the Viterbi Algorithm A question that is of interest in many applications of latent state models is this: what are the states of the latent process (here {Ct}) that are most likely (under the fitted model) to have given rise to the observation sequence? In the context of speech recognition this is known as the "decoding" prob lem: see Juang and Rabiner (1991). More specifically, "local decoding" of the state at time t refers to the determination of the state it which is most likely at that time, i.e.: it = argmaxPr(Ct = i I X(T) - x(T)). i=l,2 V ' ' In the context of feeding, local decoding might be of inter est for determining the specific sets of sensory and metabolic events that distinguish meal-taking from inter-meal breaks (Simpson and Raubenheimer, 1993). To perform local decoding, we would need the "for ward probabilities" at(i) = Pr(X^,Ct = i) and the "back ward probabilities" f3t(i) = Pr(Xt+i,..., XT \Ct = i). With these probabilities available, the probability required is then given, for t = 1, 2, ...,T, by Pi(Ct = i\xV))=at(i)(3t(i)/LT, where as usual LT denotes the likelihood of the observations {^r)}. The forward and backward probabilities can be com puted recursively; see, e.g., MacDonald and Zucchini (2006, p. 4532) for the recursions. In contrast, "global decoding" refers to the determination of that sequence of states c\, c2,..., cj which maximizes the conditional probability Pr(C&lt;T&gt; = c&lt;T&gt;|X&lt;T&gt;=*&lt;T&gt;); or equivalently, and more conveniently, the joint probability: T T Pr(C&lt;TU(T)) =6C1 n^-^K-Oj! pct(xt). t=2 t=l Global decoding can be carried out, both here and in other contexts, by means of a dynamic programming method known as the Viterbi algorithm (Viterbi, 1967; Forney, 1973), for instance as follows. Define ?H = Pr(Ci = i,Xx = xi) = 8iPi(xi), and, for t = 2, 3,..., T. Zu = max Pr (C^1) = c^~l\ Ct = z,X(T) = *&lt;T&gt;). ci,c2,...,ct_i It can then be shown that the probabilities ?# satisfy the following recursion, for t = 2, 3,..., T: &amp; = [m?*te-M7?(nt-i)}] Pj(Xt). (7) This provides an efficient means of computing the T x 2 ma trix of values ?#, as the computational effort is linear in T. The required sequence of states i\, vi,..., %t can then be de termined recursively from %t ? arg max ^Ti i=l,2 and, for t = T - 1,T - 2,... ,1: it = argmax{ftt7i|it+1(nt)}. i=l,2 One convenient feature of the Viterbi algorithm is that the computations can be carried out in logarithmic form if that is necessary to prevent underflow in the multiplications in equa tion (7), for instance. In this respect global decoding, which involves maxima of products, is simpler than the computa tion of the likelihood, which involves sums of products but is otherwise similar. 7. Models for a Heterogeneous Group of Subjects There are several directions in which the model may be ex tended if that is useful for the application intended. For in stance, we may wish to investigate the effects of subject- or time-specific covariates on feeding behavior, in which case we</page><page sequence="5">Latent-State Model for Animal Behavior 811 would need to model the subjects as a group rather than as individuals. 7.1 Models Assuming Some Parameters to be Constant Across Subjects One, fairly extreme, model for a group of / subjects would be to assume that they are independent and that, apart from nuisance parameters, they have the same set of parameters; i.e., the seven parameters aQ, ax, p0, Pi, tt\, 7t2, and A are common to the / subjects. The likelihood is in this case just the product of the / individual likelihoods, and is a function of the seven parameters listed, / values of no and (and if they are treated as parameters) / values of 8\; 7 + 21 parameters in all. At the other extreme is the collection of / individual models for the subjects, each of which has its own set of nine parameters and is assumed independent of the other subjects. In this case the comparable number of parameters is 91. Inter mediate between these two cases are models that assume that some but not all of the seven parameters listed are common to the /subjects. For instance, one might wish to assume that only the probabilities 7Ti and tt2 are common to all subjects. 7.2 Mixed Models However, in drawing overall conclusions from a group, it may be useful to allow for between-subject variability by some means other than merely permitting parameters to differ be tween subjects. One way of doing so is to incorporate random effects; another is to use subject-specific covariates. The incorporation of a single subject-specific random effect into a model for a group of subjects is in principle straight forward; see Alt man (2007) for a general discussion of the introduction of random effects into HMMs. For concreteness, suppose that the six parameters a0, ol\, Pq, P\, 7Ti, and tt2 can reasonably be supposed constant across subjects, but not A. Instead we suppose that A is generated by some density /with support [0, 1]. Conditional on A, the likelihood of the observations on sub ject i is LT(i, A) = 6P(xil)T(nll)P(xl2)r(nl2)... T(nuT^)P(xzT)l, where {xit : t = 1,..., T} is the set of observations on subject i and similarly {nit} the set of values of the nutrient level. Unconditionally this likelihood is J LT(i, A) /(A) dA, and the likelihood of all / subjects is 1 r1 Lt = U / M*,A)/(A)dA. (8) Each evaluation of LT therefore requires / numerical integra tions, which can be performed in R by means of the routine integrate, but slow the computation down considerably. Incorporation of more than one random effect could pro ceed similarly, but would require the specification of the joint distribution of these effects, and the replacement of each of the one-dimensional integrals appearing in equation (8) by a mul tiple integral. The evaluation of such multiple integrals would of course make the computations even more demanding. 7.3 Inclusion of Covariates In some applications-?although not the application which mo tivated this study?there may be covariate information avail able that could help to explain observed behavior, e.g., dietary differences, or whether subjects are male or female. The im portant question then to be answered is whether such co variate information can efficiently be incorporated into the likelihood computation. The building blocks of the likelihood are the transition probabilities and initial distribution of the latent process, and the probabilities it\ and n2 of the behavior of interest in the two states. Any of these probabilities can be allowed to depend on covariates without greatly complicating the likelihood computation. If we wished to introduce a (possibly time-dependent) co variate yt into the probabilities 7T*, here denoted 7rt(yt), we could take logit n\(yt) to be a\ + a^yt, and similarly logit 7r2(yt). The likelihood evaluation would then present no new challenges, although the extra parameters would of course tend to slow down the optimization. If the covariate yt were instead thought to affect the transition probabilities, we could define logit jn(nt, yt) to be a() + a\nt + a2yt, and similarly logit 722K, yt). 8. Other Modifications or Extensions Other potentially useful extensions are to increase the number of latent states above two, and to change the nature of the state-dependent distribution, e.g., to allow for more than two behavior categories or for a continuous behavior variable. 8.1 Increasing the Number of States If more than two latent states are required in the model, this can be accommodated, e.g., by using at equation (2) a higher-dimensional analogue of the logit transform. Any such increase potentially brings with it a large increase in the num ber of parameters, however; if there are m states, there are m2 ? m transition probabilities to be specified, and it might be necessary to impose some structure on the transition prob abilities in order to reduce the number of parameters. 8.2 Changing the Nature of the State-Dependent Distribution In this work the observations are binary, and therefore so are the state-dependent distributions, i.e., the conditional dis tributions of an observation given the underlying state. But there might well be more than two behavior categories in some series of observations, or the observations might be con tinuous. That would require the use, in the likelihood com putation, of a different kind of distribution from the binary distribution used here. That is a simple matter, and indeed that flexibility at observation level is one of the advantages of HMMs or similar models; almost any kind of data can be accommodated at observation level without complicating the likelihood computation. But because here the observations feed back into the nutrient level Nt, the feedback mechanism would need to be correspondingly modified. 9. Application to Caterpillar Feeding Behavior 9.1 Data Description and Preliminary Analysis The model was applied to sequences of observations of eight final-instar Helicoverpa armigera caterpillars, collected in an experiment designed to quantify developmental changes in the pattern of feeding (Raubenheimer and Barton Browne, 2000). The caterpillars were observed continually for 1132 minutes, during which time they were scanned at 1-minute intervals and scored as either feeding or not feeding. In order to</page><page sequence="6">812 Biometrics, September 2008 isolate developmental changes from environmental effects, in dividually housed caterpillars were fed semisynthetic foods of homogeneous and constant nutrient composition, and the recordings were made under conditions of constant tem perature and lighting. The caterpillars were derived from a laboratory culture, and so had similar ancestry and de velopmental histories. Figure 1 in Section 1 displayed the data. Some remarks can immediately be made. Firstly, despite the uniform conditions of the experiment, there is consider able between-subject variation, both as regards the density of feeding events and the apparent pattern thereof. The run lengths of the feeding events differ between subjects; see, e.g., those of subjects 5 and 6. A striking feature is that subject 8 began feeding only 6.2 hours after the start of the experi ment. Closer examination of the original recordings made on this subject revealed that it was initially eating its exuvium (molted skin), a behavior which has been demonstrated to be of nutritional significance (Mira, 2000). However, because the nutrient composition of the exuvium is very different from that of the synthetic foods, in what follows only subjects 1 7 were included in the analysis. Also noticeable is the fact that subject 3 stopped feeding more than 2 hours before the end of the experiment. This anomaly became apparent in the model-checking exercise described below. 9.2 Parameter Estimates and Model Checking The first step in the model-fitting process was to fit a model separately to each of the seven subjects, i.e., to estimate for each one the nine parameters ao, ol\, Pq, Pi, tti, tt2, A, no, and &lt;5i. In all cases &lt;$i was estimated as zero, i.e., the sub ject started in state 2. A convincing explanation for this is that, until the post molt skin has hardened, insects cannot use their mouthparts and so behave as if they were sated. In what follows we shall take it that all subjects start in state 2, and shall not further treat ^ as a parameter requiring estimation. Table 1 displays inter alia the parameter estimates for each of subjects 1-7, and the corresponding values of minus the log-likelihood. We now use subject 1 as illustrative. Figure 3 displays the observed feeding behavior, the underlying motivational state sequence inferred by means of the Viterbi algorithm, and the nutrient level. There is close correspondence between the series of feeding bouts and the inferred states. However, feeding bouts were interspersed with brief periods of nonfeed ing, which did not break the continuity of the inferred hun gry state. Similarly, occasional feeding occurred in some cases during the inferred sated state. The model thus succeeded in the aim of delimiting states according to the probability distributions of behaviors, rather than the occurrence of behaviors per se. The nutrient level for subject 1 ranges from about 0.1 to 0.5; for some other subjects the lower bound can reach zero. The parameter A determines the (exponential) rate at which the nutrient level diminishes in the absence of feeding. The associated half-life is given by log (0.5)/log (1 ? A). Thus the estimated half-life for subject 1 is approximately 25 minutes. Figure 4 displays the transition probabilities for subject 1 as a function of nutrient level. As expected, 7n decreases with increasing nutrient level, and 722 increases, and this is true for all seven subjects. Note also that 7r2, the estimated probability of feeding when sated, is close to zero. In fact 7r2 is less than 0.01 for all seven subjects, and less than 0.001 for two of them. The fact that these estimates are so close to the boundary of the parameter space has implications when one attempts to estimate standard errors. The standard errors of the parameters for subject 1 were estimated by the parametric bootstrap, but are not included here. In applying the model-checking technique of Section 5 to subjects 1-7, we were unable to reject H0 in six cases. Using the chi-squared approximation to the distribution of deviance differences, we obtained p-values ranging from 0.30 to 0.82 in these six cases. In the case of subject 3 we concluded that the model fitted is unsatisfactory (p = 0.051). Except for subject 3, Akaike's information criterion (AIC) would also select the hypothesized model. Examining subject 3 more closely, we note that some of the parameter values are atypical: e.g., the probability of feed ing when hungry is atypically low, only about 0.8. The data for subject 3 revealed the unusual feature that there were no feeding events after about time 1000, i.e., no feeding for more than 2 hours. This is clearly inconsistent with the earlier be havior of this subject. This conclusion was reinforced by a plot of deviance residuals for subjects 1 and 3, along with spline smooths of these. Table 1 Parameter estimates and log-likelihood: individual models for subjects 1-7, and mixed model with six common parameters and random effect for A Subject ct0 c\\ po Pi it\ 7r2 A n0 ?logL 1 5.807 -11.257 2.283 2.301 0.936 0.000 0.027 0.240 331.991 2 2.231 -5.284 -0.263 21.019 0.913 0.009 0.032 0.150 347.952 3 4.762 -10.124 2.900 15.912 0.794 0.004 0.080 0.740 225.166 4 2.274 -7.779 1.294 16.285 0.900 0.000 0.056 0.018 298.678 5 3.135 -7.271 1.682 10.908 0.911 0.006 0.097 1.000 332.510 6 3.080 -5.231 1.374 13.970 0.880 0.001 0.043 0.246 291.004 7 3.888 -9.057 0.617 13.341 0.976 0.003 0.054 0.375 315.188 2142.488 Mixed 2.735 -5.328 2.127 7.083 0.919 0.003 ? = 0.055 2230.090 model &lt;7 = 0.051</page><page sequence="7">Latent-State Model for Animal Behavior 813 1.0 4 x M,.. -.- -.-"'.' '- '"-- .-.'-- .-. .-.~?.--". .--.?-- --??. '-.~?--.--. . . feeding events: in m.. . . . . _ m._?mn...?,.. .,. . . i . iui.i 1.11 m 1 ii estimated to be in hungry state: q q _.1..1.j..1....1.1.IJ.I.I.I. .HJ!.n.1.1.1.1. ill. ...!.n.I 1 11 in.li. I 0.6- t^.m 0.0 -. ( ! ! | ! ! 0 200 400 600 800 1000 time (minutes) Figure 3. Feeding behavior, inferred motivational state, and nutrient level for subject 1. ?-9 " ' ~""^ y^^.I 07 - 1 j?trie"~/ -%=^^m I ?6" \^ ^ = 0.000 1 0.4- A / \ ^ = 0.027 ai_ .^^.|.z^\a.^K^+.I.I ?I-1-1-1-1-1-1-1-1-1-1? 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 nutrient level Figure 4. Transition probabilities for subject 1. 9.3 Runlength Distributions One of the key questions of biological interest that motivated this work was to assess the extent to which runs of feed ing events differ from runs in motivational state 1 (hungry), and similarly, runs of nonfeeding events from runs in state 2 (sated). The runlength statistics, for subjects 1 to 7, of the observed feeding sequences and the estimated sequences of the state "hungry," behave as expected. There are fewer runs for the latter (23% fewer on average) and the mean runlength is larger (45% on average), as is the standard deviation (20% on average). In a HMM the distributions of the runlengths in the two motivational states are geometric; that cannot be assumed to be the case here. Monte Carlo methods were used here to estimate the runlength distributions under the model. For each of the seven subjects, a series of length 1 million was generated from the relevant fitted model, and the distri bution of each of the four types of run estimated. The distri bution of the feeding runlength differs substantially from that in the hungry state in the expected way. The probability of a runlength being one is almost twice as great for feeding runs as it is for hungry runs. The mean and median for the latter</page><page sequence="8">814 Biometrics, September 2008 Table 2 Summary of models fitted jointly to the seven subjects. The number of parameters estimated is denoted by k. AIC selects the model with 7Ti and tt2 common, and BIC the mixed model, which treats X as a random effect. Model no. Description -log L k AIC BIC 1 No parameters common 2142.488 56 4396.976 4787.725 2 7Ti and ix2 common 2153.669 44 4395.338 4702.355 3 Six parameters common 2219.229 20 4478.458 4618.011 4 Seven parameters common 2265.510 14 4559.020 4656.707 5 Mixed model (one random effect) 2230.090 15 4490.180 4594.845 are greater. The duration of a hungry spell is often longer than one would conclude if one took feeding and hunger as synonymous. Unlike the distribution of the sated runs, that of the non feeding runs has a marked peak at 1. This peak is attributable to hungry subjects often interrupting feeding for one time unit, rather than subjects being sated for a single time unit. 9.4 Joint Models for Seven Subjects Five different models were considered for the observations on all seven subjects: (1) A model with no parameters common to the subjects; (2) A model with only 7Ti and -k2 common to the subjects; (3) A model with the six parameters a0, ai, Po, Pi, 7Ti, and 7r2 common to the subjects; (4) A model with the seven parameters a0, oli, Po, Pi, 7Ti, 7r2, and A common to the subjects; and (5) A model which incorporated a random effect for the parameter A and common values for the six param eters a0, ai, Po, Pi, tti, and n2. The model used for A, which is bounded by 0 and 1, was a normal dis tribution (mean (i and variance a2) restricted to the interval [0, 1]. This distribution was suggested by a kernel density estimate of A-estimates from model 3. In all cases the subjects were taken to be independent of each other and to have started in state 2. As there were only seven subjects, models with more than one random effect were not fitted because one cannot expect to identify a joint distribu tion of random effects from only seven observations. Table 2 displays in each case the log-likelihood attained, the number of parameters estimated, AIC, and the Bayesian information criterion (BIC). The model selected by AIC is model 2, which has 7Ti = 0.913 and tt2 = 0.002 for all sub jects. The model selected by BIC is model 5, the mixed model. The parameter estimates for that model appear in Table 1, along with those of the seven individual models. An interest ing point to note is how much better the mixed model is than model 4. These two models differ only in their treatment of A; model 4 uses a single fixed value, and the mixed model uses a random effect. 10. Discussion The application of HMMs to animal behavior (MacDonald and Raubenheimer, 1995) has hitherto been limited to be haviors whose consequences do not readily alter motivational state. The model here presented provides an extension that can allow for the important class of behaviors that are feedback regulated (Toates, 1986). In the present example, the states "hungry" and "sated" as used above are of course an artifact of the model, and do not necessarily correspond to the ac cepted meanings of those terms. A different way of using the model described here would be to define state 1 as the state in which the probability of feeding is (say) 0.9, and similarly state 2 as that with probability (say) 0.1. Irrespective of how the states are defined, the important point is that their delineation provides an objective means for exploring the physiological and environmental factors that determine the transitions between activities by animals. Such transitions are believed to play an important role in the evo lution of behavior (Sibly and McFarland, 1976), and the un derstanding of their causal factors is considered a central goal in the study of animal behavior (Dewsbury, 1992). The exponential filter?equation (1)?used here for the nutrient level seems plausible. It is, however, by no means the only possibility, and no fundamental difficulty arises if this component of the model is changed. Ideally, the filter should reflect the manner in which feeding affects the mo tivational process, which might be expected to vary with a range of factors such as the nutrient composition of the foods, the recent feeding history of the animals, their state of health, etc. (Simpson, 1990). In our example, the same fil ter applied reasonably well across the experimental animals, probably because they were standardized for age and develop mental history, and were studied in uniform laboratory con ditions. Interestingly, however, a study of foraging by wild grasshoppers revealed that the patterns of feeding were no less regular than those observed in tightly controlled labora tory conditions (Raubenheimer and Bernays, 1993), suggest ing that there might be some uniformity of such character istics as the decay function even in more complex ecological conditions. The models introduced here, or variants thereof, are po tentially of much wider applicability than to feeding behavior. Essentially unchanged, they may be applied to any binary be havior thought to have a feedback effect on some underlying state, and with some modification of the feedback mechanism the restriction to binary observations could be removed. In overview, by allowing for feedback regulation we have ex tended the application of HMMs or similar models to a wider</page><page sequence="9">Latent-State Model for Animal Behavior 815 range of applications in the study of behavior. We believe that these models hold potential for exploring the relation ships among observed behaviors, the activities within which they occur, and the underlying causal factors. Accordingly, they provide a step towards a much-needed objective science of motivation (Kennedy, 1992). 11. Supplementary Materials The Web Appendix referenced in Section 3.2, and the cater pillar feeding data discussed in Section 9, are available un der the Paper Information link at the Biometrics website http://www.biometrics.tibs.org. Acknowledgements We thank Lindsay Barton Browne for allowing us to use the data, and Christian Glaser for his help with the programming of the mixed-effects model. We thank the referee, associate editor, and editor for their constructive criticism. References Altman, R. M. (2007). Mixed hidden Markov models: An ex tension of the hidden Markov model to the longitudinal data setting. Journal of the American Statistical Associ ation 102, 201-210. Altman, R. M. and Petkau, J. A. (2005). Application of hid den Markov models to multiple sclerosis lesion count data. Statistics in Medicine 24, 2335-2344. Barton Browne, L. (1993). Physiologically induced changes in resource-oriented behaviour. Annual Review of Entomol ogy 38, 1-25. Bulla, J. and Berzel, A. (2007). Computational issues in pa rameter estimation for stationary hidden Markov mod els. Computational Statistics, to appear. Dewsbury, D. A. (1992). On the problems studied in ethology, comparative psychology, and animal behaviour. Ethology 92, 89-107. Forney, G. D. (1973). The Viterbi algorithm. Proceedings of the IEEE 61, 268-278. Hastie, T. J. and Tibshirani, R. J. (1990). Generalized Additive Models. London: Chapman &amp; Hall. Juang, B. H. and Rabiner, L. R. (1991). Hidden Markov models for speech recognition. Technometrics 33, 251 272. Kennedy, J. S. (1992). The New Anthropomorphism. Cam bridge, U.K.: Cambridge University Press. Lange, K. (2002). Mathematical and Statistical Methods for Ge netic Analysis, 2nd edition. New York: Springer. MacDonald, I. L. and Raubenheimer, D. (1995). Hidden Markov models and animal behaviour. Biometrical Jour nal 37, 701-712. MacDonald, I. L. and Zucchini, W. (2006). Markov models, hidden. In Encyclopedia of Statistical Sciences, 2nd edi tion, Volume 7, S. Kotz, N. Balakrishnan, C. B. Read, B. Vidakovic, and N. L. Johnson (eds), 4531-4535. Hobo ken, New Jersey: John Wiley &amp;; Sons. McFarland, D. (1999). Animal Behaviour: Psychobiology, Ethology and Evolution, 3rd edition. Harlow: Longman Scientific and Technical. Mira, A. (2000). Exuviae eating: A nitrogen meal? Journal of Insect Physiology 46, 605-610. Raubenheimer. D. and Barton Browne, L. (2000). Develop mental changes in the patterns of feeding in fourth- and fifth-instar Helicoverpa armigera caterpillars. Physiologi cal Entomology 25, 390-399. Raubenheimer, D. and Bernays, E. A. (1993). Patterns of feeding in the polyphagous grasshopper Taeniopoda eques: A field study. Animal Behaviour 45, 153-167. R Development Core Team. (2007). R: A language and en vironment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. ISBN 3-900051 07-0, URL http://www.R-project.org. Sibly, R. M. and McFarland, D. (1976). On the fitness of be haviour sequences. American Naturalist 110, 601-617. Simpson, S. J. (1990). The pattern of feeding. In A Biology of Grasshoppers, R. F. Chapman and T. Joern (eds), 73 103. New York: John Wiley &amp; Sons. Simpson, S.J. and Raubenheimer, D. (1993). The central role of the haemolymph in the regulation of nutrient intake in insects. Physiological Entomology 18, 395-403. Toates, F. (1986). Motivational Systems. Cambridge, U.K.: Cambridge University Press. Viterbi, A. J. (1967). Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory 13, 260-269. Received July 2006. Revised August 2007. Accepted September 2007.</page></plain_text>