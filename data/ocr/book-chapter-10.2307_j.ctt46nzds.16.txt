<plain_text> <page sequence="1"> Innovative Approaches  12/5/03  2:50 PM  Page 217 PART THRE E Pedagogical Partnerships  </page> <page sequence="2"> Innovative Approaches  12/5/03  2:50 PM  Page 218 </page> <page sequence="3"> Innovative Approaches  12/5/03  2:50 PM  Page 219 13 A  P EDAGOG ICAL  F RAMEWORK  FOR FACULTY- S TUDENT  R E S EARCH  AND PUBL IC  S E RV IC E  IN  T ECHN ICAL  COM- MUN ICAT ION Brad Mehlenbacher R. Stanley Dicks It has become a truism that students learn substantially more by aug- menting their traditional education with collaboration and hands-on activities, particularly with activities that they feel ownership for or that they perceive to have real-world relevance (Honebein, Duffy, and Fishman 1993; Rogoff 1990; Savery1998). Of the “seven principles of good practice in undergraduate education” listed by Chickering and col- leagues (1987, 1998), for example, the majority involve motivational and “social” and task-based dimensions for learning. Thus, effective instruc- tion encourages student-faculty contact, cooperation among students, and active learning; gives prompt feedback; emphasizes time on task; communicates high expectations; and respects diverse talents and ways of learning. It therefore seems reasonable to argue that students would benefit from working with faculty in cutting-edge research because it may pro- vide them with their first opportunities to make real contributions to the professional literature in coauthored publications with faculty. Moreover, getting students directly involved in real research gives them a richer understanding of their chosen discipline and involves them early on in their careers as potential contributors to the field. In this chapter, we describe a unique collaboration among university administrators, faculty, students, and constituents that originated in a proposed project to conduct usability testing for a part of the universi- ty’s Web site. From October 1998 to June 1999, the authors were charged by the North Carolina State Extension, Research, and Outreach Office to usability test Ask NC State, the extension branch of the NC State Web site and a creative presentation of the numerous online resources </page> <page sequence="4"> Innovative Approaches  12/5/03  2:50 PM  Page 220 220 INNOVAT IV E  A P PROACHES available for potential extension audiences. The problem driving our initial conception of the project proposal was how to integrate our teaching, research, and service goals into a multidimensional, collabo- rative effort that would produce benefits in each domain and for the var- ious audiences (or shareholders) involved. The solution was to have stu- dents across three classes, acting as research apprentices, help us per- form usability tests with various constituencies of the university who might have reason to access the Ask NC State Web site.  In this chapter, we describe how we achieved the following results:  1. Provide students with access to cross-disciplinary perspectives in cognitive psychology, human factors, computer science, industrial engineering, and technical communication 2. Provide students with hands-on experience performing usability testing on a significant Web site 3. Involve the university’s Extension, Research, and Outreach program in the pedagogical goals of graduate and undergraduate instruction 4. Perform research related to the usability of Web-based materials 5. Contribute to improvement of a part of the university’s Web site 6. Involve students in analyzing, compiling, and presenting research infor- mation for an authentic audience  Our ultimate hope was that the project would serve as a blueprint for future research projects for several reasons. The grant allowed faculty-investigators to integrate their extension, teaching, and research goals into a single project; supported both undergraduate and graduate students’ efforts to work close- ly with faculty in the evaluation and improvement of an official NC State function; encouraged the involvement of potential extension audiences, serving as test participants, in the development of materials designed to support their information needs related to NC State; gave credibility to the importance of usability testing and evaluation as an integral part of the creation of materials for use by extension audiences; and served as a possible model for future collaborations among Extension, Research, and Outreach, faculty, students, and, ultimately, industry partners.  Because the project began prior to the semester, collaborating faculty were able to design and implement usability plans for working with the </page> <page sequence="5"> Innovative Approaches  12/5/03  2:50 PM  Page 221 A Pedagogical Framework. . . 221 students to test the targeted Web site and to carefully design their syllabi to integrate the extension project. P EDAGOG ICAL / THEORET ICAL  F RAMEWORK Tebeaux (1989) and Zimmerman and Long (1993) have argued per- suasively that teaching students how to collect, analyze, and report data to various audiences and with different purposes should be a chief ped- agogical goal for technical communication instructors. Though many technical communication curricula have certainly adapted this perspective, many educational theorists still advocate dra- matic educational reform. Koschmann, Kelson, Feltovich, and Barrows (1996), for example, cite dozens of studies, revealing that “existing edu- cational systems are producing individuals who fail to develop a valid, robust knowledge base; who have difficulty reasoning with and applying knowledge; and who lack the ability to reflect upon their performance and continue the process of learning” (85). Further, our experiences have taught us that integrating experimental approaches into teaching can be extraordinarily difficult, or worse, simply fail to find the institu- tional support they require (Mehlenbacher 1997). The project that framed our pedagogical activities across three cours- es involved our university’s electronic “front door” for Web visitors with questions related to outreach, extension, and continuing education. The university’s extension site, Ask NC State, is poised to play a critical role in disseminating scientific and technical information generated and housed across various colleges, libraries, and extension services. And this site had previously documented, over a series of eight chancel- lor’s retreats in 1997–98, that NC State’s extension audiences, in their various capacities as employees, citizens, parents, political officials, and educators, were only partially aware of the immense resources that their land-grant university had to offer them.  Both authors proposed to make usability testing Ask NC State a major class project for students from various disciplines enrolled in three dif- ferent courses: 1. English 583: a special studies graduate course on usability studies (sixteen students) 2. English 517: a graduate course on advanced technical communication (fif- teen students) 3. English 421: a Web-based undergraduate course on computer documen- tation design (twelve students)  </page> <page sequence="6"> Innovative Approaches  12/5/03  2:50 PM  Page 222 222 INNOVAT IV E  A P PROACHES Our most immediate goal was to teach students and get them involved in the entire usability testing process of Ask NC State by collect- ing user data about site usage from representative extension audiences, ideally from their hometowns in North Carolina and elsewhere. In this respect, students would act as apprentice usability testers of the Web site, soliciting possible users, designing exploratory tasks, and applying usability-testing data-collection methods in actual contexts of use. Among other usability testing procedures, students were taught to man- age small focus groups, to collect talk-aloud audio- and video-recordings of user-interface interactions, to develop matrices for heuristically eval- uating Web-based materials, and to analyze and report usability findings and recommendations for an authentic audience, which in this case included sponsors at the Office of Extension, Research, and Outreach at NC State.  Across the three classes, students were taught to prepare test plans, to test materials, to identify and obtain appropriate test subjects, to per- form tests, to analyze test results, to generate a test report, to collaborate with others who tested similar subjects, and to prepare a presentation of the results (Rubin 1994). Benefits of the Ask NC State project were to include compiling rec- ommendations and suggestions for Ask NC State improvements and revi- sions, providing students with opportunities to support faculty research or to conduct their own research, and highlighting the innovative Ask NC State to students, representative extension users, and Research Triangle–based companies interested in usability practice and theory at NC State. The primary goals of usability testing the Web site, therefore, would be 1. to empirically validate that the Web site’s formal features seamlessly meet the needs and support the tasks of its users, 2. to systematically obtain and incorporate user feedback into the Web site’s development process, and 3. to report the project findings at a student research symposium held at NC State. The benefits for the principal faculty members included support from the university extension office to purchase equipment, materials, and supplies necessary to conduct usability testing and some summer research released time for compiling and presenting usability perspectives </page> <page sequence="7"> Innovative Approaches  12/5/03  2:50 PM  Page 223 A Pedagogical Framework. . . 223 findings to other units on campus. Importantly, the faculty were not aim- ing to “profit” from the support that the project received as much as they were aiming to set in place a strong foundation for providing solid research, for supporting some flexibility in instructional decisions, and for sharing usability principles with the large and diversified groups developing Web materials at NC State. THE  E X T ENS ION  CHAL L ENGE :  WHY  ASK  NC  STATE  V IA  THE  WEB ? The NC State strategic plan stresses the application of the university’s strengths in “graduate education, research, and public service, while strengthening our core mission of undergraduate education” and lists as sites where these core strengths can be applied “classrooms, farms, industries, laboratories, and conference rooms.”1 We contend that Web- based materials designed for extension audiences provide an additional forum that can serve to integrate faculty research, undergraduate and graduate teaching, and outreach and extension goals. As North Carolina State University’s Internet “front door” for extension audi- ences seeking access to NC State’s ten colleges, libraries, and numerous extension services, Ask NC State is one of twenty home page links acces- sible from the majority of NC State informational pages.2 During the week of September 13–20, 1998, Ask NC State logged over one million hits per day.3 Clearly, the need for an evaluation of the Web site’s design and usefulness is high. Moreover, audiences for university Web materials are growing expo- nentially. The NSF Indicators Report on Science and Technology, Public Attitudes and Public Understanding (1998) indicates that Web information “is likely to become a major source of reference-type information in the decades ahead, as access continues to expand.”4 And a recent study by the Angus Reid Group, Toronto, Canada, estimates that the number of Internet users worldwide will increase from the 300 million today to one billion by 2005.5 We argue that the real and potential audience for Ask NC State and other Web-based extension materials can only expand dra- matically as well. In addition to the inevitable growth of information-seeking audiences on the Web, we contend that the usability testing and design of large institutional Web sites have received minimal attention in the research literature. Because Ask NC State relies on information from more than ten colleges and institutional offices, even basic goals of consistency, </page> <page sequence="8"> Innovative Approaches  12/5/03  2:50 PM  Page 224 224 INNOVAT IV E  A P PROACHES terminology use, and searching become difficult to achieve. In one of the few manuscripts devoted to testing Web sites developed across “agen- cies, divisions, and departments,” Marchionini and Hert (1997) identify two particular challenges: The first challenge is that no single person creates such a site—these sites emerge across different departments and eventually are merged under one or a few “home page(s),” but no single individual has full authority over a site or understands everything in the site. . . . The second challenge in large insti- tutional Web sites is an inertia effect. Web sites that get tens or hundreds of thousands of hits per day build a constituency that has invested time in learn- ing navigational and general usage routines and any change will invariably bring comments, requests, and complaints that must be processed in some way, which incurs costs. (1)  Still, we felt that the real-world complexity of the project, combined with the constraints posed by multiple-audience involvement and expec- tations, offered a rich pedagogical opportunity. Bellotti, Buckingham Shum, MacLean, and Hammond (1995) support this perspective in terms of research in human-computer interaction (HCI), emphasizing that theoretically framed HCI is only achievable when “end-user require- ments of the design practitioners are properly understood, and the value of such techniques can be demonstrated” (435). Our immediate goal, therefore, was to explicate the problems we would face in manag- ing this project and to identify task-oriented approaches to addressing them in the space of little more than one semester. THE  P EDAGOG ICAL  CHAL L ENGE :  I S  THER E  A  “ R EA L -WORLD”  IN TH I S  C LA S SROOM? Boiarsky and Dobberstein (1998) recommend that documentation-writ- ing classes integrate authentic writing tasks into their syllabi, reminding us that such “assignments are not the usual writing classroom exercises, created ‘as if’ there were an audience besides the instructor. These assignments require the application of the problem-solving skills . . . in the planning, drafting, designing, testing, and revising of documents that have genuine utility for a broad spectrum of computer users” (45). But finding ideal sites for research where student activities can con- tribute to professional developments in the field is not always obvious. Of course, the appeal of using “textbook” assignments is that they are </page> <page sequence="9"> Innovative Approaches  12/5/03  2:50 PM  Page 225 A Pedagogical Framework. . . 225 often connected explicitly to the materials covered in class and have well-defined parameters and established standards for evaluation. The solution to the problem posed is frequently readily available or support- ed by preexisting models. In our experience, attempting to integrate ill- structured “projects” into either undergraduate or graduate classes is exceedingly difficult and something that many instructors learn to avoid. Authentic assignments are difficult to incorporate because facul- ty find their ill-structuredness difficult to structure, because real-world problems are often messy and therefore appear unfocused, because research does not always produce tidy results easily summarized over the course of a single semester, and because complex problems are more difficult to introduce than well-defined problems (Adams 1993; Øgrim 1991). But the professional and technical domains that many of our gradu- ating students will enter demand flexibility and resourcefulness on the part of their employees (Denning 1992), and we view this demand as an important reason to mix theory-driven approaches with problem-based or hands-on learning. A major benefit of this instructional approach, according to Kaasbøll (1998), is that  students work on real-life problems or constructed problems that mimic the complexity of the practical world. In such situations, the students have to think critically through all the information available to sort out the relevant material. When students have the opportunity to define their own problems, they become more involved in their work, and this involvement increases motivation for learning. Because students are assumed to obtain a more pro- found understanding of the subject area, assessment of problem-based learn- ing should focus more on the students’ skills in handling an ill-structured sit- uation than on recalling the textbook. (104) One area in need of significant research and elaboration is usability testing of Web-based materials, an endeavor that technical communica- tion students are already increasingly involved in as we rush to upload and invent Web-based materials that support our instructional, marketing, information, and administrative organizational needs for distributed documents and support materials (Shneiderman 1998). Another significant challenge facing the faculty-investigators was the importance of maintaining the intended focus of the three distinct graduate and undergraduate classes involved in the extension project while </page> <page sequence="10"> Innovative Approaches  12/5/03  2:50 PM  Page 226 226 INNOVAT IV E  A P PROACHES generating useful data about the Ask NC State Web site. This mainte- nance required ongoing and creative cooperation and flexibility among the instructors. Both English 421 (Computer Documentation Design) and English 517 (Advanced Technical Communication) included usability testing assignments, though neither traditionally required a for- mal, written report summarizing the results. Students were supposed to learn about the importance of audience feedback in the design of both software and documentation, even though they were not the emphasis of either course. In English 421, students contributed to the class listserv (because the class was completely Web based) and exchanged findings and observations about the Web site via email and attachments. In English 517, students presented their findings during formal oral pre- sentations; students in English 583,however, focused entirely on usabili- ty testing issues, and, therefore, the bulk of the data reported to the Extension, Research, and Outreach Office was generated as part of that class. English 583 students performed various usability tests with four main audience groups using Ask NC State, including NC State cooperative extension agents in urban counties, agents in rural counties, job-specif- ic users who can potentially benefit from the Web site in their daily jobs, and members of the general public in North Carolina. The students performed sixteen tests with nineteen subjects, employ- ing a variety of usability testing methodologies, including contextual inquiry, surveys, questionnaires, interviews, field observations, perform- ance testing, and think-aloud protocols. Usability results discussed in English 421 and English 517 supported the findings of the usability test- ing class, in addition to adding several new job-specific users to the types of audiences analyzed. THE  R E S EARCH  CHAL L ENGE :  WHAT  I S  A  “USABL E ” IN S T I TUT IONAL  WEB  S I T E ? We believe that instruction in usability testing provides an opportunity to achieve several highly desirable goals simultaneously for a technical com- munication curriculum. Because usability testing involves, indeed requires, both theoretical and practical considerations, it provides an excellent forum for accomplishing several practical, pedagogical, and professional outcomes in a single, well-coordinated effort. We also maintain, as Schriver (1997) argues, “usability testing routinely reveals important problems that document designers, even expert ones, may fail to detect” (473).  </page> <page sequence="11"> Innovative Approaches  12/5/03  2:50 PM  Page 227 A Pedagogical Framework. . . 227 Though existing research on usability testing highlights improve- ments to software products (Landauer 1995; Nielsen 1997), few researchers have applied usability testing to large institutional Web sites designed to serve broad extension-based populations. Although usabili- ty testing on traditional software applications dates back to the early 1980s (Shneiderman 1998), the usability of Web sites has only recently gained attention at conferences emphasizing usability issues, interaction design, and human-computer interaction (cf. Marchionini and Hert 1997). Research on the results and implications of usability testing on Web sites is therefore still in its early stages. A Web site can be function- ally sophisticated and aesthetically appealing without its designers understanding how the Web site is accessed in the context of use and whether the Web site is usable. Functionally, the Web site can be techni- cally innovative and can contain many more features than even required by its users. Aesthetically, the Web site can be visually attractive and graphically creative. However, none of these criteria for success neces- sarily help Web designers understand what the skills, motivations, and previous experiences of its users might be, what they are attempting to accomplish while accessing the site, how motivated to accomplish the established goals they are, or even in what types of environments they experience user interactions (speeds, machines, system configurations, for example). Linking user knowledge, preferences, and behaviors to desirable Web site “attributes” (such as consistency and layout, naviga- tional support, visibility of features, and relationship with real-world tasks) is a critical goal for all usability performance testing. In addition, Landauer (1995) and Nielsen (1997) have emphasized the substantial benefits of employing even informal usability testing in the process of designing and evaluating software programs in general, and we antici- pated that the same benefits would be brought to the Ask NC State Web site as well. A Usability Focus: How Can Multiple Methods Resist Interdisciplinary Solutions? Implicit in our methodological outlook was the goal of extending class- room-based practices in technical communication beyond the rhetorical range they usually cover. Students, faculty, and administrators learned that contemporary technical communicators can contribute significant- ly to online design efforts by incorporating techniques from human factors </page> <page sequence="12"> Innovative Approaches  12/5/03  2:50 PM  Page 228 228 INNOVAT IV E  A P PROACHES and industrial engineering into our repertoire of strategies for design- ing usable information. And, importantly, in the wired age, much of the information we prepare is not designed for traditional media but, rather, for online, Web-based distribution. Learning how to analyze the strengths and weaknesses of our online creations is becoming as impor- tant a technical communication skill as writing paragraphs or designing usable documents. To expand the scope of what technical communica- tion is and to instruct graduate and undergraduate students in the prin- ciples and practices necessary for that expansion, innovative pedagogi- cal approaches and collaborations are essential. Indeed, the very nature of the field of usability testing makes it a cross-disciplinary enterprise, involving cognitive psychology, software engineering, technical communication, human factors, and sociology. Usability testing Ask NC State was a project that, ultimately, required rela- tionships among parties that do not always interact: faculty, students, university Extension, Research, and Outreach personnel, Learning Technology Services (the campus unit charged with moving faculty courses online), the digital library initiatives department (the unit charged with developing online applications for the campus libraries), and information technology (the unit responsible for supporting dis- tributed computing on campus). Moreover, human factors–oriented fac- ulty on campus were housed in disparate departments—from computer science, graphic design, and industrial engineering to technical com- munication, psychology/ergonomics, and mathematics, science, and technology education. And student chapters of the Human Factors and Ergonomics Society and the Society for Technical Communication, along with their larger regional and national counterparts, represented natural opportunities for further relationships. In this respect, we encouraged students to seek out and share any disciplinary perspectives that might enhance their understanding of the challenge facing any university community attempting to communicate and share informa- tion with broader audiences and communities. Pedagogical Activities As technical communication faculty committed to developing theory that enhances our intellectual, professional, and disciplinary development, we are also instructors who value and encourage student application of broader principles to real-world information design situations. Schön </page> <page sequence="13"> Innovative Approaches  12/5/03  2:50 PM  Page 229 A Pedagogical Framework. . . 229 (1987) describes an instructional perspective that celebrates reflective practice as “a way of knowing” and argues that learning all forms of professional artistry depends, at least in part, on condi- tions similar to those created in the studios and conservatories: freedom to learn by doing in a setting relatively low in risk, with access to coaches who initiate students into the “traditions of the calling” and help them, by “the right kind of telling,” to see on their behalf and in their own way what they need most to see. We ought, then, to study the experience of learning by doing and the artistry of good coaching. (17) Students were charged with identifying design shortcomings of the Ask NC State Web site but were not responsible for seeing that an effec- tive redesign occurred or succeeded; that was our authentic goal, and, therefore, the usability evaluation and testing methods that we shared with them were methods that we would have practiced outside the instructional context of the three classes. Various Purposes in Context The students across the three classes were therefore encouraged to per- form tests using a variety of test methods. This encouragement allowed them to tailor the method used to the user group they were testing and to the environment in which they were doing so. It also ensured a large body of data would be collected using diverse usability methods. Agreement in the results of these various methods tends to more strong- ly support those results (Rubin 1994). The stated purposes for the tests varied, depending on which audi- ence group a student was testing and what particular types of data the student was seeking. In general, the tests were designed to discover whether people knew about the existence of the Ask NC State Web site, whether they could find it on the main NC State Web site, whether they could navigate through it successfully to find specific information, whether they could successfully search it for particular types of infor- mation, and whether they understood its navigational structure. The tests were constructed to provide both performance data and preference data. Performance data show whether users can perform specific tasks with the site and how well and quickly they do so. Preference data indicates users’ attitudes toward the site and whether they are likely to return to it. </page> <page sequence="14"> Innovative Approaches  12/5/03  2:50 PM  Page 230 230 INNOVAT IV E  A P PROACHES A Menu of Methodological Choices Most of the tests included several usability testing methods, with thirteen of the sixteen tests including some type of empirical, performance- based testing to gather performance data. These tests typically included questionnaires and interviews to elicit further data concerning both per- formance and preferences. The following list details the methods used. For those not familiar with usability testing conventions, brief definitions for each method are included. Contextual Inquiry. (A form of field interviewing that focuses on the context of a product’s use.) The tester asks detailed questions of users about the product, what they like and do not like about it, what other products they use the product with, problems and errors they experience with the prod- uct, and the entire environment surrounding its use, including not only the physical context but also the social, political, and organizational con- texts. The method provides primarily preference data. Survey. (An unstructured interview conducted remotely.) Unlike a question- naire, the survey is interactive. It is usually conducted by telephone or email. Because it is interactive, the tester can solicit information through open-ended questions. Surveys require more of the tester’s time than do questionnaires, but they can yield more valuable results due to their inter- active, open-ended nature. Provides preference data and self-reported performance data. Questionnaire. (A remote, structured interview done on paper or electronical- ly rather than in person.) The questionnaire has a specific list of questions to which users provide answers. Provides preference data. Interview. (A formal method for gathering data.) Testers prepare a list of questions aimed at providing the type of data needed regarding the usabil- ity of the product. Interviews are valuable for gathering information from users that might not surface during lab-based testing, particularly con- cerning their preferences and attitudes. Provides preference data. Field observation. (One or more visits to the users’ place of work to directly observe them using the product under test.) It affords the opportunity to learn about the users’ on-the-job tasks and to see how they use the prod- uct in their day-to-day activities. It also allows one to learn about their mental maps for the product. Provides preference data. Performance testing. (A method to determine how effectively and efficiently users can complete their desired tasks using the product in question.) Provides quantitative, empirical data. </page> <page sequence="15"> Innovative Approaches  12/5/03  2:50 PM  Page 231 A Pedagogical Framework. . . 231 Think-Aloud Protocol. (A method that has test subjects speak aloud as they interact with the product.) It is employed with usability inquiries and per- formance tests to elicit from the users’ statements related to their under- standing of the product and its mental mapping, problems with the prod- uct interface, and opinions about the usability of the product. Provides primarily preference data, but can supplement and enhance performance data. The tests yielded a large body of data, concerning both user per- formance and preferences. Overall, the results indicated that users had highly positive opinions of the site’s intended purpose, but that they were frustrated by the mechanics of trying to use it successfully. In gen- eral, the results indicated low performance levels in finding the site, understanding its relationship to the NC State extension service, navi- gating within the site, and searching for and finding specific informa- tion. The preference data generally showed that users were confused about the site’s purpose and operation, but that they found information to be very useful once they figured out how to get to it.  The students each completed a detailed usability report explaining the results of their tests. They then worked in groups, with other stu- dents who had tested the same audience, to compile a summary pres- entation of the overall results to be given to the NC State extension service. The composite report was presented in a combined meeting of the three project classes, the NC State student chapter of the Society for Technical Communication, and personnel from the Office of Extension, Research, and Outreach. Further, a written composite report was presented to the extension office (Dicks and Mehlenbacher 1999). Pedagogically, the assignment required students to duplicate condi- tions often encountered in workplace usability testing. They had to perform their tests individually, not uncommon for technical communicators. They had to collaborate with groups to create audience-specific reports and composite reports. They learned about the importance and the social and political implications of how usability information is reported to a client. They further honed their knowledge and skills in dealing with a complex rhetorical situation and in reporting information both orally and in writing.  Their final report concludes with a number of recommendations for improving the Ask NC StateWeb site, including changing the name, mak- ing the site’s purposes and functions clearer, improving navigational </page> <page sequence="16"> Innovative Approaches  12/5/03  2:50 PM  Page 232 232 INNOVAT IV E  A P PROACHES organization, repairing broken and outdated links, and repairing the search engine.  The Development of Usability Principles One of the outcomes of teaching reflective practice is the development of general principles for guiding future research and, ultimately, for folding back into future instructional practice. In this respect, our expe- rience working with students on an authentic problem—the evaluation of an operational university Web site—helped us to further develop our understanding of usability issues related specifically to Web site design. The appendix summarizes usability principles for Web site design that evolved during the course of the project and that we continue to refine and extend. Though not the focus of this chapter, we viewed the devel- opment of these principles to be explicit evidence that teaching, research, and extension activities—frequently separated historically in university environments—can feed into each other in creative and excit- ing ways. Another outcome of the experience was that several students from the classes joined other usability-related efforts on and off campus fol- lowing the semester and, in this way, contributed to design processes as well as product development. One graduate student presented with the authors on instructional usability and student learning at NC State’s 1999 Summer Institute for Distance Learning, and she later joined the university’s Learning Technologies Service as a full-time employee.  IMP L I CAT IONS  AND  NEW  D I R ECT IONS When assessing how successful or unsuccessful any long-term project has been, it is tempting to recount the positive and to de-emphasize the unresolved problems encountered in process. We know that students were excited by the authentic problem-solving situation and that their end-of-semester presentations revealed thoughtful and professional engagement in the overall goals of the project.6 But we are also aware of the proviso about teaching effectiveness that Almstrum et al. (1996) pro- vide: ‘If I’m satisfied and my students are satisfied, have I done a good job?’ Yes, if ‘goodness’ in teaching is simply a matter of mutual satisfaction. No, if ‘good- ness’ has something to do with learning, unless we establish that mutual sat- isfaction is a reliable indicator of learning. </page> <page sequence="17"> Innovative Approaches  12/5/03  2:50 PM  Page 233 A Pedagogical Framework. . . 233 Unfortunately, mutual satisfaction in instructional situations is not always correlated with learning performance (Kaasbøll 1998). Sometimes, learning can be exhausting, time consuming, difficult; it may even involve dramatic cognitive dissonance on the part of engaged students. Our experience attempting to integrate research, teaching, and service work, though rewarding, was not without challenges. First, because the authors have continued their involvement in the institutional challenge of integrating usability methods into the devel- opment of Web-based materials at NC State, we are aware that conclud- ing the students’ “experience” with usability testing at the end of the semester may have misrepresented the long-term complexity of the problem. That is, for our three classes of students, the problem of iden- tifying and analyzing potential audiences for the Ask NC State Web site ended when they presented their findings to their class and the share- holders with the Office of Extension, Research, and Outreach. But the thornier problem of creating a usable Web site for audiences external to NC State is far from over, and, in fact, the authors’ roles in that process continue to this day. Of course, many real-world problems are by nature complex and do not operate in isolation; instead, they touch on and influence other issues, political, cognitive, interpersonal, and institutional in nature (Spiro, Vispoel, et al. 1987). Moreover, Spiro, Feltovich, Coulson, and Anderson (1989) have shown that simplifying complex concepts may actually lead to erroneous interpretation on the part of students. We attempted to avoid oversimplifying the challenge of collecting data and reporting it to a motivated and interested audience. In particular, we reminded students that an early indicator of larger success would be seeing their recommendations integrated in subsequent design efforts of the Web site, rather than being congratulated for clearly presenting recom- mendations for redesign following their data-collection efforts. As well, we were constantly mindful of the nontraditional role we were asking students to play in having them serve as apprentices to fac- ulty researchers. Unfortunately, many instructors compelled to describe the engaging aspects of using authentic projects in the classroom do not acknowledge the possibility for conflict of interest. That is, by asking stu- dents to collect real data and to contribute products that will be used in actual corporate and academic environments, are faculty-researchers possibly guilty of coercing students to volunteer their services for the </page> <page sequence="18"> Innovative Approaches  12/5/03  2:50 PM  Page 234 234 INNOVAT IV E  A P PROACHES currency of course grades? And, if so, what are the students’ rights in such instructional situations? Can students choose to withdraw from par- ticular project assignments if they feel their values are being violated? In the case of a Web site aimed at providing useful information about a state university to the general public, the situation is perhaps less open to criticism, but what about the design of a Web site for Westinghouse or a brochure for a local abortion clinic? We addressed these concerns by raising them as explicit topics and by making certain that the Ask NC State Web site project did not dominate the syllabi of the three classes. And we did not feel that we were entirely alone in the university com- munity in having students engage in activities that had results the uni- versity benefits from: most psychology departments routinely require students to act as subjects in psychology experiments for course credit, and many independent study and internship programs are framed by an exchange between student labor and student learning and training. Technical communication programs need to be particularly careful to periodically address their position on issues of “sponsored” research and instruction versus theoretical isolation from the professional world around them. Despite the challenges we have raised here, our goal of combining usability testing with instruction and research has yielded valuable results for the students, the faculty-researchers, and for the university. The students sharpened their skills and knowledge of usability testing methods. They also learned about the difficulties of designing online information in a complex rhetorical domain. The faculty-researchers developed numerous relationships with other campus researchers and groups interested in usability studies. Further, they reaffirmed the value of employing diverse usability methods for testing a multipurpose com- munication medium. The university has benefited from an increased level of activity and understanding regarding the importance of per- forming usability tests on its communications with its constituents. It will also benefit from initiating a process to improve the quality of the Web site it uses for offering extension services to help fulfill its role as a land- grant institution. ACKNOWLEDGMENTS We are grateful to many people who helped us through the course of this project. They include, especially, Everette Prosise, assistant vice- </page> <page sequence="19"> Innovative Approaches  12/5/03  2:50 PM  Page 235 A Pedagogical Framework. . . 235 chancellor, and June Brotherton, former vice-chancellor of North Carolina State’s Office of Extension, Research, and Outreach, Harry Nicholos, Webmaster with the Office of Information Technology, and Caroline Beebe, director of North Carolina State’s Digital Libraries Initiative Department. In addition, the authors wish to thank the forty- three students in our English 583, 517, and 421 classes, who enthusiasti- cally participated in the experimental Ask NC State project. Without the energy and commitment of these students, none of the issues and out- comes discussed in this chapter would have been possible. </page> <page sequence="20"> Innovative Approaches  12/5/03  2:50 PM  Page 236 236 INNOVAT IV E  A P PROACHES APPENDIX EXT END ING  USAB I L I T Y  R E S EARCH Usability Principles for Web Site Design (cf. Bevan 1998; Nielsen 1994, 1997; Selber, Johnson-Eilola, and Mehlenbacher 1997) Accessibility Has the website been viewed on different platforms, browsers, modem speeds? Is the site ADA compliant? Have ISO-9000 standards been considered? Aesthetic appeal Does the screen design appear minimalist (uncluttered, readable, memorable)? Are graphics or colors employed aesthetically? Are distractions minimized (such as movement, blinking, scrolling, animation, and so on)? Authority and auth- Does the site establish a serious tone or presence? enticity Are users reminded of the security and privacy of the site? Are humor or anthropomorphic expressions used minimally? Is direction given for further assistance if necessary? Completeness Are levels clear and explicit about the “end” or parameters of the site? Are there different “levels” of use and, if so, are they clearly distin- guishable? Consistency and Does every screen display begin with a title/subject heading that layout describes contents? Is there a consistent icon design and graphic display across screens? Are layout, font choice, terminology use, color, and positioning of items the same throughout the site? Customizability Does printing of the screen(s) require special configuration to opti- and maintainability mize presentation, and, if so, is this indicated on the site? Are individual preferences/sections clearly distinguishable from one another? Is manipulation of the presentation possible and easy to achieve? Error support and When users select something, does it differentiate itself from other feedback unselected items? Do menu instructions, prompts, and error messages appear in the same place on each screen? Examples and case Are examples, demonstrations, or case studies of user experiences studies available to facilitate product learning? Are the examples divided into meaningful sections (such as overview, demonstration, explanation, and so on)? Help and support Does the site support task-oriented help, tutorials, and reference doc- documentation umentation? Is help easy to locate and access on the site? Is the help table of contents or menu organized functionally, accord- ing to user tasks? </page> <page sequence="21"> Innovative Approaches  12/5/03  2:50 PM  Page 237 A Pedagogical Framework. . . 237 Intimacy and pres- Is an overall tone that is present, active, and engaging established? ence Does the site act as a learning environment for users, not simply as a warehouse of unrelated links? Metaphors and Does the site use an easily recognizable metaphor that helps users maps identify tools in relation to each other, their state in the system, and options available to them? Navigability and Does the site clearly separate navigation from content? user movement How many levels down can users traverse and, if more than three, is it clear that returning to their initial state is possible with a single selection? Can users see where they are in the overall site at all times? Do the locations of navigational elements remain consistent? Is the need to scroll minimized across screens and frames within screens? Organization and Is a site map available? information rele- Is the overall organization of the site clear from the majority of screens? vance Are primary options emphasized over secondary ones? Readability and Is the text in active voice and concisely written (5–14 words per sentence)? quality of writing Are terms consistently plural, verb + object or noun + verb, and so forth, avoiding unnecessarily redundant words? Do field labels reside on the right of the fields they are closely related to? Does white space highlight a modular text design that separates infor- mation chunks from each other? Are bold and color texts used sparingly to identify important text (limiting use of all capitals and italics to improve readability)? Relationship with Is terminology meaningful, concrete, and familiar to the target audience? real-world tasks Do related and interdependent functions appear on the same screen? Is sequencing used naturally, if sequences of common events are expected? Reliability and Do all the menus, icons, links, and opening windows work predictably functionality across platforms? Typographic cues Does text employ meaningful discourse cues, modularization, chunking? and structuring Is information structured by meaningful labeling, bulleted lists, or iconic markers? Are legible fonts and colors employed? Is the principle of left-to- right placement linked to most-important to least-important information? User control, Are users allowed to undo or redo previous actions? error tolerance, Can users cancel an operation in progress without receiving an error and flexibility message? Are multiple windows employed, and, if so, can they be manipulated easily? Visibility of fea- Are prompts, cues, and messages placed where users will be looking tures and self- on the screen? description Do text areas have “breathing space” around them? Is white space used to create symmetry and to lead the eye in the appropriate direction? </page> </plain_text> 