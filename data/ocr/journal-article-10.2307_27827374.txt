<plain_text><page sequence="1">ENTROPY CONSUMPTION AND VALUES IN PHYSICAL SCIENCE By R. B. LINDSAY A COMMON view asserts that physics and, indeed, all science deal with matters of fact and have no place for values. In literature and the other arts, one expresses preferences and makes choices, one estab lishes values, but, in science, it seems we must take the world as we find it, describe it factually and carefully refrain from passing any judgments. In the larger issues of life, it is claimed that natural science has nothing to say; it can warn us indeed of the peril involved in dis regarding the "laws" of nature, but is silent on what we ought to do to achieve a happy existence. In fact, scientists and other people have long felt that this very quality constitutes the great strength of science? by its very nature its leaves no room for taste and involves no moral imperatives. This view has not gone unchallenged, but it seems that the time has come to emphasize more strongly than ever its inadequacy in the light of the modern methodology of science. Science is a method for the description, creation, and understanding of experience. It is, of course, not the only method for doing this, but no one can deny its success in the field to which it has generally confined itself. Science describes by endeavoring to find order among certain human sense perceptions and making statements about these with the smallest possible number of distinct terms. This usually, but not neces sarily, involves the use of mathematical symbolism. Science creates experience by devising experiments to channel and control experience, to make it active instead of merely passive. This assures that man will never run out of experience to describe. Finally, science understands by building theories which are imaginative creations of the mind or postulated pictures from which, by logical reasoning, the observed regularities of experience or "laws of nature" can be derived. It is just this last element in the scientific method which is mis understood by those who stress chiefly the factual and so-called im personal nature of science. Theories involve the construction of concepts and the introduction of hypotheses; these reflect the intuitive insight as well as the taste of the theorist. In hypothesizing he must exercise a choice, and this is ultimately a value judgment like all expressions of preference. The fact that some theories appear to be more successful than others scarcely minimizes the significance of the element of choice in their construction. Newton preferred to think of the propagation of light in terms of the motion of tiny particles; Huygens, on the other hand, felt sure that it was best described by wave motion. During the latter part of the 19th century, it seemed clear that the wave theory 376</page><page sequence="2">ENTROPY CONSUMPTION AND VALUES IN PHYSICAL SCIENCE 377 met the experimental test on the whole more successfully. However, with the advent of the quantum theory in the 20th century, the corpus cular viewpoint has been revived, albeit in a different form from New ton's. The main point here is that scientists can and do manifest strong preferences in the way in which they decide to understand experience: unique explanation is the exception and not the rule in science. Even in the interpretation of well-established theories physicists often show a surprising variety of attitudes. Mechanics is usually con sidered a solid body of doctrine about the motion of bodies?more engineering than physics, in fact. But, mechanics is a physical theory and one whose fundamental concepts and hypotheses have been looked at from many different points of view. One can, for example, put the chief emphasis on force and think of the motion of a collection of particles as due to the forces of interaction. Newton's laws of motion are then solved as differential equations, once the forces are expressed in terms of positions, velocities, and the time, and, from the solution, the state of the system at any time can be determined precisely from its knowledge at any other time. But, it is also perfectly possible to base the whole theoretical development on the concept of energy. The problem of the motion of the system of particles is then solved by setting up a certain time integral of a function involving the energy and insisting that this integral shall have a stationary value (usually a minimum) for the actual motion of the system as compared with all other possible motions performed between the same initial and final states in the same time (Hamilton's principle). Whereas the first point of view is deterministic, the second is purposive or teleological: systems behave so as to make a certain integral assume a stationary value. This is sufficient to emphasize that value judgments are by no means foreign to science. We must remember that there are fashions in theories as in other human creations; the attitudes of scientists toward the data of experience are conditioned by many factors including, for example, and in very decided measure, the total cultural environment and back ground of the scientist. The cautious reader may be willing to accept the point of view just expressed but may raise the obvious objection that science still has no place for ethical judgments: there appears to be nothing in science which says one ought to do so and so. There are apparently no scientific impera tives like the Golden Rule or Kant's categorical imperative. Naturally, any attempt by an individual to ignore in his activities the existence of a scientific law can lead to disaster and does so every day. But this hardly involves ethical precepts as usually understood. In fact, it is widely held that the whole method of science is foreign to such precepts except in so far as one is entitled to set up any precept one wishes as a postulate and then draw deductions from it. On the other hand, it would seem</page><page sequence="3">378 AMERICAN SCIENTIST curious indeed if, among all the scientific theories which have been so successful in explaining wide reaches of experience, there were no sug gestion of an ethical principle. Actually there are many. It is the principal purpose of the present paper to discuss one which presents numerous possibilities of application. We may call it the thermodynamic imperative since it is suggested by the principles of thermodynamics. In brief, and in simple language, it urges all men to fight always as vigorously as possible to increase the degree of order in their environment so as to combat the natural tendency for order in the universe to be transformed into disorder, the so-called second law or principle of thermodynamics. To understand the imperative, some elementary ideas of this great discipline must be scanned. Thermodynamics is a physical theory of great generality impinging on practically every phase of human experience. It may be called the description of the behavior of matter in equilibrium and of its changes from one equilibrium state to another. Thermodynamics operates with two master concepts or constructs and two great principles. The concepts are energy and entropy, and the principles are the so-called first and second laws of thermodynamics, which are really not laws in the strict physical sense, since they do not describe regularities in experience directly; rather they are hypotheses whose use is justified by the agree ment of their consequences with experience. The idea of energy is the embodiment of the attempt to find in the physical universe an invariant, something that remains constant in the midst of the obvious flux of change. Energy can appear in many forms : motion, heat, chemical action, electric current, light, etc. Whenever an effect corresponding to any of these phenomena is observed with the appearance of a certain amount of energy in the appropriate form, this is always accompanied by an equivalent loss in energy in some other form, so that the total remains constant. This is the content of the first law of thermodynamics, the principle of the conservation of energy. It is exemplified in practically every aspect of man's life, from the purely personal biological functions of his metabolism to the associated necessary activities of food raising and provision for shelter, warmth, and light. We live in a civilization based on power which, physically speaking, is just the rate of transformation of energy in time, and the faster the rate the more we appear to like it. But, through it all, we are faced with the apparently inexorable situation: in no way can we create energy; we may merely transform it, from the fossil fuel in the ground to the heat and electrical energy that warms and lights our homes, drives our cars and enables us to keep forever on the move, "forever" here meaning as long as we can find the wherewithal to transform. It is in the transformation process that Nature appears to exact a penalty and this is where the second principle makes its appearance. For</page><page sequence="4">ENTROPY CONSUMPTION AND VALUES IN PHYSICAL SCIENCE 379 every naturally occurring transformation of energy is accompanied, somewhere, by a loss in the availability of energy for the future perform ance of work; put in another way, every transformation reduces by so much the practical possibility of future transformations, even though theoretically they would be possible as far as the first law is concerned. In all natural processes there is a kind of one-wayness which is associated with what may be called available energy, i.e., energy available for future transformation. Think, for example, of the steam engine in which, in order to provide for the continuous transfer of heat into useful work (i.e., to run things), it is necessary in each cycle to sacrifice a certain amount of heat, which is discharged from the engine at a lower tempera ture than that at which it is taken in. We say that no heat engine can function at 100% efficiency, a direct consequence of the loss in avail ability of energy in every continuous natural process involving the trans formation of heat into work, i.e., in which no compensating work is done. The German physicist Rudolf Clausius (1822-1888) invented the con cept entropy to describe quantitatively the loss in available energy in all such naturally occurring transformations as we have just considered. Whenever such a loss in available energy takes place, as in one cycle of a heat engine, there is an increase in the entropy of the system. Stated in these terms the second principle of thermodynamics says that, for a closed system, that is, a physical system cut off from contact from the surround ing environment, the entropy cannot decrease, i.e., it must increase or stay the same. Clausius generalized this to the whole universe by assum ing that, in a naturally occurring process, the tendency is for the entropy of the universe to increase. We must be careful to note, of course, that, in local environments, one can make entropy decrease by providing ex ternal compensation. Thus, though the natural tendency is for heat to flow from a hot to a colder body with which it is placed in contact, cor responding to an increase in entropy, it is perfectly possible to make heat flow from the cold body to the hot body, as is done every day in a refrigerator. But everyone knows that this costs money! It does not take place naturally or without some extra effort exerted somewhere. The most careful examination of all naturally occurring processes (i.e., those in which external artificial influences are not allowed to intervene) has only served to confirm our confidence in the inexorable over-all increase in the entropy of the universe. It happens every time you light a cig arette: availability of energy goes down and the entropy goes up. As Clausius epitomized the fundamental principles of thermodynamics: the energy of the world stays constant ; the entropy of the world in creases without limit. If the essence of the first principle in everyday life is that we cannot get something for nothing, the second principle empha sizes that every time we do get something we reduce by a measurable amount the opportunity to get that something in the future, until ulti</page><page sequence="5">380 AMERICAN SCIENTIST mately the time will come when there will be no more "getting." This is the so-called "heat death" envisioned by Clausius, when the whole universe will have reached a dead level of temperature; and though the total amount of energy will be the same as ever there will be no means of making it available, i.e., of producing a transformation which will raise the temperature of one part above that of the surroundings?the entropy will have reached its maximum value. In contemplating this frigid fate, we are reminded of the famous Balzac story "The Wild Ass's Skin," wherein the possessor of the skin is entitled to the gratification of every wish, at the cost, however, of the shrinkage of the skin, with his inevitable death following the reduction of the skin to zero. The extension of the principles of thermodynamics to the universe is a somewhat drastic extrapolation of what is, after all, a postulate which appears to be successful through its deductions in our own immediate vicinity. Some may question the desirability of this extrapolation and may view the "heat death" as a wholly unnecessary nightmare. Yet, there is a point of view with respect to the foundations of thermodyna mics which makes it all too plausible. This point of view is the one com monly called statistical, in which one gains knowledge about complex systems containing large numbers of entities largely by virtue of the largeness of the number and without specific acquaintance with the prop erties of the individual members. Think, for example, of the life insurance tables which are able to predict about how many persons, in a given population and of a given age range, will die in a certain period of time, without being at all able to say which ones. Or, consider the molecules in a cubic centimeter of gas confined at room temperature and normal atmos pheric pressure. Their number is of the order of 10 billion billions. We cannot hope to tell much of anything specific about each one, but, just because there are so many, we can predict roughly how many will be moving on the average at any particular speed. This may seem rather mysterious until we reflect that we can always think of any aggregate of entities as changing in such a way as to make its subsequent condition more likely than its earlier one. Changes in nature proceed, in general, from less probable to more probable states. If we had some way of defining the probability of the state of an ag gregate it would therefore appear plausible to define the entropy of the aggregate as some function of the probability so that, as the probability increased, the entropy would also increase. Then, in all naturally occur ring changes, the entropy would most likely increase, and this would pro vide a logical basis for the second principle of thermodynamics. Note, of course, that it does not imply that the entropy, so defined has nec essarily to increase in any natural change; it simply is likely to do so, and the probability of this likelihood can also be computed. Suppose, to take a simple example, that all the air in a hermetically</page><page sequence="6">ENTROPY CONSUMPTION AND VALUES IN PHYSICAL SCIENCE 381 sealed room could be gathered together in a small bottle in one corner and the stopper inserted. This would take some doing, but there are appropriate devices for this purpose if we are willing to expend the neces sary energy. Now suppose the stopper is removed. No one with the simplest experience of natural events questions that the air in the bottle will rush out and fill up the room again. Numerous explanations can be given depending on the sophistication of the observer (making use of ideas of force and pressure and the like), but the simplest of all is prob ably that which says that, if the molecules of the air have a choice of staying cooped up in the small bottle or roaming around the much larger room, there is greater likelihood that they will prefer the latter. If this seems too anthropomorphic we can say that there are many more ways in which the molecules of the air can distribute themselves among the various elements of volume of the room (say all equal to the volume of the bottle) than those in which they can stay in the bottle. Thus, of any one molecule we can say that there are many ways in which it can locate itself in a volume element of the room equal to the volume of the bottle contrasted to the one way it can remain in the bottle. With this example, we have already decided on a measure of prob ability, namely, the number of different ways in which a given situation can occur. It is a particularly simple and straightforward one since it depends ultimately only on ability to count. The more ways a given state can be realized the greater is its probability (sometimes referred to as the statistical probability, to distinguish it from the probability of a given event which is usually represented by a proper fraction, as when we say that the probability of throwing a 2 when a die is cast is 1/6). The choice of a constant times the logarithm of the statistical prob ability of an aggregate (as above defined) as the measure of the entropy of the aggregate was the great contribution of Ludwig Boltzmann (1844 1906) to thermodynamics, for it leads directly to the second principle on a statistical basis and provides the statistical foundation for this whole discipline. From this definition we can indeed derive, in logical fashion, both the first and second laws of thermodynamics. The statistical interpretation of entropy and the principles of thermo dynamics lead at once to another important point of view, namely, that of the role of order in the direction of naturally occurring processes. It does not take much reflection to see a profound connection between statistical probability and order. Let us take, for example, an aggregate of atoms in a closed rectangular space of volume V and ask ourselves the number of ways in which these atoms can be distributed uniformly in the space, i.e., so that equal volume elements of the space, irrespective of position, shall contain, on the average, the same number of atoms. If there are atoms (where ^&gt; 1) the approximate expression for the number of ways becomes (2^V)V*n^(n/iV)^/2</page><page sequence="7">382 AMERICAN SCIENTIST For example, if = IO20 and = IO10, this number is approximately 4.6 1021. This may justly be termed a random arrangement. Suppose, on the other hand, we desire to distribute the atoms so that there is one of them at each point of a regular lattice formed by three sets of mutual perpendicular planes parallel, respectively, to the faces of the enclosure. The lattice consists of the points of intersection of these planes. The atoms are in this case distributed in a regular array simulating a crystal. This is a much more orderly distribution than the one first contemplated and common sense suggests that, for a given number of atoms, given volume, and given number of lattice points (of order of above), there are far fewer ways of realizing it. Precise calculation confirms this conjecture, indicating that, in this problem at any rate, the greater the number of ways of realizing a given distribution the smaller is the degree of order associated with it, and conversely. This turns out to be the case in gen eral. We can therefore introduce another interpretation of entropy. In crease in entropy means a transition from a more orderly state to a less orderly state. Let us take a more familiar illustration, namely, the distribution of playing cards in the normal pack. Everyone knows that with well shuffled cards the result is, in general, a random distribution with small chance of finding the cards in a given suit arranged in regular order. The latter, more orderly arrangement is simply statistically less prob able. It turns up far less often; there are relatively fewer ways of real izing it. Once again, high statistical probability is associated with rel atively high randomness or disorder, while small statistical probability is correlated with relatively great order. On this interpretation, the meaning of the second law of thermody namics, the law of increasing entropy, is now clear. In any naturally occurring process, the tendency is for all systems to proceed from order to disorder. The maximum entropy of Clausius is the state of complete disorder or thorough randomness, out of which no return to order is practically possible because it applies to the universe as a whole; no thing short of an inexpressibly improbable revolution could reverse the process and decrease the entropy. From this point of view, the trend from order to disorder with production of entropy is inexorable. The second law always wins in the end. A gloomy outlook indeed! But, there is perhaps a silver lining in the cloud. We have already commented that local decreases in entropy are pos sible, provided one is willing to pay the price. The most striking is that manifested by the living organism. The production of a living creature, on no matter how humble a level, is a vivid example of the transforma tion of disorder into order. From a random collection of atoms of oxygen, nitrogen, hydrogen, and carbon, with a few others thrown in, are syn thesized the remarkably elaborate chemical constituents of the living</page><page sequence="8">ENTROPY CONSUMPTION AND VALUES IN PHYSICAL SCIENCE 383 cell and these cells in turn arrange themselves functionally in intricate but orderly fashion. Here we must avoid confusing complexity with dis order. The living cell is enormously complex in terms of the constituent molecules, but these molecules are models of orderly arrangement. Life, then, may fairly be said to consume entropy, since, with the transition from disorder to order, the entropy of the universe decreases. Some en tropy disappears: we shall refer to this as a process of consumption. It corresponds equally well to the production of a highly improbable con figuration from a more probable one. Certainly, the creation of a living organism is such a move in the direction of the improbable. Nothing but living things themselves have so far been able to negotiate this synthesis. The production of life is, of course, reproduction. We must be careful to point out that the consumption of entropy in the formation of living things does not constitute a necessary violation of the second law. So far as we are aware, this consumption is accom panied by the corresponding production of entropy elsewhere in the uni verse; at any rate it seems altogether likely. But, whether this is so or not is not the important point here: the significant thing is that, in the living organism, there is an entropy consumption by very virtue of the transition from disorder to order. If we were mathematically clever enough we could calculate it and doubtless someday will. The entropy consumption associated with the reproduction of life may be thought of as unconscious or, at the most, instinctive. Rather more interesting is that connected with the conscious functioning of the human nervous system. The development of civilization itself may be, in a certain sense, looked upon as the result of the attempt of man to introduce order into his environment. On the physical side, he builds structures for shelter, transport and for the construction of other struc tures, i.e., machines of all kinds. All these imply creation of order out of disorder: a house is more than a pile of bricks or stones; it is an orderly arrangement of component parts and, thus, represents a decrease in or consumption of entropy. On the social side, man creates language, an orderly pattern of speech sounds and written signs. The communication of information from man to man replaces with order what would other wise be chaos in the relations of individuals. The social institutions of government, law and education reflect the same entropy-consuming tendency. In all these ways, man endeavors consciously to maximize the amount of order in his environment and to consume as much entropy as possible. Of course, the picture is not quite as simple as all this. There are ob vious fluctuations in the entropy consumption by living things. We recognize that destructive tendencies are exhibited by many human beings, and to this extent they are entropy producers rather than con sumers. Arsonists and murderers are clearly in this class, and, in a milder</page><page sequence="9">384 AMERICAN SCIENTIST way, the alcoholic shows the same tendency. Not wholly inappropriately is the term disorderly attributed to him. The reader can supply for him self plenty of illustrations of those who manage to produce more than their fair share of entropy in their immediate environment : they are the nuisances of society. But societ}^ as we know it, could hardly exist without large-scale local consumption of entropy. The very existence of science itself is a good example. Man's ceaseless urge to force some order on his experience so that he may understand it is to be interpreted as an entropy-consuming drive in the realm of both ideas and manual activ ities for the production of new experience through experimentation. It is to be noted that these activities are not haphazard but, in science, proceed according to a definite plan, i.e., imply a desire for greater order in human experience. It may be objected that, even in the case of living things, the second law always wins in the end. All experience points to the fact that every living organism eventually dies. This is a process in which the highly developed order of the organism is reduced to a random and disorderly collection of molecules. We are reminded that we are "dust" and to "dust" we ultimately return. But, precisely here is the challenge of the second law: while we do live we ought always to act in all things in such a way as to produce as much order in our environment as possible, in other words to maximize the consumption of entropy. This is the thermo dynamic imperative, a normative principle which may serve as the basis for a persuasive ethic in the spirit of the Golden Rule and Kant's catego rical imperative. In a society in which the quantitative promises to dominate more and more, the principle of maximum consumption of entropy offers the attractive feature that it reduces choice to a matter of counting, since, as we have indicated earlier, order versus disorder is ultimately decided on the basis of the number of ways of achieving a given distribution. Everyone can learn to count, if necessary with the assistance of the big computers which bid fair to take over more or less completely the running of the world's business in the foreseeable future. To the quantitatively minded, therefore, the thermodynamic imperative should have a special appeal. The qualitatively minded may find satis faction in looking on the principle as another way of expressing that "reverence for life" which has made Schweitzer's ethical point of view so satisfying. For all life is entropy-consuming. This is hardly the place to enter on the philosophical implications of the thermodynamic imperative. It can stir up many reactions. Some may prefer the point of view that the best course for man is to follow nature, and if nature prefers the path of entropy production and the transition from order to disorder, man might as well do the same and make no attempt to buck the second law. In fact, the pessimists like Spengler may actually take satisfaction in the thought that civilization is disintegrating</page><page sequence="10">ENTROPY CONSUMPTION AND VALUES IN PHYSICAL SCIENCE 385 and, even on the social level, the increase in entropy far outweighs its consumption?we are on the downward toboggan! The optimist, on the other hand, may point to the idea that mankind has always achieved its greatest successes by challenging nature and nature's cussedness. He would remind us that the very existence of life itself is a challenge to the operation of the second law, and that the only way to make the most of life, to fulfill its purpose, if it has any, is to continue to respond to the challenge by maximizing the consumption of entropy in every possible way. Only in this way can we successfully express our feeling that life has a meaning in the face of the remorseless natural increase in entropy of the universe. Here we let the matter rest. Each must make his own choice of view point. But, it is at least of interest that the discipline of thermodynamics with its concept of entropy provides such a clear-cut and analytically simple basis for our choice.</page></plain_text>