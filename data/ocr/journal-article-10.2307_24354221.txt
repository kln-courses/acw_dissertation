<plain_text><page sequence="1">Journal of Applied Philosophy, Vol. 15, No. 2, 1998 The Perplexing Conclusion: The Essential Difference between Natural and Artificial Intelligence is Human Beings' Ability to Deceive ALEXANDER BARZEL abstract As opposed to the computer, the human being can intentionally mislead in many different ways, can behave chaotically, and whenever he has the motivation can choose also by improvisation, non-consequent misleading, and spontaneous manners of reasoning and articulation. Human perception and the elaboration of the experience are existentially interest related, and distorted if found necessary. The arbitrariness is unlimited; human beings can initiate and produce absurd combinations, contextual failures and deceptive expressions, and do so also by intonation and body-language. These are the sources of social behaviour, of literature, arts, jurisprudence, politics, etc., all representing systems of meaning constructed by human beings, present in their 'Lebenswelt'. Human beings are flexible, sensible of emotions and well-trained to code and decode hidden fallacies, to produce wild associations, capricious-temporary conclusions, tasks that the rigid computer cannot fulfil. They can choose incorrect and dysfunctional answers; the exactness of their reasoning can be logically invalid. The computer has only inner horizons; human beings have also outer ones, in which they are able to produce multidimensional relationships. They are not truth-conditioned. The reduction of organic human thinking to the computer's mechanisms can end up in humankind's dehumanisation. Computer research over the last fifty years has been permeated with the belief that artificial intelligence (AI) will replace natural intelligence (NI). There are some researchers who narrow those expectations, mainly for two reasons: the quantity of information accumulated in the brain, and the velocity of manipulation with it by the mind surpass the capacity of a computer's memory (also of any foreseen computer), and exceed the real time needed for its effective use. The computer is merely a tool for limited objectives and cannot be expected to fulfil the goal of the human being, who acts moving freely in a manifold reality. Nevertheless, the prospects presented by enthusiastic computer scientists and technicians seem to be stronger than cautious warnings. The Leibnizian dream of mathematical formalisation of thought, based on the assumption that both disputants express valid arguments and, hence, the truth of a context can be proved by calculation, is still with us. In the Mid-Fifties a computer engineer specified the human being as a 'deficient instrument', since man's ability for efficient and quick inference is inferior to that of the computer. [1] Four decades later the computer seems to be a deficient instrument being unable to deceive, lie and intentionally mislead. The human being is able to infer non-consistently and to introduce weak concepts and metaphors; can slur over, obscure, withdraw and delay some details, cause lack of exactness, distort, produce © Society for Applied Philosophy, 1998, Blackwell Publishers, 108 Cowley Road, Oxford, 0X4 1JF, UK and 350 Main Street, Maiden, MA 02148, USA.</page><page sequence="2">166 A. Barzel humorous expressions, fantasise nightmares, vary contexts, present only partially-relevant arguments, and then again, turn arbitrarily to exact and relevant ones, that is, the human being is able to perform a 'chaotic behavior' [2]. (My use of all these terms is descriptive, not normative). Not only is a computer unable to produce humorous phrases and ones which mislead, it is also unable to understand a joke, a sarcasm, a distorted statement, and the like. 'I would want to see if the program could understand jokes. That would be the real test of intelligence'. [3] All kinds of humour bridge reason and emotion, where alone there is a place for intended inexactness and for 'half-truths'. The peculiarity of human reasoning is the skill of deviating from established knowledge also in a provocative way, using also absurd expressions. Lying isn't a peripheral phenomenon, nor some malfunction, but a necessary consequence of non-linear reasoning. Philosophers of mind discuss the distinction between 'know what' and 'know how', but neglect the question of 'know for', that is, the role of purpose in the cognitive act. Also the phenomenology of perception and Gestalt-psychology, which emphasise intentionality, do not expand the research into the domain of intentionally deceptive processing of thought. I aim at putting an essential, not merely technical distinction between NI and AI. There is an urgent need to assign the borderline by pointing to differences, beyond which the effort to compare the traits of human beings and those of the computer misrepresents both and is dangerous; the reduction of organic human reasoning to the computer's mechanism can end up in the human being's dehumanisation. Man doesn't absorb information passively from his senses as the computer receives its from external input, but takes in raw sensations, which become information only after being settled in a system of meaning and interpreted in its context by interest-laden intention. AI is incapable of interest-related understanding and representation. Interests move NI to posit sense-data in different systems of meaning, to interpret them in a deceptive manner and to articulate fallacious resolutions. To understand means to grasp the reason and purpose of the construction of a system of meaning, i.e. to comprehend the quality of relationships between its components. Since deception, too, constructs systems of meaning, a partner in speech-exchange will reveal the fallacy only if he can decode the principle of system-totalisation performed by the liar. NI is trained for this decoding procedure. 'We recognise some linguistic expressions as odd — as breaking rules — and yet we are able to understand them'. [4] The computer cannot reveal a lie because of its rigidity in accepting the input system, along with its principles and details, all closed and limited. The possibilities of fallacious composition of systems of meaning are infinite, the arbitrariness of reasoning and expression is unlimited. The computer is destined to infer valid judgements from well-defined propositions. Its logical method and its mathematical language do not allow it to output intentionally misleading answers for a goal external to the posed problem. Semantic ambiguities and defective syntactic combinations have to be, and can be avoided. In contrast, a lie can ignore any regular meaning and any valid construct of a passage. No suggested 'semantic primitives' can prevent the lie's deviations, neither can the reliance on 'conceptual dependencies' shun the lie, since its very intention is to release from dependencies [5]. © Society for Applied Philosophy, 1998</page><page sequence="3">Natural and Artificial Intelligence 167 Almost all discussions on the comparison between NI and AI are confined to perception, learning, memory-accumulation, but almost all ignore active initiation of constructing thoughts. One of those who mentioned the problem wrote: We must try to understand to what extent AI is possible, and if there are limits to the possibility of computer simulation of intelligent behavior we must determine those limits and their significance. What we learn about the limits of intelligence in computers will tell us something about the character and the extent of human intelligence. [6] A 'Copernican revolution' in cognitive science and in Al-research is now needed: the core question is not what the computer can understand and store, but what it is able to produce anew, also in opposition and contradiction to the information absorbed in it, just as NI is able to do at any time for its own reasons. Unlike the closed system of the computer, the human being is an open 'system' who can raise questions intuitively about not yet existing possibilities. Within the computer only questions in the — although broad — memory store can be raised. Human thinking puts questions when it reveals 'holes', 'dead areas' in a continuity of reasoning, which includes also not-yet-established relevances [7], When we absorb stimuli we constitute a supposedly appropriate system of meaning in which we reveal 'holes', for filling up of which we compose words considered proper for the questions. In the computer there cannot be any degrees of 'almost happened' or 'has not been realised'. Our mind reveals the not-happened-but-was-about-to-happen, by finding a 'hole'. I believe that 'almost situations' and unconsciously manufactured subjunctives represent some of the richest potential sources of insight into how human beings organize and categorize their perceptions of the world. [8] That 'unconscious' needs to be investigated, since there is no reasoning without awareness and consciousness. The would-be unconscious is but the result of extremely rapid reasoning. Again, a failure in thinking reflected in an articulate expression is, in fact, a contextual failure loosely attached to other components, and the latent ties can be found. The lack of ability to produce mistaken answers — also intentionally — is the sign that the computer is not intelligent.' . . . mistake making is a sign of high intelligence' [9]. Perhaps a future computer will be able to 'find' missing details in a preconceived whole, but it is not easy to foresee how could it discover 'dead areas' of semantics and contexts external to the memory-store, neither those which appear as counter-logical, nor those which appear in the form of intentionally distorted use of words (including intonation, accompanying body and face-language). Alternative understanding is possible for a computer only within the limits of the already existing memory-store and only in a mechanical procedure and with full functional accuracy. The human mind composes systems of meaning in which there are inaccurate and also misleading components formulated in fallacious words. The computer under © Society for Applied Philosophy, 1998</page><page sequence="4">168 A. Barzel stands systems of meaning only as a composition of details conditioned by the program. The human mind is flexible enough to compose systems of meaning; what belongs to them is open and versatile. The difference is that the computer doesn't have open semantics and, hence, it doesn't 'know' the meaning of the stored memory, and, consequently, cannot intentionally distort meanings deriveable from it [10]. It has but syntax. The . . . way of characterizing the difference between 'syntactic' and 'semantic' properties ... is that the syntactic ones reside unambiguously inside the object under consideration, whereas semantic properties depend on its relations with a potentially infinite class of other objects, and therefore are not completely localizable. There is nothing cryptic or hidden, in principle, in syntactic properties, whereas hiddenness is of the essence in semantic properties. [11] The hiddenness is the realm of possible deceptions. Even if all known logical fallacies were to be programmed, the computer would still be unable to use them in a way in which a lawyer, a political agitator or an enthusiastic preacher could. Anthony's funeral oration at the Roman Forum could not have been formulated by a computer as Shakespeare did it; neither could the ambivalent expressions of Iago waiting for Othello on the Cyprian shore, nor Hamlet's words on the 'virtue' of Ophelia. It is difficult to see how a computer will be able to be the locus of all meanings implied by the stored items in its memory. In an important sense an object's meaning is not localized . . . within the object itself. This is not to say that no understanding of any object's meaning is possible until the end of time, for as time passes, more and more of the meanings unfold. However, there are always aspects of its meaning which remain hidden arbitrarily long. [12] The 'long hiddenness' of meanings isn't of an essential character but a factual probability, while for human reasoning this 'hidden' openness is essential. The time-dimension is irrelevant in the case of the computer; in the course of time the memory-store does not expand of itself, but only if new details are added, and those will not be fixed again in a time-dimension. Thus, life-experience is developed by means of senses, but that raw-material is organised and elaborated by associative combinations and speculative conclusions, derived principles of activation — directed to aims and objectives. Computer scientists may argue that since the computer, too, stores information in its mechanism, time will come when it will be possible to program reactions divergent from the logic and from known principles, and the computer will also be able to deceive. Nevertheless, one component in that expectation remains problematic: how can it be possible to program motivation to deceive in one of the many ways? How can a computer distinguish between a situation in which it has to conclude logically and respond by direct expressions, and a situation in which temporary deception is required? The computer scientist may answer that it will be possible to program signs which turn to the equipment to produce deception in some pre established context. But this is exactly the point: a human being is free to deceive without being limitedly programmed. That is, the computer lacks, and it cannot be foreseen how it could not lack, spontaneity. Deception isn't irrational but definitely rational, being the result of strategic reasoning. There is a trend to consider spontaneity as opposite to rational deliberation, but it seems to transcend rationality only because of the extremely quick reasoning of human beings. NI © Society for Applied Philosophy, 1998</page><page sequence="5">Natural and Artificial Intelligence 169 evaluates the situation and different alternatives — even absurd ones — and the divergent decisions, those which are not derivable from the situation and from the memory-base; it weighs contrary probabilities and might decide to deceive, if that is judged necessary. Exclusive truth would do away with concrete human history, since it would put the freedom of choice out of action. Since human existence is open and situation-dependent, versatility of choice is due to the variety of eventual situations and points of view. The heuristic praxis of AI is confined to act in a limited field of choices; NI can transcend the frontiers of actually stored experience. Our brain raises images from a realm of 'wild fantasies', that is, ones which are not rooted in any previous sensation. Here is the origin of 'licentia poetica', of creativity realised in the formulation and composition of images and linguistic expressions on the basis of aesthetic, social and exhibitionist intentions, through adjustment of the details and the complex whole to initial or improvisational acts of imagination. For a computer, the origin of any such initiative remains — still or principally — problematic. Against the infinite space of choice of the human being, the zone of action of the computer is narrow indeed. The relevance of choice — embodying purposes — is far more exact than anything that can be expected from the computer, because human beings can introduce arguments and decisions which are not included in a related knowledge-base. They reaffirm every piece of information in any new connection, and reconsider a requested value in any situation, that is, human beings are always actively and flexibly present in reasoning — or, better, human beings are their reasoning. Their background knowledge is continuously formed in their Lebenswelt, which the computer cannot possess; it doesn't have needs and it doesn't deliberate and meditate before decision-making. In contrast to human knowledge, the computer is not a 'participative knowledge' [13]. The human being is prepared to choose also incorrect conclusions and utterances, for the sake of preferred solutions, even when the immediate result is manifestly undesirable, but seems to advance further goals. Intelligence is more than a tool for our 'being-in-the world': the lie produced by it might be explicitly dysfunctional for actual tasks in a given reality. Functionality in human dimensions is not necessarily conditioned by logical validity, in so far as a target can summon punctual and counter-effective misconceptions, when for an overall interest such a divergence seems to be advantageous. Reasoning is essentially associative; fallacy is meant to break down the chain of associations. There are 'mild' fallacies, in which the word or the sentence remains in the broad frame of routine meaning, and there are fallacies which transcend any connection to any established system of meanings, and can also decompose the dimensions of typicality by 'wild associations'. Fallacy is the product of consciousness, that is, of systems of meanings out of experiences and totalised by means of principles which seem to lead to the achievement of personal goals, that may also fit in with public goals. The consciousness of the human being is the result of a special kind of compromise. — A digital machine cannot itself ever require consciousness, for the simple reason that in it there do not arise hierarchical conflicts of operation. © Society for Applied Philosophy, 1998</page><page sequence="6">170 A. Barzel Such a machine can, at most, fall into a type of 'logical palsy' or 'logical stupor' when the antinomies in it multiply. [14] Compromise means non-exactness or intention-directed exactness; it can be achieved only because human reasoning is free to manipulate all kinds of components of reasoning. It does not mean that in practice we do manipulate all kinds of components, but no absurdity is absolutely outside our mental capacities. The history of culture, full of fantasmagorias, can be a good witness to this. Human experience is located in history; memory is stored, along with what was already interpreted, and what has remained non-explicit. Analogies evolved from the knowledge base rise out of systems of meaning pre-established in cultures: we don't begin history, but are born into a chain of experiences interpreted, evaluated and catalogued, and into a system of conceptual and linguistic frames. However, an individual might compound a situation which is not at all trivial and, hence, even a shared culture cannot support its understanding, since the realm of intended deviations is open. Also the relevance of a new — appropriate or fallacious — understanding of an old content is not contained in the memory-store nor in the principles of references by which a new detail will be located in a system of meaning, because those are all intention-directed. Hence, a lie can be built into the system on the ground of intentions deviant from the domain of working principles of totalisation. After an excessively quick examination of the data-base the human being is able to decide whether to move in a deceptive direction and to draw out an irrelevant particle, even a bunch of confused particles. In many real-life situations, deductive reasoning is inappropriate not because it would give wrong answers, but because there are too many correct but irrelevant statements which can be made; there are just too many things to take into account simultaneously for reasoning alone to be sufficient. [15] Nevertheless, if computerised reasoning were inductive, this would make no difference for its capacity to give good or bad, relevant or irrelevant answers, since there are too many details to be taken into account. Moreover, the argument that all reasoning is deductive and induction is but a function which accompanies deduction in concrete situations according to actual needs, cannot be annulled. The computer's use of language is considered as accurate, but in fact it isn't, since the computer is inflexible in its limited, although abundant, capacity to select words for a concrete expression wanted for a momentary purpose. Human reasoning is able to invent words for a certain task, to decline them in an unusual, also counter-regular manner and yet the receiver will understand the intention, even better than he could conceive of the rigidly well-defined words included in a computerised memory-store [16], The computer can deviate from rules only if those are programmed into it and if the deviation necessary for the sake of the argumentation is exactly defined and also if a new rule is precisely implied. Human beings are able to think and act also against known rules, can introduce words into a discourse in dialects, loose slang, temporary practice, distorted forms, and the like, if and as they are appropriate for an actual context. They can make compromises of expressions and rules ad hoc on the ground of circumstantial evidence. A computer cannot perform all that, since it is impossible to program for it all eventual points of view which invite those compromises in changing situations. Programminmg prototypes doesn't make © Society for Applied Philosophy, 1998</page><page sequence="7">Natural and Artificial Intelligence 171 any difference, since the possibilities are 'infinite' [17]. The human mind is an 'informal system' [18] — it is able to reason without rigid regularities. While the computer is becoming a commonly used tool, the English it uses turns out to be an almost 'artificial language', that is, one which isn't attached to the flexible natural use of ordinary people in everyday situations. That neutralised technical English is inaccurate and poor because it is confined to mechanical-functional exactness, and as such dysfunctional for cultural conversational deception. The inability of the computer not to express exact answers causes computerised translation to be problematic, especially of a non-technical text. The human mind is able to manipulate symbols and alternative images, the exactness of which is their inexactness. Linguistic models produced for some specific use not before needed, cannot be applied by a computer [19]. In some measure it is possible to rely on computer translation of scientific texts. In some measure only, because one restriction has to be taken into account: the less contemporary the text, the less can be translated accurately. There are changes in the meaning of scientific concepts which cannot be reflected in translation, e.g. of power, time, space in physics; normalcy, cloning in biology, etc. are conceived differently now from their sense one or two centuries ago. However, in a closed domain eventual fallacies can be revealed by experts. In the humanities and in the social sciences — better: social studies — camouflage is easy, since the arsenal of their terminology is full of value-words void of definite informative content, context-bound and evaluation-dependent, and also loaded with idea-directed ambiguities. Evidently, the computer can produce new knowledge, but is not able to falsify already constructed knowledge [20]. Exactness in the computer is due to its mechanism, while human reasoning constructs exactness for projected aims, also by the use of distorted words concepts, which in a different context would not be considered as contributing to exactness. The reason for such a mental process may arise in the very course of thinking and by intermediate conclusions not yet proved. This is intuition and spontaneity. The computer is a thoroughly determined mechanism. It doesn't choose its replies, but selects between restricted possibilities, and infers by closed formal processes a limited number of logically well formed answers, or alternatively, fails to produce any answer. Human reasoning grows within an organism involved in real, temporal circumstances, representing freedom of choice between a practically infinite number of possibilities as demonstrated by the ability to utter also nonsensical and capricious answers, that is, ones which don't refer in any sense to the problem in hand. A computer can construct only finitely many world models, while the human being can refuse to accept any of them and to symbolise a fictive model. This ability is the source of literature and of the arts, which create new relations; the one semantic and syntactic, the other formal-metaphorical ones. Communication is possible only if there is some common bundle of experience. Human reality is concrete, since it is con-crescent (as Hegel pointed out in his 'Encyclopedia'). Crescence is open and is formed by intention in current situations. A robot has and will have no experience on the ground of which it could obtain fear, hope, etc., that could be communicated. Formalisation in AI abolishes relation to concrete personal situations, while NI reflects historical, circumstantial, environmental situations: in every instance it is © Society for Applied Philosophy, 1998</page><page sequence="8">172 A. Barzel a cognitive system evolved within social connections embedded in a personal knowledge base and in a distant historical continuum of collective memory. NI also acts by wishful perspective conjectures, since the human being is also situated in imagined relationships. Due to imagination, human beings can neglect particulars and can introduce false ideas according to their preferences. Formulations are attached to such inner and outer horizons and systems, which might change with new considerations [21]. Horizons of reasoning also embrace the stored and revised lessons learned from previous decisions and utterances. Consequently, what the program in a computer knows is less than what a human being knows, or better, what a human being decides to use out of his or her knowledge-base. Human beings' individuality is the result of their life-experience in an open universe of language. Their subjectivity is constructed by accumulation and selection which imprint potential preferences of attitudes, values, views, in their intellect, all coming to serve human choice of expressions in different situations. Since brain-activity is extremely rapid, and the ability to produce combinations is extremely rich, linguistic flexibility exceeds what the most advanced computer seems to be able to produce [22]. Computer scientists do not easily accept the principle-qualitative, rather than the practical quantitative difference between NI and AI. Already at the beginning of the Eighties Hofstadter surmised the possibility of constructing an artificial neural net which would be brain-mind-like, but this would not possess meaning-dimensions 'of high-level', since neurons in however perfect mechanical interaction don't produce meanings. High-level meaning is an optional feature of a neural network — one which may emerge as a consequence of evolutionary environmental pressures. [23] However, a computer has no 'environment' for moving in and there is also no 'evolution' in it; hence, it is not open for 'options' to constitute 'high-level meanings'. But Hofstadter did not give up the hypotheses that if intelligence involves learning, creativity, emotional responses, a sense of beauty, then there is a long road ahead, and it may be that these will only be realised when we have totally duplicated a living brain [24]. He argued there also that it is perfectly obvious that a computer can be instructed to print out a sentence of illogical statements . . . Yet in following such statements having instructions, a computer would not be making mistakes. [25] In this case, as he wrote in another place, the computer would not be 'intelligent'. The human being can make mistakes, also intended ones and also without any consistency. It is not at all evident that a computer can be instructed to make open, impulsive, suddenly rising, goal-directed mistakes. A highly developed computer, similar to the human brain, which has learned to compose illogical passages, could also not be faithful to its 'instructor' and eventually not make mistakes, certainly not intentionally, on the ground of some prompt motivation. That is the good luck of the computer-industry: if this were possible, that would be the end of the computer as a reliable, accurate instrument; we would be unable to know when it would take a fancy to shift a missile from its route, to make a false response on the Internet, or to swindle a youngster in a computer-game. Evolution developed the ability of animals and human beings to deceive for the sake of survival. Persons variegate that trait beyond the need for life-protection. The transition © Society for Applied Philosophy, 1998</page><page sequence="9">Natural and Artificial Intelligence 173 from mimicking to hiding had deep significance: it was the passage from corporal reaction to mental choice. It is unlikely that man, as we know him, would have survived without the Active, counter-factual, anti-determinist means of language, without the semantic capacity generated and stored in the 'superfluous' zones of the cortex, to conceive, to articulate possibilities beyond the treadmill of organic decay and death [26]. The inflexible AI cannot solve problems in as yet uninterpreted situations which require modular reflection and an appropriate language to catch nuances of intentions. We are able to do so because our minds work with 'background assumptions' open for adjustment. We are able to think on different levels at the same time, to entertain contradictory contents and intentions, and to move between foreground and background assumptions, choosing at the same time the most appropriate articulation [27]. Human evolution disconnected the brain from linear necessity and from a limited range of choices, while it endowed the human being with the capacity to select possibilities and combinations. Consideration of an ontological distinction between brain and mind doesn't seem to be important here, since their being material or not doesn't change the fact of man's ability to deceive. Moreover, material and ideal may not be the only substances: relation between both indicates a third ontological status. The reservation from mind-body dualism then becomes irrelevant. The ability to relate, to produce multidimensional relationships between data — including ones that diverge from the evident, primordial assignation emergent from those data — is the source of deception. Human beings can evolve transformations of relations according to manifold intentions and interests. A computer cannot imitate such reasoning, even less such activity, since the computed inputs can be only what by definition they have to be. Contrary to analytic and closed mechanical reasoning, human reasoning is synthetic, analysis only serves its tasks. It preserves open time-dimensions; whenever in a given moment an argument seems to be final, the unceasing possibility of changing the composition and, hence, the conclusions, remains. The motive of a new synthesis can be an intended lie for the sake of a new objective. Also a computer programmed for conversation will not yet be able to react emotionally to lies. It is improbable that a computer will be able to compose a compliment controversial to an impression: to flatter the beloved girl when her dress is unbecoming, or to laud the poor efforts of the boyfriend to impress her. Moreover, even if a lot of details of 'falling in love' were programmed into a computer, no output could match the complex ambi- and multivalent non-constant process of human falling in love, accompanied also by deceptive but well-intended expressions [28]. Amazingly, exact honesty is counter-productive in human relations and hence, fortunately, a computer is not constrained by social rules and has no dilemmas. In spite of existing programs which produce the sounds of laughter, a computer cannot laugh, because a joke is a spontaneous outburst, not a computable speech-act; it is the result of an intentionally misleading use of expressions, which cover distorted situations and conjunctions. Programmed laughter or weeping may appear only when predetermined © Society for Applied Philosophy, 1998</page><page sequence="10">174 A. Barzel connections provoke them, but if there is the slightest digression from the program, the reaction will not come. Human beings can also react by inverse acts: to laugh when a cultural prescription is to cry, and to cry when the culturally appropriate behaviour is to laugh. A computer programmed to understand visual or vocal expressions will still not distinguish between a sick person and a malingerer lying in bed and sighing to impress a visitor. Human reasoning entails a manifold experience in constructing systems of meaning in a more-or-less identical cultural milieu and can decode the situation easily. Understanding of deception in a different culture is problematic. Cultural conventions are a kind of deception. We are able to 'translate' strange signs by plausible analogies, while distinction between the plausible and the non-plausible is culture-dependent; the computer is not able to perform such a test, even if programmed for a number of conventions, since the possible combination of signs is infinite. Human reasoning invents anthropomorphic fallacies owing to the capacity to manage infinitely open discourses, but no 'computomorphic fallacy' can emerge. Its mechanism needs stereotyped instructions for stereotyped activities. Thus, a robot can be trusted not to behave crazily for any culture dependent goals. People of different cultural constitutions reacting to the same matters of fact express their reactions by words of different indications. Cultures exist because there are differences of accuracy in understanding and interpretation of situations and in production of situations, according to views and to accumulated experience. In the course of reasoning we categorise experience not at all in fixed, but in culture- and actual-history dependent categories in which we receive the stimuli, not at all unequivocally in different situations and in response to different personal attitudes [29]. The flexibility of NI is the source of creativity demonstrated in the ability to lie not merely by bringing about a new regularity, but by abolition of any regularity. That well-known proof of the capacities of AI, chess, points to the illusion of creativity of the computer; it has only the ability to survey very rapidly many alternatives, while, in fact, the possibilities are finite and fixed by the rules of the game. The 'possibly relevant factors can be defined in terms of context-free primitives.' [30] A computer could not pass the examination of Barzel's Grandparents' Test: it cannot betray by intentionally mistaken moves to make the grandchild happy, and to return again to proper moves. An 'infinite' computer may be able to review all relevant possibilities quickly, but how could it derive irrelevant and counter effective moves for the sake of not-included, momentarily emerging objectives? And why and how could the computer return again to relevant choices once programmed to diverge [31]? Human reasoning embraces not only all relevant possibilites, but also 'all' irrelevant ones, also quickly enough for useful implementation of the intentions. ('All' in inverted commas, since principally — and also not according to what neurophysiology seems to know — not all irrelevant moves can be imagined [32]. The computer cannot relinquish the optimal move in the favour of an illusion preferred for emotional reasons, which make the very context of the grandparents' decision. If this procedure is valid for chess, it is even more valid for computed simulation in the realms of economics, politics, legal reasoning and argumentation in studies in the social 'sciences'. It is difficult to see how advertising could be designed by the computer, since its language is entirely fallacious: its semantics, its syntax, its visual and oral coordinaton — all © Society for Applied Philosophy, 1998</page><page sequence="11">Natural and Artificial Intelligence 175 serve emotion-provoking fallacies. If emotions were aroused necessarily any time when x, y, z datum appears, a computerised mechanical simulation could be foreseen. However, since their arousal is the arbitrary and occasional outcome of the appearance of sense experiences — different for each individual, and also for individuals in different cultural and historical circumstances, — and since emotions can be suppressed in the very same situation and in the very same cultural environment, and can also be shown deceptively, they cannot be programmed into a computer. Probably, a program can cause the computer to display 'enthusiasm' in its reaction to certain punctual or serial-identical stimulations, or to display desire for something in pre-established connections, but it cannot be programmed for open enthusiasm and infinite desirability. It is not 'the' human being that is happy, sorry, etc., but a certain person in his very specific situation; moreover, that very individual can avoid displaying happiness or sorrow if he intends to do so, and can also display a fallacious happiness or sorrow by pia fraus. Norbert Wiener warned as long ago as the late-Fifties of the blind accuracy of the computer [33]. If in its memory-store all judgments in previous processes are preserved, it will reckon the sentence in a certain process exactly: prison for years, days and hours. However, the computer will not be able to weigh specific individual circumstances which may claim a lighter or a more restrictive sentence. The computer is truth-conditioned, while for a human judge the logical truth-value may be counter-effective. The use of computer in ethical and in juridical discourse is highly problematic. The key-concepts 'just' and 'unjust', good and evil, and a dozen other parallel notions fail to exhaust the full range of human situations. Also the use of words in those realms which function in other connections can exemplify the case: 'cruel competition' has not the same meaning for people in the stock exchange, as it has on the sports field or for rival firms. Both 'cruel' and 'competition' are contextual, as are other expressions of evaluation. The delinquent, the judge, the lawyer and the witnesses, all are prepared to diverge from any standard mechanical use of a vocabulary, at any time which calls for a specific aptness. Similarly, a medical doctor can offer a false diagnosis when the patient's mental or the family's empathic situation requires it. A computerised 'doctor' knows and displays only accurate diagnosis, which is derived from all input details. The language of lawyers, politicians, economists, hinges on figures of speech and vague formulations which disguise interests. The computer will never be able to go in for political agitation or a lawyer's argumentation, both proceeding along arbitrarily chosen byways. Also if it were possible to program cross-references in the broadest realms of subject matter, the arena of manipulation would remain open enough. Even if the partner in all these activities had in his pocket a mini-computer, and even if (as promised at Stanford) a computer-mechanism was installed in the human brain, it would never reveal the manipulation in a diplomatic discourse — a task easily fulfilled by NI. The computer could not be a substitute-diplomat, since it cannot initiate deceptive argumentation. It lacks 'generative productivity' [34] rooted in a cultural heritage and reproduced in the historical circumstances of both negotiators. ... the depth to which you need to recognize intentions in order to feel you have understood depends on your own goal in the interaction. In some conversations — say, when negotiating peace plan — it is very important to know exactly what other people intend when they are speaking. — Once the intentions are © Society for Applied Philosophy, 1998</page><page sequence="12">176 A. Barzel recognized, the next thing to explain is what the observing agent does with the information about the acting agent's intentions. [35] All this is beyond the capacity of computers, but is within the range of routine human reasoning, which contains — essentially — eventual intentional deceptions. Optimation in AI means a search for the simplest and most advantageous solutions out of the stored possibilities for a fixed problem. Human reasoning considers optimality in a different way: it can produce it by confusion and distortion of meanings and connections of new relations between stored details, if the result seems to be optimal at a given moment and for a projected goal. Improvisation, too, is a kind of fallacious move, as opposed to what can be expected as the optimal one, on the grounds of the data in a given situation; eo ipso the computer cannot display provisory and partial improvisation as a substitute for the optimal solution prescribed by the stored data. NI co-ordinates routinely different objectives, combines intentionally the relevant expressions taken from different 'language games', and formulates passages for immediate solutions. The computer cannot imitate man's ability to deceive by syntactical, prosodical and intonational devices. A joke can exemplify this: in a defamation case the judge called on the delinquent person to admit: 'Smith is an honest person. I must apologise/' The delinquent stood and said: 'Smith is an honest person? I must apologise?' The question-mark in the intonation manifestly distorted the meaning intended by the judge. A computer is not able to produce alternative markings and intonations [36]. Consequently, a computer is unable to process different styles for the same message. Style is produced by holistic reasoning, that is, by the consideration of a full range of expressive factors in one, which cannot be formalised. The user can foresee in full measure the range of answers open to a computer, knowing the realm of the programmed field of information; answers the human brain might produce are much less foreseeable, since the human being's holistic reasoning includes also details not contained in a present collection of data, but prescribed by the planned whole. In this region of the not-yet-present intentional deceptions arise. However, while there are good reasons not to introduce vague concepts into a discussion on rational subjects, sometimes an author finds no better choice. In this way the concept of 'soul' has been introduced in order to denote organic, intentional human reasoning: Put goalism and holism together, and you have — soulism! Put predictionism and reductionism together and you get — mechanism! [37] That is, human reasoning is not predictionist in the rigid sense: its exact responses cannot be foreseen; neither is it as reductionist as is the computer, since no completely sure derivative from given details can be reduced to those very givens. There are at this time many philosophers of computing who foster ever-expanding expectations of the capacities of the computer. Chalmers is an example: The language of computation provides a perfect language in which this sort of abstract causal organization can be specified. Indeed it can be argued tht this is precisely why computational notions have such a wide application throughout cognitive sciences. What is most relevant to the explanation of the behaviour of a complex cognitive system is the abstract causal organization of the system and computational formalism provide an ideal framework within which this sort of organization can be described and analysed. [38] © Society for Applied Philosophy, 1998</page><page sequence="13">Natural and Artificial Intelligence 111 What is the criterion of being a 'perfect' language? The attribute 'this sort of organisation' hints at a limitation: perfect in its sort, in strict formalism. However, this argument is of the kind of petitio principii since it proves only what it presumes. Natural language and reasoning are more perfect just because they provide material arguments — the only sort effective in human discourse. Perfectness can be proved only by deliminating the field of relations in which the arguments are asked for. Perfectness has to include flexibility, eventual semantic deceptions, metaphors, jokes, distortions, as the human intellect functions. The resume of Chalmers himself clarifies the problem: I am happy to embrace the conclusion that if cognitive dynamics are computable then the right sort of computational organization will give rise to consciousness. [39] But the question is whether 'cognitive dynamics' and 'computational organisation' are meant to go beyond formalism, since there can be consciousness only if there are material contents of reasoning. Moreover, human consciousness as the basis of thinking comprises not only the details already present in it, but also connections in an 'infinitely' open field of relationships, including those which are obviously absurd. And, what seems to be the decisive factor: human consciousness contains also the motivation which arises from the predesigned plan and from the 'holes' to be filled up for the sake of its realisation. Natural intelligence and natural language are a 'trap' due to our ability to deceive, but we are well-trained to manipulate with deceptions and to decode them. Thus, dangers do emerge in open systems, but those very systems are the warrant of being a culture in historical time-dimensions. We have good reasons to preserve and to strengthen human culture: computers will not immunise us against our own capacities. It is worthwhile to be cautious, in spite of the conviction that Dreyfus is mistaken, in fact, in stating that man's nature is indeed so malleable that it may be on the point of changing again. If the computer paradigm becomes so string that people begin to think of themselves as digital devices on the model of work in artificial intelligence, than, since . . . machines cannot be like human beings, human beings may become progressively machines. — Our risk is not the advent of superintelligent computers, but of subintelligent human beings. [40] These fears are exaggerated, since man's ontological features cannot be changed by flawed praxis, but the human being may damage his humanness, his flexibility and creativity. The difference between natural intelligence and artificial intelligence is not one of quantity, but of quality. Human reasoning will remain open for arbitrary, intentional, non-linear response in any situation, and for task-bound deception. As long as both cultures are aware of the limits posed on them by their specific constitution, there is — as Knuth pointed out [41] — no danger in the two-cultures-split, the one which Snow warned of four decades ago. As long as computer-mechanisms are held apart from organic human reasoning, the 'two cultures' can flourish in active coexistence. Alexander Barzel, Kibbutz Kfar Hattoresh, 16960 Israel. Formerly Technion- Israel Institute of Technology. © Society for Applied Philosophy, 1998</page><page sequence="14">178 A. Barzel NOTES [1] R. JuNGK (1954/1963) Die Zukunft hat schon begonnen (Rowohlt). [2] P. S. Churchland &amp; T. K. Sejnowski (1992) The Computational Brain (Cambridge, Mass., MIT Press) p. 460. [3] D. R. Hofstadter &amp; D. Dennett, eds. (1981) Mind's I (Bantam), p. 80 and the following 'conversation' between students of physics, biology and philosophy. [4] H. L. Dreyfus (1993) What Computers Still Can't Do (Cambridge, Mass., MIT Press), p. 198. [5] M. A. Boden (1979) Artificial Intelligence and Natural Man (Basic Books) pp. 122-124. [6] Dreyfus, op. cit., p. 79. [7] M. P. Churchland (1995) The Engine of Reason, the Seat of the Soul (Cambridge, Mass., MIT Press), pp. 243ff. [8] D. R. Fjfstadter (1979) Goedel, Escher, Bach: An Eternal Golden Briad (Vintage Books) p. 642. [9] Hofstadter &amp; Dennett, op. cit., p. 89. [10] Hofstadter &amp; Dennett, op. cit., p. 323. [11] Hofstadter, op. cit., p. 583. [12] Hofstadter, op. cit., p. 582. [13] R. Born (1994) Artificial Intelligence — The case against (St Martin Press) p. XX. [14] S. Lem Non Serviani — in Hofstadter &amp; Dennett, p. 304. [15] Hofstadter &amp; Dennett, op. cit., p. 560. [16] M. P. Churchland, op. cit., p. 138. [17] c.f., M. P. Churchland, op. cit., p. 145. [18] Hofstadter &amp; Dennett, op. cit., p. 685. [19] c.f. Hofstadter &amp; Dennett, op. cit., p. 603. [20] c.f. T. Winograd &amp; F. Flores (1987) Understanding Computers and Cognition, p. 115. [21] Dreyfus and others use the concept 'inner' and 'outer horizons' in the sense in which E. Husserl used them. [22] c.f. M. P. Churchland, op. cit., p. 187. [23] Hofstadter &amp; Dennett, op. cit., p. 571. [24] Hofstadter &amp; Dennett, op. cit., p. 573. [25] Hofstadter &amp; Dennett, op. cit., p.575. [26] Hofstadter &amp; Dennett, op. cit., p. 643 (quoted from George Steiner). [27] c.f. Hofstadter &amp; Dennett, op. cit., p. 655-645. [28] c.f. Hofstadter &amp; Dennett, op. cit., p. 677. [29] c.f. Hofstadter &amp; Dennett, op. cit., p. 532. [30] Dreyfus, op. cit., p. 27. [31] c.f. M. P. Churchland, op. cit., pp. 198-200. [32] Many years after my own researching this subject-matter I found hints for the same problem in Hofstadter &amp; Dennett op. cit., pp. 82-83 and 559. [33] N. Wiener (1954) The Human Use of Human Beings — Cybernetics and Society (Avon Books, 1967). [34] W. J. Rapaport (1988) Syntactic Semantics: Foundation of computational natural language understanding — in J. H. Fetzer, ed. Aspects of Artificial Intelligence (Amsterdam, Kluwer) p. 89. [35] J. Allen (1995) Natural Language Understanding (Benjamin/Cunnings) p. 564. [36] c.f. Allen, op. cit., 625ff. [37] Hofstadter &amp; Dennett, op. cit., p. 197 (see also in Churchland, op. cit., ft. 7). [38] D. J. Chalmers (1996) The Conscious Mind: In search of a fundamental theory (Oxford, Oxford University Press) p. 321. [39] Chalmers, op. cit., p. 322. [40] Dreyfus, op. cit., p. 280. [41] D. E. Knuth (1992) Literate Programming (Stanford, Center for the Study of Language and Information), p. 7f. © Society for Applied Philosophy, 1998</page></plain_text>