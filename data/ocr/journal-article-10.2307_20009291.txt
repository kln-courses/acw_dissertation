<plain_text><page sequence="1">American Philosophical Quarterly Volume 6, Number i, January 1969 VII. GOULD THERE BE A CONSCIOUS AUTOMATON? MICHAEL ARTHUR SIMON T^\0 machines think? Could a machine ever be -*-^ said to feel anything? Are computers capable of mental activity? These questions may be ap? proached by posing two fundamental and dis? tinguishable questions: (i) Could there be created artificially an integrated physical system capable of performing in a manner indistinguishable from that of a human or an animal? and (2) Would such an automaton be actually conscious? Clearly an attack on the problem of machine mentality must focus on the second of these questions. On the other hand, though ( 1 ) is not a conceptual question but an empirical one, it has given rise to considerable philosophical speculation and argumentation. Thus many philosophers, among them notably Descartes, have attempted to support a negative answer to the question of consciousness in artifacts by arguing that there are certain behavioral tests that no automaton could ever pass.1 Nevertheless, the fact is that there now exist results of a theoretical nature that sug? gest that our present inability to produce machines that will entirely emulate persons, or even animals, may be no more than a result of technological in adequacy.2 These results, together with a wealth of experimental accomplishments concerning machines with capacities for purposive behavior, learning, and even carrying on "conversations,"3 serve to point up the relevance of and need for paying close philosophical attention to the question of whether an affirmative answer to (i) would or would not entail attributing consciousness to artifacts. For some, the question of computer conscious? ness is entirely an empirical one : being conscious is considered to be strictly entailed by having the capacity to exhibit behavior similar to that of humans.4 Other philosophers have argued that it is a necessary truth that no robot or artifact, no matter how skilled and versatile its behavior, could ever be conscious; to be a machine entails being norcconscious.5 But those who hold the former posi? tion make the mistake of supposing there to be a strict behavioral test for mental activity; they fail to realize that there can no more be criteria of states of consciousness in machines than there can be for those of humans. On the other hand, those who regard the expressions "artificially-created" and 71 1 See Ren? Descartes, The Philosophical Works of Descartes, tr. by Elizabeth S. Haldane and G. R. T. Ross (New York, 1955) vol. I, pp. 115-116. 2 The McCulloch-Pitts Theory of Formal Networks, for example, declares that any functioning that can be defined at all logically, strictly, and unambiguously in a finite number of words can also be realized by a formal network. The brain is con? strued as a Turing machine : i.e., a finite-state machine that, by scanning, one square at a time, an infinite tape ruled into squares, each containing one of four symbols, can compute any computable number, or arrive at any conclusion that follows logically from any finite set of premisses. In other words, for every mode of behavior and every mental process that can be completely and unambiguously expressed in terms of what the brain does with information, we can design a machine that will duplicate it. See, e.g., John von Neumann, "The General and Logical Theory of Automata," The World of Mathematics, ed. by James R. Newman (New York, 1956), vol. IV, pp. 2070-2098; and Warren S. McGulloch, "Mysterium Iniquitatis?of Sinful Man Inquiring into the Place of God," The Validation of Scientific Theories, ed. by P. G. Frank (Boston, 1956), pp. 159-168. See also the discussions by D. M. MacKay, such as "The Epistemological Problem for Automata," Automata Studies, ed. by G. E. Shannon and J. McCarthy (Princeton, 1956), pp. 235-251; "Mindlike Behaviour in Artefacts," British Journal for the Philosophy of Science, vol. 2 (1951), pp. 105-121; and "Mentality in Machines," A Symposium, Proceedings of the Aristotelian Society, Sup? plementary Volume 26 (1952), pp. 61-86. 3 These achievements are reviewed, and references are provided, by Jerome Shaffer in "Recent Work on the Mind-Body Problem," American Philosophical Quarterly, vol. 2 (1965), pp. 86-92; see especially pp. 87-88. See also F. H. George, The Brain as a Computer (Oxford, 1962), pp. 104-118. 4 This seems to have been the position of Turing in"Gomputing Machinery and Intelligence," Mind, vol. 59 (1950), pp. 433-460, despite his attempt to deny it; see Keith Gunderson, "The Imitation Game," Mind, vol. 73 (1964), pp. 234-245. A more obvious illustration of this view has been provided by the psychologist E. G. Boring in "Mind and Mechanism," American Journal of Psychology, vol. 59 (1946), pp. 173-192. 5 See, e.g., Michael Scriven, "The Mechanical Concept of Mind," Mind, vol. 62 (1952), pp. 230-240; and Satosi Watanabe, "Comments on Key Issues" in Dimensions of Mind, ed. by S. Hook (New York, i960), pp. 143-147.</page><page sequence="2">72 AMERICAN PHILOSOPHICAL QUARTERLY "conscious" as logically incompatible predicates appear to beg the question. For what is at issue here are not the current ways in which we use words but rather situations with which current language has not yet been confronted. Saying that there could never be a conscious automaton for this reason might be like saying that there could never be a talk? ing dog because we would never call such behavior talking. Obviously the question of whether or not there could ever be a talking dog is independent of the question of what we would call such a thing. And in any case, to decide strictly on the basis of current usage what the limits of our concepts will be in future cases is to prejudge the issue at stake. The reason why the question of whether there could be a conscious automaton can be treated neither as strictly empirical nor as a matter of logical impossibility is that the attribution of consciousness seems to require not only that the object in question exhibit appropriate behavior, but also that whoever does the attributing have a special commitment to regard the object as deserving of being so characterized. This is also the reason why saying that a machine could never be conscious does not appear to involve the absurdity that the talking dog case does. For nothing that could ever be observed would count as a refutation of this claim. As Scriven has remarked, consciousness is not a property the observation of which is equivalent to any indefi? nitely long series of behavioral observations; to determine its presence requires an "inference license."6 What I would like to argue is that the question as to whether or not an automaton of full human behavioral capabilities would be accorded con? sciousness would be made in favor of consciousness for the artifact. The position I wish to defend is that mentalistic language not only is the most natural, effective, and efficient language we have for des? cribing either a human being or anything whose behavior is similar to that of a human, but that this mode of description can be logically as appropriate for the characterization of the latter as it is for that of the former, and that the use of this terminology entails the ascription of consciousness to anything to which it is consistently applicable. I shall attempt to establish that the only adequate description of a hypothetical machine whose behavior is indis? tinguishable from that of a human would be a mentalistic one, and that it would be impossible to employ this mode of description while at the same time withholding ascription of consciousness. Let us assume, therefore, that the sort of machine we have been considering could in principle be constructed, and then consider what characteristics an adequate description of it should possess. First it should be stipulated that the terms of the des? cription of this or any machine must be consistent with its status as a (mere) physical object. The language of description must be kept free of the kind of crude and unwarranted anthropomorphism that sometimes proliferates in certain fanciful accounts of computers and lower animals. On the other hand, it is essential that the description do justice to the machine's powers of behaving purposively, learning, adapting, initiating activities, and using language. One of the major problems of characterizing an adequate description will be that of reconciling these two desiderata. Like a human being, our automaton could be described in terms of its overall behavior ; mech? anistic details of its inner workings would not ordinarily figure in our account. The question then arises as to what sort of behavioral description would be adequate. An obvious candidate for this role would seem to be that offered by behavioristic psychology : the machine would be treated as a black box, to be described simply in terms of input and output or stimulus and response. There are at least two objections that can be made to this approach, however. First, there is good reason to believe that no type of stimulus-response theory will ever prove adequate to account fully for purposive behavior in animals and men, and hence in any machine whose behavior resembled that of a human.7 Second, behavior that ordinary language describes quite simply and succinctly often requires extremely elaborate and cumbersome accounts when treated by S-R theory. There would therefore seem to be considerable impetus to search for a means of inter? preting ordinary-language accounts of behavior whereby using this type of description could be rendered compatible with regarding the automaton as nothing more than a physical mechanism. Given the range of behavior that the kind of machine we have been considering is assumed to be capable of, it would appear that what we are deal? ing with is a complex communication system. Our automaton can be viewed as an information 6 Scriven, op. cit., p. 233. 7 Cf. Charles Taylor's impressive and comprehensive critique of stimulus-response theory in The Explanation of Behaviour (London, 1964).</page><page sequence="3">GOULD THERE BE A CONSCIOUS AUTOMATON ? 73 processing or data-handling system. As such, it would receive a description not in terms of internal chemical or physical changes, or of the position of its numerous flip-flops, but rather in terms of sign processes. A communication system is appropriately characterized in terms of what it does with infor? mation. If we could provide a full account of our automaton's performances in terms of information processing, therefore, we should be able to achieve an adequate account of its behavior. It is to some of the important characteristics of this type of account that we now turn. First of all, it deserves to be noted that a de? scription of what a brain or computer does with information is not simply a recounting of the se? quences of physical symbols that constitute the units in which the machine specifically traffics, but is rather an account that indicates the semantic func? tion of these symbols. To employ the expression of Bar-Hillel and Carnap, it is the "semantic infor? mation" that figures in our account: it is not the symbols that we talk about, but rather that which is expressed by a sequence of symbols.8 A proper description of the sign processes carried out by an automaton would be expressed in terms of what these processes symbolize, not simply in terms of their physical embodiments. And if, ex hypothesi, the machine's total behavior with respect to the signals originating in its external environment were in? distinguishable from that which is characteristic of humans, then it would be equally proper to describe the machine itself as dealing with these signs as symbols. If the automaton really does behave as we do, then it must be the case that these physical signs have the same symbolic significance for the machine as for us, and hence it deserves to be characterized accordingly. A set of symbols is effective only by virtue of its content?the meaning or semantic information it conveys. And when we speak about the symbolic content of information processes, we are indicating the manifest effects with which the processing of signals is associated. Thus if we characterize the reception and processing of signals transmitted to a data-processing control mechanism from sensory instruments as the "perception and avoidance of an obstacle," or the performance of a combinatorial Operation on several discrete signal sequences as solving a problem in multiplication," we are ex? pressing what a machine does with physical input in terms appropriate to a description of the cor? responding output. As Peirce pointed out long ago, the meaning or content of a sign process is deter? mined by its "proper signif?cate effects."9 Perhaps the most striking feature of a description of an automaton in terms of the processing of infor? mation, as opposed to one in terms of internally occurring chemical and physical changes, is that such an account seems to be on a much higher level of abstraction than one that refers to inner mech? anisms. An information-processing account, by abstracting from particular physical structures, can be completely neutral with respect to whether the system being characterized is composed of tran? sistors, cardboard, or neurons. Information-pro? cessing obviously depends upon specific material configurations within an automaton, but it is only in a vague or metaphorical sense that we would say that solving an arithmetical problem or general? izing a geometrical pattern occurs inside the machine that does it. The semantic characterization of a data-processing machine is concerned with inner processes only as it concerns their functional relevance in a physically-realized communication system. Like an ordinary-language account of mental activity, it pays no attention to the details of the physical embodiments of the goings-on being described. I have suggested that a semantic account of an information-processing system would be one in which the symbolic processes carried out by the machine might be described in terms used to de? scribe the associated output. An adequate descrip? tion of this output would have to take into account the fact that the machine's overt behavior must be understood as the culmination of preceding and concomitant data-processing operations. So an adequate description of the machine's information process-mediated behavior would have to mention not merely movements, but achievements as well, such as finding a square root or threading a needle, since these are the results of certain symbol-mediated interactions between the artifact and its environ? ment. An automaton's apparently purposive be 8 Y. Bar-Hillel and R. Carnap, "Semantic Information," British Journal for the Philosophy of Science, vol. 4 (1953), pp. 147-157. It is important to avoid conflating this notion of information, which is associated with knowledge of facts or data and is bound up in the question of what is useful enough to count as information, with the technical notion of information, according to which information is simply the measure of the probability of a given signal sequence. The two notions have virtually nothing to do with one another. See Bar-Hillel, "An Examination of Information Theory," Philosophy of Science, vol. 22 (1955), pp. 86-105. 9 Collected Papers of Charles Peirce, ed. by C. Hartshorne and P. Weiss (Cambridge, Massachusetts, 1931-35), vol. V, par. 475.</page><page sequence="4">74 AMERICAN PHILOSOPHICAL QUARTERLY havior would have to be described in frankly ideological terms, i.e., in ordinary language. But what an ordinary-language description would do in such a case would be to state the semantic con? tent or functional significance of the symbolic processes that mediate output that turns out to be indistinguishable from ordinary human behavior. A machine that behaved like a human would, of course, exhibit object-directed behavior, and this behavior would involve intentionality : the "object" to which it is directed need not be an objective reality. Thus an automaton capable of exhibiting a specific and characteristic response to teacups with cracks in them, for example (as distinct from all other teacups), might on certain occasions give the crack-in-teacup response when there is in fact merely a hair in the cup. Such behavior would clearly be intentional, in the sense that the truth or falsity of its characterization as such would seem to depend on something "inside" the machine, or at least on certain undisclosed features of the machine. The question that emerges, then, is: "How in general ought we to characterize the sorts of inter? nal processes that are capable of bringing out such intentional behavior in a machine?" Thinking physicalistically, we may suppose that, for any physical system, whether animal or artifact, to exhibit behavior that could be classed as in? tentional or object directed, its inner mechanisms must assume physical configurations that serve as representations of various environmental objects and states of affairs. These configurations and the electrical processes associated with them would be presumed to play a symbolic function in mediating the behavior. I am suggesting that a description in terms of the semantic content of these symbols would turn out to be an ordinary-language inten? tional description of purposive behavior. Converse? ly, a description of the state of an object expressed in terms of jealousy of a potential rival, perception of an oasis, or belief in the veracity of dowsing rods can be interpreted as the specification of a series of symbolic processes in terms of their semantic con? tent. And such an account is correctly regarded as intentional because its truth depends not upon the existence of certain external objects or states of affairs, but only upon the condition of the object to which the psychological attitudes are attributed. The detection of any bodily or external event by an organized system involves the transmission and processing of signals to and by a central mech? anism. We have noted, however, that it is possible for such an event to be reported to or by the central mechanism "falsely": there may be no such event at all. The first of these facts leads us to produce descriptions in terms of the semantic content of messages and information, and the second provides the basis for the use of intentional idiom. It is a contention of this paper that these two types of description can in fact be identical. Intentionality is a feature of communication systems according as the semantic content of the messages transmitted requires expression in the intentional vocabulary. Thus an adequate description of an automaton capable of exhibiting behavior indistinguishable from that of a human being would amount to a semantic account or interpretation of its data handling capacities. This type of description, more? over, is "mentalistic," at least to the extent that it exploits such verbs as those used to express the perceptual propositional attitudes. Intentional de? scription is thus interpretable as a mentalistic way of reporting information processes. When we give an account of maze-running by a real or mechan? ical mouse, castling to queen's side by a chess playing machine, or running to a window by a dog at the sound of his master's automobile, we may be using a form of mentalistic description as a means of expressing the results of information processes. Such an "anthropomorphic" description can be seen as simply a way of indicating what an organ? ized system does with data it receives and stores. And if a description of this type can really be con? sidered a bona fide objective way of specifying behaviorally relevant sign processes, then an in? tentional description of an automaton's perfor? mance may indicate a commitment only to the validity of the use of a kind of abstract terminology for the description of purposive behavior. But does the extension of intentional description to automata entail application of the full range of mentalistic description in accounting for the behavior of robots? It would appear not, for there seem to be many types of mentalistic predication that are not intentional at all. As Shaffer has re? marked, there is nothing intentional about a sudden bout of nausea or "free-floating" anxiety.10 The fact that we may be justified in describing an autom? aton in terms of perceiving, believing, knowing, wanting, or hoping may not necessarily imply that we are also justified in describing it in terms of feelings and sense impressions. Nevertheless, I believe that it can be shown that the language of sensations and raw feels may be 10 Shaffer, op. cit., p. 85.</page><page sequence="5">COULD THERE BE A CONSCIOUS AUTOMATON? 75 just as appropriate to the description of an autom? aton as is the explicitly intentional idiom. In the first place, it is clear that the acquisition and appli? cation of sensation talk is no less determined on the basis of evert behavior than is that of the intentional vocabulary. Secondly, it can be argued that both types of mentalistic description play the same role with respect to characterizing symbolic processes carried out by a communication system. Each of these segments of mentalistic discourse can be regarded as an essentially theoretical account of behavior. Such a way of viewing mentalistic language has been instructively outlined by Wilfrid Sellars.11 Thoughts, desires, beliefs, and the other propo sitional attitudes have found their way into our language, Sellars argues, as theoretical posits or hypothetical constructs ; instead of using a purely behavioral language, we have developed the theory that purposive behavior is the expression of such things as thoughts. But not only are thought episodes capable of being treated as hypothetical constructs, Sellars believes, but so also are impres? sions, sensations, and feelings. Sense impressions and raw feels are analyzed as common-sense theoretical constructs introduced to explain the occurrence of perceptual propositional attitudes. The verb "to feel" is taken to be a cousin to "to see" and is assumed to have as its fundamental use such con? texts as "He felt the hair on the back of his neck bristle."12 In all cases the concepts pertaining to inner episodes are taken to be "primarily and essentially intersubjective, as intersubjective as the concept of a positron."13 Sellars does not attempt to deny that there is in a person anything like "privileged access" to thoughts and sensations, but he suggests that this feature simply "constitutes a dimension of the use of these concepts which is built on and presupposes their inter subjective status."14 Mentalistic language is thus accorded a role that is independent of the nature of anything "behind" the overt behavior that consti? tutes evidence for the occurrence of any sort of theoretical episodes. What might conceivably be objected to as a defect in Sellars' model, viz., that it may not really do justice to the subjective nature of mentalistic description as it applies to humans, turns out to be an effective justification of the use of mentalistic language for the description of a be haviorally sophisticated artifact. Having argued that only a mentalistic description can be adequate to express the complex behavioral capacities of a humanoid artifact, and that the use of such a description may be entirely compatible with regarding the automaton as nothing more than a highly elaborate data-processing machine, I now wish to try to establish the conclusion that the em? ployment of such an account nevertheless entails the ascription of consciousness to the artifact. In opposition to this conclusion, some might argue that there must be a distinction made between the mental states that we attribute to humans and the information processes that we mentalistically des? cribe in rendering an account of a machine: viz., that the former attribution presupposes the ascrip? tion of consciousness, whereas the latter does not. It might be maintained that the fact that the men? talistic idiom provides a useful and effective means of expressing the semantic content of information processes does not entail that every object to which we apply it must be conscious. Since the presence or absence of consciousness in such a machine is not something that could ever be discovered by empirical means, some might insist that the ques? tion of whether or not the machine is conscious should be treated as independent of the question of what constitutes an adequate description of its behavior. The question of what the implications of the use of mentalistic terms for the description of automata would be has been effectively raised by Putnam, who asks whether a robot really sees, thinks, and feels, or merely "sees," "thinks," and "feels," the quotation marks indicating the uniqueness of the role these concepts have when applied to arti? facts.15 The answer, he argues, depends not upon any discovery, but upon a decision on our part, "to treat robots as fellow members of our linguistic community, or not so to treat them."16 Putnam de? clares that the position that "it is possible that Oscar [Putnam's hypothetical robot] is conscious and that it is possible that he is not" rests on the mistaken 11 See "Empiricism and the Philosophy of Mind," The Foundations of Science and the Concepts of Psychology and Psychoanalysis, ed. byH. Feigl andM. Scriven (Minneapolis, 1956), pp. 253-329; "The Identity Approach to the Mind-Body Problem," The Review of Metaphysics, vol. 18 (1965), pp. 430-451. 12 Sellars, "The Identity Approach . . .," op. cit., p. 436. 13Sellars, "Empiricism and the Philosophy of Mind," op. cit., p. 320 (italics in original). 14Ibid., p. 321 (italics in original). 15 Hilary Putnam, "Robots: Machines or Artificially Created Life?", The Journal of Philosophy, vol. 61 (1964) pp. 668-691. 16 Ibid., p. 690.</page><page sequence="6">76 AMERICAN PHILOSOPHICAL QUARTERLY assumption that consciousness is a well-defined property that each robot, like each human being, either has or lacks; he calls this the "know-nothing view."17 Putnam's contention is that there can be no correct answer to the question, "Are robots conscious?" until a decision is made whether or not to extend the concept of consciousness to arti? ficially-created systems. Putnam is no doubt correct in insisting that the ascription of consciousness depends upon what application we decide befits this concept outside its customary domain. What he fails to do, however, is to explore adequately the conditions which deter? mine such a decision, and to examine the relation of the concept of consciousness to other concepts which we do apply or would apply to machines whose behavior resembled that of humans. The question of whether or not we would actually as? cribe consciousness to such a machine can best be approached, I believe, by considering how we would be most likely to describe it and what the implications of employing this mode of description would be. I have argued that an account that utilizes the mentalistic idiom for the description of anything that consistently behaves like a human being is not only compatible with regarding the object of such a description as strictly a physical mechanism but is also the most effective and economical way we have for describing it. To refuse to use mental language to describe such an object is to refuse to describe it in the simplest and most obvious terms. Whether or not any elaborate behaviorist language could ever provide an adequate description of the behavior of a human or automaton, the point remains that we do not and, I submit, would not employ such an account. It is a fact, for example, that people who work with electronic "brains" often do find them? selves talking about their machines in terms of "knowing" and "remembering." To suppose that, as automata become more complex and their perfor? mance resembles human behavior more closely, descriptions of them will become increasingly mentalistic is more reasonable than it is to make the opposite assumption. One can similarly imagine that a person familiar with these machines might become quite impatient with someone who insisted that his automaton behaves only as if it knows or sees. Not only may we expect to see our mentalistic concepts being extended as a result of technological development, but it is difficult to see how their fate could be otherwise. If it is true that we would in fact employ men? talistic language for the description of certain arti? facts, the important question that remains is whether or not the extension of these concepts to machines would imply the ascription of conscious? ness. But what does it mean to attribute conscious? ness to something? To believe that something is conscious, as Wittgenstein indicated, is to have a certain attitude toward it; the difference between viewing something as conscious and viewing it as non-conscious resides in the difference in the way we would treat it.18 Hence an answer to the question of whether an artifact could be conscious will be yielded by a consideration of what our attitude would be toward a robot that was capable of dupli? cating human behavior. Perhaps the only truly honest answer to the question "How would we treat such a robot" is: "Wait and see." Nevertheless, certain considera? tions lend support to the prediction that if anything should act as if it were conscious, it would produce in the greater part of mankind attitudes that would reveal a commitment to consciousness in the object. One may cite, for example, cases of people acting toward a plant or an automobile in ways that we interpret as presupposing the ascription of consciousness. We consider such behavior to be irrational, to be sure, but probably only because we feel that these objects do not exhibit in their total behavior sufficient simi? larity to that of humans to deserve the attribution of consciousness. Thus it is the machine's lack of versatility that forms the ground of MacKay's re? mark that consciousness is too high a prize to grant on the basis of mere chess-playing ability.19 On the other hand, anthropomorphism and consciousness ascription in giving an account of a nonbiological system may not always be so reprehensible. A per? son who looks upon a robot as conscious is surely not guilty of the same degree of irrationality as is associated with cruder forms of anthropomorphism. As an illustration of the capacity of an artificially created object to earn the ascription of conscious? ness we might consider the contemporary French motion-picture short entitled "The Red Balloon." In this film, a small boy finds a large balloon which 17 Ibid., p. 687. 18 Ludwig Wittgenstein, Philosophical Investigations (Oxford, 1958), Pt. I, sects. 299, 310, 420, 422; pp. 178, 226. 19 D. M. MacKay, "The Use of Behavioural Language to Refer to Mechanical Processes," British Journal for the Philosophy of Science, vol. 13 (1962), p. 99.</page><page sequence="7">GOULD THERE BE A CONSCIOUS AUTOMATON? 77 becomes his "pet," following him about without being held, and waiting for him in the schoolyard while he attends class and outside his bedroom window while he sleeps. No speech or any other sound is uttered by either the boy or the balloon, yet by the end of the film the spectators all reveal in their attitudes the belief that the balloon is con? scious, as they indicate by their reaction toward its ultimate destruction. There is a strong feeling, even on the part of the sceptic, that one cannot "do justice" to the movements of the balloon except by describing them in mentalistic terms like "teasing" and "playing." Use of these terms, I submit, con? veys commitment to the balloon's consciousness. An objection that might be raised at this point would be to insist that our attitude toward any? thing we knew to have been artificially created would not exhibit sufficient similarity to our attitude toward human beings to warrant the claim that we would actually be ascribing consciousness to an inanimate object. Thus Malcolm reports that Witt? genstein in lectures discussed an imaginary tribe of people who had the idea that their slaves, though indistinguishable in appearance and behavior from their masters, were all automata and had no feelings or consciousness.20 Wittgenstein considered how these slaves might have been treated by their masters : When a slave injured himself or fell ill or complained of pains, his master would try to heal him. The master would let him rest when he was fatigued, feed him when he was hungry and thirsty, and so on. Further? more, the masters would apply to the slaves our usual distinctions between genuine complaints and ma? lingering. So what could it mean to say that they had the idea that the slaves were automatons? Well, they would look at the slaves in a peculiar way. They would observe and comment on their movements as if they were machines. ("Notice how smoothly his limbs move.") They would discard them when they were worn and useless, like machines. If a slave received a mortal injury and twisted and screamed in agony, no master would avert his gaze in horror or prevent his children from observing the scene, any more than he would if the ceiling fell on a printing press. Here is a difference in "attitude" that is not a matter of be? lieving or expecting different facts.21 One can reply to this objection without denying its conclusion, although it can also be argued that there is as much reason to believe that a sufficiently clever, attractive, and personable automaton might eventually elicit "humane" treatment, regardless of its chemical composition or early history, as there is to believe the contrary. Let us assume that we would in fact treat a robot in the manner Wittgenstein suggests. Would this treatment involve the ascrip? tion of consciousness? I submit that it would. Even though our concern for our robot's well-being might go no further than providing the amount of care necessary to keep it in usable condition, it does not follow that we would not regard it as conscious. Putnam suggests that the alternative to extending our concept of consciousness so that robots are conscious is " 'discrimination' based on the 'softness' or 'hardness' of the body parts of a synthetic 'organism'," an attitude he likens to discriminatory treatment of humans on the basis of skin color.22 But discrimination of this sort may just as well presuppose the ascription of consciousness as pre? clude it. We might be totally indifferent to a robot's painful states except as these have an adverse effect on performance; we might deny it the vote, or refuse to let it come into our houses, or we might even willfully destroy it on the slightest provocation, or even for amusement?and still believe it to be conscious, just as we believe animals to be conscious despite the way we may treat them. If we are in? deed capable of becoming scornful of or inimical to a robot, we cannot plead innocent to the charge of consciousness-ascription. In the Philosophical Investigations Wittgenstein re? marks that, while it is possible under certain condi? tions to imagine that other people are automata and lack consciousness, in the midst of ordinary inter? course with others the words, "the children over there are mere automata ; all their liveliness is mere automatism," may become quite meaningless.23 I believe that a similar point can be made concerning actual artifacts : it could become quite meaningless in certain contexts to call a robot whose "psycho? logy" is similar to that of humans a mere automaton, as long as the expression "mere automaton" is supposed to imply lacking feeling or consciousness. Our attitude toward such an object, as indicated both by the way we would describe it and by the way we would deal with it, would belie any expres? sion of disbelief in its consciousness. Acceptance of an artifact as a member of our linguistic community does not entail welcoming it fully into our social 20Norman Malcolm, "Wittgenstein's Philosophical Investigations" The Philosophical Review, vol. 63 (1954), pp. 548-549. 21 Ibid. 22 Putnam, op. cit., p. 691. 23 Pt. I, sect. 420.</page><page sequence="8">78 AMERICAN PHILOSOPHICAL QUARTERLY community, but it does mean treating it as con? scious. The idea of carrying on a discussion with something over whether that thing is really con? scious, while at the same time believing that it could not possibly be conscious (an idea that Albritton tries to entertain24), is unintelligible. And to say, "The robot insisted that it is conscious, but I don't believe it," is self-contradictory. Epistemologically, the problem of computer con? sciousness is no different from the problem of Other Minds. No conceivable observation or deductive argument from empirical premisses will constitute a proof of the existence of consciousness in anything other than oneself. To the extent that we do in fact talk about other people's conscious states, however, we are committed to a belief in other minds, for it is simply false to suppose that mentalistic expres? sions have different meanings in their first-person and second- and third-person uses. But if we assume that these expressions mean the same regardless of the subject of predication, then we must concede that our use of them in describing the behavior of arti? facts also commits us to a belief in computer con? sciousness. It makes no sense to say that a thing is acting as if it has a certain state of consciousness or feels a certain way unless there is some demonstrably relevant feature that supports the use of "as if" as a qualifying stipulation. And to be unable to specify the way in which mentalistic descriptions apply to objects of equal behavioral capacities is to be unable to distinguish between the consciousness of a person and that of an automaton. If we find that we can effectively describe the behavior of a thing that performs in the way a human being does only by employing the terminology of mental states and events, then we cannot deny that such an object possesses consciousness. Consciousness thus turns out to be a property that is attributed to physical systems that have the ability to respond and perform in certain ways. An object is called conscious if and only if it acts consciously ; to act consciously is to behave in ways that have resemblance to certain biological paradigms and lack resemblance to certain nonbiological ones. If a thing behaves in ways that warrant description in terms of intelligence, thinking, deliberation, reflec? tion, or being upset, confused, or in pain, then it is meaningless to deny that it is conscious, for the very language we use to describe its behavior is itself the evidence for saying that it has consciousness. One cannot build a soul into a machine, but once he has constructed a physical system that will do anything a human being can, neither will he be able to keep a soul out of it. Received October 20, ig6y The University of Connecticut 24 Rogers Albritton, "Mere Robots and Others," The Journal of Philosophy, vol. 61 (1964), p. 693.</page></plain_text>