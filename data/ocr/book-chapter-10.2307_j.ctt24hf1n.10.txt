<plain_text> <page sequence="1"> 02THEORIES OF SCIENTIFIC  PROGRESS: HELP OR  HINDRANCE? We expect to have a problem in understanding what honeybees see because  they have a tiny brain combined with a very wide view of the world: multum  in parvo. We must draw conclusions from the way they behave. We reasonably  expect that they detect only relatively simple parts of the scene, but at first we  are unable to imagine how they see anything in a moving panorama. To make  progress, we have to devise ways of asking questions of the bees so that logical  conclusions can be made from the way they react. This chapter is about making  firm conclusions. Unlike most experimental science, there is no need for equipment, purified  chemicals or electricity. Anyone with some patterns and sugar solution could  have inferred most of what we know about bee vision at any time in the past  centuries. That did not happen. Progress was excruciatingly slow, although the  bees were eager for lessons once they learnt that sugar was available at school.  Why the delay?  At each step, progress was limited by error and the slow development of ideas,  so it took a long time to formulate each next appropriate question. At first,  the questions put to the bees never produced sensible answers. The bees were  observed, their responses to experimental change were a mystery and the  proposed explanation was just a guess. There were many unsuspected factors and  guesses that were not tested became facts. The errors blocked the imagination  of those who followed. As a result of this patchy acceptance of a mixed bag of  insight and error, there was no acceptable answer to the interesting question  ‘What does this insect actually detect?’ The question was not asked. Let us examine the development of scientific theories to see whether the ways  of thinking about explanations—what we call the philosophy of science—have  been of any help.  19 </page> <page sequence="2"> WhAT DoES ThE honEyBEE SEE AnD hoW Do WE knoW? Early theories of scientific advance There is a long and fascinating road that winds through history and explores how  the natural world was elucidated. The problem faced by the great innovators of  the philosophy of science in the past 3000 years was to find a general method  that would apply to any problem, although as things turned out, this was a  bad place to start. The process is not direct because the best questions to ask  become obvious only when the answers are anticipated. The process starts with  collecting facts of interest long before any moment of truth arrives. We have to  observe and think at the same time, followed by a dissection of the subject into  components, an effort at analysis to see what causes what, and then we must  assemble the tentative mechanisms into a coherent story.  Aristotle,1 an ancient Greek philosopher, taught that we should accumulate  facts and look for generalisations about them. In coming to his own conclusions,  however, he was usually short of facts and relied on the primitive assumptions of  the day. As a result, he was knowledgeable but often mistaken. He was unwilling  to abandon his general principles, although, on the topic of the reproduction of  bees, he admitted that there were insufficient facts to warrant any conclusion  at all.  Although real experiments had been done for millennia—for example, helpful  and fatal trials with medicinal herbs—the idea of the experimental approach and  the concept of an experimentum crucis was first systematised by Francis Bacon2  at the end of the sixteenth century. The idea was to invent a crucial experiment  that allowed the observer to decide between two alternatives. We now know  that it is a rare piece of luck to find such an experiment that is conclusive.  It can be wrongly conceived, so that the result cannot be interpreted, or there  might be more than one explanation, or new facts emerge later. Bacon stressed  that the gathering of facts must be steady and progressive, with conclusions  at each stage, and this advice was followed by great scientists such as Charles  Darwin, but Bacon had no idea how a scientific concept or theory was formed  in the first place. He advised us to be suspicious of first principles (meaning  Aristotle’s principles), but we still find them getting in the way today, such as  the belief that there is something special about symmetry or the idea that insects  see things, even if blurred.  Bacon was aware of the danger of proposing a theory and then inventing  experiments to prove it, but he could not stop the practice. You have an idea out  of the blue, then enthusiastically rush around proving it. Sadly, it was equally  useless to rely on Bacon’s pet method: induction. This is the formation of a general  principle that is consistent with a number of separate facts. Induction depends on  regular occurrences and the uniformity of natural events. It is, however, boring  to collect facts without knowing why. The opposite of induction is deduction, in  which the observations are logically deduced from data and general principles.  20 </page> <page sequence="3"> ThEoRIES oF SCIEnTIFIC PRoGRESS: hElP oR hInDRAnCE? Alone, or even together, deduction and induction are not strong enough, or  even sufficient, to generate useful experiments. The two missing ingredients are  imagination and a caution about multiple causes. Like Aristotle, Bacon argued  as though a phenomenon could have only one cause. The visual system—with  numerous receptors in parallel, multiple pathways to the brain and numerous  superimposed arrays of nerve cells, always changing with time—would never  be understood if single effects always had single causes. The classical and medieval minds tended to work in terms of rather rigid  categories with sharp boundaries. Something was either this or that. They  respected categories as though they had been created with the universe and  had an independent existence. Classifications also had a value of their own. The  categories ruled the discussion without being questioned themselves, directed  the next venture and diverted attention away from unexpected but significant  novelties. Observations were suspect, as Galileo was firmly told by the Church.  We still see the pleasure enjoyed in an armchair argument about concepts and  the definitions of terms.  More recently, we have been urged to think not only of alternative causes, but  of all the intermediate stages between them. Categories also become blurred.  I prefer to assume that the visual system operates with parallel pathways, each  with a definite function. As a first approximation, I assume that each type of  component and pathway can be analysed separately with yes/no answers if the  appropriate tests can be devised. So far, it has worked. In the late seventeenth century, John Locke (1632–1704) traced the origins of  knowledge, while David Hume (1711–76) analysed ideas about causation in the  mid-eighteenth century. Bishop Berkeley doubted the evidence of the senses  but still relied on learning and commonsense. These English philosophers were  more empirical than their continental colleagues and, in the early nineteenth  century, the differences were sharply intensified in the battle between John  Stuart Mill (1806–73) and Sir William Hamilton (1788–1856), who in general  accepted as valid anything that was intuitively obvious, especially the rules of  reasoning and the evidence of the senses. Hamilton imported these ideas from  Germany. For centuries, induction had also been relied on, with little criticism.  As already mentioned, induction is the method of inferring the general rule  from the particular instances. The more general statement that applies to many  situations is derived from a number of less general statements that apply to  only some cases. Induction is based on two principles: that nothing can happen  without a cause, and that the same combination of causes is always followed by  the same effect.  The methods of scientific induction—noted mainly because they were effective  in the industries developing in the foundries, potteries and factories, and  were indispensable for progress in physics and chemistry—were summarised  by William Whewell (1794–1866) in an influential book The Philosophy of the  21 </page> <page sequence="4"> WhAT DoES ThE honEyBEE SEE AnD hoW Do WE knoW? Inductive Sciences (1840).3 With one foot still in the past, Whewell accepted the  ancient view that the rules of thought, including the intuitive recognition of  categories in visual perception, were built innately into the human mind. They  were not to be questioned. This was in line with German philosophers, of whom  Immanuel Kant (1724–1804) was the most influential in the early nineteenth  century. Kant assumed that reason was not subject to space or time. Basically,  a reasonable cause that was proposed on intuitive grounds was accepted until  further observations made it untenable.  In the early nineteenth century, accompanying the further development  of mathematics and the exact sciences, empirical philosophy was strongly  promoted by an intellectual prodigy, John Stuart Mill. Mill put the arguments  of the English empiricists of the previous century—Berkeley, Hume and  Locke—into a systematic framework. He replaced intuition with learning from  experience, particularly by relying on numerous observations and deducing  their logical consequences. A lack of independent checks infuriated Mill, who,  in 1865, wrote a long condemnation of Kant’s support for intuition. To Mill, the  combination of induction and intuition was the way to errors of thought, and  the German philosophers were a threat to right thinking. Apart from governing India from a distance, writing a stream of articles in favour  of freeing slaves, the liberation of women and guiding the social conscience,  Mill’s contributions were crucial for the development of experimental science,  especially biology.4 In his book the System of Logic (1843), Mill laid down the  rules for the inference of causes from effects. I recommend them as a guide to any  budding investigator. Mill distinguished between necessary causes, sufficient  causes and possible causes. He accepted multiple causes operating in parallel  and repeated Newton’s advice that ‘no more causes of natural things are to be  admitted than such as are both true and sufficient to explain the phenomena’—a  principle that is usually called ‘Occam’s razor’. A necessary cause is one that is  logically required. A sufficient cause is one that is adequate but there might be  more to be said.  Second, Mill did not accept anything just because it appeared to be intuitively  so or was a reasonable guess. Even Mill’s most abstract works were aimed against  the German a priori school, called ‘Intuitionism’ and best known in the works  of Kant. Mill denied any ability or performance that was reckoned to be ‘innate’  and instead derived all human knowledge from human experience: ‘The notion  that truths external to the mind may be known by intuition or consciousness,  independently of observation or experience, is, I am persuaded in these times,  the great intellectual support of false doctrines and bad institutions’ (Mill  1843). To him, all causes, inferences, conclusions or categories were obtained by  making bare observations, noticing regularities and then deducing the causes.  A century before Jean Piaget, therefore, the development of the human mind  was an exercise in self-education by trial and error.  22 </page> <page sequence="5"> ThEoRIES oF SCIEnTIFIC PRoGRESS: hElP oR hInDRAnCE? Mill was well known and influential among the scientific community in London,  where he became Secretary of the India Office and later MP for Westminster.  Mill, once said to be the cleverest man in the world, also demonstrated that there  was more to science than observations, empirical laws and rules for scientific  investigations. As a result of his efforts, the teaching of philosophy in England  was deflected from the path led in Europe by Kant and saved England from the  Gestalt and holistic psychologists of Vienna and Zürich.  At the time, these ideas had no effect on research on the vision of the bee.  Instincts were proposed and accepted as innate as explanations of behaviour. An  experiment was an observation, followed by a guess about the cause. Most of the  philosophers of the nineteenth century and more recent times were of no further  help, being engrossed with the meanings of words and the theoretical basis of  physics, astronomy, mathematics and the relation between mind and matter.  Towards the end of the century, however, Mill’s methods were taken up in the  United States by C. L. Morgan (1890), Edward Thorndike, Margaret Washburn  and J. B. Watson, who opened the subject of experimental psychology, but little  of this spread back to Continental Europe. We can detect Mill’s influence in  the work of Herbert Spencer Jennings (1868–1947), who concluded that the  detailed behaviour of lower organisms was controlled largely by learning by  trial and error (Jennings 1906).  Mill’s rules allow us to make deductions from observations and experiments,  provided we do not ignore some hidden cause. Unfortunately, we can never  list all possible causes. Another difficulty has always been to arrange sufficient  examples so that a common cause is established. A third difficulty is that a  number of facts might be totally unrelated but we might still derive a theory  from them. A fourth difficulty is that we might be totally ignorant of the type of  system being studied—for example, whether memory is a solid-state molecular  transformation, a wet chemical reaction or a rearrangement of connections  between nerve cells—so no lasting conclusions can be made. Perhaps the most  common error is to waste time on facts that prove useless. The most dangerous  error is to postulate a hidden cause, give it a name, raise it to the status of reality  and then validate it by devising an experimental proof, while still missing the  real cause. This is called ‘misplaced concreteness’. These potholes produce errors  of deduction and account in part for the hesitant and meandering progress in  every branch of science, but especially in vision and analysis of the nervous  system, where we start with multiple causes in parallel but no map.  For experimentalists, Mill’s best contribution was originally Newton’s idea that  a postulated cause must be capable, realistically and mechanically, of producing  the effect that was observed. He went further and advised that the nature of  the postulated cause should be demonstrable by an independent means. We can  23 </page> <page sequence="6"> WhAT DoES ThE honEyBEE SEE AnD hoW Do WE knoW? translate this as ‘list the components and find out how the mechanism works,  then confirm it experimentally’. How few students of animal behaviour even  bother to list the components! How easy to label the performance ‘cognitive’!  Figure 2.1 a) Two patterns of similar area and position on the targets that are easily  distinguished by bees in the apparatus shown in Figure 1.1d. The patterns are fixed,  not shuffled in position, but are interchanged every five minutes to ensure that the  bees look at them. The square is the rewarded pattern. b) The intuitive idea that  the bees ‘compare the stored image with the current image of another shape’ by  the areas of overlap and non-overlap when they are superimposed. Source for (b): After Wehner (1981:Fig. 86). how mill’s logic was applied to bee vision Now we come to the part that requires a little concentration. Figure 2.1a shows  the success rate when bees are trained to select the rewarded black square, but  those choosing the oblique rectangle receive nothing. The two patterns were  in the apparatus in Figure 1.1d, of the same size at the same centre, to give  the bees a fair choice. The two targets were interchanged every five minutes to  teach the bees to look at them and not simply learn to go left or right. The bees  clearly learned this task, but the real score means little because it depends on  the length of the training, the hunger of the bees, and so on.  Until recently, the success of the bees would have been explained by the  difference in shape of the square and the rectangle. It was proposed that the  bees measured the region of overlap and the regions of non-overlap (Figure  2.1b). This is a general explanation that could apply to all patterns, but we have  no indication that it is the correct explanation. In fact, it was a misleading guess. 24 </page> <page sequence="7"> ThEoRIES oF SCIEnTIFIC PRoGRESS: hElP oR hInDRAnCE? Figure 2.2 a) Training patterns; the bees avoid the bar. b) The trained bees avoid  the bar when displayed versus a bar that is moved down. c) They avoid the bar  when displayed versus a bar with modified edge orientation. d) They distinguish  the edges of the original bar from the square. e) They confuse the edges alone  with the original bar, which they fail to recognise. These results show that the  trained bees recognised only the orientation of the bar edges in the expected  position. They say nothing about the square. Now turn to Figure 2.2, where the trained bees are given four tests to reveal  what they have actually learned. They distinguish the original bar in the  training from the same bar moved down (Figure 2.2b), so they are sensitive to  bar position. They distinguish the bar from a similar shape with stepped edges  (Figure 2.2c), so they detect the orientation of the edges. They distinguish the  square from a hollow bar (Figure 2.2d), which by itself tells us little; however,  the trained bees have equal preference for the hollow bar and the original bar  (Figure 2.2e). In this case, whatever makes the bees avoid the oblique black bar  is displayed on both targets in Figure 2.2e, so in the training they have learned  to avoid the orientation on the edges of the oblique bar. 25 </page> <page sequence="8"> WhAT DoES ThE honEyBEE SEE AnD hoW Do WE knoW? Figure 2.3 a) Training patterns, as before. b) The trained bees fail to distinguish the  square from a horizontal bar; or c) from a bar with stepped edges. d) They also fail  when the square and the bar are both moved; and e) when a black spot is added.  They do not recognise the square in any of the tests. Figure 2.3 shows what they did not learn. The trained bees were tested with  the original black square in the training versus other patterns. The square was  chosen equally with the rotated bar (Figure 2.3b), or with the bar with stepped  edges (Figure 2.3c), because the orientation cue was not displayed on either  target. The square was chosen equally to the bar when both were moved (Figure  2.3d) because the expected parameter was not in the expected place. Finally,  the original patterns were chosen equally when a large black spot was added  (Figure 2.3e), because this additional parameter was not expected, so the bees  failed to recognise the place. Clearly, the bees had not learned to recognise the  square or the bar, only a simple cue in the expected place. This experiment  does not exclude the possibility that bees can recognise some patterns that were  untested. In my search for the cues that bees use, I tried a large number of pairs of patterns.  Some they discriminated, others they did not. Between every pair of patterns  that bees are able to discriminate there is obviously a difference, or more than  one, which the trained bees detect. When we find a number of successfully  discriminated pairs of patterns (Figure 2.2), and the pairs have only one common  factor or common difference, we have probably found the common cause—that  26 </page> <page sequence="9"> ThEoRIES oF SCIEnTIFIC PRoGRESS: hElP oR hInDRAnCE? is, the cue that they detect, if we have persisted in looking at enough examples.  This is Mill’s ‘Method of Agreement’ in the search for factors. In the above  example, the parameter was the edge orientation at the expected place.  Although thoroughly criticised by later writers, Mill’s rules are still useful in the  crucial design stage of experiments and in making inferences from them. When  there are groups of pairs of patterns that bees cannot be trained to discriminate,  there is likely to be a common factor that is missing from all of them (Figure 2.3).  This is Mill’s method of ‘Agreement in Absence’, which is very decisive if the  number of instances is large. In all cases where an effect is missing and one  possible cause is consistently absent, there is a strong presumption that these  two circumstances are cause and effect in spite of possible multiple causes. There  might, however, be several reasons why the bees fail. We saw this in the failure  caused by the addition of a spot, which was in fact a strong salient parameter  that was easily recognised by bees.  When bees fail to learn to distinguish a pair of patterns, or trained bees fail  to discriminate in tests (Figure 2.3), we call it negative evidence. It is good  evidence of the inability to perform, which is not the same as absence of  evidence. Although it can be observed and confirmed, the bees’ failure to  discriminate still gives us a problem. We have to be careful that the same bees  can discriminate other similar pairs of patterns, so that we are sure that they  are not failing for some trivial reason. The bees might be unable to learn to  discriminate because they fail to notice the patterns or the cues displayed, the  two patterns might display the same cues or the bees might be unable to stabilise  their eyes on the targets. Similarly, when trained bees are tested, they might  fail because they detect an unfamiliar cue on the test targets or because the  preferences for the available cues are balanced on the two targets (Figure 2.3e),  so it cannot be assumed that one cue has been omitted. The ambiguity makes  the research harder and longer but the situation can be resolved with a sufficient  number and variety of tests. The solution to the difficulty is to take two patterns  that bees prefer equally despite extensive training—that is, that they cannot  learn to distinguish (Figure 9.12)—then add a parameter that bees recognise.  The resulting learning is then a positive demonstration that the parameter is  effective when the patterns are not. Similarly, when the bees succeed, we must devise control experiments to show  that there is not some other irrelevant cue, such as an odour, a difference in size,  range, position or illumination, which enables the bees to ‘cheat’. When we find  two patterns that bees easily discriminate, but with no known cause, we can  suspect that there is a previously unknown parameter.  In the study of bee vision, I assume that the bees detect certain features in  the patterns—called parameters—to which their feature detectors are innately  adapted. They remember something in the brain derived from the feature  detector responses—called a cue—that is a small part of the whole pattern. There  27 </page> <page sequence="10"> WhAT DoES ThE honEyBEE SEE AnD hoW Do WE knoW? is no evidence that the bee brain is able to reassemble the visual inputs to make  a more complete picture. This is ‘absence of evidence’ and not conclusive. There  are many pairs of patterns that look different to humans, however, which bees  are unable to discriminate in training experiments (Figure 2.3 and Chapters 9  and 11), so I infer that they generate no cue or no difference in cues. Clearly,  the bees do not distinguish or recognise them. This is ‘evidence of absence  of recognition’ in these examples. Now, to emphasise the effectiveness of the  empirical method to investigate the matter further, look again at Figure 2.1b.  The intuitive idea of overlaps and non-overlaps of shapes never surfaced in the  exposure of the parameters. It was just a guess.  Problems of applying theory With logic defined, why was progress so slow? In the late nineteenth century, apart from the efforts related to Darwin’s theory  of evolution by natural selection, biology produced almost entirely what we now  call ‘natural history’. It was a period of belief in the progressive improvement of  understanding, but the biological sciences were mostly descriptions of species,  anatomy, development, fossils, geographical distributions, physiological systems  and the chemistry of some processes in animals and plants. At the very end of the  century, detailed anatomy, histology and the physiology of the nervous system  became established. The analysis of insect vision took off in Germany with the  works of two giants, Grenacher (1879) and Exner (1891), who studied stained  sections of the insect retina with newly invented compound microscopes and  provided the basis of our modern knowledge of the compound eye (Chapter 5).  The advances in the nineteenth century were made, for the most part, by  men who were thoroughly conversant with the knowledge of their time and  who made more than one outstanding advance. Romanes was famous for his  work on the nervous systems of primitive animals. Forel was a distinguished  entomologist. von Frisch dedicated his whole scientific career to the study of  the honeybee. At the time, the interest lay in describing and making sense of  the performance. Bees were trained and the brain of the bee was recognised as  adequate for its own dedicated tasks, but there were no techniques to reveal  mechanisms. Possible mechanisms were not mentioned until the late twentieth  century. The scientists knew how to plan an experiment, and the experimental  equipment was available, but there was no fund of knowledge of what tests the  bee had previously passed or failed. The early experiments show that the early  scientists simply did not know what to do.  A number of properties of the visual system contributed to these difficulties. 28 </page> <page sequence="11"> ThEoRIES oF SCIEnTIFIC PRoGRESS: hElP oR hInDRAnCE? Systemic hurdles  1. Diversity A drag on the advance of ideas was the enormous diversity of insects with  miraculous behavioural patterns. The fascinating descriptions concealed the  lack of analysis. The first stage was the assumption that sign stimuli were used  in the recognition of mates, food or predators. The sign stimuli, however, are  the consistent signals such as colours, movements or markings, which humans  intuitively assume the animals detect. This work is still going on. It is far more  difficult to identify the real features that enter the nervous system. They are  much smaller than the sign stimuli and are probably common to all insects.  When the feature detectors have been identified from behavioural experiments,  they can be sought by electrophysiology.  2. Parallel processing As Grenacher, Exner and Ramon y Cajal showed in great detail about the turn  of the century, the insect retina has thousands of photoreceptors in parallel and  feeds into several arrays of pathways in parallel that pass through successive  layers of neurons. The first requirement is to list the anatomical components—a  formidable task not yet complete even in the most studied insect, the fruit fly.  Finding the rules of action requires hundreds of tedious physiological recordings  of the individual nerve cells during the continuing behaviour, as well as decades  of obsessive study of their connections. Having done all that, there is still a long  way to go, because the nervous system functions by the coincidences between  activities of neurons in arrays and they learn, so they are not constant. To analyse a system of elements in parallel it is useful to know what the  mechanism cannot do or what is not there. For example, if a genome is known  not to contain a particular gene, it is known that that gene is not essential for  any remaining process in the living animal, and this fact helps the analysis of  the gene’s function elsewhere. Whether or not he had a motive, if the suspect  proved his absence, he could not have been the agent. This is the correct use  of the evidence of absence. In law, it is the theory of the alibi. Further, if there  was no motive to kill grandma, no sign of violence and there was no-one around  at the time, grandma probably died of natural causes, so a positive deduction  might be made. Positive evidence of absence is perfectly valid and is essential  when there are many possible causes that can be eliminated one by one. Critics  of negative evidence, please note. There are other ways to separate multiple causes. In the 1920s, R. A. Fisher  (1935) developed novel experiments randomising different treatments of  agricultural crops with different amounts of fertilisers, soil types and water, to  separate the effects of possible multiple causes. Right from the beginning of our  work with trained bees, we eliminated some possible causes by randomising  those aspects of the stimulus that we wanted the bees to ignore, as in the papers  29 </page> <page sequence="12"> WhAT DoES ThE honEyBEE SEE AnD hoW Do WE knoW? by Lehrer  et  al. (1988) on the measurement of range irrespective of size or  position and van Hateren et al. (1990) on the discrimination between horizontal  and vertical bars irrespective of bar width or position. The method appeared  pathetically late in the analysis of insect vision. From 1909 on, bees were trained to discriminate a pattern or between a pair  of patterns displayed on movable targets that were shuffled in position to  prevent the bees learning their locations (Figure 1.1d). This had the unexpected  consequence that the bees ignored local landmarks outside the training targets  (Chapter 12). From 1926 onwards, the trained bees were also tested with patterns that were  related to the training pattern or various parts of it. The method was progressively  refined after 1990 as more cues within the bees were recognised and their limits  found. It was essential that some of the test patterns contained the necessary  cues and others did not. Results obtained using all the methods, in the hands  of many researchers using a variety of training and testing techniques, were  eventually all explained by the same few simple cues in parallel, as listed in  Chapter 9. 3. Feedback loops One of the great hurdles that had to be surmounted in the analysis of the  nervous system was the control of the action by a part of the action itself.  The feedback loop makes the system look purposeful. In the nervous system,  a feedback loop can be within the animal—for example, the sensors at our joints  and in the muscles keep our limbs in constant positions under a varying load.  We would not be able to stand up without these feedback loops. The loop can be  outside the body. We guide our hands and feet with our eyes. We hear our own  voice and adjust it under control. Feedback loops were not really understood  until cybernetics became popular in the 1960s, and it took decades before the  experimentalists found even poor methods of analysis by breaking or clamping  the loops, or more refined methods by replacing the natural loops with artificial  ones. 4. Preconceived ideas Despite Mill’s efforts, a trust in intuition and acceptance of causes that looked  reasonable were serious sources of error in every decade. Intuition was  represented in every aspect of bee vision in the twentieth century—not only  by the Gestalt psychology that influenced Mathilde Hertz and, through her,  other studies of bee perception. We will find numerous examples in the coming  pages, even though Morgan (1890) advised readers to ‘endeavor to distinguish  observed fact from observer’s inference’ and ‘to apply Occam’s razor, especially  to proposals of cognition in animals’.  30 </page> <page sequence="13"> ThEoRIES oF SCIEnTIFIC PRoGRESS: hElP oR hInDRAnCE? Almost universally, it was assumed that the behaviour of insects was genetically  innate and that a sign stimulus initiated a predetermined act. Quite the contrary:  if the insect is allowed to initiate its own movements, it quickly modifies them  by rapid learning that eliminates errors and selects the effective responses.  This was described by Jennings (1906) as ‘reaction by varied or overproduced  movements, with selection from the varied conditions resulting from these  movements’. We now know that this applies to simple motor movements and  to relearning the effects on sensory inputs by all parts of the central nervous  system, even in the posture of the legs and the supposedly fixed opto-motor  response. The most insidious preconception, damnably difficult to escape, is to read human  sensations into animal systems—and in this case to assume that bees ‘see’. For  most of the twentieth century, this anthropomorphism led to an assumption,  derived from the study of primate brains, that in the brain of the seeing bee  there was a structural layout of the image—a bad guess that was not excluded  until 2005 (Chapters 11 and 12). Backsliding into intuitionism From about 1900 onwards, two theories of general application and wide  influence appeared in studies of vision. First, Wertheimer (1912) found that  the mechanistic concepts of his time failed to explain why human perception  seemed to proceed from an assessment of the whole to the recognition of the  parts. Nowadays, there is abundant evidence of human top-down processing, as  in visual search and size constancy. Wertheimer was followed by Koffka (1924)  and Koehler (1925) and the Gestalt theory spread—notably to bee vision. The  word ‘Gestalt’ is translated as ‘configuration of a whole’. The modern equivalent  is the recognition of configurational layout.  In human vision, the Gestalt theory included numerous laws and factors. The  most important is that the human visual system transforms the stimulus into  the most perfect that the situation allows—for example, a ring of dots becomes  a circle. Characteristics such as regularity, inclusiveness, symmetry, simplicity  and unity are detected intuitively, even if present in an imperfect form. Similar  patterns or objects that move together are seen as groups and are classified into  categories. An enclosed surface is seen as a shape. This was the accepted theory  when Hertz researched bee vision. Mill would have suggested that Gestalts were  all learned. The second corrosive influence was the development of ethology by Konrad  Lorenz and Niko Tinbergen, who used intuitive concepts such as ‘fear’ and  ‘drive’. In the 1930s, Lorenz published numerous observations of the behaviour  of free-living animals and, with Tinbergen, produced a comprehensive theory  of instinctive behaviour. The responses were ‘innately controlled movements’  31 </page> <page sequence="14"> WhAT DoES ThE honEyBEE SEE AnD hoW Do WE knoW? initiated by a ‘specific releasing mechanism’ (internal) or a ‘sign-stimulus’  (external). They were driven by ‘reaction-specific energy’ that could accumulate  and also be ‘depleted’ by the activity itself. Actions were directed with an  intention of obtaining food or a mate or driving away a rival. Behaviour was  basically stereotypical and inherited, but could be modified by learning.  The  theory was developed into a hierarchical model with many layers and  parallel pathways. In my terminology, this was mostly detailed description of performance,  however wonderful and colourful, followed by an intuitive guess about causes,  followed by a redescription of the performance in terms of the supposed causes.  There seemed to be an intellectual block to empirical analysis of mechanisms.  In a biting critique, Lehrman (1953) found serious flaws:  It involves preconceived and rigid ideas of innateness and the nature of  maturation. It habitually depends on the transfer of concepts from one  level to another, solely on the basis of analogical reasoning. It is limited  by preconceptions of isomorphic resemblances between neural and  behavioral phenomena...Any instinct theory that regards ‘instinct’ as  immanent, preformed, inherited or based on specific neural structures is  bound to divert the investigation—from fundamental analysis.  Mill would have loved that. After World War II, ethology became a powerful force in Germany and spread  with Tinbergen to Oxford, then moved to Madingley Hall, Cambridge, with  Thorpe, Hinde, Bateson and many others. Even in my student days, however,  Gray and Lissmann (1946) were taking pot shots at innate behavioural patterns.  Throughout the 1960s and 1970s, the ethologists said they made an enormous  effort to understand mechanisms by studying behaviour, but they in fact  redescribed it in other words. The available theory could never be satisfactory  because the amount of information in a behavioural pattern was dwarfed by the  vastly greater information required for its explanation.  In his Biographical Memoir of Niko Tinbergen, Robert Hinde (1990) summarised  the conflict between ethology and comparative psychology, with an effort to  paper over the cracks: ‘both sides were partially right.’ This was whitewash and  rubbish. Gradually, the battle died down, however, as it became obvious that  ethology was descriptive natural history of whole animals, mainly vertebrates.  Those with the techniques to study neural mechanisms of behaviour formed their  own subject, neurobiology, with their own Congress of Neuroethology, but that  alone did not help to identify the mechanisms of bees’ visual discrimination,  because most of the researchers on bees had been educated in the Continental  system based on the philosophy of Kant and his successors. The whole subject  was dominated by German biologists who somehow thought that empiricism  was immoral. 32 </page> <page sequence="15"> ThEoRIES oF SCIEnTIFIC PRoGRESS: hElP oR hInDRAnCE? Empirical laws For the past 200 years, physical scientists have studied a system by changing  the input progressively and recording quantitative data from the responses.  Investigators looked for regular relationships between inputs and outputs,  which were called empirical laws. The search for an empirical law was useful  because a theory might be found to be consistent with it. When they were not  able to see why such a law should exist, they hesitated to extend the law to cases  varying much from those that were in fact observed. Mistakenly, the search for  empirical laws was taught as a way to proceed scientifically. I still remember  those never-to-be-forgotten practical physics classes at school in which we  measured the volume of air at different pressures and the length of a copper bar  while it was heated. We then described the empirical relationships as laws that  were explained in terms of the motion of molecules, which became more and  more real as we listened to our enthusiastic physics master. In fact, we had no  evidence of molecules in our experiments—and they did not come into science  by this route.  In the case of vision, the exact relation between input and output is even less  useful. In the bee, these searches for exact relations between the features of  the pattern and the percentage of correct responses hampered progress. Dozens  of such papers by Crozier and his colleagues at Harvard and by Selig Hecht  at Columbia were published in the Journal of General Physiology in the 1930s.  This approach was futile when applied to a system such as vision, with many  separate channels of transmission in parallel.  Starting on a new tack, Lindauer and Wehner in Frankfurt looked for empirical  laws from 1965 until about 1973. Wehner (1967) trained bees to come to a square  cross, or alternatively to a regular striped grating, and then plotted the choices  between the training target and the same target rotated by various angles.  A theory could be found for one set of results or the other, but not for both  (Chapter 4).  In one favourable example, Wehner (1969) trained with a broad single bar and  then plotted the scores of the discriminations between the original bar and the  same bar at different angles. The scores fitted the increasing mismatch between  the positions of the areas of black on the training and the test targets (Figures 2.1b  and 4.5). To explain the results, Wehner proposed that the bees remembered a  copy of the training pattern and compared it with each test pattern, but this  guess led to the erroneous conclusion that the image was remembered in the  brain of the bee. Explanations consistent with the data Most experimental data are explained by making an intuitive hypothesis that is  compatible with them, such as postulating that a cat ate the missing cream. It is  33 </page> <page sequence="16"> WhAT DoES ThE honEyBEE SEE AnD hoW Do WE knoW? not good enough, however, to accept a theory just because it is consistent with  the data. This was, however, the commonly accepted way to proceed. The cat  starts as a guess and, even if never seen, it can become an explanation of other  events. Later authors refer to the cat as though it was seen eating the cream.  If there really was a cat, however, it might have been innocent. Much of the  discussion about theory in the scientific advances of the twentieth century was  an effort to cover up the sad consequences of accepting erroneous explanations  that were compatible with the data. In fact, like sex, everyone was doing it,  and it was regarded as an ultimate necessity. Everyone hoped they had the true  theory and that they would not be responsible for erroneous concepts. Popper and the method of disproof There is a way to make progress. If there is no cat, we can look elsewhere for  the cream. This is the ‘theory of disproof’ attributed to Karl Popper (1935),  whose ideas were popular with students about 1950. In Popper’s view, progress  requires a definitive experiment that excludes the cat. In the study of honeybee  pattern vision from 1914 to 1989, the cat was the idea of parameters as parts of  the image and memories within the bees, or alternatively the idea of a complete  retinotopic memory of the image, but for some strange reason, the critical  experiments to exclude either of these popular proposals were never done.  The contribution of Popper—the idea that theories could never be verified, only  disproved—partially resolved the otherwise difficult acceptance of induction.  Theories can never be proved because unforeseen observations can appear in  the future. After Popper, theories that were consistent with the facts could be  accepted as valid for the time being, but only if the effort to disprove them had  been made. The disproof might have been useful if the discipline had in fact  been applied when a theory presented itself, but none of those working on  bee vision ever validated their theories. Also, until the 1990s, different theories  were presented separately without reference to one another. So much for Mill,  Popper, Kuhn and the rest of the philosophers of science. The new paradigm In the mid-twentieth century, explanations based on intuition or empirical laws  gave way to a new attitude, based on fiddling with the mechanism; it was the era  of Meccano, mending a bicycle or a radio, which led to understanding signals  and machines. With the development of electrophysiology and cybernetics after  1945, the idea developed rapidly that sense organs acted as filters in parallel  pathways. Recordings from neurons in the visual systems by Kuffler (1953 on  the cat), Barlow (see below) and Burtt and Catton (1952 onwards, on the locust)  revealed examples that responded to complex stimuli such as a moving spot  of the right size, but not to a simple flash of light. In 1952, at a supposedly  34 </page> <page sequence="17"> ThEoRIES oF SCIEnTIFIC PRoGRESS: hElP oR hInDRAnCE? private meeting in Cambridge while I was a research student, Horace Barlow  gave a seminar on the responses of frog retina ganglion cells, including one type  sensitive to a small black spot, with an inhibitory surround. An American spy  at the meeting distributed a report to all holders of US Office of Naval Research  (ONR) grants in the United States (Mollon 1997) and Ted Bullock certainly knew  of it. Much later, however, Lettvin et al. (1959) created widespread interest  when they published similar work. That year, I visited Jerry Lettvin on the way  to California to work on a book with Bullock and was less than impressed by  their equipment, methods and flash in the pan.  In fact, the promise of this work was not fulfilled, partly because in invertebrates  the neurons recorded were not representative and not identified; the real cues  were not well defined and neuron specificity was not really demonstrated. The  main reason, however, was that recording from only a single neuron at a time  simplified the data, but blinkered our understanding about arrays in parallel  and coincidences between neurons. In the years 1955–62, innate edge detectors  were found in huge arrays in vertebrate retinas and mammalian cortex. Jander  (1964) was stimulated by these studies of vertebrates. He combed the literature  and proposed the general hypothesis that small feature detectors were the basis  of vision in ants and bees; but, without the necessary equipment or techniques,  it was only a guess. Later, McCann and Dill (1969) recorded from edge detectors  in the optic lobe of a fly. Those working on insect visual behaviour flatly  ignored these indications. There is some excuse for heading the wrong way  when signposts are undeveloped, but no excuse when signposts from related  disciplines emerge.  Forty years later, we are no further advanced in identifying the part played  by any neuron for anything subtle in the visual repertoire of the honeybee (or  other invertebrate). There has been a huge effort on the illusory unit motion  detectors of the fly, but negligible electrophysiology of pattern vision of any  invertebrate because no-one knows what features have been detected.  kuhn and progress Thomas Kuhn (1922–96) argued that scientific topics passed from an initial  chaotic pre-paradigm stage into a productive period of growing understanding  and satisfying results that fit the theories of the time. There is nothing there to  object to. Ultimately, perhaps because new methods or technology are found,  there is a crisis because anomalies accumulate and the accepted theory is no  longer tenable. At this point, a new experiment or theory points the way to the  next period of advances, and the cycle repeats.  At first, a new theory should be simple—that is, it should propose few new  postulates or variables with acceptable relations between them. To survive, the  theory must pass every test and there should not be a better theory. The new  35 </page> <page sequence="18"> WhAT DoES ThE honEyBEE SEE AnD hoW Do WE knoW? theory must not account only for previous results and anomalies, it must predict  further developments. Kuhn observed that from time to time a new technique—a  new means such as the availability of electric power or a real discovery such as  of x-rays—altered the way facts were collected and explained and there was  then a shift away from the old towards the new. The new paradigm would not  be acceptable to all, especially to those whose career was based on the old one.  All ‘theories’ became transient and replaceable. The route was familiar to Mill. Collection of data led to an inference or guess  of causes. As a paradigm developed, a new theory would become more and  more confirmed. At best, this was induction; at worst, mere intuition. Next, it  was essential to design experiments that would probably establish or perhaps  exclude the new theory. No philosopher, however, explained how to design  the right experiments, which required intuition, lots of thought and extensive  knowledge of the literature. The new territory, filters and neurons  In vision, the way forward was indicated by the arrays of single neurons in the  primary visual cortex in mammals that each responded to an extremely simple  feature—notably, the orientation of a moving edge (Hubel and Wiesel 1959). In  this case, every neuron could be represented as a spatio-temporal filter with its  own field, but the meaningful signal was carried by the coincidences of many  neurons in parallel. The artificial-vision fraternity enthusiastically embraced  the concept—as reviewed later by Hinton et al. (1986)—but robot vision needed  much more.  Filter theory, like electrophysiology or neuron anatomy, is not sufficient to  explain vision. First, it is essential to discover how many different kinds of  parallel paths are active at any one time, exactly what is the meaningful part of  the signals they carry, the excitation in each, how they are interconnected, the  field sizes, the destinations, the time delays and finally the central reckoning.  This leads slowly to an understanding of the way the parts work together.  A systems analysis with interacting boxes (Figure 7.1) can be made only after  the difficult behavioural analysis has been done—not before. Second, the visual  system relies on the visual feedback from the movements that it controls. Vision  is active, so the pinned-down preparation is only a beginning. The new postwar paradigms in physics—notably, information theory,  cybernetics, feedback, signal-to-noise ratios and modulation transfer functions— and the expectation that useful artificial vision would quickly follow, sustained  the enthusiasm for recording from amphibian and arthropod visual systems.  The studies of mammalian vision in medical schools led the way. In the period  36 </page> <page sequence="19"> ThEoRIES oF SCIEnTIFIC PRoGRESS: hElP oR hInDRAnCE? from 1966 to 1974, it became established that at the front end of vertebrate  vision there were arrays of simple filters in parallel that detected local intensity,  directional movement of local contrasts and orientations of edges and contrasts. A few pioneers enthusiastically proposed that behaviour would be understood  in terms of nerve-cell interactions. As more and more results emerged from a  greater variety of preparations, however, it slowly became obvious that this  ideal would never be achieved, because the information content of behaviour  was so low compared with that of neurons. Even in 1981, Rudiger Wehner, in  his long review, could not reconcile the new work on the neurons of the optic  pathways with the visual performance known at the time. It was also essential  to direct research to the analysis of behavioural responses that might explain the  activity of the neurons. In the 1970s, new techniques became available for identifying single dye-marked  neurons that were also recorded with a microelectrode, so theoretically, the  physiological interactions could be reconciled with the anatomical pathways.  There was some excitement at the prospect of working inwards from the retina  towards the brain to discover what features of the image were detected. There  was no final understanding, however, even of the directional motion detector  neurons of the fly’s lobula plate, which were exceptionally large. Even in this  case, the results were at first misleading because an animal with a fixed head  could not initiate the movements that were part of its vision.  In the insect optic lobe there are dense arrays of neurons in parallel and in  series and it has proved impossible to track a sufficient fraction of the synaptic  connections or decide which are inhibitory or excitatory. Many of the  neurons—and perhaps all the small ones—function by graded potentials and  have no action potential. Although it was possible, through the second half  of the century, to record from many of the large ones in the insect optic lobe,  behaviour was not explained by neuron responses. The reasons why are that  the units of visual behaviour are not known; bees are freely moving and their  vision is active, with continual learning from visual feedback from their own  movements; they respond to only a small part of the image seen by the human  eye; only a few parameters have been identified; and sensory integration depends  on coincidences. Moreover, the behaviour of interest—visual recognition—was  stored in an unknown language and appeared only briefly. When we recorded neurons in the deep optic lobes of insects 45 years ago  (Horridge et al. 1965), we found that very simple stimuli such as flashes and  moving edges were adequate and the field sizes were very large. Some responded  to sound, touch or body movements. How could a group of neurons with these  properties carry the signals for vision? Certainly, the reassembly of an image  would be impossible. Only much later, I gradually learned that the parameters  that bees detected and the cues within their processing system were also very  simple and were summed in local eye regions, and that they functioned as a  37 </page> <page sequence="20"> WhAT DoES ThE honEyBEE SEE AnD hoW Do WE knoW? parallel array. In brief, large arrays of a few types of small feature detectors are  summed together to form a few cues in each local region of the eye. The relation  between behaviour and neuron activity lies in the coincidences of the responses  of neurons with different inputs that lie in parallel in local regions. Again, the  idea of multiple causes can be traced back to Mill. Endnotes 1. Aristotle (384–322 BC) wrote the Organon. See Westaway (1937). 2. Francis Bacon (1561–1626), an English essayist, philosopher and politician, wrote the Novum  Organon. See Westaway (1937). 3. Whewell also wrote Novum Organon Renovatum, as well as History of the Inductive Sciences (1837).  See Westaway (1937). 4. Recounted in his autobiography (Mill 1873). 38 </page> </plain_text> 