<plain_text><page sequence="1">Instructionel Science Elsevier Publishing Company, Amsterdam — Printed in the Netherlands MODELS FOR SOCIAL SYSTEMS AND FOR THEIR LANGUAGES* GORDON PASK System Research Ltd., Richmond, England 1. Introductory Overview Many sorts of models are used in the behavioural and social sciences. These models belong to several logically distinct classes and they are used and identified with the real systems they represent in rather different ways. Each class of model is useful in the right place. However, we are apt to adopt a slightly cavalier attitude and to simply "make models" and "use models" indiscriminately until we come up against some sort of dilemma. This article reviews the field of behavioural and sociological models in an attempt to point out the proper distinctions and some of the snares. Occasionally, for example in connection with individual and so cietal innovation, we shall be in a position to break essentially novel ground. Part 1 of this paper, starting at 1.1, is chiefly a classification of models as such. The classes established in the discussion are ( 1 ) statistical models, (2) functional models, (3) normative models and (4) organisa tional or programmatic models. A further class, (5) hybrid models, should properly be introduced in Part 1, but for various reasons, I found it necessary to defer consideration of these models until 2.10, at the end of the paper. In developing the classification, it has been necessary to allot a disproportionately large space (a) to a discussion of the neglected concept of a "norm" and (b) to a discussion of "artificial intelligence" models. In Part 2, we examine the use and identification of each type of * This paper was originally read at a Wenner Gren Symposium in New York in 1963. It was revised in the light of Rappaport's work on the Tsombega which he first discussed on that occasion and presented (the present text), at a Wenner Gren Symposium convened at Altmunster Am Trauensee, by H. von Foerster in 1966 (a companion paper, given at the same meeting but dealing with man-machine experiments, "A Cybernetic Experimental Method and Its Underlying Philosophy" has recently been published in International Journal of Man-Machine Studies(\91\) 3, 279—337). I would like to thank Prof B. N. Lewis for retrieving the manuscript from his files; likewise MrMacDonald Ross; to thank Mr B. C. E. Scott for editing the manuscript and Miss M. Kershner for checking it. 395</page><page sequence="2">model. As a prerequisite, it is necessary to distinguish between the descriptive metalanguage used by an observer or model builder, and the object languages used by the elements of a communication model or the components of the real system it represents. It turns out that there are at least a couple of logically distinct identifications; one depending upon a closed analogy relation; the other (pertinent for evolutionary models and for models representing creative activity and some sorts of concept learn ing) depending upon an open analogy relation. Further, there are many significantly different ways in which identification is accomplished. We look at these diverse types and methods of identification and conclude with some comments on innovation as a process to be represented by a special type of model, the hybrid model mentioned above. 1.1. STATISTICAL MODELS Statistical models describe the regularities observable in a particular class of data. It is often possible to abstract from the regularities and to infer a "law" that accounts for the consistent mode of behaviour; how ever, a "law" of this kind is a descriptive law; it does not govern the behaviour but describes a pattern. To avoid confusion with several other sorts of law (to be discussed later) I shall refer to descriptive laws as "principles" (this is a slightly awkward word, but it is the best I can think of). One example of a statistical model is Zipf's model for language usage. The basic data are counts of word frequencies, mostly derived from written text. One well-known regularity adumbrated by this model is the nearly linear relationship between word frequency and the rank ordering of frequencies. The "principle" inferred from many regularities is a conservation principle. Very roughly, Zipf asserted that language is used as though the ensemble of users aims to minimise the effort (or predicted work) entailed by communication, given the contingency that the utteran ces of a transmitter can be understood by the recipient. A less familiar statistical model is Wilkins' model for social home ostasis. In this case, the basic data is a count of the number of individuals in a population characterised by a specific value of one or more cogent attributes, for example, the attribute "occupation" with values butcher, baker ... or the attribute "dominance" with values high, medium ... If the population is in dynamic equilibrium, it will be characterised by a particular, time invariant, distribution of frequencies, such as the distribu tion shown for a single attribute in Fig. 1. Moreover, this distribution is inherently stable. If an individual is found to or is made to deviate from the mean value A in Fig. 1 there is a tendency for his deviance to be 396</page><page sequence="3">corrected by social pressures, so that the statistical status quo is preserved. The regularity exhibited by this model is thus a distribution with one or more "peaks" and because of the inherent stability, we are justified in calling the points where these "peaks" occur, the indices (with respect to the given attribute) of descriptive norms of the society. The principle inferred from a number of regularities of this type is social homeostasis; deviant individuals are acted upon by forces which maintain the status quo. (Of course, Wilkins says much more than this; his argument in favour of a homeostatic principle is especially telling, because, if a sufficient number of deviant individuals depart from descriptive norm A in Fig. 1., the mass effect establishes descriptive norm B. An individual can exist as an i or as a "legal deviant" B, though he cannot easily exist in the behavioural hinterland between A and B.) attribute "occupation" Fig. 1. Example of the distribution for a single attribute in Wilkins' model for social homeostasis, with local mean values A (Butcher), B (Baker), etc. 1.2. FUNCTIONAL MODELS Functional models are descriptive and explanatory devices. They are reducible to basic units or building blocks and conversely they can be assembled from these units by the application of composition rules. The behaviour of the units is known a priori (or is assumed by way of axioms); commonly, our knowledge about the units is gleaned from investigations that are not related, through a hypothetico deductive chain, with data pertaining to the field in which the model is applied. To make these notions explicit, the functional model consists of: 1. Stimulus-Response or input—output units to be identified with sub systems in a real system. 2. Inputs to and outputs from each unit, to be identified with the coupling betweem sybsystems in a real system. 3. Causal or systemic relations that characterise the behaviour of a unit. 397</page><page sequence="4">4. Composition rules which must be obeyed in the assembly of units to form a functional model (and which admit the reduction of any model to its constituent units). Functional models are very common in brain physiology. Typically, the units of (1) are identified with some features of neurones or small groups of neurones; the input and output of (2) represent some features of the synaptic connections between neurones; the causal or systemic relations characterising the behaviour of a unit of (3) reflect properties such as all or none action, threshold, impulse summation, inhibition, facilitation, and so on; the composition rules in (4) allow the adjunction of units according to some topology, i.e. in laminar arrays with lateral inhibitory connections between adjacent members of a lamina. Suppose we are given a particular functional model Z, and suppose that its structure tallies with some features of the structure of a real system (such as the gross anatomical features of a cat's brain). If a simulation of this model (which may either be a mathematical simulation or a real "working model") exhibits an interesting property like "learn ing" which is also manifest in the real system, then Z provides a possible functional explanation of "learning"; that is, Z demonstrates that it is possible to explain this property in terms of (1), (2), (3) and (4). The demonstration is non-trivial insofar as "learning" is not part of the behaviour of the individual units. It does not exclude other explanations for "learning"; nor can it, so long as our knowledge of the unit behaviour stems from one class of observations (such as "studies with microelec trodes"), whilst "learning" is a predicate of a different class of observa tions (psychological studies). Equally, the units may be identified with men, groups of men, tribes or societies. Any model in which well defined automata interact with one another as an image of social intercourse is a functional model, and so are most of the models of physical anthropology. One recent functional model, especially typical of the social sciences, is due to Merelman, who explains some aspects of "political legitimacy" in terms of certain principles of human behaviour [namely, reinforcement learning theory and cognitive dissonance). The units of (1) are interpreted as human beings; the "inputs" of (2) are statements and reinforcements delivered from the body politic and the "outputs" consist in the com ments elicited by these statements: the causal and systemic relations defining the behaviour of the units (as required by (3)], are principles of conditioning and learning imported from the field of experimental psy chology; the composition rules of (4) determine a number of permitted modes of interaction between the members of a population. 398</page><page sequence="5">The functional explanation offered by Merelman specifies how, in psychological terms, some governments are accepted as legitimate; that is, how particular sorts of composition enjoy this stable, "legitimate" or "morally acceptable" property, whereas others do not. 1.3. A DIGRESSION INTO THE MANY TYPES OF NORM In a moment, we shall discuss normative and programmatic models. Before doing so, it is necessary to digress into the logic of norms — the "Deontic Logic" developed by von Wright — at least to the extent of considering several importantly different uses of the word "norm." First of all, "norm" may mean a "descriptive law." Whilst this is a perfectly respectable usage, it would lead to confusion in the present context (and we have already dubbed any "descriptive law" a "principle" to emphasise that no sort of obedience is involved). The meanings of "norm" that I wish to discuss — (1), (II), (III), (IV) and (V) below — do entail the ideas of "obedience" and "prescription" (though the norm may, of course, be described as well as prescribed). The following list rests upon von Wright's discussion and terminology, though the categories are not identical. (I) Prescriptive norms. The norm is a state of affairs brought about by a law that prescribes the norm; commonly, this is a law of the state, applicable to citizens. The law is issued with an authority accepted (usually, though not absolutely) by the citizens concerned and it is enforced by sanctions or threats. (II) Universal norms. These again are prescriptive norms; however, the authority of the lawgiver is not made explicit. We suppose that the underlying law is universally accepted, or at any rate, that it is accepted within a given universe of discourse. "Natural Law," in the sense of d'Entreves,1 has this calibre; so does a divine law; so did the laws of Justinian. (III) Norms depending upon conditionally accepted legal rules. The rules in question may be the rules of a game, the rules of membership in a club or society, or the rules of a language. They are conditionally accepted in the sense that I obey the rules of Canasta only if I wish to play this game (and, presumably, aim to win it); I obey the house rules of the Reform Club insofar as I wish to use their elegant premises (and, perhaps 1 In the sense of ethical theory and the theory of jurisprudence. I do not mean a "law of nature" in the sense of a descriptive physical principle. 399</page><page sequence="6">to be a "Reformer" in the sense of certain clauses in the regulations); I obey the rules of a language if I wish to speak, to hear, to understand and be understood. But, in order to achieve these several goals of being a player, a member, or a communicant, I logically must obey the rules. Obedience inheres in the property of being these things. As I understand it, these "rules" are precisely the category of von Wright's "rules." Similarly, there are norms depending upon conditionally relevant material or the causal constraints of a functional model. "If you want to accelerate, press the pedal," expresses the sort of constraint I have in mind. The rule is a causal rule to do with the functional character of motor cars; it is relevant insofar as I am in a motor car and wish to drive faster. The prescription is mandatory if I want to achieve this goal; if not, it is irrelevant. Roughly speaking, prescriptions of this type are von Wright's "Technical Directives." (IV) Norms depending upon commands and instructions. If a com mand is accepted by A? at a certain time (or contingent upon precondi tions that arise at a certain time), then Ws, behaviour satisfies a norm prescribed by this command until the time at which the command terminates. Rescher has recently exhibited the diversity and fine structure of commands. But all of them have this much in common; they are issued by someone with some authority to someone, given certain conditions, at some time and until some time. It is evident that the norms of (I) and (II) have the status of commanded norms; they belong to a special class of commanded norms. (In Rescher's terminology they are distributive com mands, accepted by all the members of a system or population and the temporal dependence is suppressed.) (V) Conventional norms. The majority of norms are hybrids of the relatively pure types (I), (II), (III) and (IV). One hybrid of especial consequence is the sort of norm established by a custom or convention. Von Wright points out that a custom is partially a prescription in the sense of (I) and partly a rule of conduct in the conditional sense of (III). In some cases, it may also savour of (IV) — (for example, in ritual proce dures). A few comments are necessary. (A) The notion of a "goal" is inextricably bound up with the idea of a "norm" [though we only made this explicit in (III)]. In (I), there is a goal (namely, securing a behaviour compatible with the prescriptive norm) that is accepted (though not, perhaps, absolutely) by all creatures of a certain kin. This state of allegiance to the goal is secured by social sanctions and rewards. Hence, the goal is partly a 400</page><page sequence="7">property of the individual in a society and partly a property of the society itself. In (II) we are in a position either to infer that the goal is universally accepted, or unconditionally enforced. In (III) and (V) the goal is one way or another contingent and individually selected. In (IV) the command selects the goal (one of many conceivable goals) that individual M pursues. (B) Norms may be described by a model maker as well as prescribed. In our discussion of statistical models, for example, we referred to the descriptive norms A and B. Strictly, the frame of reference (with respect to attribute 1 and attribute 2 should always be stipulated, though in practice, when the matter is obvious, this nicety is omitted. (C) Equally, of course, norms may be described by the participants (i.e. the members of the statistical population), and their description of the status quo is apt to differ markedly from the observer's or model maker's description. (D) Except in the purest and most abstract cases, the norm must be described by anyone or anything to whom or which it is effectively prescribed. In other words, normative statements are necessarily interpreted. They are not just stimuli. This is particularly evident in the case of state achievement commands (to use Rescher's nomencla ture). These are commands of the form "Be a gentleman" which stipulates a norm (being a gentleman) without saying how this objective should be achieved.2 Here, it is evident that the recipient must describe a criterion of goal achievement and use this in order to produce subgoals that are part of the goal. Usually these subgoals will establish conditional norms, like those in (III) which, in turn, are described. Finally, of course, the original command gives rise to instructions for subgoal achievement and this process is guided by a descriptive feedback signal. 1.4. NORMATIVE MODELS According to the rather narrow definition we shall adopt in this paper, a normative model is a structure described by the model maker, observer or experimenter, which he employs to prescribe what ought to happen in or what ought to be done by a real system. Thus, the model is prescriptive (in the sense that a model for teaching is chiefly prescriptive, 2 In contrast an "action command" might state "Wash" and "Wear a dark suit" and "Be considerate to ladies" and "Develop your knowledge of literature" and so on. Action commands might conceivably be analysed in terms of stimulus and response sequences, although I do not think this analysis is possible. 401</page><page sequence="8">whereas a model for learning is chiefly descriptive).3 The following list illustrates some types of the normative model. ( 1 ) An engineering design which represents a system and is used to prescribe the construction of a physical artifact is a normative model. The model builder (in this case "the designer") is in a position to "enforce" his model, and he knows that the artifact will work because he has a well validated functional explanation of the system. The norm established, is degenerately, a type II norm. Of course, the engineering design is one limiting case of a normative model, for the same model can be used descriptively as a functional model for the real artifact. This argument effectively bridges the gap between functional and normative models. (2) The models of communication theory, such as the simple Shan non type channel of Fig. 2., rest upon a type II norm which states that the transmitter T and the receiver R are able to produce and receive syn tactically legal (that is, prescribed) sequences of signs from the prescribed alphabet of a sign system. Source of noise which perturbs communication channel Fig. 2. Simple Shannon-type communication model. (3) Most political theories are normative models, though they are often in practice mixtures of normative and functional models. Any of Weldon's "mechanistic" states depends upon a norm of type I and any of Weldon's "organic" states upon a norm of type II. Hobbes Leviathan describes a mixed normative and functional model. Its functional com ponent assumes certain behavioural properties of the units (which are identified with men) and the normative component prescribes certain organisations that prevent the chaos which (Hobbes believed) would result from the uncontrolled aggregation of men. The Talmud is prescriptive model for being an orthodox Jew. Taken as the vehicle for a type II norm, the model builder shares a common 3 This distinction is inadequate in the long run. It is a fiction to suppose that "teaching" and "learning" can be separated in such a neat and tidy fashion. 402</page><page sequence="9">belief with the individuals to whom the prescription is addressed, and the authority is God. Taken as a type III norm, the Talmud constitutes a set of rules for membership in the Jewish faith and nation. (4) Business games and game theoretic models as a whole are based upon normative models that prescribe certain rules which, it is assumed, will be obeyed by the participants. The norm in question is of type III. (5) The rules of a language as prescribed by a book of grammar involve a type III norm, and constitute a normative model. However, the generative and transformational grammars discussed later are more than normative models. (6) Models for communication amongst small groups, such as those of Bavelas and Christie, are normative models, again prescribing norms of type III, or occasionally of type IV. (7) Models used in connection with any human psychological ex periment are partially normative. Experimental instructions given to the human subject are assumed to establish some type IV norm. (8) Models for conventions and traditions rely upon type V norms in a rather subtle fashion. Typically, the experimenter questions the in habitants of a society about the conventions (say, the marriage conven tions) that they adopt. Next, as a result of his questioning, he establishes by computational methods the alliances that would occur if these conven tions were (in fact) accepted. Finally, he determines whether or not this result tallies with reality and thus accepts or rejects his model. Notice that in this case, the norm is not imposed by the model builder. It is a norm described by the model builder as the characteristic norm (as he sees it) of a society. The really important point is that although these normative models may (and usually do) presuppose the acceptance of rules or conventions by the members of or participants in a real system, with which the model is identified, the model does not describe the process of acceptance or interpretation. No part of the model images an interpretative mechanism. By the same token, these models neither involve any mechanism for saying how internally specified conventions arise, or how internal state ments and instructions are generated or received; nor do they comment upon the construction or modification of norms. Rather, they demon strate the consequence of accepting certain norms, and that is all they do. To summarise these points; the statements (discourse and interac tion) imaged by a normative model are statements taken in extension only 403</page><page sequence="10">0not taken in intension); issues of meaning are not imaged by the model. Meaning is, of course, involved in the establishment of the model. In the case of all the examples except (1), the prescribed model must be accepted by the participants in a real system whenever the model is used. However, it remains true that intensions are not imaged within the model itself. 1.5 ORGANISATION MODELS AND PROGRAMMATIC MODELS Normative models prescribe and describe regulations. Organisational models prescribe and describe algorithmic processes, including processes that interpret statements about norms or regulations and processes that construct norms and issue instructions. Unlike a normative model, but in common with a functional model, the organisation model is reducible to units associated with goal achievement or command interpretation (in the fashion of 1.3.(V)D). Hence, it is able to explain the working of a real system with which it is identified (in the sense of 1.2). If an organisation model is represented as a linear sequential processor (a linear sequence of instructions acting upon data or other instructions), it is a programme (in the sense of a computor programme) and we shall call it a programmatic model. It follows that statements within an organisational model are con strued intensionally; we are crucially concerned with the meaning of statements to the participants and, in particular, with the interpretation placed upon statements and how this interpretation occurs. However, it must be admitted that the distinction between a nor mative and an organisation model is often hazy; we cannot always be certain whether the observer does the prescribing and describing, or whether someone or something in the real system (with an image in the model) assumes the prescriptive and descriptive role. The point is illustrated by a game theoretic model for arbitration, devised by Braithwaite. In this model, Braithwaite considers the logical calibre of a 2-person, non-zero, sum game (a partially co-operative and partially competitive game). An arbitrator, C, looks at the strategic pos sibilities of the game and points out various outcomes and various sym metry properties. On these grounds, he seeks to convince A and B (who are rational, though basically avaricious creatures) that they would mutually benefit from agreeing to a partially co-operative solution. Fur ther, he points out ways in which the agreed solution can be rationally enforced by the other participant if either of them cheats in the sub sequent moves. It is evident that if we take C and place him on a par with an 404</page><page sequence="11">observer or experimenter, then this is a normative model. For C, the arbitrator, uses the game as a strategic paradigm and indicates to us how the game may be played. He is talking ethics, which is the context in which the model was originally advanced. If, on the other hand, C is embedded in the real system as an arbitrator who talks to the participants, an image of the arbitrator will appear in the model. So will the inter pretation of the discourse between A, B and C. Moreover, since the discourse is of various types (statements about moves, descriptive state ments of properties of the game, prescriptive statements about how the game ought to be played and statements of agreement between par ticipants) it will be necessary to discriminate these different types of discourse within the model. Such a model is an organisation model (some of George's partial^ co-operative game models have this status), and if fully developed, it can be a rather elaborate structure. Because of the complexity entailed in a fully developed organisation model, many models of this type are not expressed in a thoroughly mathematical fashion.4 To grasp their significance, it is important to realise that they could, in principle, be mathematicised. The value of clothing them in mathematical garments is dubious, and they are per fectly respectable and useful as they stand, provided that their logical calibre is recognised. Some organisation models are cited below. A couple of types are considered, separately in 1.6 and 1.7. ( 1 ) The models advanced by ethologists to account for various sign stimulus (or releaser) organised behaviours (such as mating). See, for example, Tinbergen (1953). (2) Wynne Edwards' model for population density control. Briefly, Wynne Edwards puts forward a convincing argument that many pheno mena of display, mimicry and directive behaviour have a specific signalling function, rather than being biological epiphenomena. They mediate com munication in a specific density control system; for example, the singing and territorial excursion of male birds prior to mating, serves (I) to indicate male density to other male birds, and (II) to provide information about the resources available in a given habitat. From (I) and (II), an individual male is provided with a diffference signal relevant to density control (notably, this is a difference signal that is a property of the local group of animals). Now, within any group, there is an established dominance hierarchy, wherein any male individual has a place (the hier archy is established by convention in some cases, and by stress-mediated 4 The chief exceptions to this rule are the artificial intelligence and linguistic models which we cite in 1.6. and 1.7. 405</page><page sequence="12">hormonal mechanisms in others). The lowly individuals, "excluded males," are not allowed to mate and breed (for the hierarchy sets up a prescriptive norm in this respect). According to the present hypothesis, the relative number of "excluded males" in the local group is determined by the intensity of the difference signal which thereby serves as a feed back to establish the group goal of securing a reproduction rate (hence, a subsequent population number) that fits the resources of the environ ment.5 The group goal will, in many cases, be contrary to the innate goals of the individuals, i.e. reduction of various sex drives. Thus, the model involves a logical hierarchy of goals to be sharply distinguished from the merely "ordering" hierarchy of dominance. (3) Rappaport and Vayda's model for the ritual regulation which maintains a condition of mutualism between the local population of a tribe (the Tsombega) and the local population of partly domesticated pigs (upon which the Tsombega depend, in various ways). Since the ritual cycle can be written as a sequential programme, this is a programmatic model, which is shown in a simplified form as the chart of Fig 3. As in the case of Wynne Edwards' model, it involves a logical hierarchy. There is a perceptual level at which conditions of the environ ment are interpreted as a function of the state of individuals (for example, to produce the signal of female dissatisfaction which is chiefly responsible for determining the moment at which a pig festival starts). Next, there is a level at which conventional or traditional norms are established in the milieu of a particular tribe (this is the level at which the flow chart is written). Finally, there is a level at which a tribe is defined. The terse comment of Fig. 3, "A man belongs to the tribe with whom he plants the Rumbim," conceals the crucial fact that the whole "regulation algorithm" contains a clause for defining the domain in which it operates. (4) Bateson's model for the cultural "Double Bind," whereby the conventionally encoded organisation of a culture is modified. Briefly, it is argued that the maintenance of homeostasis in a social group depends upon the existence of a suitably encoded organisation, such as (3) above, whereby conflicting tendencies trigger off compensatory reactions. This organisation personifies the culture and is essential for its physical in tegrity. If a pair of cultures having incompatible organisations are juxta posed, a "Double Bind" situation occurs, as a result of which the cultural organisation pattern is necessarily modified. Bateson has also considered an analagous process operating in the genesis of familial schizophrenia. 5 My account is too brief to do justice to the argument from which it is derived, or to the many and varied mechanisms that are brought into play in density control. The interested reader should consult Wynne Edwards' book. 406</page><page sequence="13">Fig. 3. Ritual regulation of Tsombega. 407</page><page sequence="14">Once again, the model is accommodated in an hierarchical structure and this structure is mandatory. (5) Brodie's model and Laing, Phillipson and Lee's models for inter personal and group interaction [which are closely related to (3) and (4) above]. (6) Barnett's model for cultural change and innovation. (7) Alexander's model for architectural design processes in various societies. (Alexander distinguishes "unselfconscious" societies in which homeostatic adjustment of design takes place automatically because there is a lack of any architectural theory, and because any building is subjected to the pragmatic tests of construction; and "selfconscious" societies, in which there is an architectural theory and in which dilemmas can occur due to the abstract and possibly impractical transformation of designs.) The levels in the model are levels of abstraction. (8) Most of the models of group dynamics and social psychology, at any rate insofar as they deal with the control of social groups. 1.6. ORGANISATION MODELS USING ARTIFICIAL INTELLIGENCE Models for individual perception and concept learning coming under the rubric "Artificial Intelligence" deserve a special attention. Any "artificial intelligence" can be represented as a computer programme and is fully and mathematically stated. I believe that these models have considerable importance in anthropology and the social sciences, (a) because people in a society are continually learning concepts (in this respect, the models are applicable as they stand) and (b) because there appears to be an isomorphism between the algorithmic entity "Cognitive Structure" (the name given to the code for a conceptual organisation in one of these models) and the conventions, traditions and role structures that characterise a society. If so, "artificial intelligence" programmes can be given a societal interpretation. Since models of this type are scattered about the psychological and information processing literature and inadequately documented in the present field, the comments that follow bring out a few salient features of these models; they do not describe the models. (A) An "artificial intelligence" is a model for problem solving; the problems posed for solution may entail classification according to speci fied criteria, the construction of a taxonomy to be used for a later classifying activity, the playing of a game or the conduct of a control and decision process. The problems are presented to the model in a language used by the model. This language is also used to describe and prescribe 408</page><page sequence="15">solutions. Our first point, however, is that such a model involves a language that it uses. (B) In each case there is an overall goal to be satisfied. Some sorts of artificial intelligence may also be able to pose problems that go beyond this overall goal, that is, to innovate. All sorts of artificial intelligence must be able to pose problems within the overall goal, that is, to set up subgoals which, when achieved, lead to the overall goal. (C) I believe it is useful to regard any artificial intelligence as com posed of goal achieving units, of the type shown in Fig. 4. These are called TOTE (test, operate, test and exit) units by Miller, Gallanter and Pribram, and they are isomorphic with Rescher's canonical image for the intension of a "state achievement" command. They are also isomorphic with a (slightly modified) version of Ashby's "ultrastable system", and with my own "control units" and "analogy operations". The composition of goal achieving units (using the symbolism for TOTE units) is illustrated in Fig. 4 and the instantaneous organisation of an artificial intelligence Exit when test satisfied Domain of TOTE unit Fig. 4A. Single TOTE unit. Fig. 4B. Composition of TOTE units. 409</page><page sequence="16">model can be represented by a structure of this sort, which is a "cognitive structure". Hence, the model is "reducible" to goal achieving units. (D) Superficially, the picture is simple enough. On closer scrutiny, it turns out to have a number of subtleties. What is tested? It is, at least, a description; a description of properties of the domain upon which the unit operates (which may, of course, be a description of states of other units, just as well as states of some symbolic environment). What issues as a result of the test? It is, at least, a prescription; a prescription for actions that will achieve or that are expected to achieve the goal. What is a goal? For the moment, we comment that a goal is described, in the language used by the artificial intelligence system, by an expression denoting a subset of values of whatever descriptive attributes or properties are used by the goal achieving unit, a relation between the unit's prescriptions and descriptions and an edict of relevance. What sets up a goal? It is an instruction or a command to achieve some norm. What does the unit do? It aims to set up the norm. But, in doing so, it performs an act of abstraction in the language used by the model; in other words, a goal achieving unit is also an abstractive unit. (E) Hence, a competent artificial intelligence model necessarily con tains a hierarchy of abstraction; the levels in this hierarchy are charac terised by degrees of abstraction. But another, quite distinct, hierarchy is usually also involved. (F) Although the instantaneous organisation of the model can be depicted by a cognitive structure, its general organisation entails some thing that changes or adapts this structure (as by learning). It is possible to represent this "something" (whilst retaining the possibility of reducing the model to goal achieving units) if we introduce an hierarchy of control. Suppose that the lowest level in this hierarchy is the original structure and notice that this structure operates upon a domain of problems. The next level in the hierarchy of control is a similar structure, also consisting of goal achieving units, which operate upon the domain of possible first level structures, or, more succinctly, that operate upon a domain of first level goals. In other words, the higher level units modify and build the first level structure. (G) Hencev, an artificial intelligence model capable of "learning"6 is a manifold hierarchical, or in McCulloch's terms, an heterarchical organisa tion. 6 Any non-trivial model is capable of learning in the required sense, although as Minsky points out, the learning process is rarely made explicit. 410</page><page sequence="17">1.7. ORGANISATION MODELS USING GRAMMATICAL STRUCTURES Amongst the most important programmatic models are the generative and transformational grammars described by Chomsky and other linguists. These models represent the apparatus for interpreting or producing7 the utterances of a natural language insofar as this apparatus has a plan that is common to all normal language users. Most of these models deal only with syntactic rules; however, there are grounds for the belief that models of semantic interpretation can be constructed in essentially the same fashion. Within this format, a natural language is not viewed as a collation of stimulus response behaviours (as the parole of De Saussere or even as a description of parole). Instead, the fundamental entity is a cognitive structure shared by the language users (approximately, the langue of De Saussere), and we are invited to consider (a) the sorts of grammatical rules capable of producing, analysing and interpreting "strings of words" (or utterances) in this language, and (b) the classes of abstract automata (amongst them, "artificial intelligence" models) that will satisfy these grammatical rules. Since grammars are discussed in another contribution to this meeting, I shall confine my comments to a few points to do with hierarchical structure and the distinction between models for individuals and for classes of individuals. (A) First of all, any grammar that fits a natural language contains a hierarchy of rules; for example, transformation rules are at a higher level in this hierarchy than phrase structure rules. (A phrase structure grammar is a particular type of generative grammar based upon highly restrictive rewriting rules of the sort shown in Fig. 5). The distinction between levels is chiefly based upon a distinction of domain. Thus, phrase structure rules operate upon a domain of signs for words in the vocabulary of the language and for syntactic entities such as "noun phrase". The derivation (production or interpretation) of a string in the language is obtained by applying these phrase structure rules sequentially, and the derivation may be depicted by a tree of the type shown in Fig. 6, which is called a "structural description" of the string. In contrast, transformation rules operate upon a domain of "structural descriptions", and not upon a domain of signs for words in the vocabulary. Hence, the transformation 7 It can be shown that models of this type are dually interpretable as models for classes of speakers, and as models for classes of hearers. Given a language L, and a model M, we may, with only technical adjustment, specify a class A (M) of devices that produce the legal strings of L; and a class B (M) of devices that identify the strings produced in L, and that analyse their structure as legal or illegal. 411</page><page sequence="18">rules are higher in a hierarchy of rules than phrase structures rules. It is also and non-trivially true that transformation rules are inherently less restricted in form; anything we can do with a phrase structure can be done with a transformation rule, and transformation rules can do other things as well. S •- NP ♦ VP NP »- article (a) + noun (n) VP verb (v) + N P a »- a,the,etc. n »- man, dog, etc. v bites,sits,etc. means "is written as" S means sentence NP means noun phrase VP means verb phrase Fig. 5. Example of rewriting rules for a phrase structure grammar. Fig. 6. Structural description of a string of a phrase structure grammar. Grammatical models are also characterised by quite different sorts of hierarchy. There is, for example, the hierarchy of abstraction within the language, as seen by the language user; there are the hierarchies inherent in a categorical grammar; there are hierarchies engendered by the concept "degree of grammatically" (of a structure or an utterance). These exemplars will be sufficient to demonstrate that a grammatical model (in common with an artificial intelligence model) is manifold hierarchical in form. (B) We have argued that models of this type are plans for classes of automata which image classes of language users. They prescribe how people ought to use a language, how they say they use it, and how they 412</page><page sequence="19">normally do use it. But they do not represent the working of an individual language user. They fail to do so: (1) Because even an ideal, unrestricted and logical language user must satisfy these normative rules, rather than obey them in detail. Phrase structure rules, for example, lead to the strictly sequen tial concatenation of symbols. We do not contend that the strings of symbols produced by a language user must be produced by this sort of sequential computation.8 Whilst still satisfying the rules, these strings might be produced by an essentially parallel process, or by a vast number of artificial intelligence models which entail sequential, but entirely different types of organisation. (2) Because real language users are not "ideal" in the sense that a digital computer is "ideal." They have limited computing capabilities and limited memory capacity. Further, unlike an ideal ised automaton, they are impelled to continue working. They cannot be turned off. If the real language user is neither listening nor speaking, then he is reorganising the data in his memory (at any rate, providing he is conscious, and providing "language using" is taken to cover a whole gamut of attentive and constructive activities). In order to model these quirks and limitations, it is necessary to abstract from the proporties of a computer (such as a brain). But programmatic models are abstracted from programmes which may be run in any computer, either the special "digital computers" used by logicians, or the biological computing machinery of a brain. (This comment is particularly important in connection with grammars that prescribe what organisations may occur, but it also applies to "artifi cial intelligence" models which offer a much tighter account of what organisations do occur.) 1.8. DISTINCTIONS BETWEEN FUNCTIONAL AND ORGANISATIONAL PROGRAMMATIC MODELS Both functional and organisational models describe processes. Both functional and organisational models are reducible to units. Yet I have made a firm distinction between functional and organisational models, 8 Though they give us a great deal of insight into the psychological workings and, in some cases, there is evidence that sequential concatenation and substitution are psychologically cogent. This is more apparent, however, in the organisation of simple motor skills, like typewriting, than it is in connection with speech. 413</page><page sequence="20">and before we go any further, I wish to defend it and to exhibit some of its limitations. What differences are outstanding when we scrutinise a pair of models (one functional and the other organisational)? (I) The functional model represents features abstracted from a com puting mechanism, whereas the organisational model represents features abstracted from an organisation or a programme. (I shall return to this important point later.) (II) In a functional model, the basic units are "stimulus response" or "input output" units characterised by a behavioural rule or an input output mapping; in an organisational model, the basic units are goals (in a programmatic model, "goal achieving units"). (III) In a functional model, we talk about the interaction between units, whereas, in an organisational model, we refer to discourse in a language (or in some of the cases cited in 1.5, a conven tional structure equivalent to a language). (IV) At the most, a functional model is contingently hierarchical; the hierarchy is introduced as a convenience or as a figure of speech that carries the right flavour with it; as when we say the nervous system is hierarchical because "the cortex is on top of the mid-brain" and also "the cortex seems to dominate some aspects of mid-brain activity." An organisational model is neces sarily hierarchical. (V) This property is a consequence of the fact that the signals and messages in a functional model are denotative or taken in extension only. In an organisational model, they always have a connotation as well as a denotation. They have an intension as well as an extension. So far as (II) is concerned, could we not say "the basic units of the functional model are goal achieving units" — in which case it becomes an organisational model? Yes, we could say so; but, in fact, it would be necessary to invert all of the differences in (I), (III), (IV) and (V) as well, so nothing would be gained by saying it. We can, of course, describe a goal achieving unit within a functional model, and we can use the word "goal" rather loosely; for example, in saying "the goal of the thermostat is to maintain the room temperature constant." Here 'goal' is just a shorthand for describing the existence and feedback connections of a thermostat (all of which are components in a functional model). But there is every difference between this use of the word and its usage in statements like: (a) "The goal of the programme is 414</page><page sequence="21">to solve problems" or (b) "The goal of the embryo organisation is to develop into an adult." Here, we are talking about an organisational invariant; the invariant is somehow coded as a description in the system. For (a) the required description is explicitly placed at a higher level in the organisational hierarchy. For (b) the description is partially embodied in the genetic codes of the cells, partly in the structure that exists when the statement is made, and partly in the environment. Further, this goal may be achieved by many possible computing mechanisms, or it may not be achieved by any of them (the latter is obvious when we say "the goal of the organism is to survive" — which makes sense if "organism" refers to the organisa tion, but very little sense if "organism" denotes a biological machine). When a great deal is known about a physical system, a functional and an organisational model for the same system come into correspondence. The visual system of the frog is known, from the work of Lettvin, Matturana, McCulloch and Pitts, to consist in a number of perceptual filters which are well-defined biological computing mechanisms. In the colliculus, receptive fields of the retina are mapped on any of four laminae in strict one-to-one correspondence. At any corresponding point, each lamina receives an excitation proportional to the value of one property (extracted by a perceptual filter) of the excitation of this particular receptive field in the retina. The motor apparatus of a frog, which produces motions like "putting out the tongue to catch and ingest food" or like "jumping away" is fed by connections derived from these layers. Only certain excitations of the retina (such as the excitation due to the moving fly, or to a large shadow) produce an appreciable motor output. This comprises a functional model, which is obviously highly selective and discontinuous; i.e. it is unresponsive to pin points of light or to uniform illumination, or to most things apart from moving flies and shadows. The corresponding organisational model starts with a language used by the visual modality of a frog. In this language, certain symbols denote significant events, such as the motion of a fly or a large shadow (these are, of course, the signals that excite the functional model). The model describes its visual world in terms of a perceptual space, having co ordinates that measure the values of four descriptive attributes. (The values of these variables are, of course, isomorphic with the output excitations from the perceptual filters in the functional model.) A percept is a subset of points in the visual space. In particular, the connotation or intension of the symbol that denotes "moving fly" is described by one percept or connotation, or intension; and that of the symbol that denoted "large shadow" is described by another. The goal of a frog (that is, of the organisation "frog") is "to survive" 415</page><page sequence="22">in the sense that the biological machinery wherein it is embodied is neither eaten by predators, nor starved through lack of food. In respect to this goal, the intension of the symbol denoting "moving fly" is "Some thing you eat; put your tongue out!" The intension of the symbol denoting "large shadow" is "Something you avoid; jump away!" We can certainly bring the organisational and the functional models into cor respondence by invoking the true statement that the intension of a symbol is the organisation of a programme of the computing machinery that interprets it. Since the frog acts in its visual modality as a simple automaton, there is only one possible interpretation, and the correspon dence is precise. But it is not, even in this case, an identity. In the organisational model, the frog is seen as a programme related to the programme of its environment. (Notice that the environment programme is usually a social and historical structure; if we had modelled the auditory modality that the frog uses for mating, then the environ mental programme would have been social.) In the functional model, the frog is seen as a biological computing machine. This brings us back to (I) and the fact that the primary distinction between an organisational model and a functional model is to do with its ontology; with those features of reality with which its variables are identified. The fact is particularly crucial when we consider evolutionary systems or processes involving insight and innovation. Gunther has discussed the ontological issue and has pointed out the necessary distinc tions. In the present article, I shall concentrate upon a number of practical points which are, however, closely related to Gunther's philosophical argument. In Part 2, I shall contend that the methods of using and identifying an organisation and a functional model are completely differ ent. 1.9. SOME DIFFICULTIES As a result of making some quite plausible distinctions between different types of model, we are in peril of confusion. First, it is not always obvious who prescribes or describes what (is it the model builder or is it someone or something imaged in the model). Next, so far as hierarchical structured models are concerned, the status of the hierarchy is uncertain (is it always imposed by the model builder, or does it reflect a property of the real system, imaged in the model). I have omitted the rather cumbersome apparatus needed to avoid these confusions in order to demonstrate that such an apparatus is desirable, if we wish to talk about models for communication systems wherein statements are taken intensionally (and this is precisely what we 416</page><page sequence="23">do aim for in the social and behavioural sciences). The apparatus will be essential (not merely desirable) when we come to examine the use and identification of models, rather than models per se. Hence, I shall intro duce it in the next part of the paper without apologising for an argument which might, in the absence of this preamble, seem unduly pedantic.9 2. The Identification and Use of Models for Systems 2.1. LANGUAGES L * AND L To avoid the confusions that beset our discussion in Part 1, it is essential to make a very clear distinction between the act of talking about communication and the process of communication itself (either within the model or within the physical system it represents). I shall use Cherry's nomenclature for this purpose. When talking about a model, we employ an observer's or experi menter's metalanguage. This metalanguage may be ordinary (or scientific) English. (It could be a formal system,10 though formalism is, in the present case, unnecessary.) The "metalanguage" which we shall designate L* is used: (1) To describe the model and any communication that goes on within it. (2) To describe abstract structures and relate them to the funda mental model from which they are derived. (3) To discuss the model with other observers and to pose testable hypotheses (modified models, new models). (4) To generalise the model and relate it to other models by the use of unifying principles. (5) To describe and prescribe the identification of the model with a physical system (by specifying procedures for measurement, observation and experimentation). 9 The full apparatus, required if we are to provide a logical analysis of intension, has been developed by Martin (who comments upon the case of societal as well as individual intensions). This is a necessarily complicated work in which it turns out that there are really many sorts of intension, not just one. In the present paper, we shall not carry logical analysis beyond the level of verbal and rather imprecise comment, because our primary concern is the practical application of models, and this degree of complication can thus be avoided. However, Martin's argument is at the back of my mind, and, so far as I know, provides the only starting point for a thorough treatment of the subject. 10 It must be in a precise account, such as Martin's. 417</page><page sequence="24">We shall attach particular importance to item (5) above, but all of these activities are adumbrated by the phrase "talking about" the model in L*. Apart from the statistical models, all of the models we have con sidered entail interaction between members of an ensemble of subsystems that are imaged within the model (for example, by the units in a func tional model). Some models also entail communication (the use and interpretation of symbols that are part of a symbol system with well defined rules). When communication does take place, I shall again use Cherry's terminology and call the interpreted symbol system an object language, designated L. In common with the model, L is described in L* by the model builder. But it is not the same as L* and it must not be confused withL*. To emphasise this point,/,* is used for talking about the model; L is used for communication within the model or within the real system it represents. 2.2. IDENTIFICATION Models come into existence as hypotheses posed by an experimenter or observer and described in L*. An act of abduction is involved (or sometimes an act of deduction from an already validated part of the model, which allows the experimenter to be more specific and to fill the holes in an original structure). In either case, the experimenter is free to pose any logically consistent hypothesis; hence to construct any logically consistent model. Identification is the process of placing a model in correspondence with the real system it is supposed to represent. (In this process, putative models are severely winnowed; not every logically consistent model can be identified with reality.) Identification is clearly required in order to test an hypothesis (either some new model or the extension of the old one), to confirm or to validate a model in any way. Subsequently, identification is needed in order to use a model for predicting or controlling the behaviour of a real system and identification is entailed whenever the experimenter uses a model to explain the working of a real system (and communicates his explanation, by dint of discourse in L* to his fellows). We shall lose nothing by assuming a single situation, Fig. 7, in which a model is identified. This is a paradigm for control, prediction or experiment. The model builder and user (called the "experimenter") is anxious to measure the state of a real system, to stimulate it, and possibly to modify its parameters. The identified model serves as the basis on 418</page><page sequence="25">which, or the framework in which, all of these activities are carried out and the results interpreted in L*. Variables in the model (described in L*) are placed in correspondence with properties of the real system. The character of a state measurement, the character of a stimulus and the set of possible relations between stimuli and states, states and measured states, and stimuli and measured stimuli, are interpreted in L*, with reference to the model; in this sense, the experiment or the behavioural control process imaged in Fig. 7 is "relative to the model." If the experi menter is testing a hypothesis, then there will in addition be parts of the model that are "hypothetical," and his subsequent acceptance or rejection of these parts will depend upon the measurements made within the already accepted framework. Observations of real system and observations of model Observer or experimenter Identification process maintains analogy between model and real system Environment of model Environment of real system Fig. 7. Process for identification of a model with a real system. This is a paradigm for experiment, prediction and control. Logically speaking, identification is an analogy of the form "A is to B as a is to b" that is stated in L* (the number of terms A,B, . . . and a,b, . . . is unlimited). Here, A, B are components in the model, and a, b are subsystems of a real system. The "is to" relations comprise the structural substance of the model. (They are correlations in a statistical model, causal or systemic relations in a functional model, and program matic or syntactic rules in a normative model or an organisational model.) The "analogical" relationship symbolised by "as" in the L* statement, is a 419</page><page sequence="26">relation determining the relevance or irrelevance of the properties of objects a, b to objects A, B. Thus, the analogy relation holds between objects a, b in one universe of discourse to do with real systems, and objects A, B in another universe of discourse to do with models. It follows that L* must be a metalanguage capable of describing these universes of discourse and their possible relationships. The analogical relationship says what properties of a real system are relevant to a model (and may be indexed by the values of model vari ables). In a functional model, for example, the input and output properties of the subsystems identified with units in the model are relevant; most other properties are irrelevant. Again, in most of the normative population models, the age, sex and race of real individuals are relevant; their hair colour and personality are irrelevant. We shall consider two sorts of analogy relation, closed and open. Take any identified model; if it is possible to say "relevant" or "irrele vant" for any property that is cited then the analogy involved in the identification is closed. If not, but (to avoid triviality) if some properties are relevant and some irrelevant, the analogy is open. Thus, an analogy is open when, at a given instant, certain properties have undetermined relevance (neither relevant nor irrelevant), though they may become relevant (or irrelevant) as a result or further experimentation or due to an internal change such as an "innovation." Identification involving closed analogy is far more familiar and we shall deal with it first. But open analogy characterises the identification of models for evolutionary systems and for creative activity, "innovation" and insight. 2.3. METHODS FOR ESTABLISHING THE IDENTIFICATION OF A MODEL For each type of analogy, there are also two essentially different ways in which the identification can be brought about by the experi menter. I shall call these "the method of physical science" and "the method of rational discussion." The method of physical science is most important in connection with statistical or functional models and requires little comment. State vari ables in the model are identified with relevant properties of the real system by specifying procedures for measurement (insert a strain gauge, an audiometer or the E.E.G. electrodes). Stimulus variables are identified in a similar fashion (shine a light at the retina, pass a current of 10 microamps, and so on). In each case, the experimenter must nullify the effect of irrelevant properties by reducing the variation of unwanted parameters and trying to exclude the perturbation of relevant by irrele vant variables. But all this is covered by the dictum "maintain constancy 420</page><page sequence="27">and repeatability of the experimental conditions." However, we should notice that whenever an identification is brought about by the method of physical science, this identification is under the experimenter's control entirely; ideally so, at any rate. In particular, the real system has no hand in identification; it has no responsibility for maintaining the identifica tion. The method of rational discussion is quite different, and is applied only to normative and organisational or programmatic models. The experi menter starts off with a foreknowledge of the possible goals and aspira tions of the components in the real system and he usually knows quite a lot about their possible cognitive structures (in the sence of 1.6). This knowledge may be gleaned in various ways; when the components are men, the experimenter makes a general inference of similarity with him self. The inference is supported by the fact that they are also made of flesh and blood (ultimately, that depends upon a whole set of other models, identified by the method of physical science); it is also supported by behavioural observations, by the fact that men talk and understand him, and so on. When the components are animals, this knowledge is based upon the data of a natural historian — in strict contrast to the data of a biologist working within the framework of physical science. Essen tially, the experimenter places himself in the world inhabited by the creatures concerned. He "sees things through their eyes." He guesses the connotation of the signs emitted by other animals and the exigencies of the environment, and checks his guess by observing behaviours which, in this frame of reference, are goal-directed. All of this is described in L* and it may or may not constitute part of the model. (In general, it is part of an organisational or programmatic model, and it is not part of a normative model. We return to this point later.) The next step in the process is the establishment of a common language in terms of which (prior to the identification of the model) the experimenter can discuss his proposed experiment. Let us first consider the case in which the components (participants is a politer word) are men. To begin with, it will also be convenient to assume that we are identifying a psychological model with the mentation of a real subject (for example, one of the artificial intelligence models of 1.6). The common language may either be L* or L. If it is L*, the experimenter must assume that the subject can understand at least part of it. If it is L, the experimenter knows that the subject can do so, by definition. He uses the common language to engage in a discussion about the experiment and to obtain the subject's agreement to accept certain symbolic constraints upon which the identification depends. In psycho logy, the entire discussion is usually called "issuing the experimental 421</page><page sequence="28">instructions," but in logic (and in practice also) it is much more of "reaching an agreement." For a problem solving or concept learning experiment, this agreement involves the acceptance of a particular one of the possible goals (in this case, to solve problems of a given class), the acceptance that certain stimuli denote problems of this class and the acceptance of a ritual wherein the subject designates solutions by certain response selections. If the discussion that leads to this agreement takes place in L*, then the experimenter issues the "instructions" in a way that a colleague could understand. If the discussion takes place in L, he gives "instructions" for a naive or even abnormal subject. The discussion process is readily generalised to many subjects, as in the case of several participants in a small group experiment or an entire society. The distinction between discussion in L* and in L is generalised to the distinction between sophisticated and native speech. Conversely, the experimenter can use rational discussion to discover which of the possible goals are in a subject's mind at the moment concerned, and which of the possible denotations are currently accepted. Having discovered these facts, he modifies his model to fit the particular subject (rather than imposing an agreed model upon the subject). This technique is adopted in identifying the grammatical or linguistic models of 1.7. In either case (agreement or discovery) it is evident that the onus for maintaining the identification of the model rests largely upon the subject's shoulders. In contrast to an identification established by the method of physical science, the subjects as well as the experimenter are responsible. The subjects participate in the identification process and the experimenter may be forced to participate in the experiment by engaging in further discourse in order to maintain the identification if the subjects fail to keep their agreement. For cultures that are very different from our own (Martians, perhaps, or very primitive people), we necessarily rely upon the technique of discovery and the interpretative discourse is correspondingly one sided. Animal communities serve as a useful limiting case. With a few excep tions,11 the natural historian cannot really "talk to" the beast in any cogent sense of the word. Instead, he forms an internal image of the creature and its world, "talks to" this internal image and modifies his experimental conditions so that his model can be identified. (For 11 The dolphin is certainly one of them. Some of the man-animal relationships described by Lorentz and mentioned by horse riders, dog owners and animal trainers probably also qualify for inclusion. 422</page><page sequence="29">example, he presents a frog with stimuli that are signs for flying insects, rather than points of light which have no significance for the animal and to which it is entirely unresponsive; he presents a song bird with sign stimuli that are significant releasers; and so on.) It is still true that the activities of the natural historian and the sober, pre-experimental discussion with a group of trained subjects, are part of the same continuum. In each case, the identification is brought about at a symbolic level. It depends upon the subjects' continuing acceptance (innate or agreed) of certain symbolic constraints. The entity that is identified with the model is necessarily an organisation rather than a thing. 2.4. IDENTIFICATION USING CLOSED ANALOGY RELATIONS (I) Statistical models. Statistical models, such as those discussed in 1.1, are identified by the method of physical science; further, they are descriptive models so that the experimenter need only choose real system properties corresponding to relevant state variables. Having made his selection he specifies measuring procedures (like counting the number of words of a given class in a particular text, or the number of individuals in a population characterised by the value "Baker" of the attribute "Occupa tion"). Of course, the measurement specification also involves a process for recognising the required type of word or individual; but all of this is a standard part of scientific methodology (it is not peculiar to the social and behavioural sciences). Indeed, only a couple of special comments are needed, namely: (A) Although some statistical models, such as Zipf's model, entail the mention of a language, this is not treated as an object language in the sense of 2.1. Natural language, in Zipf's model, is treated as a source of data rather than a communication medium. (B) The "principles" of a statistical model are comments made in L* about the model or the data source. As we insisted in 1.1, "principles" are in no way laws within the model. (II) Functional models. The functional models of 1.3 are also identi fied by the method of physical science. Here, in addition to measuring state indicative attributes, the experimenter must select areas of system properties that correspond to stimulus and parametric variables in his model. The state space of the model is also partitioned; the partition demarcating units; the states of a unit corresponding to the conjoint 423</page><page sequence="30">evaluation of the properties of real sybsystems. Consequently, the identi fication of a functional model is more complicated than the identification of a statistical model, but it is not essentially different. In particular, there is still no real object language in the sense of 2.1. "Interaction" between the units is a surrogate for "communication." Since interaction serves to correlate the activity of the units, the experimenter can talk about this interaction as though it is discourse and he is at liberty to formalise the interaction as a sort of "pseudo L". However, his "pseudo L" is a construct that he invents and imposes upon the model (not upon the real system); he neither discovers it as the object language used by the real system, nor does he (in a sense we discuss later) ask the real system to adopt it for experimental purposes. As shown by Ashby, the closed analogy in the identification of either statistical or functional models is a homomorphism, in respect to the relevant analogical properties.12 Hence, the model is an abstraction from the real system that preserves relevant relations. (Ill) Normative models. Normative models are identified either by the method of rational discussion, or by a degenerate form of this method (let us dub it "absolute command"). Hence, in the model of 1.4(4) we ask the participants in a real system to play according to the rules of the game; in 1.4(6) and (8) we ask them to obey certain societal rules; in each case, we obtain their agreement or even force them to agree (perhaps we arrange the conditions so that the laws are mutually enforced by the participants). Apart from the engineering model of 1.4(1), all of the normative models involve an object language L, in terms of which the participants communicate. There is an obvious sense in which the institu tion and acceptance of L is a prerequisite for accepting the rest of the model (in 1.5(5) the game prescribed by the model is simply to use the object language). In fact, the normative model is built around an object language. The model builder "starts with" L and it is no accident that L has a special property in identification, namely that L as used in the real system is isomorphic with the image of L in the model. This is due to the fact that the abstraction needed to establish L is performed by, or is assumed to be performed by, the participants in the real system. The experimenter builds an L into his model that is in one to one correspondence with the L that exists, or that he can impose upon the real system with the real system's agreement. 12 Or another relation preserving "simplification." 424</page><page sequence="31">The identification of the normative models cited in 1.4(1) and 1.4(2) deserves special attention. Of these, 1.4(1) is an engineering design used prescriptively, as a normative model in building an artifact (but conversely, the same design can be regarded as a well validated functional model for this artifact). We do not discuss the design with bits of machinery; we issue an "absolute command" with an authority based upon our knowledge of well validated functional models, and certain deductions made from them. (In fact, we do not really "command" the machinery. We instruct a willing project engineer or a clever automaton to build the artifact according to the design.) This case is also exceptional insofar as the object language is, at most, a "pseudo L." The communication model of 1.4(2) is again identified through an absolute command. Here, however, we may argue that L is a genuine object language, rather than a "pseudo LIn a communication model, the primary prescription is L; the components T and R of Fig. 2 corre spond to stochastic subsystems that obey the rules of L. Since the real subsystems T and R are prescribed by the model, they have built in mechanisms for "dealing with" and (in a weak sense) interpreting the expressions produced in L. In a very weak sense, R "is able to comprehend all of the messages sent by T." But R cannot, of course, describe its state of expectancy or anticipation; regarding the output of T, it is simply built to deal with the output of T, and terms like "expectancy" or "anticipation" are no more than convenient, but anthropomorphic phrases. In other words, whenever we talk about properties of T and R in respect to L, we do so in terms of L* and the properties are predicates of L* designata. Consider, for example, the statistical properties of communication theory (measures of selective information and its converse, uncertainty). The model of Fig. 2 is identified with a real system, primarily to allow the experimenter to cite values of these properties.13 Given the identified model, the experimenter can readily compute "R's uncertainty regarding the output of T" or various other uncertainties and contingent uncertain ties. But his property statement is a statement in L* about the stochastic properties of T and R as constrained by L. "R's uncertainty about the output of T" is just a "convenient phrase." Really the experimenter is talking about an objective uncertainty; the value of this property does not really designate R's expectation or R's anticipation (R's degree of doubt or R's "subjective uncertainty"). It is the predicate of anL* description. 13 In order to evaluate these properties, the vocabulary and syntax of L must in the first place be specified. 425</page><page sequence="32">These comments refer to all normative14 models (the communication theory model is only exceptional insofar as it is identified by an absolute command, not in any other way). They also refer to all property state ments that are made whilst the identified model is being used by the experimenter. Thus, it is necessary to make a very sharp distinction between the act of identifying a normative model and the act of using it. For, as we have just argued, normative models [with the exception of those in 1.4(1) and 1.4(2)] are identified by the method of rational discussion. As we stated in 2.3, this method involves a common language that is understood by the components in the real system and the experimental instructions are countered by statements of agreement or acquiescence that come from these components. Evidently, the identification statements of rational discussion have a connotation to the components or subsystems; they are taken intensionally. Either we must say that the common language is L* or that L is used as a real and identified language and not simply a formal system, so far as the process of identification is concerned. In contrast, when the normative model is used by the experimenter, L is a formal system. We can only maintain the rather tidier view that L is always a formal system if we adopt the convention that the common language used for identifying a normative model is L*. Given this convention, the only interpreted language is L* and the only connotation involved is the experimenter's connotation for L* expressions. If we adopt this convention (and it is implicit in the narrow defini tion of a normative model given in 1.4), then the players in a game (or even the participants in a situation that is not superficially game-like) have no connotations as such. Their goals and beliefs and attitudes are imaged by L* statements that describe them as particular sorts of automata. We say "he aims to win" as a figure of speech. Really, "he is represented by an automaton, the L* description of which acts as though he aims to win." Further, in this format, the goals and their descriptions cannot change. Once identified, the structure of the model is entirely static. Hence, we cannot use a normative model to describe concept learning. It is a cumbersome vehicle even for a description of adaptive processes. (IV) Organisational models. Like normative models, organisational and programmatic models are identified by the method of rational discus sion, augmented by the method of physical science. Once again, the primary task of identification is to establish L. By definition, the organisa 14 At any rate, in the restrictive sense of 1.4. 426</page><page sequence="33">tionel or programmatic model is built around L, as a processor or set of processors of L. Now, in contrast to a normative model, the L of an organisational model is always an interpreted object language. It is never merely a formal system.1S Its expressions have a connotation (an intension) for subsystems of the real system represented by the model. The possibility of having such an object language stems entirely from the hierarchical character of these models, which we stressed in 1.5, 1.6 and 1.7. A real subsystem represented by an hierarchical model is able to make descriptions of its own internal state, its own computations, its own goals and its own predictions; further, it can prescribe actions.16 In addition (and in many of the most important cases) it can also communicate descriptive and pre scriptive property statements to other subsystems that are denoted by the model, and (if it does so) this communication takes place in L. Thus, for example, there is a perfectly good sense in which the experimenter may talk about "subjective uncertainties" or "degrees of doubt" entertained by parts of the system with which an organisational model has been identified. To cite a rather banal case: an artificial intelligence programme can readily describe the amount of computation that will be needed on average in order to solve a problem presented to it in L. Within a suitable format, it can communicate this description — either to some programme that forms part of a model, or to some other part of the programme (such as an executive routine or a decision routine). It thus communicates a property statement in L (and equally well, it may communicate statistical property statements that are inter pretable in L as uncertainties or degrees of doubt). L is not given, as it is in the communication theory model of Fig. 2, as a fait accompli. To parallel the discussion of 2.3, the detailed structure of L may either be discovered by the experimenter, or imposed upon the system17 and agreed to by the components of the system18. 15 Gorn points out that any formal language, such as a formal programming language or logical language, has a connotation for somebody (i.e. the computer programmer or the logician who uses it), however abstract and uninterpreted it may appear to be. We concur. But for the present purpose, we also require (as Gorn does, in some cases) that L has an interpretation within the real system. 16 Or we might say "the model has an hierarchical structure because it applies to a real system of this sort." 17 Upon the system, not just upon the model. 18 When an organisational model is identified, the common language for rational discussion may either be taken as L* or L. In the former case, all of L may be imposed, not merely its "detailed structure." In the latter case, L exists, but a restriction may be imposed. Hence the caveat "detailed structure." 427</page><page sequence="34">The details of L are imposed in the identification of an artificial intelligence model (as in 1.6 and 2.3) with the subject in a problem solving experiment. Similar comments apply at a societal level when the models of 1.5(8) are identified with small problem solving or data processing groups in the context of a laboratory experiment. Finally, men or animals may be trained to use a particular L. The most dramatic exemplar is Ferster's work, in which chimpanzees were trained to use a language with the structure of arithmetic. There is also a sense in which any contextual restriction involves an imposed L. For the socio-architectural model considered in 1.6(7), it is true that L is a natural language; but when the model is identified, its domain is restricted to architectural theory and practical matters to do with the construction of buildings. Much the same can be said for some of the models in 1.5(6), wherein the domain is restricted to particular sorts of design or invention. The discovery of L often involves a perplexing mixture of the method of rational discussion and the method of physical science. In identifying models of 1.7, for example (models for the grammar of a natural language), the experimenter wishes to establish correspondence between the rules of the language L as understood (and possible described19) by the population, and the phrase structure and transforma tional rules (described in L*). To do so, he questions members or the population and asks them to describe their rules; this entails the method of rational discussion. Later, however, he checks whether these rules really apply by behavioural observations that entail the method of physical science. Similar comments apply to the models cited in 1.5(3), (4) and (5), where the experimenter is aiming to establish cultural invariants, rather than grammatical rules. Finally, for the animal society models of 1.5(1) and (2), the experi menter is faced with a grossly exaggerated form of the dilemma encoun tered with an unfamiliar culture and the method of rational discussion becomes differentiated, as in 2.3, into the method of the natural historian. 2.5. CONVERSATIONAL SITUATIONS The methods of identifying organisational models that have so far 19 As in 2.3, if the culture is unfamiliar, the rules are not directly described. In this case, the experimenter must use discourse that involves gesture, common emotive terms, or ostensive definition, in order to identify a cognitive structure from which a set of rules is deduced. 428</page><page sequence="35">been considered are suitable for situations in which the model is essen tially complete and substantially invariant. With small refinements they can also comprehend situations in which the real system learns or other wise changes its characteristics, provided that the learning process does not entail gross change in the cognitive structure of the system (requiring corresponding modifications in the cognitive structure of the model). To be precise, the present techniques can be extended to deal with goal directed adaptation and learning processes that involve predictable parametric modifications in the model. They cannot, however, deal with learning situations of a type I shall call conversational situations, wherein the participant or participants engage in essentially open-ended concept learning. Consider a conversation between a pair of participants, (A and B in Fig. 8), Insofar as A and B get to know one another and manage to exchange ideas or build concepts, they are learning about one another, and this learn ing involves the internal identification in A's mind of a model MA (B) for B, and in B's mind of a model MB(A) for A. The language used for the conversation is, of course L (for A and B are members of, or participants Ma (B) Mb (a) Fig. 8. Conversation between A and B. A identifies a model with B, M^(B). B identifies a model with A, MB(A). in, a real system, though we describe their conversation in L*). Hence, the identification process is "internal." In fact, A and B usually restrict L as their conversation proceeds, to a private language (of a form that depends upon A's model for B and fi's model for A). Insofar as this occurs, the internal identification process is a precondition for maintaining intelligible discourse between A and B. As I have described it, the "internal" iden tification savours chiefly of mutual discovery, but in the majority of conversations, one of A or B is dominant, and in this case the restriction of L and the entire identification process looks more like an imposition by agreement. According to the subject matter of the discourse, we say the dominant participant "teaches" or "extracts information from" the other participant. I wish to consider a special class of lop-sided conversations suggested 429</page><page sequence="36">in Fig. 9. The dominant participant A, has the further property that he acts as the L* interpreter of the current L discourse. In particular, as the internal identification changes, he carries out an external identification between a structure describing the relations of the internal models MA (B) andMb(A), andan external organisational model M*. Depending upon the subject matter of the conversation, we call A a "Teacher" or an "Inter viewer". A Mr (A) Fig. 9. A, the dominant participant, acts as L* interpreter of L discourse. If the external identification of M* is to involve a closed analogy in L* then M* must remain invariant throughout the discourse. The con ditions that must be imposed in order to secure the invariance of M* are very severe indeed (we return to this matter in 2.9). The contraints are so severe that many sorts of teaching and interviewing would be excluded. Nevertheless, we shall assume, for the moment, that these constraints have been imposed and that M* is invariant. Let us call the constrained system a "restricted conversation" between A and B. Even within a "restricted conversation", participant A has a dual role, for (1) he maintains rapport with B by an internal identification process that allows him to keep track of the changes that occur when B learns, and (2) he performs an external identification between the external organisational model M* (which may be a model for tuition or a model that specifies salient attributes of B), and the relation between MA (B) and Mb{A). So long as B is learning, continual reidentification, external or internal, must take place. This rather elaborate process of continual reidentification is the only way to represent the conversation in terms of 430</page><page sequence="37">an L* model M*.20 Interviewing and Tutorial techniques are ubiquitous in the social and psychological sciences. The element of participation may be minimised, but it cannot, logically speaking, be excluded. It is important to be clear about the character of conversational situations, and to recognise them when they occur. 2.6. FORMS OF L What sort of language is Ll The question is general, but of particular interest in a conversational situation. L in Fig. 2 is a simple, closed, object language (a sign system) and this may suffice for the most rudimentary releaser organised exchanges. In general, however, the L of an organisational or programmatic model may either be: (I) A restricted natural language, or an open ended language with logically similar properties, or (II) A closed but stratified language, L = L°, Ll, . . . wherein the levels of discourse L°, L1 refer to levels of control or abstrac tion in the hierarchy of the model or real system (notice that in Fig. 2 we have the limiting case of L = L° ). In (II), the levels of discourse serve as (at least partial) internal metalanguages (not to be confused with L*). Thus, L1 is capable of describing at least some of the L° syntax as well as being a vehicle for descriptive and prescriptive statements about properties of the objects denoted by terms in L°. The structure of a communication system with a stratified object language is shown in Fig. 10. If L° and Ll have separate vocabularies (as they may do) then the hierarchy of channels is a physical as well as a symbolic entity. Fig. 10 could, for example, represent the discourse between a subject and a computing machine in an on line controlled concept learning experiment, where the machine is acting as the dominant conversational participant of 2.5. Here L° is a channel used for posing problems and obtaining solutions, L1 is a channel used for making statements about classes of problems or about the way in which problems are posed. Using an L of type (I), property statements are still internal metalinguistic comments, but they are demarcated by meaning designators 20 Participant A need not be a man. The man can be replaced by a suitable computing machine, and I have used this mechanical technique, which has a number of advantages if the universe of discourse of L is well structured, in experiments on individual and small group learning. 431</page><page sequence="38">Fig. 10 Structure of a communication system with stratified object language — the levels L° and L1 corresponding to levels of control. and qualifying phrases of the form "P is a Q on this occasion." Here, P may be substituted by a statement in L and Q may be substituted by "command" or "prepositional statement" or "property description," as required. Hence, an apparently "unstratified system of type (I) is "locally stratified" (meaning, stratified in context, by its user). A patient talking for a psychiatrist (producing a stream of consciousness) is using an unstratified L as though it were Ll . The metalinguistic discourse modelled in 1.5(4), (5) and (6), is all "as though it were L1 " discourse in an unstratified L. Human natural languages come into category (I) (there are a few exceptions, such as technical fields were discourse is consistently strati fied, and Japanese respect language). But human beings can also deal with stratified language systems perfectly well, and they may be forced to do so in symbolic procedures or at a man—machine interface. Conversely, a few of the languages used in animal communication belong to (II), notably the language systems used by the dolphin. 2.7. PRAGMATIC AMBIGUITY Either stratified object languages or unstratified open-ended object languages accommodate the type of discourse we require (strictly, the former type does so only if there are sufficient levels; that is, if the hierarchy can be extended indefinitely). But there is an important distinc 432</page><page sequence="39">tion between stratified and unstratified discourse; namely that stratified discourse is referentially unambiguous, whereas unstratified discourse or conversation is liable to a referential ambiguity that Gorn calls "pragmatic ambiguity." This is an ambiguity in the L* description of the L discourse (or in the L discourse itself) regarding the level of reference of L expressions. In extreme cases, it may lead to paradox21 and various types of "indecid ability;" i.e. situations in which no explicit course of computation is prescribed by the cognitive structure of the real system or the organisation model that represents it. We should emphasise that pragmatic ambiguity is not particularly troublesome in the actual conduct of a conversation or a control process. We live with it every day, and our communication is a great deal more efficient because we do so. We do not go beserk when we encounter paradox or come to a halt when a problem is unsolvable. We merely act in an "illegal" fashion (i.e. we depart from the algorithms which we use to describe our cognitive structure, and do "something" that is momentarily apposite). However, if the experimenter is anxious to avoid any ambiguity (as he is in a precisely controlled experimental situation) he will prefer to use a stratified L. Consequently, whenever he is in a position to impose L by agreement, he is likely to impose a stratified L. For instance, this is the reason why L in the man machine system of Fig. 11 is a stratified L. 1 Fig. 11. Simple adaptive teaching machine. 21 Gorn provides exemplars in his discussion of this matter. 433</page><page sequence="40">Now the levels of discourse in L correspond to levels in the organisa tional hierarchy of the model itself. We thus arrive at the important conclusion that the hierarchical structure of an organisational model is neither entirely "invented" by the experimenter, nor "inherent" in the real system. Insofar as the real system is capable of learning, having connotations and needing a rather elaborate L, it is potentially and inherently hierarchical, but it commonly engages in L discourse or L control within an unstratified framework (control statements are "locally stratified"). When that is true, the potential hierarchy is "locally mani fest" also (in connection with particular goals and objectives). On the other hand, if we aim to make unambiguous L* statements about the L discourse of a system with this inherent structure, we are bound to impose a stratification, and in this sense, the hierarchy in the model is invented by the experimenter to suit his own purpose. It is not, of course, sufficient to impose any stratified L. There must be sufficient levels of discourse to accommodate the type of communica tion in which the participants engage. In particular, man is posessed of an innate need to learn, to abstract and to make sense of his environment. Lacking an adequate L, he uses the L provided in an illegal manner. This point is amusingly illustrated by my own work on man—machine discourse. Consider a simple conversational learning situation, Fig. 11, in which a man is required to make correct mechanical responses to a sequence of stimuli (he knows what a correct response is in principle, but he must acquire the skill of making it consistently). The stimuli are generated by an adaptive machine which receives a feedback signal indicating the subject's proficiency, and which modifies the difficulty of the stimuli by providing cue information in such quantities that learning rate is maxi mised (the machine, of course, contains a rule based upon our ideas of how a man learns). Here, L° is a simple stimulus and response language. The knowledge of results signal in Fig. 11 is strictly an Ll statement, since it refers to a property of L° discourse, but it is a unidirectional statement so that, in this system, the discourse is confined to L°. At least, it is in principle, and whilst the subject makes legal L° statements that are attempts to produce correct responses, given the stimuli. In practice, something rather different takes place. The subject begins to make sequences of responses that the machine, acting as an L participant, interprets as mistakes (and which we must interpret in L* as mistakes according to the legal usage of L° that is described in L*). These sequences finish at points that correlate with changes in the difficulty level and it can be shown, fairly convincingly, that mistakes of this kind are really an attempt on the subject's part to manipulate the response of 434</page><page sequence="41">the adaptive machine. Since L° contains no expressions that refer to the action of the machine, they are illegal L° statements (they constitute a misuse of L° ). Further, if we rigidly confine the discourse to L°, we suffer an inherent ambiguity, a pragmatic ambiguity or ambiguity of reference, regarding the status of these expressions. Do these sequences constitute genuine mistakes or are they an attempt to control the machine? This ambiguity can be resolved by stratifying the system so that it has the form of Fig. 12, in which the discourse occurs in L1 as well as in L°. As before, the L° discourse refers to stimuli and response selections. The L1 discourse consists in property statements (akin to and including the knowledge of results signals) and statements made by the subject (that modify machine parameters). Of course, this stratification is imposed upon the system and the subject need not use it. Fig. 12. Conversational teaching system. However, in practice he does so and the empirical finding is that L1 is used as a control channel and the misuse of L° is eliminated. So is the ambiguity suffered by the experimenter in his L* description of the process, for if L = L°, L1, he can associate L° "mistakes" with real mistakes and Ll statements with "control" statements. This exemplar is typical of conversational situations. Our subject is using L° as though it were an unstratified object language (which is contrary to our syntactic rules). In this particular experiment, we could have avoided the introduction of L1 by altering the syntax of L° so that it 435</page><page sequence="42">became legal to use it as an unstratified language and by introducing a mechanical L interpreter able to detect real and spurious mistakes (in which case, knowing the L interpretation rules, we should be able to discriminate between real and spurious mistakes in the L* description). But the possibility of doing so arises only because we know the subject's goal,22 for instance, the goal of learning the skill. In the systems we shall discuss in 2.9, the subject is able to construct his own goals. 2.8. OPEN ANALOGY FOR FUNCTIONAL MODELS The embryologist has a perfectly good idea of the entity "embryo." Quite reasonably, he wishes to retain the integrity of this entity and to observe and model its development using a model for "an embryo" throughout the process. Let us assume that he also wishes to use a functional model for this purpose.23 It is quite evident that the real system properties relevant to the model for an embryo undergo change as the embryo develops, and that (logically speaking) the embryologist is in the position of periodically reidentifying his model. Cellular and biochemical properties are initially relevant. At a later stage, tissue properties are relevant; and at a later stage again, the properties of organs and systems such as the nervous system. Conversely, the properties that later become relevant are initially irrele vant (there is no nervous system, no tissues have yet differentiated). Hence, the identification of the functional model for an embryo contains properties that have an undetermined relevance and constitutes an open analogy in the sense of 2.2. Of course, given the goal that the embryo will ultimately develop into an adult, it is possible to attempt an organisational model for the embryological process. In practice, this is a very useful expedient. We say that property A will be relevant at stage a and that property B will be relevant at stage b, and so on. But the organisational model is neither the model for an embryo in a functional sense (the relations within it are not causal relations), nor is the adult causally determined (our assumption of the goal in the organisational model is backed up by experience only). Further, it is not obvious that a causal model for the entire process is 22 Or assume that we do. If this assumption breaks down, the ambiguity obviously reappears. 23 This assumption is not altogether justifiable. The Cybernetic Models used by Waddington are both organisational and functional models; the organisational com ponent, involving an hierarchy of control processes; the genetic, epigenetic and metabolic "levels." 436</page><page sequence="43">logically possible. Events like evocation may depend upon many different factors, and there is no reason to suppose that a contingent disjunctive proposition about the factors involved can, in principle, be reduced to a conjunctive or relational form. The sociologist and the anthropologist occupy a position strongly reminiscent of the embryologists' position when they contemplate and model the evolution of social and biological systems. However, unlike the embryologist, they are rarely able to specify a goal for an organisational model24 so that the question of whether or not an organisational model for the evolutionary process could be reduced to a gigantic functional model does not seriously arise. In practice, the functional model is reidentified periodically and properties with undetermined relevance become relevant or irrelevant as needed, in order to maintain the integrity of the system. The situation is thus analogous to the internal discourse in a con versational system. The experimenter is acting like the participant A in the lop-sided conversation. In particular, the reidentification forces him to change his model (in the same way that A had to change his internal model for B). Only in this case, it is the external model that is changed to keep pace with the evolution of the real system [whereas in 2.5 M* is invariant, in this case the open analogy involves a time varying model M*(t)]. The canonical form of this process is von Foerster's concept of a self organising system, and one important consequence of the process is that any system property, such as the property "evolution" is a predicate, not only of the observable behaviour of the system, but of the time varying model also.25 24 This does not imply that organisational models are useless; on the contrary, they throw a great deal of light upon evolutionary processes. The comment simply under lines the fact that we are here looking forwards, whereas the embryologist is also able to glance backwards from the goal to the origin. 25 Von Foerster argues that a system is a self organising system, if, and only if, dR/dt &gt;0 for all dfef, where R is a redundancy and T is a certain interval. By definition, R=l—H/I where H is the information entropy of the system and I is the maximum information entropy. Notice (1) that H and / are de fined in L* with respect to a particular model M*\ (2) That / depends only upon the model and usually changes with it, though H depends upon the behaviour observed within the framework of M*. The condition dR/dt &gt;0 may be satisfied either by suitable changes in H or in I. However, if I is fixed, the system cannot be a stable self organising system for large T, since H must approach 0 to maintain dR/dt &gt;0. Typically, stable self organising systems are associated with changes in H due to some sort of adaptation in the real system and changes in /, induced by the changes in M* that are imposed by the experimenter. 437</page><page sequence="44">2.9. OPEN ANALOGY IN ORGANISATIONAL MODELS When people learn to solve problems, they use analogy relations that are specified as metaphors in L (let us call these "L analogies" and make a very definite distinction between them and the "L* analogies" concerned with identifying a model). Typically, L analogies are similarities between problem solving operations or classes of problem solving operations; for example, given a problem a that is solved (to yield the solution b) by an operation P, and given a problem a that cannot initially be solved, the subject applies an L analogy, z, to P yielding an operation p, applicable to and producing a solution j3. Here, z has the form "a is to ó as a is to j3," the "is to" relations being P and p and the analogical relation being P(z)p.26 I shall call the domain of a class Z of L analogies, z, a coherent universe of discourse, U. The coherent universe of discourse may either be delimited by the relevant properties of the objects a, b, a and ¡5, or by properties of the operations P and p. If the operations are TOTE units, including facilities for recognising the properties of objects upon which they operate (as we assume), then U may be equivalently defined in either manner. The sort of problem solving that goes on in manipulating puzzles (party puzzles, mathematical gimmicks, most experimental situations) is confined to a single universe of discourse. But it is entirely possible that several coherent universes of discourse, Ui and U2 say, are coexistent even though the analogical relations are distinct. The early gramophone designers, for example, worked in a universe of discourse, U¡ to do with sound boxes and acoustic tubes. Many of them must have been aware of another universe of discourse, U2, to do with the construction of electrical networks, and it is pretty certain that a lot of these people were adept at handling the L analogies of class Z, and of class Z2 pertinent to each .of Ui and U2. However, until the 1920s these particular Ux and U2 were distinct and the operations related by z in Zx were held to be distinct from the operations related by z in Z2. When the problem solving and learning activity is of this calibre, we may in principle give an account of the TOTE operations and L analogies that the problem solver has at his disposal. It is precisely this sort of learning and problem solving that takes place in the restricted conversa tion of 2.5. Indeed, we can state the conditions imposed upon the restricted conversation (to secure the invariance of M* and the possibility of identifying the conversational model by a closed L* analogy) as F = "The set, Z, of L analogies that appear as L metaphors in the A,B 2(&gt; The L analogies can, of course, be used in other ways in problem solving. 438</page><page sequence="45">discourse and the A,B universe of discourse, U, have an L* description as part of M* (usually, M* contains other elements; further, there may be several disjoint entities Zj, Z2, or U¡, U2 )". Now, many very reasonable sorts of conversation fail to satisfy condition F\ in particular, conversations that entail the posing rather that the solving of problems, or that involve insight and innovation. These processes all rely upon the construction of essentially novel L analogies (over and above the use of L analogies as part of the learning we have already mentioned). In order to comprehend these broader sorts of conversation and learning, we must discard the invariance of M* and identify our model by an open L* analogy. To show what I mean by "constructing a novel L analogy," we shall assume that L is stratified, that L- L°,Ll, and that there is a pair of disjoint universes of discourse, Ul and U2. In this case, the corresponding organisational or programmatic model, M*, is an hierarchical cognitive structure based upon L. Its lowest level, L°, contains descriptions of problems, solutions and operations, p. Level Ll consists in a description of Ui and of U2 and a pair of disjoint lists Zx and Z2 of L analogies z. When A and B "construct a novel analogy," I mean that they relate Ui and U2 in one of several ways. Typically, they assert that an operation Pi, applicable to an object in Ui, is analogous to an operation p2, which is applicable to an object in U2 ■ The assertion has the form p i (v)p2 ; but v (the novel analogy) does not appear in the lists Z, or Z2. This process, which could be dubbed "innovation," may be of social importance (when it represents "discovery" of the useful relation between acoustical systems and electrical networks) or it may be mundane and almost trivial (when somebody recognises a useful equivalence between objects or situations). In either case, the production of v involves A and B in describing the relation27 between Ux and U2 ; and, for this purpose, A and B require an internal metalanguage, say L2. In other words, A and B can only innovate if they are able to extend an hierarchy of levels of discourse. But M* is based upon L = L°, L1. Hence we cannot identify the unrestricted conversation with this organisational model. The model must be changed to accommodate the introduction of L2 discourse. Of course, we could have started with M* based upon L = L°, Ll ,L2 instead of L = L°, L1. But the origin is arbitrary. However we started our model, M* is "partially undefined at the outset" insofar as we credit A and B with the propensity to build up levels of discourse (to innovate, or to make novel L analogies) and insofar as they exhibit this propensity. We express this fact 27 Upton and Sampson develop this point and give some excellent exemplars in their discussion of verbal analogy. 439</page><page sequence="46">by replacing M* by a time varying model The time variation of M*(t) comes about as a result of a continual L* reidentification that keeps pace with the innovation in L. The L* analogy involved in the identifica tion is an open analogy in the sense that it contains L* analogical properties of initially undetermined relevance (and it is in exactly this sense that M* is "undefined at the outset"). Most unrestricted conversations take place in an open-ended unstrati fied object language, rather than a stratified object language. In this case a local stratification is imposed by A and B when they innovate. But, because the stratification is local, it is often hard to interpret and it is excruciatingly difficult to keep track of continued innovation. This is especially true when L has the same form as L* or when L is a part of L*, as it is in a great deal of clinical psychological work in experiments with discussion groups, and so on. Here, it is entirely possible to confuse L statements and L* statements, to mix up L analogies and L* analogies, and to tangle with the body of model making dilemmas that are recog nised by clinicians and social psychologists. All of these comments about innovation apply with equal cogency to the microcosm in which the participants A and B are coalesced into a single problem-solving individual. They also apply, more usefully perhaps, to macroscosms in which the cognitive structures of A and B are replaced by the conventional and traditional organisations that characterise a social group. Bateson's "Double Bind" of 1.5(4), Laing's type of resolved impasse mentioned in 1.5(5) and the refined and highly selective proces ses of social invention all come, in consonance with Barnett and others, under the rubric "innovation." Maybe certain discontinuities in the evolu tion of species and the evocation effects of embryology do so also. Insofar as these comments do apply, the L* model is necessarily identified with the real system by an open analogy relation. 2.10 HYBRID MODELS We have considered the structural concommitants of innovation and have commented upon their consequences for the experimenter or model builder. But We have said little or nothing about what innovation is, or why it occurs. Nor can 1 say anything very definite. However, in conclu sion, 1 would like to advance some tentative hypotheses about innovation and creative activity. These hypotheses are applicable to societies and cultures, as well as individuals. But it will be convenient to develop the argument in connection with an individual who exists in a social milieu, such as the unrestricted conversation between A and B. Further, I shall confine the discussion to the production of an undifferentiated innova 440</page><page sequence="47">tion, like the construction of the L analogy v, before it is known to be a useful L analogy. The differentiation of useful innovations occurs in L discourse. It entails metalinguistic levels of discourse for criticism and selection. It is a very elaborate conversational process. But we have indicated it, in outline, already. Recall the distinction between functional models and organisational or programmatic models; the former are abstractions from computing mechanisms, whereas the latter are abstractions from the programmes run in computing mechanisms. My contention is that innovation seems to be such an odd phenomenon just because we try to make the act of innovation a property of one sort of system or the other (of the com puting machine described by a functional model, as we do in saying that innovation occurs because of a chance perturbation of a machine or brain, or of the programme described by an organisation model, as we do in saying that innovation is the outcome produced by running certain types of heuristic programme). Both of these systems are real and perfectly respectable ontological entities. However, it happens (according to my hypothesis) that innovation is a property of both sorts of entity, not of one or the other alone. In abstraction, innovation is thus a property of a hybrid model;28 a hybrid of an organisational and a functional model. Hybrid models are philosophically intriguing because they have, in a certain perfectly definite sense, a "mixed ontology." That is, the onto logical class that is designated by the entities in an organisational model is distinct from the ontological class that is designated by the entities in a functional model, and I conjecture that these ontological classes are disjoint.29 The hybrid model has a "mixed ontology" insofar as it contains entities designating objects in each class. At the moment we know very little about the logic and character of hybrid models. However, they do not appear to be altogether intractable. Because of this, it will be useful to indicate the way in which I believe that innovation and creative activity depend upon the separate domains of a hybrid model. A digital computer is a highly specialised computing medium. It is designed to mirror its programme accurately. It is under the command of its user insofar as it can be turned on or of or given completely unam biguous instructions. Although it has definite capacity limits, these are 28 The embryologists have used hybrid models discursively, as mentioned in 2.8. In my own work, I have used hybrid models to represent some restricted forms of learning. 29 My argument relies upon Harre's idea of the family continuity of a sequence of observations, and I am using the phrase "ontological class" in Harre's sense "the class of events observed in a possible family continuous sequence." 441</page><page sequence="48">unimportant because its storage units can be extended and because when ever it runs up against a limit, it is designed to inform its user of this fact (so that he can extend the storage or take some appropriate remedial action). In contrast, the biological computing mechanisms (whether individual brains or social collations of brains) that act, more or less in McLuhan's sense, as the media in which cognitive structures are processed, have a number of quirks and imperfections. We need only cite a couple of these to make our point, namely: (1) These biological mechanisms are impelled to compute about something. They cannot be turned off. They must process information either gleaned externally (from a suitable field of attention) or internally (from long term memory). The crux of the matter is crystallised in the statement that they have a built in curiosity drive. If they are given no input, they must find an input. (2) These mechanisms are of very restricted capacity so that given a programme, they may be unable to execute it. In this case, they do something which is a mechanically determined abbreviation of, or devia tion from the computation they were instructed to perform. The genesis of this capacity limit is fairly elaborate; briefly, it stems from the fact that a brain acts like a number of capacity limited sequential computing machines embedded in a rudimentary parallel computor that selects amongst them. Once that a particular sequential processor is selected, the brain is momentarily committed to use it, in the sense that certain autononomous or trigger like processes are set in motion. For the present purpose, we need not delve further into why it is that a brain does not simply stop when it comes up against a limit. The programmes we need to consider are cognitive structures specified in L [such as MA (B) and MB 04)]. It is important to recognise that these structures are, by definition, L describable [they may also be described in L*, for example, by an organisational model M* (t)]. Now, according to the present hypothesis, these L programmes are run in biological computing mechanisms which are L* described by a functional model of the sort discussed a moment ago. It is first necessary to comment that in many conditions, these programmes would, if they were run in a digital computor, bring the machine to a halt (because the programme is too lengthy, because it demands too much storage capacity, or, most important of all, because it leads, through pragmatic ambiguity, to an inherently paradoxical situa tion). However, when the programme is run in a biological computing mechanism, no halt occurs; something mechanically determined is done (though this "something" is usually L illegal at the instant it is done, and is not L describable at the instant it is done). 442</page><page sequence="49">We next comment that even if the programme contains no limits or ambiguities, the process of running it in a biological computing mecha nism may lead to events that have no immediate L description. Typically, this occurs because the mechanism must do "something," even though the programme gives it no instruction apart from "stop computing," which it cannot execute. In either case, the impasse is resolved by the production of an "undescribable" event, which I take to be an undifferentiated act of innovation. It could, for example, be the act of constructing the novel analogy relation, v. If we look at the genesis of v from an organisational point of view, it is evident that only some of the analogical properties of v are determined at the instant when v appears (these properties may be L described; the rest cannot be L described). If, on the other hand, we look at the genesis of v from the point of view of a hybrid model, then all of the analogical properties of v are determined, only some of them (those that are L describable) refer to the organisational component of the model, whereas the others, that cannot be L described, are mechanically dependent properties imaged within the functional component of the model. In fact, this hypothesis tallies quite well with the data concerning situations that engender innovation (various sorts of unsolvability, boredom, drive satisfaction, and so on). It also tallies quite well with the fact, stressed by Koestler, that innovations become capable of description in stages, as they are selected and differentiated by socially oriented discourse (or in the miniature system of our A and B conversation). We are led to predict that the great majority of innovations are trivial, and being useless, become discarded. However, my comments would be largely vacuous apart from a basically Cybernetic insight. The insight is that, although organisations or programmes are not at all the same as computing machines, and although these classes of object are ontologically disjoint, they are mutually dependent in the case when the computing machine is a biological or social computing medium or mechanism. The fact is that biological computing mechanisms, unlike digital computors, rely upon programmes for their survival; they rely, at the individual level, upon programmes for satisfying vital needs — the basic goals of the organism. They rely, at the social level, upon programmes for regulating population density and for achieving societal goals. Conversely, the development and the maintenance of an organisation depends upon the existence of imperfect computing machines. Expressed in programmatic form and written immutably or run in a perfect digital computor, it cannot change (nor can it really be preserved without limit). 443</page><page sequence="50">The development of an organisation, its infusion with variety (and, in the long run, its survival), are motions that occur when it is embodied in an imperfect biological or sociological medium. According to my hypothesis, these motions have the calibre of innovation. Bibliography Alexander, C. Notes on the Synthesis of Form. Harvard University Press, 1964. Ashby, W. R.,An Introduction to Cybernetics, Chapman and Hall, 1900. Barnett, H. G. Innovation: the Basis of Cultural Change. New York: McGraw-Hill, 1953. Bateson, G. "Cultural problems posed by a study of the schizophrenic process." In: Symposium on Schizophrenia (Ed.: A. Averbach). New York: Ronald Press, 1959. Bavelas, A. Communication patterns in task-orientated groups. J. Acoust. Soc. Am., pp. 725-730, 1950. Braithwaite, R. B. Theory of Games as a Tool for the Moral Philosopher. Cambridge University Press, 1955. Brodey, W. M. Changing the Family. New York: Potter, 1968. Cherry, C. On Human Communication. New York: Wiley, 1957 Chomsky, N. Syntactic Structures. The Hague: Mouton, 1957. Christie, L. S., Luce, R. D. and Lacy, J. "Communication and learing in task orien tated groups." M.I.T. Research Laboratory: Electronics Technical Report 231, 1952. D'Entreves, A. P. Natural Law. London: Hutchinson, 1951. de Saussure, F. Cours de Linguistique Generate. Paris: Bally, Sachehaye and Reid linger, 1949. Ferster, C. B. Personal communication. George, F. H. (1963). "Pragmatic machines." Proceedings DAGK conference, Karls ruhe, Neuere Ergebnisse der Kybernetik. München: R. Oldenbourg. Gorn, S. (1962). "The treatment of ambiguity and paradox in mechanical languages." Recursive Function Theory 5. Proceedings of a symposium on pure mathematics, American Mathematical Society, pp. 201—218. Gunther, G. (1962). "Cybernetic ontology." In Yovitts, M. C., Jacobi, G. T. and Gold stein, G. D., eds., Self-Organising Systems 1962. Washington: Spartan Press. Harre, R. (1962). Theories and Things. London: Sheed and Ward. Hobbes, T. (1962). Leviathan. London: Fontana Books. Koestler, A. (1964). The Act of Creation. London: Hutchinson. Laing, R. D., Phillipson, S. and Lee, R. (1966). Interpersonal Perception. London: Tavistock Publications. Lettvin, J. Y., Matturana, H. R., McCulloch, W. S. and Pitts, W. (1959). "What the frog's eye tells the frog's brain." I.R.E. Transactions 47, pp. 40—59. Martin, R. M. (1963). Intension and Decision. New York: Prentice-Hall. McCulloch, W. S. (1965). Embodiments of Mind. Cambridge: M.I.T. Press. Miller, G. A., Galanter, E. and Pribram, K. H. (1960). Plans and the Structure of Behaviour. New York: Holt. Minsky, M. "Steps towards an artificial intelligence." Proc. I.R.E. 49, pp. 8—30, 1961. Rapoport, R. Pigs for the Ancestors, Yale University Press, 1969. 444</page><page sequence="51">Rescher, N. The Logic of Commands. London: Routledge &amp; Kegan Paul, 1966. Shannon, C. E. and Weaver, W. E. Mathematical Theory of Communication. University of Illinois Press, 1949. Tinbergen, N. Social Behaviour in Animals. London: Methuen, 1953. Upton, A. and Samson, L. W. Creative Analysis. New York: Dutton, 1963. Von Foerster, H. "On self-organising systems and their environments." In M. C. Yovitts and S. Cameron (Eds.): Self Organising Systems. New York: Pergamon Press, 1961. Von Wright, G. H. Norm and Action. London: Routlege and Kegan Paul, 1963. Weldon, T. D. States and Morals. London: Murray, 1946. Wilkins, L. Social Deviance. London: Tavistock, 1963. Wynne Edwards, W. C. Animal Dispersion. London: Oliver and Boyd, 1963. Zipf, G. K. Human Behaviour and the Principle of Least Effort. Cambridge, Mass.: Addison-Wesly, 1949. 445</page></plain_text>