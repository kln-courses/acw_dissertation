<plain_text><page sequence="1">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY DANIEL H. CALHOUN I We all know that technique becomes a way to emblazon the patterns of substance. Literary criticism has told us so. So should it be with the tech- niques picked up by historians. New quantitative methods may do their part to implement a technocratic society. Quantitative methods rest, though, on logical foundations. Because those foundations develop with their own tempo, and their own relation to the underlying culture, the innovating of particular devices within historical practice can lose touch with fundamental change. Such a change has taken place since 1963, to- ward liberating inquiry from bounds on its scope. The transit of new tech- niques into history has itself been one item in this underlying culture, yet it has reflected foundational developments in only a fragmentary way. Three distinct levels of discussion work here. (1) After 1930 a new ideal took hold in scientific thought, identifying sophistication with probabilistic assumptions and with the unprovability of total systems. (2) Coincidentally there spread the concept of a Cosmopolitan-Local dimension in politics, cutting across a supposedly less sophisticated distinction between conser- vative and liberal. And (3) more specifically, quantitative social scientists developed a set of scaling tLel niques that they often used to identify fac- tions or issue dimensions in politics. Starting in simplistic form in the 1930s, these scaling devices heave necessarily had to cope with the mul- tidiniensional oar compakx arrangements that people make. Sophisticated managers of political scenes can create structures of comprehension that are hardly perceived by local interests or by one-idea politicians. If com- plex, do such structures then require an appropriate logic? All three of these levels should be affected, in some long run, by founda- tional developments in modern logic, and especially by a new current that has Romwed front set theory since 1960. Logicians have aimed more and more to evaluate the detailed results of models. They have luxuriated less the mere demonstration of uncertainty and uunprovability. The types of thought under discussion have all reflected such innovation, but unevenly. Such interaction as has worked among these three levels raises questions</page><page sequence="2">258 DANIEL H. CALHOUN about the timing of scientific development in separate fields. There are questions of convergence: why did many developments come to a focus at once, in the early 1960s, without much obvious communication between fields? But there are also questions of lag: Have historians, holding back in the realm of common-sense technique, missed the extent to which intellec- tual developments have brought new support to eighteenth-century notions of 'common sense'? While a general intellectual history of our time might pick up a broad interplay between ideas of Cosmopolitanism and ideas of comprehension, we can ignore most such connections for the moment, in order to lay out with greater precision the connection between work in scale analysis and developments in logical theory. Recent logic has advanced by examining the fine implications of certain ideas of approximation and inclusion. The models that have developed bear key resemblances to the more advanced scaling models, which a few historians and political scientists have explored. For that reason, we should consider whether the broader impli- cations of the new logic should reflect back into emergent thought in the social sciences. It is easy enough to find examples of interaction between thought about the Cosmopolitan-Local "continuum" and thought about scaling proce- dures. Even when historians have tried to measure Cosmopolitanism with crude-seeming instruments, this work has also managed to parallel the impulses at work within logical innovation. Consider Main's elaborate study, Political Parties before the Constitution. Writing about the 1780s, he described a gradation of social types, from Cosmopolitan to Local, among the men who made up state legislatures. He considered separate measures such as wealth, occupation, religion, even the extent of a man's social contacts (which Main called "world view"). The overlap of these measures produced a natural gradation. In his conclusion, he also presented tables that implied a spectrum of political positions, which Main had scaled in some way. But in the long body of his work, instead of scaling votes, Main adopted a simple trichotomy into Localists, Cosmopolitans, rand Neutrals. For each state legislature of his period, he classified men as party Cosmopolitans if they voted with a Cosmopolitan pattern two-thirds of the time, as Localists if they voted against it two-thirds of the time.' On the surface, this tactic might appear gross. Any talk of many-valued or continuLous-valued logics would stand in elegant contrast to Main's crude categories, Yet, as will appear, one relevant sophistication has found ways to condense many- valued scales back into simple truth-choices. Of course, if Main's trichotomy were applied in intelligence testing, it 1. Jackson Turner Main, Political Parties beibre the Constitution (Chapel Hill, 1973).</page><page sequence="3">CONT[NUAL ISION AND COSMOPOLITAN ORTHODOXY 2) 59 wouid classify people as Geniuses, Dolts, and Ordinaries. Not even the most bigoted standardizers have gone so far. From the first they have graded test items by difficulty, and have built cumulative scales, under one name ;r another. Guttman began to give one pattern to th1is scaling at the end of the 1930s, and fed it into the extensive attitude-testing that was carried out by Stouffer's "American Soldier" project. If attitude toward the Army could be scaled, then so inighit civilian social and political at- titudes. From there the application to legislatures in past time was an inevitable. step. There, applications by historians (and by political scientists using histor- ical matter) have varied from the mechanical through the sophisticated to the critical. Some historian s have used GuIttman scales simply as a way to extract party identifications from an array of votes. Others have used it as attended, to elicit- degrees of allegiance to some position. But the method itself, ev'en though cook-bookish, seeks constructs at a higher order of complexity than "party,' "bloc," or "faction." If a scale exists within a body of votes, it entails some cumulative arrangement from legislators who least favor to those who most favor whatever tendency is being scaled. Bills I. I 1 1 1. 1 1 1 1 0 l 1 1 I I I I o 0 0 1 1 1 1 1 1 Legislators 0 0 0 1i 1 I I 1 1 O 0 0 0 1 1 1 1 1 0 .9 0 0 Q 0 0 0 1 Tests for sinmlarity, such as factor analysis, work poorly here, because they would describe Ithe extreme measures of the scale as opposite mem- bers of one factor, while putting the moderate measures into a separate sceae. The Guttman scale, instead of putting final emphasis on bloc oppo- sitiLn, concentrates on relations of scope and inclusion. Since it collects all Cuvmu111andve 'rmearures i ito a single scale, it is concerned molre with the scalabilitty of a vot-e, than with its positive or negative weight, -more with questions of intelleCtuall comprehension than with questions of who wins or Loses. For genuine two-party legislatures, with disciplined voting, a Guttman scale will produce a two-step dic-hotomy. Some practitioners have seemed to aQssume that this is whtat ?calfig was supposed to do. In effect, Main is in that camp, though he made no pretense at cumulative apparatus. Others have souIht to identify genuine di tensions or gradations. Working on the Confederate congresses, for which no clear party system operated, and on which political historians had developed n-o orthodox interpretation, Be-</page><page sequence="4">260 DANIEL H. CALHOUN ringer and Alexander demonstrated that certain issue dimensions did in fact emerge as the Confederacy approached its final crisis.2 The technique identifies the articulation of issue-consciousness out of an inchoate back- ground. It would thereby also be suited to find an issue being polarized into party dichotomy. That process could conceivably have worked during the 1780s, and it would have involved important questions about how leaders or people came to define a party-like situation. But Main's indifference to the particular techniques of cumulative scaling precluded such insight. Another sophistication in legislative scaling has asked whether a single dimension would account for the behaviors of a legislature. Aydelotte pro- duced a dramatic negative for the Parliament of the 1840s: liberalism on the usual political or religious questions was not the same thing as liberalism on matters like factory regulation. Members' votes on one dimension could not be predicted from their votes on the other dimension - not at any first-order level of consistency. A similar discrimination applied to the variously sectional and economic scales that Silbey, or Alexander, found in American Congresses of the same generation.3 (In their case, they simply laid out the disparate scales that they had found, but without giving readers Aydelotte's neat clinching demonstration that the scales could not be made to fit together. This failure to specify one's negative test is a common lack of scientific elegance in legislative applications of the scaling technique. One form is Main's failure to demonstrate what the next smaller scales were for his legislatures - and therefore, presumably, how unimportant those neglected scales were.) When investigators have failed to allow for gradations on a single scale, or for possible multiple scales, they might vellf argue that the necessary techniques would run to complex Cut-andtry, pushing legislators and mea- sures into various arrays to see if they would fit. Main might well have felt that such effort, for several sessions each of thirteen different legislatures, would have been prohibitive, The difficulty with this supposedly practical objection is that Guttman and others have long since gone ahead to work out programmable al- gorithms that can take a batch of legislative votes and determine at one pass how many independent scales are needed to account for the votes, who belongs to which scale, and, which bills belong to which scales. These are not cheap programs to run, and they have pro 3blems but they do exist, 2. Thomas B. Alexander and Richard E. Beringer, The Anatomy of the Confederate Con- gress. A Study of the Influences of A'fember Characteristics 01n Legislativ'e 1Voting Beoo 'tor, 1861-1865 (Nashville, Tenn., 1972). 3. W. 0. Aydelotte, "Voting Patterns in the British House of Commons in the 1840s," Comparative Studies in Society and History 5 (19633), 134-163. Joel H. Silbey, The Shrine of Party: Congressional Voting Behavior 1841-1852 (Pittsburgh, 1967). Thomas B. Alexander, Sectional Stress and Party Strength: A Study of Roll-Call Votintlr Patterns in the Jaited States House of Representatives, 1836-1860 (Nashville, Tenn., 1.967).</page><page sequence="5">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 261 and they dan produce a summary statement for a whole legislature on one or two pages.4 In fact, a simpler summary, giving the basic content and relative numerical importances of each scale, could be given for any legisla- ture in half a page. That is, it would have been possible to lay out in fifty pages or so a conspective, exact statement of all the subtleties that Main swept under the rug. This would have cleanly released Main's text for descriptive and narrative presentation - much of which he did do, and well. Where Main may have rejected excessive technique for his own practical reasons, others have argued that the epistemology of mere technique is itself an important modern ideology. Take the argument that C. Wright Mills presented against one of the leaders in scaling theory. Two different people had worked on scaling within Stouffer's "'Anmerican Soldier" proj- ect: Guttman himself, and Lazarsfeld. While Guttman's work at that time had still been directed toward a concrete, one-dimensional analysis of dichotomous items, Lazarsfeld had worked mostly on the theory of ques- tionnaire analysis. Before the War, Lazarsfeld had figured in the migration of European intellectuals to the United States. In Austria about 1930, he had already begun to work on differences in personal scope as a sociologi- cal dimension. After the War, when Merton and Merton's students began to talk explicitly about Cosmopolitanism, Lazarsfeld saw this as simply a continuation of his earlier interests. Quite easily he used examples of Cosmnopolitan-Local scales in some of the essays he now wrote about "latent structure analysis." He was refining scale techniques in directions that interested him.5 This juncture of the Cosmopolitanism topic with elaborated teclnique defined much of what Mills denounced as "abstracted empiricism." In Mills's view, such elaboration marked people who consid- ered themselves sophisticated enough to work within the system. An im- perfect identity, or strain, connected Cosmopolitanism taken as a technical attributLf of some investigators, with Cosmopolitanism taken as an attribute that investigators diagnosed in the outside world. This strain was thenr looped in an additional knot by the political complications of the McCarthy e'ra. Stouffer argued in Comirinunism, Cojnjormitv and Civil LibJerties that 4. Geoietric Representations of fRelational Data: Readings inl Multidiimelnsional Scaling, ed. James C. Lingoes (Ann Arbor, Mich., 1977). Duncan MacRae, Jr.. Isslses an,2d Parties in Legislativne Voting: Methods of Statistical Analysis (New York, 1970). 5. Paul F. Lazarsfeld. "The Sociology of Empirical Social Research" (l962), in his Qiial- itatiye Analysis: Historical and Critical Essays (Boston, 1972), 361-369. Lazarsfeld, 'An Episode in the H4istory of Social Research: A Memoir, "I'erspectit'es in Atnerican HistorY 2I (1968), 270-337. Robert K. Merton, 'Patterns of Influence: A Study of Interpersonal Influence and of Communications Behavior in a Local Community" in Comnnifnicationis Research, 1948-1949, ed. Paul F. Lazarsfeld and Frank N. Stanton (New York. 1949), 180-222. Lazarsfeld, 'The Algebra of Dichotomous Systems" in Studies in Itein A naly 'sis and Prediction, ed. Herbert Solomon (Stanford, 1961), 111-157. Paul F. Lazarsfeld and Neil W. Henry, Latetif Structure Analysi~s (New Yolwrkl 1968).</page><page sequence="6">262 DANIEL H. CALHOUN only cosniopolitans could reliably protect someone like Mills against the forces of reaction.6 In this climate of opinion, Main began his own career. His first article, on Virginia politics in the 1780s, presented most of the political struggles in that state as a class matter. In appended notes and captions, though, Main shifted abruptly to the argument that proto-Federalists were those who had extensive ties to the outside world. McDonald, writing about the same time, offered a similar theme in his work on the groups that backed the federal Constitution.7 Main's sympathies lay with a more populistic or progressive view of the period than McDonald's; thus, when he came later to use a simple dichotomous technique for analyzing legislative votes, that method was still voting for Mills versus Lazarsfeld. This ideological climate of the 1950s and 1960s has some importance for longer-range theoretical development, because it created among academics a presumption that technical elaboration implied sophistication, which in turn implied some accommodation with establishment interests. While those implications may have been valid in a short run, the actual outcome of theoretical development was to reverse any such epistemological trend. Technique itself, pushed far enough, would transcend its own preciosity. But the assumption of the 1950s, that thinkers faced a necessary opposition between resolution and pluralism, made it difficult for the academic world to assimilate and popularize a really new approach. Now, any teacher who has presented the Guttman scale to a methods class knows that the clever student may ask awkward questions. The fact that these questions can be disposed of does not make them unimportant. A scale is supposed to identify the degree that a legislator possesses of some quality. But suppose that the managers of a legislature arrange affairs in such a way that the "true" votes for one measure are reduced, in order to secure votes for some other measure. Suppose that the traded measures do not "trulv" even belong to the same scale. The Fesults tay badly confuse whatever it is that scalability is supposed to represent. Note, too, that the criticism re-introduces questions of power at a higher order of analysis. It treats not just factions (that is, first-order sets), but the whole question of deals or sets of factions anrd of how they are arranged. The objection can always be disposed of, for finite examples of scaling, simplyi by the introduction of additional scales, dimensions, or epicycies. This can never, of course, deal with the question of whether the higher-order co; figuration provides a better description of politics. 6. C. Wright Mills, The Sociological Imagination (New York, 1959). Sarm el A. Stouffer, Commuinism, Conf trmitv, and Civil Liberties: A Cross-Section of the NAation Speak's Its Mind (New York, 1955). 7. Jackson T. Main, "Sections and Politics in Virginia, 1781-1787," William aind Mary QuarterAly 12 (1955), 96-112. Forrest McDonald, Wc thle People. The Economic Origins of they Constitution (Chicago, 1958).</page><page sequence="7">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 263 {I The student query about Guttman scales is, in fact, an informal statement of one of the major problems in twentieth-century logic: Can higher orders of nested complexity be represented within -models that are accessible to human intuition? This question has taken several forms in mathematical theory. Often, popularized versions of mathematical results have been used to justify positions that intellectuals have taken in general discussion. Cer- tain kinds of results cannot be proved or computed by finite methods. They are therefore inaccessible to ordinary human logic. The decisive cultural type becomes therefore the intellectual who has at least access to the facts of inaccessibility. But what happens when the very meaning of accessibility changes? During the 1960s, Apter and Ladd argued that technological obsoles- cence marked all the Yahoos of society who were the real opposite to the new cosmopolitans. For them, as they built rationale for development pol- icies, ideology was little more than the thought patterns to which obsolete minds resorted.8 Zbigniew Brzezinski picked up this notion when he was arguing down the romantic radicals of 1968. In Between Two Ages, he talked up global politics and a "global city." Once, the "static" logic of a merely industrial society had promoted "ideological" thinking. Now, the media have come to promote disparate views of reality, not compressible into formal systems, even as the re- quirements of science and the new computative techniques place a premium on mathematical logic and systematic reasoning. . . . the increasing ability to reduce social conflicts to quantifiable and measurable dimensions reinforces the trend to- ward a more pragmatic approach to social problems, while it simultaneously stimu- lates new concerns with preserving "humane" values.9 In the tissue of allusion, three points stand out: the reference to formal systems; the confidence in measurement as a process related to science, computation, and logic; and a belief that the strain of intuition against logic provides a basis for administrative pragmatism. One pivotal term here looks so abstract that we might miss the fact that it is a metaphor, an epistemological tag lifted whole from discussions about logic. "Formal systems" are the kind of bounded axiomatic structure that could never uphold an empirical science. After all, Kurt Godel had seemed to demon- strate that such a system must leave unprovable some plain truths in the 8. Ideology and Discontent, ed. David E. Apter (New York, 1964), Introduction. Apter, The Politics of Modernization [1965] (Chicago, 1967). Everett Carll Ladd, Jr., Ideology in America: Change and Response in a City, a Suburb, and a Small Towvn [1969] (New York, 1972). 9. Zbigniew Brzezinski, Between Two Ages: America's Role in the Technetronic Era (New York, 1970), 12.</page><page sequence="8">264 DANIEL H. CALHOUN very mathematical language for which it is designed. If logic fails for logic, how could it do for the real world? Writers who oppose each other on explicit ideology may even use the same mathematical metaphors to support the status of reasonable intellec- tuals. Alvin Gouldner, who had earlier done much to spread Merton's concept of a Cosmopolitan-Local dimension, was now working in a mode less journalistic than Brzezinski, and certainly more radical, but he still picked up the same popularized version of G6del: "that formal systems are unavoidably lacking in self-sufficiency and must rest on assumptions out- side their own stipulations." For him, this spelled the failure of enlighten- ment rationality, and fixed a pragmatic, common-sensical bound to free speech: "Unless there is some limit on what may be said, when, and to whom, there is no possible predictability in human discourse, and no pos- sible way of maintaining a consistent and logical line of discourse." Gould- ner's long career as a spokesman for "representative bureaucracy" merged here with Apter's or Brzezinski's dislike for participatory ideologues.'0 In this analogizing from metamathematics to politics, an orthodoxy had come to prevail within academic metaphor. Such a faddish structure of allusion, especially when it seems to entail an unchallengeable sophistica- tion, must be held up to its own standards of judgment. Did Gddel's proof imply what Brzezinski and Gouldner came to think? Did it put a static limitation on how human scope and competence should be defined? Would Brzezinski's and Guttman's confidence in quantifiable dimensions fall itself afoul of the standard, intelligent student criticism? Another crucial term in the Brzezinski formulation is the reference to "social conflict" as quantifi- able. But conflict creates problems for scalability, and these problems con- tradict the easy assumptions about logic that both Brzezinski and Gouldner built into the structures of their argument. The idea of the insufficiency of formal systems belongs to only one stream of development in modern argument. The idea of the scalability of conflict belongs to a divergent stream, technically compatible with the first, but leading toward quite different conceptions of human scope. The first stream would deny that complex notions are accessible to ordinary intui- tion. The second determines that, given proper scientific research, all complex ideas can be modeled in a form accessible to visual intuition and its logical analogues. In the development of logical principles, the idea of the inaccessibility of proof has been, not contradicted by, but transcended by the idea of the accessible character of experimental results. To clarify the relation between these two streams, we need to examine 10. Alvin W. Gouldner, The Dialectic of Ideology and Technology: The Origins, Gram- mar, and Future of Ideology (New York, 1976), 43, 143-144, 268 n.; Patterns of Industrial Bureaucracy (New York, 1954); "Cosmopolitans and Locals: Toward an Analysis of Latent Social Roles,' Administrative Science Quarterly 2 (1957-58), 281-306, 444-480.</page><page sequence="9">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 265 the way that proof themes and visualization themes have interacted since the- end of the nineteenth century. I The inciting figure in one was David Hilbert; in the other, Georg Cantor. Hilbert laid out in 1900 his program of twenty-three unanswered ques- tions in mathematics. One of the broadest was his Number Two: how to demonstrate from the axioms for elementary number theory that the theory itself is consistent. To solve the problem fairly, the proof would have to be "finitistic,' using only limited numbers of axioms or deduction steps.12 From the outset, this goal was presumptuous. In a world of power, where Nietzschean intuitions seeped through Wilhelmine orthodoxy, Hilbert's proj- ect expressed a piece-wise, clerkish confidence that ignored power, a hope that comprehensive systems could be stated by a mere cumulation of ele- ments. In contrast, he listed as his Number One question the Continuum Hypothesis already stated by Cantor, one of the undoubted mystic origi- nals of modern thought. Cantor came to the continuum problem in 1884, as part of a drive to vindicate infinite quantities in mathematics. Though he raged as much as worldly mathematicians against any resort to infinitesimals, he believed that infinitely large collections are objects in the mind of God. When he described successive types of infinity, each incommensurable to those smaller, some working academics treated him as mad. As if to oblige them, he fought back with flights of apprehension that took him finally to the asylum. He was vindicated in time (not in eternity) by twentieth-century set theory. A notion of visualization inhered in the continuum problem, though not because Cantor pushed for images. When he considered sets to be actual collections of objects, he meant mostly abstract mathematical objects. He condemned metaphors that grounded infinities in space or time, rather than in arithmetic.'3 But beyond any question about whether infinite sets actu- ally exist, there was a serious question about the logical complexity of the entities that mathematicians may use, and therefore about the complexity of the spaces within which human beings may then try to visualize any 11. The technical interaction between proof techniques and continuum is noted, with some apprehension about the ambiguity of the situation, in Alan M. Turing, "Systems of Logic Based on Ordinals" (1939) in The Undecidable, ed. Martin Davis (Hewlett, N.Y., 1965), 207-208. 12. J. Fang, Hilbert, "Towards a Philosophy of Modern Mathematics," II (Hauppauge, N.Y., 1970), ch. 6. 13. Joseph Warren Dauben, Georg Cantor: His Mathematics and Philosophy of the Infinite (Cambridge, Mass., 1979), 108-110, 133-149. The relation between visualization and continua is discussed with considerable grace by Marvin Minsky, "A Framework for Repre- senting Knowledge" in The Psychology of Computer Vision, ed. Patrick Henry Winston (New York, 1975), 211-277. Minsky does not treat the correspondence between his "frames" and points in high-order number systems. This correspondence is the practical matter in the Continuum Hypotheses.</page><page sequence="10">266 DANIEL H. CALHOUN reality. Ahnong the classes of infinity, a first consists of the natural numbers that can be used to count things in sequenCe. Now, any distinct property of things can be defined simply by the set of things that has that property. If all things are indexed by the natural numbers, then the number of distinct properties of things corresponds just as simply to the set of all distinct sets out of the natural numbers. Because there is no counting procedure that will line up the natural numbers one by one against these property sets, we have here two levels of infinity - of which the first is designated N; in Cantor's notation. But properties can also be collected, into properties of properties. Since the sum of a thing's properties defines it as an object, the space of undifferentiated monads is 8X, the property space may be N,, the object space may bel8, the space of collections of differentiated objects may be 83, and so on. 14 Can the space of all possible coalitions of natural objects be visualized? That is, can even one- or two-dimensional phenomena in that space be visualized? This becomes a tricky question precisely because both 8S and81 do have credible visualizations. Where 8, corresponds to the natural num- bers, 8, corresponds in thickness of texture to the real numbers - the natural numbers plus all the infinite decimal fractions. Since the natural numbers correspond to discrete points on a line, and the real numbers to "all" points on the same line, these two levels of complexity share the same capacity for being visualized along the "continuum." But is the second level of complexity identical writh the continuum? Cantor conjectured that it is. This is his Continuum Hypothesis (hereafter, CH). By it, the universe of real motion is exactly one step up any ladder of complexity from the universe of movie frames that only simulate motion: there can be no distinct kind of complexity between these two levels. On the surface, this seems an easy idea to live with. Note, though, that it has one intellectual implication that is either pessimistic or toughly realistic, depending on one's mood. If the continuum is what we know human beings can visualize, do all higher levels of complexity lie beyond it, in a thicket of logical thorn-bushes that the eye can never penetrate? If all that is involved here were the external, physical world, we might scoff at the problem. For a long time, people who applied mathematics to concrete physical problems were inclined to accept the identification be- tween space and the real number system, and equally inclined to ignore any higher order of cardinality. But it had been known for some time that logic(ll ideas could be given models in spatial ideas. Without rigor, this feeling had worked centuries earlier in the classical and renaissance ideas of an art of memory: a technique for indexing ideas by places in a familiar 14. The shift from 'is" to "may be" in this sentence finesses both the Continuum Hypoth- esis itself and the strategies that are used for proving the independence of such hypotheses from standard axiom systems.</page><page sequence="11">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 267 space's Boolean algebra made the notion formal by identifying truth with the whole of acceptable space - with the 'universe of discourse." If the elements of that universe were no more numerous than 8t, then all true statements could be visualized. But elements can include data, which can include not only property sets but also object sets and coalition sets. If CH were actually false, then the continuum would have to be equivalent to 82 or something even more numerous; those higher orders of complexity would then fall within the spaces or contexts that are open to ordinary human intuition. Through the 1920s, both Cantor's and Hilbert's conjectures were still open, unresolved. Each problem served as a grand metaphor for the ques- tion of the accessibility of ideas to people. One conjecture denied that high ideas could be found by just going to the window and looking at them as they flowed by. The other asserted that high ideas could be reached by the stacking-up of finite building blocks. These two related problems were to be resolved a full generation apart. The resolution of one, but without the other, was to define the fundamental character of the intellectual paradigm that reigned as orthodoxy from 1931 to 1963. By the end of the 1920s, many scholars were homing in on themes that bore indirectly on Hilbert's challenge. In physics, Heisenberg and others were formulating the indeterminacy principle. In philosophy, Arthur Lovejoy was arguing that the adequacy of rational arguments could be decided by indexing them to historical examples, even though the value of the ideas at issue was in no way bound to the examples. Specifically, questions about the variety of entities in the universe could be indexed to evolving views on the "Great Chain of Being." It was not difficult to conclude that earlier centuries had been wrong when they used formal principles to project "all" creatures (such as paupers, or mermaids, or angels) into logical slots in the organic hierarchy. Lovejoy's final inference from this exercise he put in language appropriate to the early 1930s: The world of concrete existence, then, is no impartial transcript of the realm of essence; and it is no translation of pure logic into temporal terms - such terms being themselves, indeed, the negation of pure logic. It has the character and the range of content and of diversity which it happens to have. No rational ground predetermined from all eternity of what sort it should be or how much of the world of possibility should be included in it. It is, in short, a contingent world; its mag- nitude, its pattern, its habits, which we call laws, have something arbitrary and idiosyncratic about them. But if this were not the case, it would be a world without a character, without power of preference or choice among the infinity of possibles. 15. Frances A. Yates, The Art of Menoty (Chicago, 1966).</page><page sequence="12">2 68 DANIEL H. CALHOUN If we may employ the traditional anthropomorphic language of the theologians, we may say that in it Will is prior to Intellect.16 Gbdel published his own solution in 1931, of course in pure logic. Work- ing with formal axiom systems for arithmetic, he showed first how every formula in a language for arithmetic can be indexed to a unique whole number. This incidentally confirmed what was well-known anyway: that any such theory has some model whose elements are no more than count- ably infinite. The theory itself can therefore be visualized - though this leaves open the question whether complex data structures can be visual- ized. More to the point for 1931, Gddel showed that there exist true state- ments that can be defined within the language of a system, yet can be neither proved nor disproved from its axioms. This applies especially to statements about the theory itself. If new axioms are added to take care of the unprovable statements, there then arise unprovable statements that involve those new axioms.17 Once Gbdel gave this demonstration, other mathematicians such as von Neumann and Turing joined him with applications to different areas of theory - or even to practice. The "Turing machine" was both an ideal description of computing procedures, analogous to Gddel numberings, and a vehicle for demonstrating that some functions are not computable. Later study on working computers and their limitations was thus an extension from Gddel's paradigmatic formulation. The transit of Turing's ideas from innovative concept to technological practice followed roughly the same chronology as the assimilation of Keynes's monetary ideas from theory into treasury practice. Much the same applied to Lovejoy's idea of an autonomous history of ideas, themselves unconditioned by the historical environment in which they worked. This conception fed eventually into the curricular practice by which undergraduates were taught how to "un- derstand poetry."I18 For practical thought about the real world, though, the more important abstract problem was that of the continuum. Theories of logic can be stated in models that have only countably many elements. Each such model can in principle be visualized. It may even serve to describe higher-order en- 16. Arthur 0. Lovejoy, The Great Chain of Being (Cambridge, Mass., 1936), 332. 17. G. T. Kneebone, Mathematical Logic and the Foundations of Mathematics (London, 1963), ch. 8. Alonzo Church, review of R. B. Braithwaite, Introduction to Kurt Gddel, On Forinall' Undecidable Propositions of Principia Mathematica and Related Systems, transl. B. Beltzer (Edinburgh, 1962), in Jourtnal of Symbolic Logic 30 (1965), 358-359. Ernest Nagel and James R. Newman, Gbdel's Proof (New York, 1958). 18. A similar timing is described also for the stages of information theory in Wendell R. Garner, Uncertainty and Structure as Psychological Concepts (New York, 1962), 8-16. And compare the immediate origins of the sociological distinction between Cosmopolitan and Local in the 1932 work reported in Carne C. Zimmerman, The Changing Community (New York, 1938).</page><page sequence="13">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 269 tities. But 4the visualization of real-world entities depends on the nature of complex data structures, on the exact character of counting, and therefore on whether data can be counted out within the continuum. During the 1930s, Gbdel then addressed himself to CH. He stated a first result in 1938, much in the vein of his work on proof theory. Building on techniques that von Neumann had already outlined, he showed how to "construct" a universe of sets from the bottom up, accumulating at each ordinal level all those sets that could be built from the preceding level by certain fundamental operations. This would be an orderly but full universe. Points at the level of t; would already equal the complexity or cardinality of the continuum. If a suitable axiom could be posited - that all sets are constructible in the way Gbdel defined - then CH held. It certainly could not be disproved, unless the constructible universe were to be scotched at the same time. '9 Such were the innovators in the new outlook: Gddel, Heisenberg, Einstein, von Neumann, Keynes, Turing, Lovejoy. They took up what had been supposed to be reductionist techniques, but demonstrated the invalidity of reductionist ideas, therefore the need to support idea systems by assump- tions or policies brought in from outside. They also (pace Heisenberg) became the ultimate advisers to Atlantic governments that were trying to muster their own energies against totalitarian assumptions. Though Heisenberg stayed in Germany, the social meaning of the new outlook was indexed by the shift of Gbdel, Einstein, and von Neumann to the Anglo- American world. In all these developments, one crucial intellectual distinction was that between the local and global variables that operate in a system. The terms themselves have become conventional in designating the levels of variable used in a computer program. In larger analogy, though, they apply to the relation between the Hilbert conjecture and its Gbdel fate. Hilbert had conjectured that entities "local" to a system - its axioms - could be used to evaluate the whole system in truth-space. Gddel showed that this eval- uation required variables external or "global" to the system. Now Hil- bert's confidence had persisted into the 1920s, when middling people were becoming anxious about their ability to comprehend and stabilize systems. Faith in local building blocks moved toward a compulsive romanticizing of hearth and cradle. In politics, those who would not accept global systems were becoming the cells of a cancerous "local" system. If Hilbert's original program had been a conceptual "populism," some 19. Kurt Godel, The Consistency of the Axiom of Choice and of the Generalized Continuum-Hypothesis with the Axioms of Set Theory (Princeton, N.J., 1940). But note the dictum on the constructible universe in John L. Bell and Mosh6 Machover, A Course in Mathematical Logic (Amsterdam, 1977), 517: "far too neat and tidy - like a police state."</page><page sequence="14">270 DANIEL H. CALHOUN key figures in the West resisted the intellectual anti-populism that was grow- ing up around Indeterminacy and Gddel's Proof. One strain within German romantic socialism had accepted communalism, but never the racist hearth-and-cradle line.20 Einstein, in his American refuge, kept up a so- cialism of sorts. More important to theory, neither Einstein nor Gbdel ever accepted the cult roles into which they were being put by academic sophis- tication. Einstein's discomfort with a dice-playing God is familiar enough. So are his dogged later efforts to develop a Unified Field Theory that would transcend the fragmentary character of relativity and particle theory. Less familiar are Gddel's second thoughts about finitism, in 1958, or his occasional collaborations with Einstein on Unified Field Theory. These collaborations, finally, are of a piece with their shared intuitions about the importance of visualization in physical theory. For Einstein, to accept visualization was to distrust Indeterminacy and the wave-particle an- tinomies. But for Gbdel, to accept visualization was to distrust the finality of his own work on C H.21 That distrust, he stated. In 1947 he published the only real popularization that he ever wrote, a short article on the continuum problem for a journal aimed at teachers. There he concluded with his statement of faith: that someone might find a new axiom, convincing enough to join a basic list for set theory, and strong enough to disprove CH. Teachers should let the next generation of mathematicians realize that the problem still existed.22 Early in 1963, word spread that someone at Stanford had answered the call. Paul J. Cohen was brought to give a command performance, lecturing at the Institute in Princeton. Gbdel himself communicated the result for publication. In it, Cohen offered, not a new axiom, but a new method of proof. With this method, called "forcing," he proceeded to build new models for the universe of sets - models, not axioms. One model was Gddel's constructible universe, in which CH would hold. In another model, CH did not hold. The Hypothesis, neither provable nor disprov- able, was "independent" of the axioms.23 In one sense, Cohen's result 20. Eugene Lunn, Prophet of Commuwlnity: The Romzantic Socialism of Gustav Landauer (Berkeley and Los Angeles, 1973). 21. Kurt Godel, "'Uber eine bisher noch nicht benutzte Erweiterung des finiten Standpunk- tes," Dialectica 12 (1958), 280-287. Dag Prawitz, "Ideas and Results in Proof Theory," Proceedings of the Second Scandinavian Logic Symposium, ed. J. E. Fenstad (Amsterdam, 1971), 235-307. Gaisi Takeuti, 'Consistency Proofs and Ordinals," Proof Theo1y Symposium, ed. J. Ditter and G. H. Muller (Berlin, 1975), 365-369. Gerald Holton, "On Trying to Under- stand Scientific Genius," American Scholar 41 (1971-72), 95-110. 22. Kurt Godel, "What is Cantor's Continuum Problem?' in American Mathematical Monthly 54 (1947), 515-525. 23. Cohen, "The Independence of the Continuum Hypothesis," National Academy of Sciences, Proceedings 50 (1963), 1143-48; 51 (1964), 105-110. Cohen, "Independence Results in Set Theory" in The Theory of Models: Proceedings of the 1963 International Symposium at Berkeley, ed. J. W. Addison, Leon Henkin, and Alfred Tarski (Amsterdam, 1965), 39-54. Cohen, Set Theory and the Continuum Hypothesis (Reading, Mass., 1966).</page><page sequence="15">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 271 confirmed the whole ideology of unprovability. But it did this by demon- strating a model in which the pessimism about vision did not hold. Such an outcome satisfied both the realistic determinations of the early, public Gddel and the idealistic hopes of the continuing Godel. The problem now is not so much to note the fact of the outcome, as to explain several things about it: why a first step, took the simplistic form of Guttman scaling, why the final breakthrough came only after the 1950s, what the actual structure of the breakthrough meant in general intellectual terms, and why its implications failed to be given the same cult treatment that Gddel's Proof had received after 1931. III The question of why the trend began early in social and psychological scaling is part of a much larger question in technological history. Historians of culture and education have often lamented the application of machine ideas to human process: the factory model in the early nineteenth century, the efficiency model in the twentieth. This lament is a comfortable reification that falsifies the direction of events. Sweat-shops, monitorial schools, and Methodist circuits preceded factories in most communities. Goods were produced for the market by people in the first instance, not by machines, and the obvious way to increase production was to impose a quasi-monastic or Puritan discipline on half-idle people. The strain of doing this may have been one incentive for imposing a monastic discipline on replaceable parts. Similarly, in the twentieth century, mobilization for wars on poverty or totalitarianism was in the first place a mobilization of people - of their abilities and social patterns. This meant classifying individuals by aptitude or intelligence levels, and it might also mean classifying social groups. From the very earliest psychological tests, these efforts raised questions of ordinal complexity. Often, though, busy test-designers chose to think about ability as something scaled by degrees of difficulty, in a simple linear man- ner. Because older children generally did all the things that younger chil- dren did, and then some, this always entailed cumulative scaling of the kind that Guttman later clarified. Much of this test construction ignored the fact that successive levels of difficulty might entail successive levels of com- plexity. The original form of the Guttman scale was then simply a way to put individual responses into some usable series. It was an item in social mobilization. In principle it was vulnerable to the same ideological manipu- lation as intelligence tests, where one-dimensional tests served a racist outlook, and multi-dimensional served a personnel-manager's outlook.24 24. Raymond B. Cattell, Abilities: Their Striteture, Growth, and Action (Boston, 1971). Lloyd G. Humphreys, "The Organization of Human Abilities," Americanz Psychologist 17</page><page sequence="16">272 DANIEL H. CALHOUN But its most sophisticated multi-dimensional forms developed among those investigators, like the later Guttman himself, who worked in a contempla- tive rather than pragmatic vein. Even while Guttman's techniques became more intricate, his results appeared more and more as immediate visual images. In contrast to any simple arguments -about dimensionality, there devel- oped two attitudes of proud sophistication about the social uses of formal theory. Each of these attitudes belonged to a distinct social universe. One was typified by Lazarsfeld's criticism of Guttman scaling: test re- sponses should be treated as probabilistic, not determinate. Writers on learning theory or utility tried to make their fields over in some image of the new physics. Social mobilization was less a matter of pushing people into slots than of tapping any random energies that might become available. Since categories were approximate, the formal membership-relations of set theory seemed irrelevant. The mere classification of individuals and groups was extraneous to the management of social processes. Social scientists who thought in this way did not always remember that mathematicians were willing to classify quite abstract "objects" into sets. Neither did they yet perceive where a more experimental kind of set membership would lead theory. If probabilistic ideas marked Anglo-American thought, a different form of sophistication appeared in France in an apparent coincidence between pure mathematics and social philosophy. The mathematics in question was "sheaf theory." It began with characterizations of the "local properties" of a space, then proposed ways to assemble those descriptions into a bun- dle that would characterize "global properties" of the space.25 In the same vein, Sartre opened his philosophical path with a description of individual action in concrete situations. Asking then how such action could be as- sembled into wholes, he developed an explicit Theorie des ensembles pratiques. The adjective in "practical sets" was his warning that he would (1962), 475-483. J. P. Guilford and Ralph Hoepfner, The Analysis of Intelligence (New York, 1971). 25. The characterization of sheaf theory here is taken from Richard G. Swan, The Theoty of Sheaves [1958 Oxford lectures] (Chicago, 1964). Basic papers are to be found in H. Cartan, Se'ninaire (Ecole Normale Superieure, 1949/50, 1950/51). See also the bibliography in Glen E. Bredon, Sheaf Theory (New York, 1967); the historical introduction to P. T. Johnstone, Topos Theory (London, 1977); and the survey discussion in Saunders Maclane,. "Sets, Topoi, and Internal Logic in Categories," Logic Colloquium '73: Proceedings of the Logic Col- loquium, Bristol, July 1973, ed. H. E. Rose and J. C. Shepherdson (Amsterdam, 1975), 119-134. The eventual bearing on CH is spelled out in Myles Tierney, "Sheaf Theory and the Continuum Hypothesis" in Toposes, Algebraic Geometric and Logic, ed. F. W. Lawvere (Berlin, 1972), 13-42. Note also Lawvere's determination to interpret category theory as a form of "objective dialectics." Lawvere's ideological projections are considerably more in- teresting than the orthodox variety in Wolfram Heitsch, Mathemnatik und Weltanschauung (Berlin, 1976).</page><page sequence="17">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 273 avoid rationalistic analogies. Formal set membership could entail only a "partial totality," in contrast to the "total totality" of historical conscious- ness. Having stated this precaution, he then built his own continuing dis- course on precisely the question of how individual actions could be joined into wholes. If Lazarsfeld challenged determinate set notions from the viewpoint of a probabilistic orthodoxy, Sartre challenged determinacy from the standpoint of an isolated activism that would mobilize itself into total relevances.26 In the meantime, up through the 1950s, Anglo-American power exerted real effects on the kinds of mobilization that seemed urgent, and therefore real constraint on the kinds of data that required serious attention. Where Sartre's concern for mobilizing individuals into social action had some analogue in the concerns of the then-obscure New Left, it had equal analogue in the task of getting civilians to accept dangerous service in armies. But it was precisely this social mobilization, at once complex and risky, that people thought they could ignore as long as the nuclear monopoly protected them. The end of that monopoly raised the possibility that responsible military planning would have to allow for infantry wars and tactical complexity. It also raised the possibility that responsible moral thought might project individuals into complex forms of resistance to the military. Concerns about mobilization, about the analysis of rival cultures and of rival productive systems, began to develop a diffuse ideological urgency. In the short run, the concrete military demands of the late 1950s directed attention toward certain kinds of methodological problems, and these in turn seemed to call for a re-examination of logical foundations. Note the specific questions: How could missiles be designed to target Soviet cities by means of accuracy rather than size? How could overflight photographs be unscrambled to yield exact information? How could machines be pro- grammed to translate Russian-language texts? And note the slightly less specific questions: can linear tests be used to place an individual in a slot requiring a complex mix of intellectual styles? Can objective scaling identify the political role of an historical individual? Can a computer language treat complex patterns as kinds of data, and identify some correct patterns? In general: can simple, separate objects be made to achieve complex unified ends? This set of questions operated, though, on two distinct levels. One was the problem of achieving continuum-level solutions within finite scales: in the most literal case, the geographical targeting problem. The other was the problem of achieving higher-order comprehension within continuum scales: in the classical case, the problem of whether human beings perceive struc- 26. Sartre, Critique de la raison dialectique (Paris, 1960), 132-134, 172.</page><page sequence="18">274 DANIEL H. CALHOUN tares as wholes. Some problems could be stated in first-order terms; others required higher-order or contextual statement. During the 1950s, first-order or targeting problems were being solved, but higher-order were not. Quite early, missile-design solutions were indicated in von Neumann's 1952 lec- tures on the synthesis of reliable automata from unreliable components.27 But the language-translation problem was one in higher-order targeting, and it proved to be impossible, in any terms beyond the crudest. Computational mobilization moved in a coherent trajectory of its own, from Turing in 1936 to von Neumann in 1952. Cultural 'mobilization" moved in a different manner, if at all. Scott, recalling the situation among logicians in this period, feels that they suffered from a "first-order disease" that kept them from assembling various fragments of technique into a solution of the CH problem. And his only real explanation is that "history" did not extrude a solution at that time.28 Precisely then, because an internal account of events within logic fails to suggest why connections got made only after 1960, it is reasonable to look outside the technical sphere of mathematical logic, into the global cultural context. At least in Anglo-American society, the very urgency of simplistic targeting problems directed energy away from the human prob- lems of comprehension and vision. Only when simple strategic solutions were being exposed as inadequate did the scene open for a more abstract, therefore more human approach to theoretical problems. By 1960, part of the problem seemed to exist as a conflict between behaviorists, such as Skinner or Suppes, and gestaltists, such as Chomsky. 27. J. von Neumann, "Probabilistic Logics and the Synthesis of Reliable Organisms from Unreliable Components" in Automata Studies, ed. C. E. Shannon and J. McCarthy (Prince- ton, N.J., 1956), 43-98. This whole volume is a brilliant statement of the maximum that can be achieved with probabilistic handling of n0-sets (or at most si). Note K. de Leeuw, E. F. Moore, C. E. Shannon, and N. Shapiro, 'Computability by Probabilistic Machines," 183- 212. Cohen, in contrast, was to apply probabilistic or forcing considerations to N2-structures (or even higher-order). 28. Dana Scott, "Foreword" in J. L. Bell, Booleon-Valued Models and Independence Proofs in Set Theoty (Oxford, 1977). For a tantalizing indicator of the whole juncture, see Summaries of Talks Presented at the Summer Institute for Symbolic Logic, Cornell Universi- ty, 1957, Sponsored by the American Mathematical Society under a Grant from the National Science Foundation, 2nd ed. (Princeton: Communications Research Division, Institute for Defense Analysis, 1960). Many papers in this collection strain unconsciously toward the Cohen result of 1963. Especially. Dana Scott, in "Completeness Proofs for the Intuitionistic Sentential Calculus," moves close to Cohen's concept for forcing. Alonzo Church's "Applica- tion of Recursive Arithmetic to the Problem of Circuit Synthesis," the one paper substantially revised for the IDA edition, discusses "potentially infinite" automata in a way that might easily be generalized to Cohen's non-CH model as an automaton. Note that the pragmatic sponsorship of the meeting could not have encouraged participants to purge themselves of the "first-order disease." The gathering was large. Cohen was one of the few (potential) logicians who did not give a paper.</page><page sequence="19">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 275 Could finite automata, such as computers, serve as a valid model for human learning? If languages depend on contexts that are complex to some higher order, how can people learn them in real time? Chomsky hypothe- sizes that learning was logically impossible unless maps of that context existed already in the genetic structure of the brain29 But out of the behaviorist tradition there came also the psychometrician Shepard, concerned to show how mere ordered preferences could be built up into definite multi-dimensional pictures. This effort connected up easily with the work of Guttman and Lingoes. Kruskal gave it a machine solu- tion: start with a whole visual map of how various ordered scales might fit together, then refine the map through successive steps of approximation until it achieved a sufficient fit. This was still a finite procedure, but it stated a general schema for dealing with complex structures in those finite terms. 30 The "solution" to the cultural problem was to consist of demonstrations that choice was inherent in the situation. One could either not solve the problems or solve them, depending on how one wanted to build the uni- verse. The first demonstration, that it was possible to build an unvisualiz- able universe, had come from Gbdel in 1938. A very different approach to building universes was taken by Lazarsfeld and Guttman, in their psychometric tinkering: by Shepard and Kruskal; and by Cohen, in the decisive 1963 result. Note again, and this time in more formal terms, just what the Guttman scale does. It takes an array of traits for a population, and arranges them in degree of rarity. The scale is successful if each successive trait includes also all the related less rare traits. This produces a "chain" or "linearly ordered set." There are two ways by which this arrangement guarantees a reduction in complexity. First, even with an unbounded number of traits, the number of distinct, positive property sets can never be greater than the number of individuals in the experimental population. But second, the arrangement permits a decision method for handling the transpositions and exceptions that occur in the real world. For any actual data, the best ordering is that which produces the fewest possible trans- 29. Patrick Suppes and Richard C. Atkinson, Markov Learning Models for Multiperson Interactions (Stanford, 1960). Noam Chomsky, review of B. F. Skinner, Verbal Behavior, in Languages 35 (1959), 26-58. Chomsky, Reflections on Language (New York, 1975), 195-198. 30. Roger N. Shepard, "The Analysis of Proximities: Multidimensional Scaling with an Unknown Distance Function," Psichometrika 27 (1962), 125-140, 219-246. Joseph Kruskal, 'Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis," Psychoometrika 29 (1964), 1-27. Multidimensional Scaling: Theory and Applications in the Behavioral Sciences, ed. R. N. Shepard, A. K. Romney, and S. Nerlove (New York, 1972). Shepard, "Representation of Structure in Similarity Data: Problems and Prospects," Psychometrika 39 (1974), 373-421.</page><page sequence="20">276 DANIEL H. CALHOUN positions. Of course, exceptions might well suggest additional scales, or new dimensions, or more complex structures. Those are a bother. The working investigator, as Lazarsfeld suggested in 1950, simply " 'forces' the higher order and especially the ultimate frequencies into the 'nearest' latent class structure.' "3 1 When Cohen took up CH, he used both of these explanatory tactics, though in far more sophisticated versions. He was working with the rela- tion of finite scales to the infinite sets out of which they came, or toward which they might point. Now, it so happens that Cohen's demonstration, especially in the Boolean version provided by Solovay and Scott, can be given concrete and possibly not too vulgar form as a refinement of the criticisms that Lazarsfeld and bright students have made of the "simple" Guttman scale.32 Since the steps in this refinement become successively more complex, let us consider them one by one. The object here is to identify general strategies, not to state or even summarize the proofs. (1) First, we will accept Guttman's own idea that the population being scaled is thought of as being drawn from an infinite population.33 Since we are now talking about individuals, we will be considering only a countable infinite pool of potential "voters." Partly because of this shift from finite to countable (but also to fit some of the technical requirements of the Cohen model), we will redefine our scale so that we are talking only about positive votes. A finite arrangement out of such a countable pool might then look like: 31. Lazarsfeld, in Measurement and Prediction, ed. Samuel A. Stouffer (Princeton, N.J., 1950), 467. Compare the suggestions on reduction and substruction in Lazarsfeld, "The Algebra of Dichotomous Systems" in Studies in Item Analysis and Prediction (Stanford, 1961), 142-150. Lazarsfeld's casual notion of forcing prescribed a way to treat as unimportant some experimental complications already observed. Cohen's philosophical concept provides a way to define truth in terms of the already observed items of an open experimental sequence, even though future terms might conceivably vary. 32. The exposition here depends heavily on the elementary discussion in J. L. Bell, Boolean-Valued Models and Independence Proofs in Set Theory. An admirably literate but less compact survey of several approachs is Yu. 1. Main's A Course in Mathematical Logic, transl. Neal Koblitz (New York, 1977). See also J. Barkley Rosser, Simplified Independence Proofs. Boolean Valued Models of Set Theor' (New York, 1969), and Gaisi Takeuti and Wilson M. Zaring, Axiomatic Set Theory (New York, 1973). All of these deal with the relation between Cohen's original forcing approach and the more explicitly "probabilistic" Boolean- valued models. The Bell is especially important because, published under the general editor- ship of Dana Scott, it contains implicit and some explicit notational hints to the interpretation of non-CH Models as "machinery" for the handling of higher-order data structures. 33. Guttman makes this suggestion most clearly in his "A New Approach to Factor Anal- ysis: The Radex," in Mathematical Thinking in the Social Sciences, ed. Lazarsfeld (Glencoe, Ill., 1954), 296 - where he notes also a multiple regression argument that hints strongly but of course unconsciously toward Cohen's notion of generic set. Guttman is quite explicit, though, on comparing the selection of finite from infinite sets to the use of finite neurons to model "infinite" intellectual results.</page><page sequence="21">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 277 Bills 1 1 1 1 1 1 1 Legislators 1 1 (2) Next, the Cohen approach adopts a conception of truth grounded in the data-handling strategies of our generation. Truth is what we accumulate knowledge about, whether the "we" is a computer designer providing struc- tures by which data may be ordered in memory, or a legislative manager working intuitively to determine the issues into which he may categorize his members' possible behavior - or a logician trying to determine the conditions under which the formal universe may be shaped one way rather than another. Ideally, every new batch of voting data that we collect should be consis- tent with every previous batch: in Guttman's term, the votes should "scale." At least, we should be able to order statements about voting by "how true" they are, or how probable. A legislative manager may be trying to assemble a program that mobilizes members who respond reliably to "the" tax-reduction issue; but his specific bills may not tap perfectly the underlying issue-content of tax-reduction-ness, and the specific legislators in his stable may not respond perfectly to that issue as a political feed-bag. In either direction, he may find only probabilities of reaching the interest that he is trying to build. Whether we are making statements about concrete factions or about abstract sets, all statements may be indexed with labels according to where they fall in ordered sequences of fuller discovery. We can accept prob- abilistic, "fuzzy" statements about empirical sets along the way. In the end, we hope to reach definitive statements about essentials. (3) Still, we recognize that we can gather only finite batches of specific information - votes cast, or data to be stored in memory, or examples of logical relationship. All our knowledge is experimental and "local." Yet each map is a condition that limits the interests we can define, or the truths we can discover. Each is what Cohen called a "forcing condition."34 (4) We can connect the set of finite maps into a whole, infinite uni- verse of ordered statements. This whole universe, a way of calculating on relations of inclusion and sequence, will be a Boolean algebra.35 In it, 34. For the interpretation of the non-CH models as a description of a kind of experimental process, see A. Gregorzyk, "A Philosophically Plausible Formal Interpretation of In- tuitionistic Logic," Indagationes Mathematicae 26 (1964), 596-601; and M. C. Fitting, In- tuitionistic Logic, Model Theory and Forcing (Amsterdam, 1969), 21. 35. Compare another approach to using finite sets of events to construct probability inea.-</page><page sequence="22">278 DANIEL H. CALHOUN logically necessary statements will have a value or probability of 1, while false statements have a value of 0. The connection of finite maps to the complete algebra depends on a couple of distinct features: - In the first place, the entities that are to enter the system are conceived of as "regular open sets." In analogy to one painting style, they are pointilliste areas that can show us all about an object without ever presenting hard outlines. In political function, they are collections of erasable legislative choice, not popula- tions of legislator images with cartoon boundaries. In the whole model,- this requirement makes certain key probabilities go to 0, not remain intermediate or fuzzy. - Each finite map is transformed into such an area if it is identified with the whole set of random variables that can take values for any individual on any property, though pinned down in this case at those values that happen to be specified in that one finite map. Up to this point, what is being described is like a data structure for a computing system. It is more realistic, in the sense that it allows for prob- abilistic statements, for phenomena that do not have defined boundaries, and for a world that may be quite random once we open our gaze beyond experimental findings. (5) We must specify what we can include in each finite vote array. If that map extended no further than what would be the property space if CH held, then nothing would violate CH in this model either. Only if we allow for more do we get more. We have been talking about an unbounded pool of legislators or voters, but about only one such pool, acting out its choices within one political situation. Now consider the setting for a complex ethical calculus. Let us acknowledge that the same group of people can be considered in relation to many different situations, or aspects. Further, we know that bright people consciously (and anybody subconsciously) can act with the realization that a particular deed is accomplishing things in two situations (two moral spheres, or two political contexts) at once. Each such composite realization may constitute an ethical balancing act, carried out in a situational manifold that is simply a set of aspects joined at a higher order of complexity. The problematic objects in these manifolds will be more than countable in possible number; indeed, they can be stacked to any desired level of complexity. Let us consider that we are selecting our finite experimental data sets, not just out of the countable population pool, but out of the whole array of ways of looking at the population pool, taking the pool in each of very many possible aspects, up to some order of complexity that in simple models would violate CH. We are now doing something that is quite sures on infinite collections: Duncan Luce, "Utility Theory," in the reported UNESCO semi nars of 1960 and 1962, Mathematics and Social Sciences, ed. Saul Sternberg (Paris, 1965).</page><page sequence="23">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 279 realistic. We are talking about finite experimental cases (and we know that experimental science happens in the real world), selected out of a complex set of structures (and we know that the world is really like that).36 This is the structure that would develop if we built a cumulative vote scale in which the entities "voting" on various bills were whole inter- factional deals as such. Individual legislators might still be entered in the scale, but in the sense that any legislator is simply the one-person case of a political deal. If the Guttman scale could be described as the experimental cumulating of an interest, then this structure is the experimental cumulating of afile of realistic strategies. We are talking, not just about finite scales, but about how to embed finite scales in a random or experimental universe. Turned the other way around, we are talking about how to anchor the various universes of random possi- bility, each into a particular pattern of experimental findings. Look on this variety of processes as things that concern us. We are talking about the embeddings of finite scales in random possibility, or about the anchorings of the random to the empirical. And here is the result: no matter how complex we make our array of situational manifolds, the number of these embeddings or anchorings will count out to be no greater than the number of points in the continuum -a that is, no greater than the number of distinct sets of elements in our random population pool. The embeddings will be just as complex as the situational manifolds we chose to consider, but they will map out within what we can visualize. IV Like all good mathematical profundities, this can also be stated as a truism. If we construct theories about a probabilistic universe, we will still be seeking our empirical evidence in finite, consistent sample results. Our theories may deal with extremely complex situations. In models where CH holds, these situations would be far more numerous than the sets we find by our sampling process. But if we are dealing with a probabilistic universe, and if we are taking our samples out of a countably infinite pool, 36. Compare the pressure toward treating a whole experiment as a single pattermned sample or trial, in Clyde H. Coombs, Robyn M. Dawes. and Amos Tversky, Mathematical Psychol- ogy: An Elenmentary Introduction (EngLewood Cliffs, N.J., 1970), 288. But the technical in- sight that non-CH models can be built with sets of any order of complexity seems to date from Robert Solovay, -2xo Can Be Anything It Ought To Be" in Theory of Models, ed. Addison, Henkin, and Tarski (Amsterdam, 1965), 435. For the suggestive, only implicit analogy to complex structures as data types, see Dana Scott, " Data Types as Lattices" in Logic Con- ference Kiel 1974, ed. G. H. Muller et al. (Berlin, 1975), 579-651; and Joseph E. Stoy, Denotational Semantics: The Scott-Strachey Approach to Programming Language Theory (Cambridge, Mass., 1977). Scott's foreword to the Stoy volume should be compared with his introduction to Belles Boolean-Valued Models.</page><page sequence="24">280 DANIEL H. CALHOUN there is, exactly a zero probability that we will ever find the same two empirical sets confirming different complex theories. Merely finite samples, out of an infinite pool, can "never" come up twice the same way. If we take our universe to be probabilistic, all the theorems of set theory will still be absolutely true, with a probability of one. But the counting process will behave in a probabilistic, common-sensical way. G6del's Proof of 1931 had amounted in its own way to a truism: that we cannot reach absolute truth about truth by a bootstrap process. It should not be surprising that the non-CH models have a plain truth at the core of their complexity. The plainness does not keep it from being important. It does not state a fact about the physical universe, but a fact about what happens if' we want to look at the universe in a certain way. It says that, if we accept the probabilistic world that seemed formidable in the new physics, then we shall end up in a situation where all complex theories about the world can be distinguished along some visualizable scale. Pur- sued to its logical lair, the probabilistic universe does not cancel intuition; it gives license to intuition. Note, though, that the pursuit does not mean pursuit by illogical means, intuitive in a woolly sense. Significantly, people like Scott have been deeply involved in working out theories of data and functions used in computing. Where the early applications of Godel's Proof had pointed out that certain functions are not computable, computer theorists have since put most of their energies into maximizing what can be computed. Compare the discussion of computer semantics by Stoy, who draws heavily on the work of Scott. Computer languages need to evaluate func- tions that may have unending decimal fractions as their "answers." Stoy concludes: "So the whole thing relies on being successfully able to refine approximations by refining other approximations - for that is in general the only way in which we can compute with infinitary objects inside a finite machine (or, indeed, in a finite universe)."37 Now, the process of accumulat- ing forcing conditions toward a complete set is just the process of ac- cumulating successive refinements toward the information available about a question. It is an algorithm. A simple Guttman scale is a selected al- gorithm, designed to pin down a political tendency. The central feature of Lazarsfeld's argument, or of the Boolean interpretations of Cohen, is to treat the successive approximations as achieved within a probabilistic uni- verse. Stoy's language can then be used to re-word CH: Cantor's conjecture had said that higher-order infinitary objects (such as tricky deals and scientific 37. Stoy, Denzotatiotial Semnuntics, 50. Compare William W. Rozeboorn, "Scaling Theory and the Nature of Measurement,' Sy'nthese 16 (1966), 232, which offers an early, unintended suggestion toward interpreting the "Cohen real" as a reliable but approximate discrimination between more remote transfinite variables.</page><page sequence="25">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 281 dualisms) cannot be "computed" inside continuous machines (such as human, vision). If the scope of human vision can be taken as a reasonable metaphor for the continuum, then the non-CH model simply states the conditions under which vision can indeed "compute" on a phenomenologi- cal universe. If people argue clearly, and do their research homework within a probabilistic model, they will get visualizable results. The stipulation may put heavy demand on scientific dedication, but the outcome is reassuring. A probabilistic universe is not one in which phe- nomena become so obscure that outside authority must impose its own perceptions. It is precisely a universe in which vision is possible. In it, vision will 'always" be able to distinguish different real sets of things to correspond to different possibilities in higher-order theory. Why did that conditional optimism fail to win quick acceptance beyond the ranks of logicians? One problem lay in the fact that the initial publiciz- ing of the result compared it to the discovery that Euclid's parallel postu- late need not hold. But parallelism has something to do with the physical world, whereas probability may be only an attribute of how we think about the world. It was misleading to imply that the non-ClI model might have such direct physical meaning.38 But there was more to it than that. And here I must put the matter in terms of what I sense was the intellectual climate of the 1960s. What I say is thus only an invitation to others to contribute their own viewpoints. The or- thodoxy of the new physics and the new macroeconomics had come to be identified with a severe downgrading of finitary processes. Intuition was felt to be an ideological last resort used by little, obsolescent people who could not keep up with a probabilistic world. People who tried to build up little ideas into big, 'a la Hilbert, would only end up confined in totalitarian solutions. This was certainly the attitude that academic sophisticates took toward student machine-breakers during the 1960s. The dichotomy be- tween Cosmopolitan and Finitary had become a vested intellectual interest which persisted during a period of some social crisis, even though a quite different way of conceptualizing the problem had already been set down on paper. And of course, on many levels both tinvial and profound, the intui- tive judgments being offered then were not always attached to clear chains of logic. Hardly anyone was ready to talk about rint-uition as an outcome 38. G. Kreisel, "Two Notes on the Foundation of Set-Theory," Dilecdtica 23 (1969), 109-110. For other but quite varying skeptical comments see: Abraham Robinson, "For- malism 64' in Logic, Methodology aind Philosophy of Science. Proceedings o f the 1964 International Congress, ed. Yehoshua Bar-Hillel (Amsterdam, 1965), 228-246; Robinson, "From a Formalist's Point of View," Di'lectica 23 ('1969), 45-49; Cohen, "Comments on the Foundations of Set Theory'' in Proceedings of Symposia in Pure Afthematiccs, vol. 13, part I (1971), 9-15; Joel I. Friedman, 'The Generalized Continuum Hypothesis Is Equivalent to the Generalized Maximization Principle," Journal of Symbolic Logi.c 36 (1971), 39-54.</page><page sequence="26">282 DANIEL H. CALHOUN grounded in logical procedure, or about perception as a process grounded in logic. Some investigators, confronting the difficulty of working with higher- order configurations, retreated into defining visualization as a lower-order process, and even as one that should be kept as low-order as possible. This retreat showed plainly in Shepard's handling of the dimensionality prob- lem. During tile 1960s, multidimensional scaling showed itself capable of reproducing such natural structures as the color wheel, out of data on how people compare color chips. But Shepard now argued that an analysis based on purely local comparisons between relatively similar colors would generate a one-dimensional solution, corresponding to the "real" scale of the wave-lengths of light.39 This argument moved in a direction quite op- posite to the way sheaf theory would use local properties to imply the global properties of a space. It was of a piece with Brzezinski or Gouldner re-applying, compulsively, the old anti-i-nitist position. Given this dilemma between new and old styles of visualization, we might ask what the new visualization would recommend to historians using quantitative techniq tes. Scaling techniques, for example, produce modest finite segments out of some complex data universe appropriate to a forcing model. For specific investigations, such may be enough. But the existence of the more elaborate models, and the guarantee they offer to vision, make it reasonable for workers on complex phenomena to extend their own techniques out over larger segments of a data universe. Further, if the convergence between practical technique and high theory is a continuing process, which investigators discover naturally in the tissue of their intel- lectual lives, then there should already be techniques developing that em- body one aspect or other of the non-CH model. In some ways, this has begun to happen, On one methodological edge of the social sciences, Guttman and Lingoes have produced many ways to describe multidimensional order strictures, thus embracing wider and nmore representative segments out of a complete set of forcing conditions. From computer technology, the whole field of pattern recognition has grown out to be a scientific discipline in itself, strugglingng to deal with higher-order objects.40 Within the practice of legislative scaling, several investigators have worked toward one aspect or another of the sophistication embodied in the 3 9. Shepard, Representation of Structure." Psx'chomnetrika 39 (1974). 385-391. K. Fukunaga and D. R. Olsen, 'An Algorithm for Finding Intrinsic Dimensionality of Data.-a IEEE Transactions on Computters C-20 (1971), 176-183. 40. For two diffusWe but suggestive comments, see: A. Lerner. "A 'Crisis' in the Theory of Pattern Recognitirn' in Frontiers of Pattern RecoCnition, ed. Satosi Watanabe (New York, 1972). 367-372, especially the final section "On a World Where Intuition Is Possible"; and G. Hirsch, ''Math6matisation et realit6," Dialectica 29 (1975), 12, 20-23.</page><page sequence="27">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 283 non-CH models. For some, such as Aydelotte working on the Parliament of the 1840s, or Beringer and Alexander working on the Confederate Con- gress, this has meant insisting on the multidimensionality of a situation. Aydelotte's Parliament, or English society back of it, had to be conceived as the kind of structure that could precipitate out his separate scales. In Beringer's Confederacy, war was a crucible or algorithm that refinednd the information" available about individuals and therefore revealed the poten- tial for scalable behavior. An even clearer approach to the scaling of higher-order structure is provided by Veisberg, using the Compromise of 1850 as a case to argue his ' proximity scale" against the conventional Guttman scale. A mere cumulative scale would not account easily for the way that pro-Southern, moderate, and pro-Northern groups aligned on any particular compromise measure. The measure might constitute a central point from which voter patterns fell away in either ideological direction. Legislators voted for a measure according to how closely it approximated their own ideal posi- tions, not according to how intensely or probably they felt the appeal of some ideological extreme.41 It would be possible, of course, to break Weisberg's scales dowxn into separate cumulative scales. But this means that the proximity scale is a particular kind of higher-order structure. defining certain sets of scales by a unifying criterion. If one should find a formal pattern describing the relation between Aydelotte's scales for Par- liament, this pattern would be a higher-order structure in the same sense as Weisberg's proximity scale. Either would meet the common student objec- tion to the cumulative character of Guttman scales. Either would then pose the substantive question: what kind of historical situation would account for the appearance of particular structures? Closer to the probabilistic or Boolean-valued models, Duncan MacRae has adapted scale analysis to the crude "cotton-gin" technology offered by matrix algebra. The original GJuttman approach had required investigators to pick tediously over data, trying out various combinations of bills and legislators, then various ways to dispose of error, in order to determine what woulet prodtice the best sense of scalability. DespitLe criteria like the coefficient of reproducibility," these procedures continued to be much a matter of search and pick. Matrix algebra has long, of course, pro vided systematic t-cechiniques fc&gt;r separating any array of items into its implied patterns of similarity or set-rr.embership. MacRae conjectures that a whole multidimensional Guttman scale Could be approximated at ornce if the ma- trix analysis bUegan wid-i a measure of the rnutual sca/ability of any two bills, rather than a measure of their sinilarity. He obtained from this a 41. Herbert F. Weisbeig, "Scaling Models foer Legislative Roll-Call Analysis," American Political Science Reiiew 66 (1972), 1306-15.</page><page sequence="28">284 DANIEL H. CALHOUN result that looked like a conventional component analysis, except that the coefficient for any one bill on a particular component was a kind of Boo- lean value for how far that bill belonged to that scale. A value of .70 for a NATO-related bill, on a foreign-affairs component in the 1955 Congress, meant that the legislators suggested had that "probability" of responding to an underlying foreign-affairs content when they came to that bill. Thus, where the rawe Guttman approach would search for a group of legislators, all of whose votes would scale on a particular set of bills, MacRae's quasi-Boolean approach would identify certain fluctuating themes of rele- vance that ran through individual behaviors. It was this conception of the social world as probabilistic that Lazarsfeld had stated in his comment on the original Guttrnan scale42 For any new application, one key question is: how close does an inves- tigator come to formulating scales that embrace higher-order relations as part of his theory?43 Main, for example, divides his treatment of any legislature into one part where he uses zero-order analyses, and another part where he ventures into higher-order: into his formal non-scaling of votes, and his quite, informal scaling of social characteristics. Scales can be imagined on his social phenomena, which would extend to higher-order sets - that is, social relations as such, or properties on property sets. The non-CH model does suggest that higher-order relations might be handled in the same sequences as lower-order sets. That this may happen in theory does not mean that anyone has devised a clear, non-cumbersome way to juggle the necessary arrays. For most people, the "visual" intuition of such relations will take a form that does not on the surface seem visual. It will mean conceiving of society as many complex objects that can be placed in ordered relations of power and prestige. Where a scale reaches to higher-order relations, such as technological styles or social classes, each item on the scale may be too compie. for a mhchanical Guttman array, yet the concept of ordering may just as genuinely apply. When we say that a social system can be visualized, we do not mean that it can be laid out in economistic array. Rather, we mean that sequences can be posited among its higher-order obj-ects and that these sequences will povie- distinct 42. MacRae, Issves and Parties in Legislative Voiting cbh. 57, 7. Lazarsfet'ld, MAdathematical Thinking in the Social Sciences., 12-14. 43. Compare Laus Witz and John Earls, ".A Representation of Systems of Concepts by Relational Structures' in Mlathemnatical Models of Social and Cognitive Structtiies: t on- trihbutions to the Mlathematical Develeopmnent f Anithropology, ed. Paull A. 'allonoffl (Urbana, Ill., 1974), 104-120. Systems for defining such structures as computer data types have been advanced in languages such as A~lgol68, PL/1, and Snobo14. For one among many discussions on the implermentartion of highEr-order structures, see Paul E. Weston and Stephen M. Taylor, ''Cylinders: A Relational Data Structure' in Data Structures in? rogratnining Languages, ed. Julius T. Tou and Peter Wegner (Gain-esville, Fia., 1971), 398-416.</page><page sequence="29">CONTINUAL VISION AND COSMOPOLITAN ORTHODOXY 285 points in conceptual space for whatever we need to distinguish in order to decide between complex theories. The non-CH model offers a permissive statement about the development of further techniques. A scale, or a forcing condition, is like a wooden ship launched to make its way through virtually random combinations of wind and wave. It may be anything from a simple plank to an elaborate square- rigger. The array of possible examples of some coherent style corresponds to a complete set of forcing conditions. Up to whatever level of complexity the designers are taking into account, these designs will entail a particular conception or model of what the natural world is like. Given the com- plexities that are thus taken into account, this conception of the world is a kind of non-CH model, within which two things are true. First, all the standard mathematical rules by which elements are combined in the whole world apply also in this model. Second, though the hydrodynamic theories assumed in this model may seem as if too complex to visualize, the furthest thing over which we assert control can be visualized: namely, the set of strain points between the design style and the environmental model that it implies. Designing a testable theory, and especially one for real historical condi- tions, is like designing and launching a ship. We can never have any guarantee that the world-model we assume is complex enough to make the theory safe in the real world. But we are assured that the strain points between the design and its world will always be intuitively distinctive, no matter how complex we choose to make that design. Our measurements may safely raise the complexity of what we choose to scale, because the problem space in the entailed model can never get beyond what we can perceive. This applies to much of historical practice that eschews quantity and scale. Any style of discourse that discusses complex phenomena, and that presumes to explain "what came next," is outlining an ordered struc- ture up to som be evel of complexity. If the elements that make up that style are finite, we can extend the scope of that style to complex levels without any fear that the problems entailed will escape the field of vision. This license applies both to structural scaling and to dialectical narration. While only great art Imay succeed in describing problems, the problems them- selves cannot come too thick for us to articulate. The non-CH results offer us a strictly formal opportunity to transcend the intellectual pessimism of the 1931-1963 years. If we accept a prob- abilistic conception of the universe and work with it, then it becomes possible to discriminate any two complex theories with distinct sets of evidence drawn from a visualizable universe. A full acceptance of the os-ison between concrete at-d probabilistic leads tovard confidence in in- tuition, riot away from it. The Gddel result of 1931, like the Cohen result of 1963, was a statement</page><page sequence="30">286 DANIEL H. CALHOUN about the nmanner in which people can take points of view toward the universe. The mannier of'1931 constrained us toward a sense that the com- plexities of a random world make that world forever impossible to grasp. The manner of 1963 opens for us a sense that an acceptance of randomness makes comprehension reasonable. Finally, the optimism of the 1963 manner contradicts the pessimism that is supposed to have overtaken the intellectual world since the end of the 1960s. Every cliche about the 1970s seems to say that public temper has ignored the open tendencies of the 1963 result. Only among logicians them- selves, or among computer theorists who work at the level of high logic, do we find the kind of productive industry that would seem appropriate to the new manner. Here, the theorists may be the cocks that we keep to crow at sunset before an earthquake. They are done crowing just now. They are busy scratching. UniNersity of Califjorlia Davis</page></plain_text>