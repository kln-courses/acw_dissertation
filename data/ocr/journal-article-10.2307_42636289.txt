<plain_text><page sequence="1">J Log Lang Inf (20 1 3) 22: 1 1 5-1 37 DOI 1 0.1007/s 10849-01 2-9 168-y Reasoning About Truth in First-Order Logic Claes Strannegárd • Fredrik Engström • Abdul Rahim Nizamani • Lance Rips Published online: 13 February 2013 © The Author(s) 2013. This article is published with open access at Springerlink.com Abstract First, we describe a psychological experiment in which the participants were asked to determine whether sentences of first-order logic were true or false in finite graphs. Second, we define two proof systems for reasoning about truth and falsity in first-order logic. These proof systems feature explicit models of cognitive resources such as declarative memory, procedural memory, working memory, and sensory memory. Third, we describe a computer program that is used to find the smallest proofs in the aforementioned proof systems when capacity limits are put on the cognitive resources. Finally, we investigate the correlation between a number of mathematical complexity measures defined on graphs and sentences and some psychological complexity measures that were recorded in the experiment. Keywords First-order logic • Proof system • Bounded cognitive resources • Truth C. Strannegárd (S) • F. Engström Department of Philosophy, Linguistics and Theory of Science, University of Gothenburg, Gothenburg, Sweden e-mail: claes.strannegard@gu.se C. Strannegárd Department of Applied Information Technology, Chalmers University of Technology, Gothenburg, Sweden A. R. Nizamani Department of Applied Information Technology, University of Gothenburg, Gothenburg, Sweden L. Rips Department of Psychology, Northwestern University, Evanston, IL, USA Springer</page><page sequence="2">116 C. Strannegârd et al. 1 Introduction What truths are humans able to identify in the case of first-order logic (FO) on finite models? This is a fundamental question of logic, linguistics, and psychology, which nevertheless remains largely unexplored. In this paper, we study this question sys- tematically using psychological experiments and computational models. We combine elements of proof theory and cognitive psychology and extend earlier work on propo- sitional logic (Strannegârd et al. 2010). 1 . 1 Psychological Complexity Human reasoning concerning FO truth lends itself perfectly to exploration using stan- dard methods of experimental psychology. This holds also for other model-theoretic logics (Ebbinghaus 1985). Fix a vocabulary r and let MOD be the set of finite r-structures for which the domains are subsets of some fixed denumerable set. Also, let SEN be the set of FO r-sentences and define Truth = {(A#, A) e MOD x SEN|Af (= A}. A psychological experiment pertaining to Truth and a given participant can be set up by specifying the experiment's procedure and a finite set of test items TEST C MOD x SEN. An example of such a test item is given in Fig. 1. An experiment of this type yields response times and correctness data for the items of TEST. It also yields a set POS ç TEST consisting of those elements of TEST that the participant classified as members of Truth. The sets POS and Truth generate a division of TEST into four subsets: true positives, false positives, true negatives, and false negatives. These experimental data can then be approximated using computational models, for which the predictive power can be evaluated using statistical measures such as the (Pearson) correlation. 1 .2 Mathematical Complexity What factors influence the difficulty for a human to determine whether M (= A for a finite model M and FO sentence A? Are they the properties of the sentence, the model, or some combination of such properties? The properties of the sentences are clearly important. For instance, the truth-value of 3 xE(x, x ) is generally easier to determine than the truth-value of Vx3yE(x, y). Which properties of sentences are important here? Is it length, quantifier depth, parse tree depth, number of negations, type of connectives, a combination of these properties, or something else? It is also clear that the properties of the models matter. For instance, the difficulty of determining the truth- value of Vx3yE(x, y) clearly depends on the properties of the model. Which properties of models are important in this connection? Is it cardinality, number of edges, some version of Kolmogorov complexity, a combination of these, or something else? ^ Springer</page><page sequence="3">Reasoning About Truth in First-Order Logic 1 1 7 Fig. 1 Screenshot of the interface used for presenting the test items. The correct answer to the item shown is "Yes" One strategy for predicting the difficulty of determining truth is to combine n complexity measures on sentences with m complexity measures on models using some function / (e.g., a polynomial) of n + m variables. Despite the apparent generality of this strategy, it might be inadequate regardless of the choice of /. In fact, if the interplay between sentences and models is more intricate than that, then this strategy is bound to fail. An entirely different strategy is to assume that truth-values are computed and that the properties of such computations are the most promising indicators of difficulty. This is the strategy we use in this paper. 1 .3 Models of Human Reasoning Cognitive architectures, such as SOAR (Laird et al. 1987), ACT-R (Anderson and Lebiere 1998), Clarion (Sun 2007), and Polyscheme (Cassimatis 2002), are compu- tational models that have been used to model human reasoning in a wide range of domains. They typically include explicit models of cognitive resources, such as work- ing memory, sensory memory, declarative memory, and procedural memory; these cognitive resources are known to be bounded in various ways, e.g., with respect to capacity, duration, and access time (Kosslyn and Smith 2006). 1 .4 Models of Logical Reasoning In cognitive psychology, several models of logical reasoning have been presented in the mental logic tradition (Braine and O'Brien 1998) and the mental models tradition Ô Springer</page><page sequence="4">118 C. Strannegârd et al. (Johnson-Laird 1983). Computational models in the mental logic tradition are com- monly based on natural deduction systems (Prawitz 1965); the PSYCOP system (Rips 1996) is one example. The analytical focus of the mental models tradition has been on exploring particular examples rather than developing general computational models. Human reasoning in the domain of logic is considered from several perspectives in Adler and Rips (2008) and Holyoak and Morrison (2005). Stenning and van Lam- balgen (2008) consider logical reasoning both in the laboratory and "in the wild" and investigate how problems formulated in natural language and situated in the real world are interpreted (reasoning to an interpretation) and then solved (reasoning from an interpretation). Formalisms of logical reasoning include natural deduction (Prawitz 1 965 ; Jaskowski 1934; Gentzen 1969), sequent calculus (Negri and von Plato 2001), natural deduction in sequent calculus style (Negri and von Plato 2001), natural deduction in linear style (Fitch 1952; Geuvers and Nederpelt 2004), and analytic tableaux (Smullyan 1995). Several formalisms have also emerged in the context of automated reasoning, e.g., Robinson's resolution system (2001) and Stâlmarck's system (2000). None of these formalisms represent working memory explicitly, and most of them were constructed for purposes other than modeling human reasoning. We suspect that working memory could also be a critical cognitive resource in the special case of logical reasoning (Gilhooly et al. 1993; Hitch and Baddeley 1976; Toms et al. 1993). Therefore, we will define our own proof system that includes an explicit model of working memory. 1 .5 Structure of the Paper In Sect. 2, we report the results of a psychological experiment concerning FO truth. In Sects. 3 and 4, we present proof systems for showing FO truth and FO falsity, respectively. In Sect. 5, we present resource-bounded versions of these proof systems. In Sect. 6, we present a number of mathematical complexity measures, some of which are defined in terms of the resource-bounded proof systems. In Sect. 7, we compare the psychological complexity measures that were obtained in the experiment with the mathematical complexity measures. Section 8 contains a discussion and Sect. 9, finally, presents some conclusions. 2 Experiment In this section, we describe a psychological experiment concerning Truth. 2.1 Participants The participants in our experiment were ten computer science students from Gothen- burg, Sweden, whom were invited through email. These students had previously stud- ied FO in their university education. They belonged to various nationalities, were aged 20-30 years, and included one woman and nine men. â Springer</page><page sequence="5">Reasoning About Truth in First-Order Logic 1 19 2.2 Material Fifty test items were prepared for the experiment. Each item consisted of a finite graph and an FO sentence. The task was to determine whether the sentence was true in the graph. A Screenshot of the graphical user interface is shown in Fig. 1 . The test comprised 24 true items and 26 false ones, which were presented in an order that was randomized for each participant. The logical symbols used in the experiment were the quantifiers V and 3 and the connectives -% a, v, and «*. The vocabulary r included the binary predicate E(x, y) (for edges); the unary predicates Red (jc), Blue(jt), and Yellow(jt) (for colors); and the constants 1 , 2, 3, and 4 (for nodes). Let L(r) be the set of FO-formulas defined in the usual way over the vocabulary r. A total of 50 test items, consisting of pairs of graphs and ¿(resentences, were prepared. They were selected manually on the basis of estimated level of difficulty from randomly generated lists of graphs and sentences. All nodes were labeled with unique numbers and colored either yellow, red, or blue. Some of the test items are given in Appendix A and the full list can be found in Nizamani (2010). 2.3 Procedure The experiment was conducted in a computer laboratory at the Department of Applied Information Technology, University of Gothenburg. Each participant was assigned an individual computer terminal. A short practice session was conducted before the test to familiarize the participants with some sample problems. The experiment was conducted in a 25-min session, followed by a 10-min break, followed by another 25-min session. The answers and the response times were recorded for each participant and each item. Two aggregated complexity measures were computed for each item. The accu- racy is the proportion of participants who answered the item correctly. The latency is the median response time among the participants who answered the item correctly. The median was used, rather than the mean, to reduce the effects of extreme response times due to external disturbances. 3 The Proof System Tm In this section, we describe a proof system Tm for proving sentences to be true in a fixed finite model M . Please observe that this system is not a classical proof system in which only logical truths can be derived, but a system in which all truths in some fixed model M are derivable. The system Tm is a rather straightforward rewrite system with some modifications and extensions. The proofs are linear sequences of sentences. The system is local in the sense that checking whether a sequence of sentences is a proof can be done by only looking at two consecutive sentences at a time. The proofs are also goal-driven , Springer</page><page sequence="6">1 20 C. Strannegàrd et al. which means that they start with a proof goal, i.e., the desired conclusion, and then use rules to reduce the goal to the true statement T. The proof system has two ingredients, axioms and rules. The axioms model declar- ative memory content and visual information about models and sentences, whereas the rules model the procedural memory. The main rule (Substitution) is based on the principle of compositionality, which is that meaning (and truth-value, in particular) is preserved under the substitution of logically equivalent components. The axioms are used as side-conditions to this rule. For example, substitution of A for B is only allowed if A &lt;+ B is an axiom. Substitution is a deep rule, which means that it allows subformulas appearing deep down in the parse-tree to be substituted. 3.1 Formulas We shall now enrich our set of formulas L(r) by adding (i) bounded quantifiers and where Q is a set of constant symbols in r and VqxA has the intended meaning Vjc( VCGß x = c A ); (ii) abstraction boxes for modeling visual perception of formulas, [A]; and (iii) contexts for assigning values to variables, [x' = c' , . . . , jt* = c*]. Definition 1 (Formula) The set of L*(r)-formulas is defined by the following BNF grammar: A ::= Atom | | A - A ' QxA | A[x' = c' , . . . , jc* = c*] Atom ::= P(t',...Jk) | [£] where • is one of a, v, or Q is either V, 3, Vß, or 3ß, Í2 is a set of constant symbols in r; c' , . . . , c* are constant symbols in r; P is a predicate in r; and B is an L(r)-formula. Definition 2 (Satisfaction relation) The satisfaction relation for formulas in L*( r) is defined in the standard Tarskian way with the following extra clauses, where s is an assignment: - M '=s 3°x A iff M Kv 3*( Vceß x = c A A), - M (=¿ VßxA iff M '=s Vx( Vc.eí2 x = c A), - M '=s [A] iff M Kv A, and - M '=s A[j ci = ci,..., **=&lt;*] iff A# K| C'/x 1 Ck/x Jtj Next, we define what it means for a relation symbol R in a formula to be substituted by a formula. Definition 3 (Substitution) If R is a relation symbol of arity k, A a formula in L*(r U {/?}) and B a formula in L*( r) with the free variables x = jcj, . . . , we define the substitution A[B(x)/R] to be the formula we obtain from A by substituting the leftmost occurrence of the symbol R with B[t' , . . . , tk/x' , . . . , **], where R occurs as R(t' ,...,**) and t' is free for jc/ in B (if not, the notation A[B(x)/R] is undefined). Observe that substitutions are not carried out inside abstraction boxes [Cj. &lt;0 Springer</page><page sequence="7">Reasoning About Truth in First-Order Logic 121 3.2 Axioms The axiom set r is constructed as the union of three sets: the logical axioms, the sentence axioms and the model axioms. 3.2. 1 Logical Axioms The set of logical axioms is denoted /&gt; . The members of this set are the logical truths listed in Appendix B. This list was compiled from a number of textbooks on basic logic, including (Huth and Ryan 2004). 3.2.2 Sentence Axioms The second ingredient of r comes from the idea to include parsing of a formula in the proof system itself. When we must check the truth of a formula, we first must parse the formula. This action is modeled in the system using abstraction boxes, which are considered black boxes that the agent cannot look inside. By using the rules of the proof system, the agent may unwind the formula one step at a time. Formally, we add one abstraction box for each formula in the language. The abstraction box corresponding to the formula A is denoted by [A]. These boxes are in many respects similar to atomic formulas; the free variables of the abstraction box {A} are defined to be the same as the free variables of A. This idea is closely related to the template logic introduced in Engström (2002), which was constructed to understand non-standard formulas in non-standard models of arithmetic. In a bounded version of the proof system that will be introduced later, we will restrict the complexity of the formulas that may be used in a proof. Thus, the abstraction boxes may be used to reduce this complexity. The drawback of this reduction in complexity is that the substitution rule (see Sect. 3.3.1) may not substitute inside abstraction boxes. To include parsing in the system, we add, for each pair of formulas A and B of L(r) and each variable Jt, the following axioms to the set of sentence axioms: - If A is atomic, [A] A. - [-»A] &lt;-&gt; -»[A] - {A • Bj ++ [A] • [Z?J, where • is one of v, a, and - If A is atomic, {A • B} A • [/?], where • is one of v, A, - and (similarly for B). - [ßjcA] &lt;+ ÔJt[j4], where ß is one of V, Vß, 3, and 3ß. - If A is atomic, [ßjtA] QxA , where ß is one of V, Vß, 3, and 3ß. Thus, by using these axioms and the substitution rule, we can replace a template symbol in a formula by a more complex formula. By repeating this procedure, we may eliminate all abstraction boxes from a formula, thus replacing {A J by A. However, this result is achieved only with a considerable increase in complexity. Ô Springer</page><page sequence="8">1 22 C. Strannegârd et al. 3.2.3 Model Axioms Let M be a finite structure in which all elements are named by some finite set of constants Qq. To analyze the quantifiers in formulas, we require axioms that specify the range of quantifiers and the truth of quantifier-free sentences. If A is an L*( r)- formula (including abstraction boxes), we denote the corresponding formula without abstraction boxes by Af. The set consists of the following formulas: 1. A and A ++ T, whenever A' is a quantifier- free formula that is true in M . 2. -«A and A _L, whenever A! is a quantifier- free formula that is false in M . 3. Vjt A Vß()jc A. 4. 3jc A A. 5. Vßj t &gt;4 y&amp;'ic)x A , where Af |= A'[jt = c] and A' is quantifier-free. 6. 3qx A A, where M ¥ A'[x = c ] and A' is quantifier-free. 7. V^jc (Z? - ► A) X A , where Í2f = { a'M |= B[x = a]} and B is an atomic formula. 8. SQx (B A A) ++ X A, where Q ' = [a'M '= B[ x = a]} and B is an atomic formula. In the above formulas, Q is any set of constants. The set of axioms r is defined as the union of 7&gt;, / $, and r m (which depends on the model M). 3.3 Rules In this subsection, we present the rules of Tm . 3.3.1 Substitution This is a deep rule in the sense that formulas can be substituted deep in the parse tree of a sentence. Only one occurrence of a formula can be substituted at a time: A[B(x)/R] LE/FI/MI ir/rT/MI A[C(x)/R] LE/FI/MI ir/rT/MI This rule may only be applied if C *+ B e T and FV(C) = FV(Z?). We label the rule by LE (Logical Equivalence), FI (Formula Inspect), or MI (Model Inspect) when the equivalence B +&gt; C comes from /"V, or / a/, respectively. In the case of MI, we allow all contexts in A to be used for determining whether C B is in / a/. I.e., if y is the list of all contexts in A then B may be replaced by C if (Cy) (By) e Tm1 For example in a model where P(c) holds we may use MI to deduce (T a B)[x = c] from (P(x) a B)[x = c]. In short, Substitution enables us to deduce A[C(x)/R' from A[B(x)/R] whenever C *+ B e r and C and B have the same free variables. 1 This can be defined in a precise manner, but we omit the technical details here. Ô Springer</page><page sequence="9">Reasoning About Truth in First-Order Logic 123 3.3.2 Strengthening We may replace the goal to show B to be true with the goal to show A to be true whenever we know A -&gt; B to be a logical truth: -M- s 0 A s 0 To apply this rule, we require that A -&gt; B e /&gt;. 3.3.3 Truth Recall If we have derived a formula that we know to be a logical truth, we have succeeded: A ~Y~ TR To apply this rule, we require that A e /&gt;. This concludes the definition of the proof system Tm- 3.4 Proofs Definition 4 (Proof in Tm) Suppose M e MOD and A e SEN. A proof of A in Tm is a sequence of sentences (Ao, A', . .. , An) such that Ao = [ Aj , An = T, and A¡+' follows from A¡ by one of the rules of Tm- By letting the first sentence in the sequence be {A} rather than A , we aim to model the idea that an individual who is presented with a sentence perceives the structure of the sentence gradually, starting from perceiving nothing and possibly ending before all the details have been perceived. Formula inspection can be used for gradually perceiving and parsing the sentence. As an example of a proof in Tm , let us prove that the sentence given in Fig. 1 is true in the model given in the same figure: p|l.2.3.4)jrV|l.2,3.4)}&gt;(g/t&lt;e(3;) 3(1.2,3.4)^(1,2.3.4)^/^) _ ^E(Xj ^ S (A [JÍ - 3] - &gt; 3jcí4) |y(i.2,3.4 )y(Blue(y) -&gt; -&lt;E(x, &gt;&gt;))]][* = 3] ^ (V&lt; ^ ^y'Blue{y) -&gt; ^E(x, y)])[jr = 3] &lt;yP-Wy[Blue(y) -»• -£(*, y)j)[x = 3] MI Çi^y{Blue(y) -+ -E(x, y)])[jc = 3] wt wt Ml (yWyjBluejy) -» ^E(x,y)j)[x = 3] &lt;y»ylBlŘie(y)^-:E(x,y)Ux = 'S] ^ ^ Note that the abstraction boxes may be removed in the beginning of the proofs by zooming in on the formula using iterated applications of Substitution (FI). Abstraction boxes are strictly speaking unnecessary at this point, but they will play an important role in Sect. 5, in which bounded versions of proof systems will be considered. Ö Springer</page><page sequence="10">1 24 C. Strannegârd et al. 3.5 Properties The system Tm includes simple models of the following cognitive resources: - declarative memory, which is modeled by the logical axioms in the set /&gt;; - procedural memory, which is modeled by the rules; - sensory memory, which is modeled as a buffer to hold sentence axioms from the set / $ (modeling visual perception of sentence structure); - working memory, which is modeled as a buffer to hold temporary proof goals. Now, let us show the adequacy of Tm for Truth. Proposition 1 Suppose M e MOD and A e SEN. Then , M |= A iff A is provable in Tm- Proof Right-to-left is soundness: We prove that in any proof of Tm, if A occurs, then M '= A. This is done by induction starting from the bottom; the trivial base case is when A is T. For the induction step, all we must check is that truth is preserved (in the sense that if A may be deduced from B and M |= A, then M [= B) by the rules in Tm - This is straightforward. Left-to-right is completeness: Suppose M '= A. Note that by essentially using Substitution (FI) followed by Substitution (MI), we may reduce any sentence A to a propositional formula in which only T and _L are atomic formulas. Regardless of the details of this propositional formula, we can then use Substitution (LE) to produce strictly shorter formulas at each step until either T or ± is reached. Because M '= A, however, the soundness of the system forces the proof to end with T. 4 The Proof System Tm In this section, we describe a proof system Tm for showing sentences to be false in a fixed finite model M . The formulas and axioms of Tm are the same as for Tm • 4.1 Rules Now, let us define the rules of Tm • 4.1.1 Substitution This rule is identical to the Substitution rule of Tm- 4.1.2 Weakening We may replace the goal of showing B to be false with the goal of showing A to be false whenever we know B - &gt; A to be a logical truth: -f-w A A To apply this rule, we require that B - ► A e /&gt; . ^ Springer</page><page sequence="11">Reasoning About Truth in First-Order Logic 125 4.1.3 Falsity Recall This rule makes use of a set /&gt;, which contains contradictions only. We omit the exact definition of 7&gt;, which is a list of textbook contradictions in the style of Appendix B. If we have derived a formula which we know to be a contradiction, we have suc- ceeded: -j-FR To apply this rule, we require that A e Tf. 4.2 Properties Definition 5 (Proof in Tm) Suppose that M e MOD and A e SEN. A proof of A in Tm is a sequence of sentences (Ao, A ' , . . . , An) such that Ao = [A], An = _L, and A¡+' follows from A¡ by one of the rules of Tm- Now, let us prove the adequacy of Tm for showing falsity in M . Proposition 2 Suppose M e MOD and A e SEN. Then , M A iff A is provable in Tm- Proof This proof is analogous to the proof of Proposition 1 . 5 The Bounded Proof Systems BT m and 1ST m In this section, we define resource-bounded versions of the proof systems Tm and Tm- For this purpose, we require a precise definition of sentence length. Definition 6 (Formula length) The length |j4| of an L*(r)-formula A is defined as follows: - |v4| = 1 when A is atomic. - PJI = 1. - |i4[*i = c i, ...,** = ck] I = 1 + 'A'. - hA| = l + 'A'. - 'A • B' = 1 + |j4| + |Z?|, where • is either v, A, - ► or - I Qx A I = 2 + ' A', where Q is either V, Vß, 3, or 3ß. Now we can define our two bounded proof systems. Definition 7 (BT m and BT m) Let BT m be the proof system obtained from Tm by adding the following restrictions on the proofs: - Working memory limit. The maximum length of a sentence that can appear in a proof is 8. - Sensory memory limit. The rule Substitution (FI) is only allowed on sentence axioms of / $ of maximum length 7. The proof system BT m is defined analogously from Tm- â Springer</page><page sequence="12">1 26 C. Strannegàrd et al. In the bounded proof systems, the abstraction boxes play an important role because they enable certain sentences to be provable that would not be provable without them. For instance, note that the FO sentence T v A, where 'A' &gt; 6 is valid and that this validity can be deduced without looking inside the right disjunct. This sentence is not allowed to appear in a proof in BT m , however, because its length exceeds 8. However, we have the following proof in BT m • [Tv Al Tv [Al TR (T v A) 6 Mathematical Complexity Measures In this section we define a number of mathematical complexity measures on items (M, A). The complexity measures are as follows (the ranges refer to the items appear- ing in the experiment). 1. Sentence length. The length of A. Range: 5-9. 2. Quantifier count. The number of quantifiers appearing in A. Range: 1-3. 3. Negation count. The number of negations appearing in A. Range: 0-2. 4. Cardinality. The number of nodes of M . Range: 3-4. 5. Edge count. The number of edges of M . Range: 3-6. 6. Linear combination. A linear combination of sentence length, cardinality, and sentence length * cardinality, whose coefficients can be fitted to experimental data. 7. Working memory. The minimum size of the WM required for proving A in BT m or BT m - Range: 3-8. 8. Proof length. The minimum number of steps of a proof of A in BT m or BT m • Range: 4-38. 9. Proof size. The minimum size of a proof of A in BT m or BT m , i.e. the minimum sum of the lengths of the formulas appearing in the proof. Range: 6-164. Let us provide some motivation why these particular complexity measures were cho- sen. Measures 1-5 are standard complexity measures in logic. Measure 6 (Linear com- bination) illustrates the possibility of combining such complexity measures in various ways, e.g. via polynomials. As suggested in Sect. 1.2, there may be a priori arguments for believing that these complexity measures are inadequate for the present experiment. Measure 7 (Working memory) models the maximum strain on the working memory and measure 8 (Proof length) models the length of the shortest train of thought leading to the desired conclusion. Measure 9 (Proof size) is based on the idea that latency in the context of logical reasoning depends on some notion of computational workload. By identifying the working memory load with formula length, we get proof size as a measure of com- putational workload. Thus measure 9 models the minimum amount of data that must flow through the working memory for the problem to be solved. â Springer</page><page sequence="13">Reasoning About Truth in First-Order Logic 127 Table 1 Mean latency and mean accuracy for true and false items True False Latency (seconds) 33 37 Accuracy (%) 74 67 Table 2 Correlations between latency and some mathematical complexity measures True False Sentence length 0.35 0.63 Quantifier count 0.42 0.73 Negation count 0.15 0.12 Cardinality 0.01 -0.02 Edge count -0.28 0.09 Linear combination 0.38 0.64 Working memory 0.53 0.66 Proof length 0.49 0.76 Proof size 0.54 0.76 Table 3 Correlations between accuracy and some mathematical complexity measures True False Sentence length -0.22 -0.02 Quantifier count -0.22 -0.05 Negation count -0.33 -0.56 Cardinality -0.41 0.38 Edge count -0.04 -0.10 Linear combination 0.46 0.50 Working memory -0.32 -0.19 Proof length -0.21 0.13 Proof size -0.24 0.07 We implemented automated theorem provers for the systems BT m and BF m in the functional programming language Haskell. Proofs of minimum length and minimum size were generated for all items appearing in the experiment. For instance, the proof appearing in Sect. 3.4 was generated by our theorem prover, as a proof of minimum size. 7 Results Table 1 shows the mean latency and mean accuracy recorded at the experiment. Table 2 shows correlations between latency and the mathematical complexity measures defined in Sect. 6. Table 3 shows correlations between accuracy and the same com- plexity measures. Ö Springer</page><page sequence="14">1 28 C. Strannegârd et al. 8 Discussion In this paper, we studied human reasoning in the case of FO truth using standard methods of cognitive psychology. This reflects our view that there is nothing special about problem-solving in the domain of logic, and therefore, it can be explored using ordinary experimental methods and modeled using ordinary models of cognitive psy- chology that would apply equally well to mental arithmetic, Sudoku, or Minesweeper, for example. 8.1 Comments on the Experiment In a preliminary investigation, we observed that the difficulty of determining truth was affected by the manner in which the models were presented graphically. For instance, determining whether a graph is complete seems to be simpler if the graph is drawn in a regular fashion. To mitigate this problem, we drew our graphs by placing the nodes equally distributed on a circle. As is usual in experiments of this type, the answers given by the participants are potentially problematic because guesses, interaction errors (e.g., hitting the wrong but- ton accidentally), and distractions in the experimental situation (e.g., coughing attacks) can affect the individual results substantially. Another potential problem relates to the instructions. In our particular experiment, we learned afterwards that some of the participants thought that if two variables x and y appeared in a sentence, then auto- matically X y . Our instructions failed to address this point. On the aggregate level, some of the problems on the individual level might par- tially cancel out, but then new problems arise on the modeling side. In fact, to model experimental data on the aggregated level, one must develop a computational model that represents some sort of average of the participants. Strictly speaking, this might not be possible using our present type of computational model, which was designed for cognitive modeling on the individual level. These factors should be borne in mind when evaluating the results. 8.2 Comments on the Proof Systems The bounded proof systems described in Sect. 5 can be modified in several ways to suit different human role models. One way is to modify the axioms; a second is to modify the rules; a third is to change the working memory and sensory memory capacities; and a fourth is to add models of other cognitive resources. Intuitively, proof-size reflects how comprehensive a thought must be, i.e., how much information must be processed to produce the correct answer. Proofs that require more information processing might take longer to process and be more prone to error. One may perhaps think of the smallest proofs as the smartest proofs, relatively to a given repertoire of cognitive resources. As is often the case with cognitive modeling, this complexity measure can be criticized for being too coarse. For instance, it does not directly reflect the effort of searching for a proof; instead, it reflects the effort required to verify the steps in a proof that has been found. Although the efforts required for finding a proof and verifying Ô Springer</page><page sequence="15">Reasoning About Truth in First-Order Logic 1 29 the same proof may be somewhat correlated, this limitation certainly allows for future improvements to the model. 8.3 Comments on the Results The data in Table 1 indicate that the True items were easier to solve than the False items. Among the complexity measures analyzed in Table 2, Proof size has the highest cor- relation with latency, both for True and False items. Second comes the closely related complexity measure Proof length. This might indicate that complexity measures that are defined in terms of computations are more adequate than those defined in terms of standard properties of models and sentences. Some of the complexity measures on sentences in Table 2 fared relatively well. In Sect. 1 it was argued that in general, properties of models can dramatically affect the difficulty of determining truth. The reason why those complexity measures on sentences were relatively successful in this particular case might be that the models that were used in the experiment were quite homogeneous. In fact, Cardinality varied between 3 and 4, and Edge count varied between 3 and 6. Therefore, the complexity measures that considered only sentences were perhaps not sufficiently challenged in the present experiment. All of the complexity measures analyzed in Table 3 have relatively low correlations with accuracy. We do not know why the contrast to latency is so pronounced. The correlation values for Negation count stand out here and provide some support to the idea that negations increase the probability of error. 9 Conclusions In this paper, we presented (i) the results of an experiment pertaining to Truth, (ii) two proof systems for deriving membership and non-membership in Truth using bounded cognitive resources, and (iii) an analysis of the correlation between psychological com- plexity measures and different mathematical complexity measures, including proof size. The results indicate that proof size was more successful than the other complex- ity measures in the case of latency. The approach that we use enables predictions about latency to be made for arbitrary elements of Truth. To evaluate the usefulness of this* approach, which combines elements of proof theory and cognitive psychology, more experiments are needed, in particular experiments with more heterogeneous test items. Open Access This article is distributed under the terms of the Creative Commons Attribution License which permits any use, distribution, and reproduction in any medium, provided the original author(s) and the source are credited. Appendix A: Experimental Data To enable a deeper understanding of the experiment, let us list items 1-9 of the exper- iment and give their associated data. A full description of the items used can be found â Springer</page><page sequence="16">130 C. Strannegârd et al. Table 4 The sentences of items 1-9 Item Sentence 1 3x(Blue(x) A Vy'Blue(y) -► E(x , y)]) 2 Vx(Red(x) 3&gt;&gt;3zl£(jt, y) A E(y , z) a /?ed(z)]) 3 3*[K&lt;?//tfu;(.*:) a Vy^E(x, &gt;01 4 VjcVyl-£(jc, y) -» Ke//^(y)] 5 Vjc(3-y[ye//ř;w(&gt;') A E(x , y)J Yellow(x)) 6 VJC([£(jc,JC) £(3,JC)) 7 3jc3y| /?ed(jt) a Red(y) A £(*, y) a £(&gt;&gt;, jc&gt;] 8 3jc3.y3z|E(jc, y) A £(&gt;&gt;, z) A -£(z, jc)J 9 Vjcl-i?grf(jc) -&gt; V&gt;&gt;£(jc,y)] Table 5 The models of items 1-9. On the top row, items 1-3 are shown, and so on in Nizamani (2010). Tables 4 and 5 list the sentences and models of items 1-9, respec- tively. Table 6 shows the data recorded at the experiment concerning items 1-9. Table 7, finally, shows the mathematical complexity measures of items 1-9. ô Springer</page><page sequence="17">Reasoning About Truth in First-Order Logic 1 3 1 Table 6 Response times of the participants P 1 -P 1 0 on items 1 -9 PI P2 P3 P4 P5 P6 P7 P8 P9 PIO M A 1 47.2 32.4 26.9 49.3 57.6 72.7 10.7 66.6 48.3 8 2 30.3 67.9 35.3 42.6 65.7 54.9 70.5 70.5 60.3 8 3 17.0 23.2 21.2 12.4 47.1 60.0 29.9 30.7 45.4 29.9 9 4 14.0 68.2 41.1 2 5 52.2 31.4 24.5 60.9 71.0 53.3 52.7 6 6 29.1 49.3 73.5 21.5 20.2 29.1 5 7 19.2 29.8 22.9 18.6 19.3 22.2 24.4 22.2 7 8 39.6 25.7 16.4 58.8 37.5 28.2 73.3 26.0 32.8 8 9 21.0 31.5 K7 48I5 26.3 4 Here M abbreviates median response time and A abbreviates accuracy Table 7 The mathematical complexity measures of items 1-9 Item SL QC NC C EC WM PL PS Truth-value 1 13 2 0 3 3 5 8 32 False 2 19 3 0 3 3 6 13 71 False 3 11 2 1 4 4 4 9 31 True 4 11 2 1 3 3 4 5 17 False 5 13 2 0 4 4 6 38 164 True 6 13 1 1 3 3 3 4 7 False 7 17 2 0 3 4 4 5 17 True 8 18 3 1 4 5 4 8 27 True 9 11 2 1 3 3 5 8 25 False Sentence length (SL), Quantifier count (QC)' Negation count (NC), Cardinality (C), Edge count (EC), Working memory (WM), Proof length (PL), and Proof size (PS). The last column represents the truth-value of the item Appendix B: Logical Truths In this section, the members of the set /&gt; are listed. Note that all formulas listed below are logical truths. Below A , B , and C stand for formulas of L*( r). Remember that if A is an L*(r)-formula then A ' is the corresponding L(r)-formula with the abstraction boxes removed. We assume that VxA includes the case of xA and similarly with 3. For every axiom A appearing in the list, we also include the sentence .4 T as an element of /&gt;. B.l Truth-Table TaT â Springer</page><page sequence="18">132 C. Strannegârd et al . A vT T v A A ^ T ± -&gt; A (Avi)«» A (_L v A) A (A A T) A (T A A) A (T -*■ A) +&gt; A (A - ► -L) -m- -&gt;A {A ■*+ T) +*■ A (T ■*+ A) +&gt;■ A (_L &lt;+ A) -&gt;&lt;4 (A _L) -h- -i A B.2 Idempotence (A v A) «-&gt; A (A A A) -H- A A -&gt;■ A A ++ A B.3 Commutativity (A A B) (B A A) (A v B) -o- (Z? v A) (A ++ B) ++ (B ++ A) Springer</page><page sequence="19">Reasoning About Truth in First-Order Logic 1 33 B.4 Associativity ( A A B) A C •&lt;-&gt;• A A (j B A C) A A (B A C) +&gt; (A A B) A C (Av B)v C &lt;+ A v (B v C) A v (B v C) ++ (A v B) v C ((&gt;4 &lt;-&gt; B ) &lt;-&gt; C) (y4 &lt;-&gt; (Z? &lt;-&gt;- C)) (A •*-&gt; (B &lt;-&gt; C )) &lt;-&gt; ((/4 ■&lt;-&gt; Z?) C) B.5 Distributivity (A a B) v (A A C) +&gt; A a (B v C) (A v B) A (A v C) ++ A v (B A C) B.6 De Morgan (-•A A ~~*B) - &gt;(/' V B) -»(-»A a -&gt;B) ++ (A v B) -•(A a -&gt;B) &lt;-&gt; ( - &gt;/4 v B) -&gt;(-&gt;A a B) &lt;-&gt; (A v -*B) (-1^4 v -iß) -i(A a B) v -&gt;B) *+ (A a B) -&gt;(A v -*B) &lt;-» ( *A a B) v B) &lt;-&gt; (A a -£) B.7 Negation - A A (-*A v B) (A -&gt; B) ô Springer</page><page sequence="20">1 34 C. Strannegârd et al. (. A A -fi) -&gt;(A -&gt; B) (^B -* -&gt;A) ■+*■ (A -*■ B) (A - &gt; -^A) -&gt;A (-■A -»• A) &lt;b. A B.8 Excluded Middle Av-&gt;A -*Av A A v -&gt;A v B -&gt;Av Av B BvAv-&gt;A B v -&gt;Av A Av Bv -&gt;A -&gt;A v B v A A v (-&gt;A v ß) -•Av (Av B ) B V (Av -&gt;A) B v (-»A v A) A v (Z? v -&gt;A) -&gt;Av (Bv A) B.9 Quantifier Expressions VxT V0A £} Springer</page><page sequence="21">Reasoning About Truth in First-Order Logic 1 35 VnxA (Vß'&lt;&lt;U A Aix = c]) 3 qxA ^ (3ßM'U v A[x = c]) Vßj kA A A[x = c]) 3s2 X A ++ (3ß'(&lt;'lI/t,l v A[x = c]) (Vjt/4 A VjC B) Vjt (A A B) (3xA v 3jcZ?) 3jc(j4 v Z?) 3jc3jy4 ++ 3y3xA VxVy/4 «-&gt; VyVjc^ B.10 Implications (AaB)-&gt; A (A A B) B A (A v B) B -&gt; (Aw B) (A &lt;r&gt; B) (A -+ B) (A ++ B) -* (B -» A) VjcA -&gt; i4[jc = c] A[x = c] - ► 3jC/4 3x(A A ß) -&gt; 3xy4 3x(y4 Aß)-&gt; 3JCZ? Vjc(i4 A ß) -&gt; Vjt¿ Vjc(A A Ä) -► VJC£ 3x(A A Z?) -» (3jt/4 A 3jcZ?) Ô Springer</page><page sequence="22">1 36 C. Strannegârd et al. Vjc(-A) -* Vjc (A B) VJC B -&gt; VJC(A -&gt; B) VxA -&gt; Vjc (A v B) VJC B -&gt; Vjc (A V ß). References Adler, J. E., &amp; Rips, L. J. (2008). Reasoning : Studies of human inference and its foundations. Cambridge: Cambridge University Press. Anderson, J. R., &amp; Lebiere, C. (1998). The atomic components of thought. Mahwah, NJ: Lawrence Erlbaum. Braine, M. D. S., &amp; O'Brien, D. P. (1998). Mental logic. UK: L. Erlbaum Associates. Cassimatis, N. (2002). Polyscheme: A cognitive architecture for integrating multiple representation and inference schemes. PhD thesis. Ebbinghaus, H. D. (1985). Extended logics: The general framework. Model-theoretic logics (pp. 25-76). Engström, F. (2002). Satisfaction classes in nonstandard models of arithmetic. Licentiate thesis, Chalmers University of Technology. Fitch, F. B. (1952). Symbolic logic: an introduction. New York: Ronald Press. Gentzen, G. (1969). Investigation into logical deduction, 1935. In M. E. Szabo (Eds.), The collected papers of Gerhard Gentzen. North-Holland Amsterdam. Geuvers, H., &amp; Nederpelt, R. (2004). Rewriting for Fitch style natural deductions. In Rewriting techniques and applications. Springer (pp. 134-154). Gilhooly, K. J., Logie, R. H., Wetherick, N. E., &amp; Wynn, V. (1993). Working memory and strategies in syllogistic-reasoning tasks. Memory &amp; Cognition , 27(1), 1 15-124. Hitch, G. J., &amp; Baddeley, A. D. (1976). Verbal reasoning and working memory. The Quarterly Journal of Experimental Psychology , 28(4), 603-62 1 . Holyoak, K. J., &amp; Morrison, R. G. (2005). The Cambridge handbook of thinking and reasoning. Cambridge: Cambridge University Press. Huth, M., &amp; Ryan, M. (2004). Logic in computer science: Modelling and reasoning about systems. Cam- bridge, UK: Cambridge University Press. Jaskowski, S. (1934). The theory of deduction based on the method of suppositions. Studia Logica , 7, 5-32. Johnson-Laird, P. N. (1983). Mental models. Cambridge, MA: Harvard University Press. Kosslyn, S. M., &amp; Smith, E. E. (2006). Cognitive psychology: Mind and brain. Upper Saddle River, NJ: Prentice-Hall Inc. Laird, J., Newell, A., &amp; Rosenbloom, P. (1987). Soar: An architecture for general intelligence. Artificial Intelligence , 53(3), 1-64. Negri, S., &amp; von Plato, J. (2001). Structural proof theory. Cambridge: Cambridge University Press. Nizamani, A. R. (2010). Anthropomorphic proof system for first-order logic. Masters thesis, Chalmers University of Technology. Prawitz, D. ( 1 965). Natural deduction. In A proof-theoretical study, volume 3 of Stockholm studies in philosophy. Stockholm: Almqvist &amp; Wiksell. Rips, L. (1996). The psychology of proof. Bradford. Robinson, A., &amp; Voronkov, A. (2001). Handbook of automated reasoning. The Netherlands: Elsevier Sci- ence. Sheeran, M., &amp; Stâlmarck, G. (January 2000). A tutorial on Stâlmarck's proof procedure for propositional logic. Formal Methods in Systems Design , 76(1), 23-58. Smullyan, R. M. (1995). Logic, First-Order (second corrected edition). New York: Dover. (Berlin: Heidel- berg, New York: First published in 1968 by Springer). Stenning, K., &amp; van Lambalgen, M. (2008). Human reasoning and cognitive science. Cambridge, MA: Bradford Books MIT Press. &lt;0 Springer</page><page sequence="23">Reasoning About Truth in First-Order Logic 1 37 Strannegârd, C, Ulfsbäcker, S., Hedqvist, D., &amp; Gärling, T. (2010). Reasoning processes in propositional logic. Journal of Logic, Language and Information, I9( 3), 283-314. Sun, R. (2007). The importance of cognitive architectures: An analysis based on CLARION. Journal of Experimental &amp; Theoretical Artificial Intelligence, I9( 2), 159-193. Toms, M., Morris, N., &amp; Ward, D. (1993). Working memory and conditional reasoning. The Quarterly Journal of Experimental Psychology , 46(4), 679-699. â Springer</page></plain_text>