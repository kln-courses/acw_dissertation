<plain_text><page sequence="1">THE DANCE OF EVALUATION: HUSTLE OR MINUET Eva L. Baker Center for the Study of Evaluation UCLA Graduate School of Education The practice of education is poised on an insecure technological base. Despite massive infusions of rhetoric, little prog- ress has been made to transform the operations of instruction into other than blurred attempts at craft. At present, the scientific aspect of education is limited principally to discipline based research conducted with far-term outcomes. In only restricted areas, under highly con- trolled conditions, have the responsibili- ties of the education profession derived benefit from scientific analysis. Dance is an art form with a technological base and a transpositional motive. Al- though dance and educational practice are both performed en chorale, they dis- play more particular similarities. Con- sider the references to the hustle and the minuet, two well-known dances. While separated greatly in historical time, they both share common elements. They, for instance, were both popular expressive forms, based on highly controlled pro- gressions of rigidly stylized steps. Thus, their suitability as metaphors for evalua- tion activities should be immediately perceived. As evaluations, the hustle and minuet differ along other continua, from energetic to passive, or exuberant to reserved. Notice, however, that these dances, as evaluation, both contribute at a marginal level to the serious pursuits of their times. 26</page><page sequence="2">Emerging technology in the field of edu- cation mirrors none of the art but some of the other attributes of these dances. For example, the tendency in education is to codify and stylize operations, e.g., technological procedures, and grind at them insistently either until some higher authority, usually political, questions ones' motives, or until a new procedure is proffered. Some procedures are adopted with high hopes and energy and others fit more clearly into the estab- lished order. In instructional develop- ment, specifically, we have seen almost mechanical commitment to different procedures and guidelines: rules for creating, first, linear programs, then, branching instruction, and then, multi- component programs, have been articu- lated. The field of evaluation, at once older and less mature than that of in- structional development, seems destined to continue the same errors. For evalua- tion, as it became regularized into models, frameworks, and procedures, has spawned a series of rules for action which rarely receive sustained reflection. A review of the role of evaluation in in- structional planning and development is suggested, with a focus on an examina- tion of the purpose and utility of evalua- tion for educational technology. A critical question, almost lost in the clutter of contending models and proce- dures of evaluation, must be raised about who is to assume the evaluation role. Functions generate labels and labels have a way of becoming anthromor- phized. Technology created first techni- cians, then technologists. Development transmuted into developer; evaluation became evaluator. This linguistic fact is of minor interest except for the connota- tion such designators suggest. Who is best qualir,.- to evaluate a development project7 An evaluator or anyone else7 Certainly, any right-minded person would choose the individual who is named with the function, for special ex- pertise is clearly implied. Another prob- lem with anthropomorphic process is that a complex set of activities, which requires participants with diverse prepa- ration and expertise, is reduced to a sim- ple procedure, evaluation, where a sim- ple, all-purpose individual, the evalua- tor, can handle the entire show. Should, then, development give over the evaluation function to evaluators?7 If de- velopment activity is stripped of its eval- uation component, what is left? A pessi- mistic view of the state of applicable findings from instructional research would suggest that, without evaluation, very little work remains for those charged with development responsibility. They might create instructional design based on a few stable principles of learning. They may base program structuring de- cisions on rather firm but far between findings from cognitive psychology. De- velopment personnel may make media selections by rehearsing any one of a number of sets of arbitrary, and usually, economically inspired rules. They may manage component development and integration by using systems such as PERT, Critical Path Analysis or other planning procedures. But the relinquish- ment of evaluation functions to another external group gives up the most potent development capability there is, the capacity to detect and manage change. It is evaluation, at least at present, that drives the development act. Development can be conceived as a con- tinuum where we begin in ambiguity and attempt to move toward clarity. At the outset of a development project, we may only have the sketchiest informa- tion about what we will end up with. For example, we will know what age or experience learners we are designing ma- terials or curriculum for...probably. We will know the broad area of subject matter we are to treat.., .probably. We will know how much we have to spend ... definitely. A few overzealous people might even have outcomes to which the curriculum or product is directed speci- fied in advance. During the course of the development effort, we attempt to gain better understanding of our process and what our likely outcomes will be. We learn to identify learners with precise areas of need and history for our prod- ucts. We determine what settings will form the probable contexts of product use. We design, revise, and redesign component tests of programs in order to formulate a proper effort which em- bodies appropriate instructional charac- teristics. In the end, we hope that we can say that the product we have developed is planned for a particular group or groups of respondents for use in a speci- fied class of settings and with results re- liable at given levels, when appropriate time and implementation guidelines are met. It should be clear that throughout this process, the need to evaluate is per- vasive. Instructional development staff may have a wide range of expertise in the evaluation area, but they usually share some familiarity with the concepts of formative and summative evaluation. As described by Scriven (1967) and Markle (1967) using slightly different terms, formative and summative evalua- tion are assumed to have different pur- poses. Summative evaluation should provide a comparative judgment of merit under conditions as objective as possible. The purpose of this form of evaluation is to make a selection among competing al- ternatives, a topic we will return to later. Formative evaluation differs in its pur- pose for data are collected to improve a program under development, to allow adjustments to be made in order to maxi- mize the desired outcomes of the pro- gram. Thus, the formative evaluation contributes to the redevelopment and re- finement of the product as it shifts along the continuum from ambiguity to pre- cision. It is clear that instructional de- velopment must keep the reins on forma- tive evaluation activity. Rather than guidelines, some simple rules of thumb can be described for formative evaluation. First, one should collect extensive information from rela- tively few subjects during initial design phases. This suggestion is based on three probable states of the world. First, the early versions of the product and pro- gram may be poor. Why expose great numbers of subjects to unreliable instruc- tion7 Second, it is likely that significant aspects of the program are susceptible to revision, for instance, the objectives, formats, instructional approaches and so on. Thus, diverse information sources are desirable. Third, the practical matter of processing data from many subjects is a serious impediment. Development mo- mentum can be stopped and the fluidity of the project lost by extensive data analyses and interpretation requirements at the wrong time in a project's life. Another major guideline is that the di- versity of data one collects reduces as the project moves toward completion. Es- sentially one collects less information, but from more different kinds of people. It should be remembered that conserva- tion of resources requires that only us- able information be assembled, that is, data which can be employed to revise the program. The need to be comprehen- sive should be overcome in an effort to make the evaluation act instrumental to the efficient completion of the project. With regard to the relationship of sum- mative evaluation, that is the program verification, it is probably not in the de- velopment staff's or developing agency's interests to conduct a summative or com- parative evaluation. Whenever what is known as "summative evaluation" seems 27</page><page sequence="3">appropriate in a development cycle, the decision for product selection will likely be surrounded by political as well as ef- fectiveness criteria. For this reason, de- velopment personnel might seriously consider avoiding the entire prospect of summative evaluation. A brief reflection of the actual conditions of summative or comparative evaluations might make the point more strongly. First, data from any comparison between curriculum or products are likely to be inclusive. This likelihood, in fact, is the basis of our parametric statistics. Thus, there will be a force for the status quo, and a tendency against innovation when the decision is based purely on empirical data. Second, in a comparison between a commonly formated program, e.g., printed materials, in contrast with multi- media development, the short run utili- tarian aspects of the contending pro- grams, when divided by costs, will al- most always support the more conserva- tive development effort. The long range benefits, for instance, of introducing stimulating variation in school activities, are, in the first place, almost never as- sessed by appropriate dependent mea- sures, and in the second place, the amount of time appropriate for such a long term comparison is usually imprac- tical for all participants. In addition, in- novative looking materials may be threatening both to the users of instruc- tional programs, teachers, and adminis- trators, and to the funders of education, for example, state legislators. The paradox is that while development people recognize the importance of com- parative product tests in their activity, the summative evaluation has little to recommend itself to product develop- ment staff on a strictly practical basis. Further, unless summative evaluations are contracted externally to the organi- zation of the developed product, they are inevitably suspect. Summative eval- uations should be conducted indepen- dently by agencies or groups for whom there are no contingencies for support- ing the effectiveness of the product under study. (Who really believes the Pepsi challenge advertising campaign, con- ducted, as it is, by Pepsi-Cola employees. Perhaps if Canada Dry Ginger Ale monitored the comparisons, we would have developed a greater sense of trust in the findings.) If summative type evaluations are to be conducted by the development agency, however, they should be limited to a few clear purposes. For instance, they seem to be appropriate for promotional ma- terials required for broad implementa- tion. Data from summative evaluations can also be used formatively, perhaps to improve user manuals. But summative evaluation asks potentially to document our failures without recourse. Thus, it must be the minuet of this piece, a con- trived exercise of limited practicality for development staff. Formative evaluation is where the hustle, energy and produc- tivity intersect and continue to contrib- ute to the improvement of instruction. For current evaluation of our govern- ment-supported and for-profit efforts as well as for the persisting longevity of in- structional development itself, we should subscribe more evaluation models de- rived more from Motown than from Mozart. References Markle, S. M. Empirical testing of pro- grams. In P.C. Lange (ed.) Programmed Instruction, 66th Yearbook of the Na- tional Society for the Study of Educa- tion, Part II. Chicago: NSSE, 1967. pp. 104-138. Scriven, M. The methods of evaluation. In R. W. Tyler, R. M. GagnW, and M. Scriven (eds.) Perspectives of Curricu- lum Evaluation, AERA Monograph Series on Curriculum Evaluation, No. 1, Chicago: Rand McNally, 1967. pp. 39-83.</page></plain_text>