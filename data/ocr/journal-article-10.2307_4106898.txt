<plain_text><page sequence="1">WHAT SORTS OF MACHINES CAN UNDERSTAND THE SYMBOLS THEY USE? Aaron Sloman and L. Jonathan Cohen I-Aaron Sloman I Introduction I am grateful for this opportunity to discuss with philosophers some difficult issues common to philosophy and AI. It is increasingly difficult to keep up with all the relevant literature, and only mutual aid can prevent time-wasting re-invention of wheels and blundering down blind alleys. My topic is a specialised variant of the old philosophical question 'could a machine think?'. Some say it is only a matter of time before computer-based artefacts will behave as if they had thoughts and perhaps even feelings, pains or any other occupants of the human mind, conscious or unconscious. I shall not pre-judge this issue. The space of possible computing systems is so vast, and we have explored such a tiny corner, that it would be as rash to pronounce on what we may or may not discover in our future explorations as to predict what might or might not be expressible in print shortly after its invention. Instead I'll merely try to clarify what we might look for. Like Searle ([11, 12]) I'll focus on a specific type of thought, namely understanding symbols. Clearly, artefacts like card- sorters, optical character readers, voice-controlled machines, and automatic translators, manipulate symbols. Do they understand the symbols? Some machines behave as if they do, at least in a primitive way. They respond to commands by performing tasks; they print out answers to questions; they paraphrase stories or answer questions about them. We understand the symbols, but do they? Is real understanding missing from simulated understanding just as a real wetness is missing from a simulated tornado? Or is a mental process like calculation: if simulated in detail, it is replicated? If 'understanding' denotes some logically private internal</page><page sequence="2">62 I-AARON SLOMAN state which can only be defined ostensively by pointing inside yourself, then the question whether machines can understand becomes undiscussable, like the question whether the earth is or is not the at same point in absolute space as it was a year ago. Two people 'pointing' inside themselves cannot be sure they are talking about the same thing when they ask whether machines, or even other people, have it. Arguments from analogy need a theory which indicates why certain common aspects of the body or brain might be sufficient to produce understanding. And that requires some kind of non-ostensive, functional, analysis of what understanding is, just as questions about identity of locations require locations to be relative to a framework of reference. In that case, understanding is defined in terms of a collection of capabilities with a certain structure and certain functions. It is not a simple state, and it may be present in different degrees of sophistication. In this functional sense there is a discussable question whether machines could ever understand the symbols they manipulate. This does not imply that there will be a determinate answer. We'll see that our ordinary concept of 'understanding' denotes a complex cluster of capabilities and different subsets of these may be exhibited in different people, animals or machines. To ask 'which are necessary for real understanding?' is to attribute spurious precision to a concept of ordinary language. Hence there is no clear boundary between things that do and things that do not understand symbols. I shall list 'prototypical' characteristics of human use of symbols with understanding, and discuss conditions under which these characteristics might be found in machines. Then instead of answering either 'yes' or 'no' to the question whether suitably programmed computers can understand, we note that within the space of possible 'behaving systems' there are indefinitely many cases, some sharing more features with human minds, some fewer. The important task is to analyse the nature and the implications of these similarities and differences, and not to argue about which cases existing labels 'really' fit. The space of possible systems is not a continuum. There are many discontinuities that make a difference to functional capabilities. So we are not talking about differences in degree,</page><page sequence="3">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 63 like differences in speed or memory size, but differences in structure and function, like the difference between having eyes and not having them, or the difference between legs and wheels. There is not just one crucial division. There are very many differences between amoebas and people, no one of which is the 'essential' one which makes us conscious or intelligent, just as there is no one 'essential' difference between chess and football. Analysing complex capabilities and distinctions in the space of possible systems can help theoretical biology by presenting a framework for questions about the evolution of behaviour. It can help psychologists by clarifying the nature of the capabilities they are attempting to study. It can help computing science and artificial intelligence by identifying precise new engineering targets for the future. Philosophers can help by identifying confusions, gaps and errors in the analysis of capabilities we all know about, and extending the analysis to a far wider range of mental concepts. Ordinary philosophical analysis needs to be extended by adopting what Dennett [3] calls the design stance. For example, by analysing possible computational mechanisms instead ofjust behavioural or phenomenological analyses, we can hope to achieve theories with greater generative power, and therefore greater depth and clarity. II What is understanding a language? I use the word 'language' loosely as equivalent to 'notation', 'representational scheme', 'symbol system' etc. Very roughly, a language L is a system of symbols used by some agent U in relation to a world W. 'System' implies a generative notation, with compositional semantics. I use 'agent' without implying purposiveness. For now I want to leave it open whether the use of symbols presupposes purposiveness in all cases, though it obviously does in some. The word 'use' may be thought to imply purposiveness, but I intend it to be taken in the sense in which a plant uses oxygen, without having any purpose or intention. A voice-driven juke-box would relate spoken numbers to a world of records in a rack. The juke-box's limited behavioural repertoire makes it inappropriate to describe it as doing</page><page sequence="4">64 I-AARON SLOMAN anything more than relating certain symbols to objects. It does not, for instance, relate symbols to properties, relations or states of affairs. Its world W contains a very restricted class of events, and every symbol refers implicitly to that class, merely indicating which object (which record) should partake in the event. The sorts of symbol-users we'll be interested in will generally be far more sophisticated. They may be able to use symbols in L to refer not only to objects but also to different properties, relations, events, processes, or actions in W. And they can do different things with their symbols. In simple cases evidence that U uses a symbol S to refer to object O consists of bi-directional causal links. (a) Occurrences of S, manipulated by U, may cause U to do something involving O. For instance, finding 'the big red block' in an input string may cause U to pick up a certain block. (b) A happening involving O may cause U to do something with the symbol S. Sensors detecting that a certain block is moving might cause U to build a structure containing the string 'the big red block'. Symbol manipulations need not be externally detectable. A computing system may do things internally which cannot be inferred from its behaviour, and it may have neither tracing programs, nor access to an output medium capable of displaying the internal detail (see ch 10 of[13]). Unlike behaviourists I am talking about the very kind of internal behaviour which behaviourists try to analyse away. A full analysis would distinguish different kinds of: (a) symbol media, (b) grammatical rules (c) semantic rules (d) mechanisms for manipulating symbols, (e) symbol users, (f) worlds, and (g) purposes for which symbols might be used. This paper discusses only a subset of this rich array of possibilities. Symbols are structures that can be stored, compared with other structures, searched for, etc. They may be simple or complex (i.e. composed of parts which are symbols). They may be physical, like marks on a piece of paper, or virtual symbols, i.e. abstract structures in a virtual machine, like 5-D arrays in a computer (see [17]). They may be internal or external. They need not be separable physical objects or events, since a single travelling wave may 'carry' different symbols simultaneously, and a network of active computing nodes may have several patterns distributively superimposed in its current state in</page><page sequence="5">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 65 holographic fashion. A set of bits may represent one Godel number corresponding to a set of sentences. Symbols include maps, descriptions, representations, of all kinds, including computer programs, and non-denoting symbols, like parentheses and other syntactic devices. (In fact, anything at all can be used as a symbol.) The symbols need not be used for external communication. Meaning and understanding are often assumed (e.g. [8]) to be essentially concerned with communication between language users. As argued in [14], this is a mistake, since understanding an external language is secondary to the use of an internal symbolism for storing information, reasoning, making plans, forming percepts and motives, etc. This is prior in (a) evolutionary terms, (b) in relation to individual learning, and (c) insofar as the use of an external language requires internal computations. Representation is prior to communication. Objects in the world W may be concrete (e.g. physical objects) or abstract (e.g. numbers, grammatical rules). They may be external, or internal to U. W need not be uniquely decomposable into objects, relations, etc. E.g. a human torso is not uniquely decomposable. Like symbols, the objects may exist in a virtual world, embodied in a lower level world, like a virtual machine implemented in a lower level computer. Many programming languages refer to objects in a virtual world, such as lists, arrays, procedures, etc. Similarly social systems form a virtual world embedded in a psychological and physical world. III The structure of the concept 'understanding' A prototypical set of conditions for saying that U uses some collection of symbols as a language L referring to objects in a world W is presented below. Different combinations of conditions define different concepts of'language', 'meaning', 'understand- ing', etc. Asking which is the 'right' concept is pointless. Some are 'structural' conditions concerned with what mechanisms for understanding do. Some are 'functional' conditions, concerned with what understanding is used for, and how the mechanisms contribute to a larger functional architec- ture.</page><page sequence="6">66 I-AARON SLOMAN We can treat the conditions as a set of axioms implicitly defining 'use of symbols with understanding'. We'll see that events and processes in a computer can constitute a model for a significant subset of the axioms. Moreover, it is not just an abstract model. Unlike simulations of (e.g.) tornadoes, computer models of mental processes can have the same causal relations to the rest of the world as natural mental processes. People outside the model can relate to a machine model as to the real thing (though some may not wish to). A robot may obey commands, answer questions, teach you things. But a simulated tornado will not make you wet or cold. We'll see that computers can manipulate internal structures and use them as symbols associated with what Woods, in [21], calls a 'directly accessible' world W consisting of both entities within the machine and more abstract entities like numbers and symbol-patterns. (Cohen [2] also points this out.) Later, we discuss reference to an 'external' world. IV Prototypical conditions for U to use L to refer to W L is a set containing simple and complex symbols, the latter being composed of the former, in a principled fashion, according to syntactic rules. U associates some symbols ofL with objects in W, and other symbols with properties, relations, or actions in W. These conditions are satisfied by most computer languages, though machine codes generally have very simple syntax. A computer can associate 'addresses' (usually bit-patterns) with locations in its memory (possibly a virtual memory) and other symbols with their contents and relationships. The symbols cause processes to be directed to or influenced by specific parts of this internal 'world' W. Some of the symbols specify which processes-i.e. they name actions in W. Various sorts of properties and relations may be symbolised in a machine language, e.g. equality of content of addresses, neighbourhood in the machine, arithmetic relations, etc. Instructions have imperative meanings because they system- atically cause actions to occur. They may have independently</page><page sequence="7">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 67 variable components, e.g. object, instrument, manner, location, time, etc. If U is a computer and L its machine code, the semantic relation is causal: 'S refers to O for U' = 'S makes U's activities relate to or involve 0, and facts involving O affect U's use of S' where O may be an object, property, relation or type of action. Some objects referred to in world W may be abstract, e.g. numbers. Computers can use certain symbols to denote numbers because they are manipulated by arithmetical procedures and used as loop counters, address increments, array subscripts etc. (Compare [2].) Computers can count their own operations, or elements of a list that satisfy some test. This has much in common with a young child's understanding of number words-they are just a sequence of symbols used in certain counting activities ([13] ch. 7). What a complex symbol S expresses for U depends on its structure, its more primitive components and some set of interpretation rules related to the syntactic rules U uses for L. I.e. L has compositional semantics ([6]) This is true of many computer languages. E.g. what is denoted by a complex arithmetical expression, or a complex instruction, depends on what the parts denote, and how they are put together according to the syntactic rules of the language. A distinction can be made between the reference and the sense of symbols, i.e. between what they refer to and how they refer. A simple example to be found in computers would be the difference between two numerical expressions which necessarily denote the same number, but as the result of different calculations. Similarly, two expressions may access the same internal data but via different routes. It is sometimes suggested that real use of a language</page><page sequence="8">68 I-AARON SLOMAN requires that the mapping between symbols and objects be arbitary, e.g. unlike 'clouds mean rain'. This is partly true of computer languages. However, total arbitrariness would be inconsistent with compositional semantics, and the use of systematic names. U can treat the symbols of L as 'objects', i.e. can examine them, compare them, change them, etc., though not necessarily consciously. This applies to computers. Symbolic patterns used to refer can also be referred to, compared, transformed, copied, etc. It is not clear whether other animals can or need to treat their internal symbols as objects. This may be a pre-requisite for some kinds of learning. Certain symbols in L express conditionality. This underlies flexible and creative thinking, planning, or acting. We can distinguish (a) 'if used in conditional imperatives, (b) 'if used as the standard boolean (truth-functional) operator and (c) 'if used in conditional assertions. (c) is not found in the simplest computer languages. (a) and (b) are found in machines. By examining W, U can distinguish formulas in L that assert something true from those asserting something false. Computers typically use symbols for Boolean operations e.g. 'or', 'and', 'not' and two 'truth-values'. They are taken as truth- values partly because of their role in conditional imperatives. Truth-values can be assigned by examining internal states or arithmetical relations. U can detect that stored symbols contain errors and take corrective action. E.g. programs can attempt to eliminate wrong inferences derived from noisy data, e.g. in vision, and plan-executors can check whether the assumptions underlying a plan are still true. This supports a richer conception of a truth-value thanjust two arbitrary symbols.</page><page sequence="9">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 69 A complex symbol S with a boolean value may be used for different purposes by U, for instance: questioning (specifying information to be found), instructing (specifying actions), asserting (storing information for future use). S functions as a primitive question in a conditional instruction where action depends on the answer to the question. In low level machine languages there is not usually the possibility of using the same symbol to express the content of an imperative as in 'Make S true'. I.e. machine codes do not have 'indirect imperatives' with embedded propositions. However, AL planning systems do. (See [Boden 1978] for a survey). Most computer languages include requests and instructions, but not assertions. However, it is easy to allow programs to record results of computations or externally sensed data, or even results of self- monitoring. The symbol S may specify the content of an assertion in one context ('store (S)'), a question in another ('if S then . . .' or lookup (S)'), and an instruction in a third ('achieve (S)'). I.e. role is determined by use rather than form or content. (This mirrors the distinction between mental states and their contents.) U can make inferences by deriving new symbols in L from old ones, in order to determine some semantic relation (e.g. proofs preserve truth, refutations demonstrate falsity). Work in AI has demonstrated mechanisms for doing this, albeit in a restricted and mostly uncreative fashion so far. L need not be a fixed, static, system: it may be extendable, to cope with expanding requirements. Many computer languages are extendable. Adaptive dialogue systems are beginning to show how a machine may extend its own language according to need. But deep concept formation is still some way off. It is not clear which animals can and which cannot extend their internal languages. Without this, certain other forms of learning may be impossible. U may use symbols of L to formulate goals, purposes, or intentions; or to represent hypothetical possibilities for purposes of planning or prediction.</page><page sequence="10">70 I-AARON SLOMAN Simple versions of this sort of thing are AI planning systems. Only a system whose functional architecture supports distinctions between beliefs, desires, plans, suppositions, etc., can assign meanings in the way that we do. Merely storing information, and deriving consequences, or executing instructions, leaves out a major component of human understanding, i.e. that what we understand matters to us. For information to matter to a machine it must have its own desires, preferences, likes, dislikes, etc. This presupposes that there are modules whose function is to create or modify goals-motive generators. Full flexibility requires motive-generator generators. Deciding and planning require motive comparators and motive-comparator-generators. This is spelled out a little more in [15]. Motives generated internally over many years, refute the claim that a machine can exhibit only desires of the programmer or user. Such a machine would use symbols in L for its purposes. This is an important boundary in the space of possible behaving systems. Without this structure a machine might understand well enough to be a slavish servant, but could not be entrusted with tasks requiring creativity and drive, like managing a large company or minding children. L may be used for communication between individuals. This adds new requirements ([21]), which I shall not discuss, since representation is prior to communication. All the conditions so far listed for U to use a language L in relation to a world W are consistent with U being a computer. Several do not even require AI programs, since modern computers are built able to use symbols to refer to a world W containing numbers, locations in memory, the patterns of symbols found in those locations, properties and relations of such patterns, and actions that change W. V Does the computer really understand? Searle's claim that computers appear to understand only because people interpret the symbols, i.e. the process has only 'derivative' intentionality, ignores the fact that a substantial portion of the structure of the concept of 'using a symbol with a meaning' is exemplified even without AI programs. Associations</page><page sequence="11">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 71 between program elements and things in the computer's world define a primitive type of meaning that the computer itself attaches to symbols. Its use of the symbols has features analogous to simpler cases of human understanding, and quite unmatched by juke boxes. So it does not interpret symbols merely derivatively: the causal relations justify our using intentional descriptions, without anthropomorphism. To simulate or replicate human types of intentionality, including beliefs, desires, plans, fears, attention and self-consciousness, requires the embedding of individual mental processes in a suitable network of co- operative processes with intricate division of functions. In short, though structural requirements for at least the simplest sorts of understanding are relatively easy to achieve, functional requirements are harder. We know how to make mechanisms capable of producing intentional states. However, to be intentional processes like human mental processes, the symbol-manipulations must themselves have additional causal powers: the power to affect beliefs, desires, plans, and the actions they produce. This requires connections with additional procedures and data-bases concerned with the use of symbols in a manner characteristic of beliefs, desires, plans, etc. All this is possible even if W is a purely internal world, like the world of a dedicated, enthusiastic mathematician. VI Reference to inaccessible objects Machines can refer to their own internal states, to numbers, and to symbolic patterns, i.e. what Woods [21] calls a 'completely accessible' world because semantic links between symbols and things in this world are directly derived from simple causal links and the way symbols are used. In order to be useful as robots, or friends, machines will need to refer to external objects, events, locations, etc. The problem of external semantic linkage is harder to deal with. How can a system use symbols to desribe objects. properties, and relationships in a domain to which it has no direct access, and only incomplete evidence, so that it can never completely verify or falsify statements about the domain (like unobservables in physics)? Some external reference uses external causal links, such as sensors and motors. But direct links are often impossible,</page><page sequence="12">72 I-AARON SLOMAN e.g. referring to events remote in space and time, or even to hypothetical objects in hypothetical situations. What alternative types of semantic link might there be? A key idea is that implicit, partial, definitions (e.g. in the form of an axiom system) enable new undefined concepts to be addded to a language. (Compare [1]) on 'meaning postulates' and [21] on 'abstract procedures'.) A collection of axioms for Euclidean geometry, in the context of logical inference pro- cedures, can partially and implicitly define predicates like'line', 'point', 'intersects', etc. The axioms constrain the set of permissible models. Similarly, a congenitally blind person may attach meanings to colour words not too different from those of a sighted person, because much of the meaning resides in rich interconnections with concepts shared by both, such as 'surface', 'edge', 'pattern', 'stripe'. A Tarskian semantic theory does not, in general, allow meanings to be fully determinate, since it will always be possible (except in very simple cases) to add further axioms constraining the possible models, and adding precision to the meanings of the terms. It is also generally possible to add axioms postulating additional entities and new relations between those entities and the previous ones, just as science advances partly by postulating new sorts of entities: atoms, genes, etc. Combining our previously discussed internal causal links with Tarskian semantics, allows symbol-users to refer to their own internal states and also to very general possible states of possible worlds. This would permit mathematical thoughts and inventing possible physical universes and engaging in hypothetical reasoning about their inhabitants, properties, etc. Are external causal connections required for thoughts about particular objects in the environment? (Compare Woods, McDermott [16]) VII Causal links are required for reference to actual particulars No matter how many new symbols and axioms are added, Tarskian semantics will not of itself force the symbols to refer to any particular bit of reality rather any other actual or possible bit of reality which has a similar structure and a similar network of relationships.</page><page sequence="13">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 73 So the meanings defined simply by a set of axioms will always be totally universal, unless some of the symbols have a different sort of meaning, which attaches them to some individual portion of reality, for instance symbols whose connections enable a machine to refer to its own innards, as described above. Even without links through sensors and motors, an intelligent system might have symbols for a number of general relationships defined axiomatically, which could be used to express thoughts about how portions of the internal world are related to inaccessible objects. Examples of such relations are 'causes', 'before', 'inside', 'beyond'. How exactly 'cause' might be defined axiomatically is an old and unsolved problem. A sophisticated reasoning system might use the meta-level notion of a type of relationship whose detailed definition is not known, to build descriptions of relationships (of unknown types) between accessible objects and others (possibly of unknown types). Such a thinker might think of its own internal states as embedded in a larger structure, and start speculating about the properties of that structure, which it could refer to as: 'this world'. Symbols causally linked to input and output transducers (sensors and motors) would have the ability to anchor reference to external particulars. Another example would be the use of demonstratives like 'here' and 'now' (and implicit use of such things in tensed verbs), which are linked to portions of space and time merely through the spatio-temporal nature of the system using them. (Compare Evans [4].) Attachment to specific portions of reality can be inherited by axiomatically defined terms, provided the axioms link them to other terms which have a more direct link. This does not imply that the external descriptors are explicitly definable in terms of symbols describing 'sense-data' as phenomenalists have supposed. (For more on this see [13] chapter 9.) Moreover, the inherent indeterminacy of Tarskian meanings explained above can never be totally removed by links to symbols with more direct semantics. At best the indeterminacy will be partially reduced. For example, links between the concept 'electron' and what we can observe in a range of experiments leave it open for the concept to be further specified in the future by theoretical and empirical discoveries concerning the internal nature of electrons and their causal powers.</page><page sequence="14">74 I-AARON SLOMAN VIII Loop-closing semantics for no-propositional symbols I don't really believe that birds, baboons or babies use logic with Tarskian semantics to enable them to perceive and act on things in the world. Yet there is no doubt that many animals have rich mental lives including thoughts of external objects. Might something other than logical and propositional representations explain this? A generalisation of Tarskian semantics may be more generally applicable to intelligent systems. There is no reason to suppose that all internal representations must be propositional. There are good reasons for using a variety of forms of representations, including analogical representations such as diagrams, maps, ordered lists, etc. (See [17]). We can define a non-Tarskian model for the internal representations which play a role in percepts, beliefs plans, etc., namely an external environment which can coherently close the feedback loops. This notion of coherent causal closure will be relative to the system's ability to have precise and detailed goals and beliefs. How specific the mapping is between internal representations and external structures will depend on how rich and varied is the range of percepts, goals and action strategies the system can cope with. Like Tarskian semantics, 'loop-closing semantics' leaves meanings indeterminate. For any level of specification at which a loop-closing model can be found, there will be many consistent extensions to lower-levels of causal structure (in the way that modern physics extends the environment known to our ancestors), which remain adequate models in this sense. Even for a given level of description the internal representations may be more or less specific: for instance there will generally be infinitely many possible hidden extensions to visible portions of objects consistent with what you know about the world. Your friend may have warts under his shirt. The notion of loop-closing semantics presupposes a compu- tational architecture rich enough to support distinctions between different sorts of internal causal roles of symbols, in particular distinctions between (a) established beliefs (including percepts), (b) hypotheses awaiting confirmation, (c) goals, and</page><page sequence="15">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 75 (d) plans and instructions. It is far from obvious what sort of design can support such role distinctions, and the consequential loop-closing model theory. Some causal link is required if symbols are to refer to particular physical objects, like the Tower of London, or physical properties found in our world, such as magnetism. Without causal connections with the environment a thinker could only think (existentially quantified) thoughts about an abstract possible world, or very abstract and general thoughts about this world. External links differ in kind. Besides visual, tactile, and other sensory links it is possible to have communication with other agents via a keyboard or other devices. I believe these are also capable of pinning down reference. Causal links can be more or less direct, and can convey more or less rich information. Communication via another agent is indirect, and generally provides limited but abstract and general information, but it is still a causal link, like fossil records. So, using symbols to refer to an external world does not require that the world actually be directly sensed and acted on by the specific symbol-user. IX Extending 'mentalese': concept learning A language may be extended by the addition of new axioms and procedures, partially and implicitly defining some new primitive symbols, and modifying the meaning of old ones. The history of concepts of science and mathematics shows that not all newly- acquired concepts need be translatable into one's previous symbolism. After such learning, there is no clear functional distinction between the original concepts and the accreted language: we can memorise facts, formulas and instructions in English, instead of always having to translate into 'mentalese'. Hence, contrary to Fodor [5], different humans (or machines) may use different 'mentalese' even if they all started off the same. X The essential incompleteness of semantics We have seen that both Tarskian and loop-closing semantics</page><page sequence="16">76 I-AARON SLOMAN leave symbols with partially indeterminate meanings. Causal links like added axioms, reduce, but do not remove, the indeterminacy. This incompleteness is evident in theoretical concepts of science, but can also be demonstrated in ordinary concepts. In a sufficiently complex thinking system, even the language used for describing its own internal state will have this kind of indeterminateness and incompleteness, because of the problems of internal access sketched in chapter 10 of [13]. XI Can a computer distinguish 'true' and 'false'? It is not clear how to distinguish a 'true' from a 'false' boolean value, since formally they are symmetrical. The manual may say that 1 stands for 'true', but formally 1 could equally be interpreted as 'false', 0 as 'true', 'and' as 'or', 'or' as 'and' etc. Could there be an asymmetry in the use of the symbol for 'true' and the symbol for 'false'? One source of asymmetry lies in mechansims that check assertions, instead of always blindly assuming them correct: an elementary form of self-consciousness. 'True' might label a tendency to survive thorough checking. But the connection is not simple, for the result of checking may be wrong. A 'redundancy convention' could produce asymmetry. Instead of using explicit booleans, adopt a convention that one of the boolean indicators is redundant: it is signified merely by the presence of a formula in an information store or a communication. Given negation, 'true' and 'false' then both become redundant labels. A deeper asymmetry lies in connections between beliefs and autonomous motives. True beliefs are those which (generally) enable desires to be satisifed by rational planning. Again the connection is not simple, for a true belief can lead to a disastrous plan. XII Can understanding be truly duplicated, or only simulateda Many readers will object to the suggestion that if certain formal conditions are satisfied by the processes in a machine, then it understands. This has been called the 'Strong AI' thesis. A</page><page sequence="17">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 77 common way of arguing against it is to describe a process which conforms to the allegedly sufficient conditions yet clearly does not involve understanding. One supposed counter-example is a person who does not understand Chinese taking the place of a computer running a program allegedly capable of producing such understanding. Searle [ 11, 12] claims it would not if he were the person. Another type of example might be a subset of the atoms in a giant storm cloud, or some other randomly moving agglomeration-in principle some subset might happen to form a pattern which could be mapped onto the execution of a program. This would not mean that a storm-cloud had mental states. Another example might be a random number generator which happened to produce a succession of Gbdel numbers representing states of a machine following the program. Full discussion of these objections would require analysis of different ways in which a program may relate to processes which 'instantiate' it. Random connections clearly do not have the reliability required for a process which plays the role of understanding within an intelligent system. Though it is not so obvious, the same could be said of a process in whichJohn Searle acts as a computer. The lack of reliability would be due to the potential for Searle's motives, beliefs, distractions, tiredness, etc. to interfere with the running of the program. Thus the process would not satisfy the same set of counterfactual conditional descriptions as the process in a fully integrated intelligent system. A more complete discussion would show how certain sorts of local unreliability may be required, to allow more global processes to interrupt, modify, re-direct, or abort sub-processes if they do not conform to global requirements of the system. Thus local unreliability or unpredictability may enhance global coherence and reliability. This leads to the conclusion that not every process which happens to have the right formal properties would constitute understanding (or any other mental state). The underlying mechanisms and the relationships to other parts of the system must have the right causal properties. There is nothing to prevent a computer having those properties, as far as I know. But the alleged refutations of the Strong AI thesis involve</page><page sequence="18">78 I-AARON SLOMAN systems which don't have the right properties. So they are not refutations after all (compare [18]). If machines are to have mental states and processes of their own, they must have mechanisms with the right dispositional qualities. For example, merely having some kind of giant lookup table which enables an appropriate response to be produced in a very large set of possible situations would not be adequate. Ordinary understanding of a language involves having a capability with infinite generative power, not achievable by a finite condition-action table, even if the table was large enough to survive a lifetime of testing. Understanding involves having dispositions or capabilities which go beyond the behaviour actually produced. (Compare Cohen's distinction between 'simulated parotting' and 'simulated understanding' [2].) None of this proves the Strong AI thesis correct, of course. But it shows that setting up the right causal conditions for understanding (or other mental states) is not a trivial matter. Refutations of strong AI must address themselves to systems where the reliability conditions are satisfied, not just the formal conditions. XIII Conclusion A 'design stance' helps to clarify the question whether machines themselves can understand symbols in a non-derivative way. It is not enough that machines appear from the outside to mimic human understanding. there must be a reliable basis for assuming that they can display understanding in an open-ended range of situations, not all anticipated by the programmer. I have briefly described structural and functional design require- ments for this, and argued that even the simplest computers use symbols in such a manner that the machines themselves associate meanings of a primitive sort with them. I have shown that a computer may use symbols to refer to its own internal states and to abstract objects; and indicated how it might refer to a world to which it has only limited access, relying on the use of axiom-systems or perception-action loops to constrain possible interpretations. These constraints leave meanings partly indeterminate and indefinitely extendable. Causal links reduce but do not remove indeterminacy.</page><page sequence="19">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 79 The full range of meaningful uses of symbols by human beings requires a type of architectural complexity not yet achieved in AI systems. There is a complex set of prototypical conditions for understanding, different subsets of which may be exemplified in different animals or machines, yielding a large space of possible systems which we are only just beginning to explore. Our ordinary labels are not suited to drawing a definite global boundary within such a space. At best we can analyse the implications of many different boundaries, all very important. This requires a long term multi-disciplinary exploration.' 'Acknowledgements: The author has a fellowship from the GEC Research Laboratories, and has benefitted from discussions with members of and visitors to the Cognitive Studies Programme at Sussex University, especially Margaret Boden, Steve Torrance, and Bill Woods. REFERENCES [1] Carnap, R., Meaning and Necessity Pheonix Books 1956. [2] Cohen, L. J., 'Semantics and the computer metaphor, in R. Barcan Marcus, G. Dorn, P. Weingartner (eds) Logic Methodology and Philosophy of Science VII, Amsterdam: North-Holland, forthcoming. [3] Dennett, D. C. Brainstorms, Harvester Press 1978. [4] Evans, Gareth, The Varieties of Reference, Oxford University Press, 1982. [5] Fodor, J. A. The Language of Thought Harvester Press 1976. [6] Frege, G., Translations from the philosophical writings, ed. P. Geach and M. Black. Blackwell, 1960. [7] Hempel, C. G. 'The Empiricist Criterion of Meaning' in A. J. Ayer (Ed.) Logical Positivism, The Free Press, 1959. Originally in Revue Int de Philosophie, Vol. 4, 1950. [8] Lyons, John, Semantics Cambridge University Press, 1977. [9] Pap. A., An Introduction to the Philosophy ofScience, Eyre and Spottiswoode (Chapters 2-3), 1963. [10] Quine, W. V. O., 'Two Dogmas of Empiricism', in From a Logical Point of View, 1953. [11] Searle, J. R., 'Minds, Brains, and Programs', with commentaries by other authors and Searle's reply, in The Behavioural and Brain Sciences Vol 3 no 3, 417-457, 1980. [12] Searle, J. R., Minds Brains and Science, Reith Lectures, BBC publications, 1984. [13] Sloman, A., The Computer Revolution in Philosophy: Philosophy Science and Models of Mind, Harvester Press and The Humanities Press, 1978. [14] Sloman, A., 'The primacy of non-communicative language', in The Analysis of Meaning: Informatics 5, Proceedings ASLIB/BCS conference Oxford, March 1979, Eds: M. MacCafferty and K. Gray, Published by ASLIB. [15] Sloman, A. and M. Croucher, 'Why robots will have emotions' in Proc. IJCAI Vancouver 1981.</page><page sequence="20">80 I-AARON SLOMAN [16] Sloman, A., D. McDermott, W. A. Woods 'Panel Discussion: Under What conditions can a machine attribute meaning to symbols' Proc 8th International Joint Conference on AI, Karlsruhe, 1983. [17] Sloman, A., 'Why we need many knowledge representation formalisms', in Research and Development in Expert Systems, ed M. Bramer, Cambridge University Press, 1985. [18] Sloman, A., 'Strong strong and weak strong AI', AISB Quarterly, 1985. [19] Sloman, A., 'Did Searle attach strong or weak strong AI?', in A. Cohn and R. Thomas (eds) Proceedings AISB Conference, 1986. [20] Strawson, P. F., Individuals: An Essay in Descriptive Metaphysics, Methuen. 1959. [21] Woods, W. A., 'Procedural semantics as a theory of meaning', in Elements of Discourse Understanding Ed. A. Joshi, B. Webber, I. Sag, Cambridge University Press, 1981.</page><page sequence="21">WHAT SORTS OF MACHINES CAN UNDERSTAND THE SYMBOLS THEY USE? Aaron Sloman and L. Jonathan Cohen II-L. Jonathan Cohen Though I am in general rather sympathetic towards Professor Sloman's computationalist position, and also agree with much of the detail in what he has said, I think it would be best ifI confine my remarks here to a few main points on which I either disagree with his argument altogether or prefer to formulate it in a substantially different way. I When Searle' attacked the claims of what he called 'strong AI', he took those claims to be typified by the thesis, first, that in executing one of Schank and Abelson's programs' for answering questions about a story a computer can literally be said to understand the story and provide the answers to question, and, secondly, that what the computer and its program do explains the human ability to understand the story and answer questions about it. So strong AI, in Searle's conception of it, approaches the computer-mind analogy from both ends of the comparison and declares an identity in appropriate cases. On the one hand, it claims that computers can have cognitive states like under- standing: on the other, it claims that when humans are in such states they are computing. Searle argued against strong AI by proposing a thought- experiment. Imagine (it may well be true) that you are an English-speaker but totally ignorant of Chinese. Imagine also that you are locked in a room and given a set of rules, in English, for correlating one batch of Chinese writing from another in accordance with their respective shapes. These rules instruct 'J. R. Searle, 'Minds, brains and programs', The Behavioral and Brain Sciences 3, 1980, 417-457. 2 R. C. Schank and R. P. Abelson, Scripts, plans, goals and understanding, Hillsdale, N.J.: Erlbaum, 1977.</page><page sequence="22">82 II-L. JONATHAN COHEN you which set of Chinese symbols to output for each set that may be put into the room. Then so far as the Chinese writing is concerned, said Searle, you would be behaving like a computer: you would be performing computational operations on formally specified elements. But, though it might appear to a Chinese- speaker that you were answering questions and answering them intelligibly, you would in fact not be understanding them at all. So, Searle concludes, a suitably programmed computer that took your place and functioned likewise would also be understanding nothing. (Indeed, Searle's conclusion could have been rather stronger: such a computer would not even be behaving as if it must have understood the questions and answers.) Nor, therefore, could such a computer and its program provide a model for the psychological explanation of understanding. How does Sloman meet Searle's attack? Reversing the order in which Sloman presents his arguments in defence of strong AI, one could say that he offers two arguments specific to Searle's thought-experiment, and another argument that operates on a wider front. The first Searle-specific argument is just this: in describing his thought-experiment Searle was wrong to claim that the person in the locked room was behaving like a computer, because a computer would be systematically more reliable. The person's relative lack of reliability would be due to the potential for his motives, beliefs, distractions, tiredness, etc. to interfere with the running of his program, i.e. with his continuing to act in conformity with the English-language rules that he has been given. A local unreliability or unpredictability might enhance global coherence and reliability, says Sloman, but that would not be the situation with the person in the locked room. And Sloman's second Searle-specific argument is as follows. Searle, he says, supposes that the strong Al thesis envisages a type of program which merely provides a giant look- up table that enables an appropriate response to be produced in a very large set of possible circumstances. Instead, Sloman claims, a machine that is to have mental states and processes of its own must have infinite generative power, because understanding involves going beyond the behaviour actually produced. So, on Sloman's view, the person in Searle's locked room would differ from an intelligent computer not only in not being sufficiently reliable, but also in not possessing an infinite generative</page><page sequence="23">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 83 capacity. In short the fact that the person does not understand the Chinese texts that he is manipulating is irrelevant to the question whether appropriately programmed computers can understand linguistic texts, because he differs from such a computer both in respect of the manner in which he executes his program and in respect of the kind of program that he is executing. But these are not strong arguments. First, Sloman himself makes the point elsewhere that an intelligent computer should have its own desires, preferences, likes, dislikes, etc. So, if these sometimes generate errors or interruptions that make the execution of a program unreliable, there is a good excuse for the person in the locked room to perform unreliably in such respects. The person would be behaving more intelligently if he did so, rather than less. And, if the errors or interruptions were due (as anything caused by, say, tiredness would be due) to adventitious physical causes, then again we have no shortage of analogous breakdowns in the behaviour of computer hardware. Secondly, the person in the locked room could easily be instructed in such a way as to make the reply to each new question a little longer than the reply to the last question. All he would need for the purpose would be appropriately recursive syntactic rules for tacking shapes on to shapes, where these additional shapes had a meaning that Chinese speakers would accept as relevant. But despite having this infinite generative capacity he could still be rightly claimed by Searle not to understand the Chinese texts that he produced. Indeed, Sloman's line of reasoning here is scarcely well designed to be a source of support for strong AI. What Sloman is arguing is that there are good reasons why the person in the locked room should not be regarded as functioning like a computer. Yet, according to the second of the two claims made by strong AI, when people are carrying out any kind of intellectual performance they are computing. So, since under- standing and executing instructions to correlate various shapes in various ways is certainly to be regarded as an intellectual performance it must follow from strong AI that the person in the locked room is indeed functioning as a computer. Whatever the weakness in Searle's argument, it cannot usefully be criticised in</page><page sequence="24">84 II-L. JONATHAN COHEN the way that Sloman proposes, because Sloman's proposal implicitly gives away half of the position for which he purports to be contending. What would be the good of bolstering up the thesis that Schank and Abelson's machine understands its story if the price that had to be paid for this was an admission that human understanding is non-computational? I conclude therefore that Sloman's critique of Searle's thought-experiment does not succeed. If strong AI is to be defended against Searle's attack, some other strategy must be employed. II Sloman does have another line of defence for strong AI, and most of his paper is devoted to it. He argues that our ordinary concept of 'understanding' denotes a complex cluster of capabilities, and that different subsets of these may be exhibited in different people, animals or machines. Hence, he says, there is no clear boundary between things that do and things that do not understand symbols. Rather, there are indefinitely many cases, some sharing more features with human minds, some fewer. So the important task, on Sloman's view, is to analyse the nature and the implications of these similarities and differences, and not to argue about which cases existing labels 'really' fit. But here, as often elsewhere, an appeal to the vagueness or open texture of a term does not meet the underlying thrust of a philosophical argument if the argument is only superficially concerned with the semantic analysis of that term. In reality Searle's thought-experiment is concerned with facts, not with meanings, and can readily be taken as a contribution to the very research task that Sloman describes. What it seeks to show is that there is an important cognitive capacity which is easily recognisable as such by anyone but is not captured by programs like Schank and Abelson's despite the equivalence of externally observable manifestations. No matter what we call this capacity the fact that it exists is obviously relevant to the ambitions of current research in artificial intelligence and computational psychology. Searle used the term 'understanding' to denote the capacity but it would quite misinterpret his motivation to assume that his overall purpose was to analyse some supposedly precise or close-textured concept of understanding. Rather the</page><page sequence="25">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 85 thought-experiment was designed to support the much more general thesis that 'instantiating a computer program is never by itself a sufficient condition of intentionality.'3 Moreover what was particularly ingenious about this thought- experiment was that it appeared to present a direct refutation of the validity of Turing's classical test for telling whether a machine can think.'4 It apparently posited a situation very similar to the one that Turing envisaged, and then exploited a possibility that Turing had not taken into account. Turing's idea was that questions might be asked of a man and of a woman, in separate rooms, to determine which respondent was a woman and which a man; and if the interrogator decided that issue wrongly, on the basis of the answers, just as often when a machine took the place of the man as when the actual man was responding, then we could reasonably take such a machine to be thinking. Correspondingly Searle pointed out5 that the pro- grammed person in his locked room might get so good at following the instructions for manipulating the Chinese symbols, and the programmers might get so good at writing the programs, that from the point of view of anyone outside the locked rooms the person's answers might become absolutely indistinguishable from those of spontaneously responding native speakers of Chinese. Yet, as Searle rightly insisted, the person himself would know-in some quite central sense of 'under- stand'-that he understood none of the symbols or texts which he was manipulating. Similarly, suppose that the questions and answers in Turing's test were in printed Chinese, with the interrogator and the woman being Chinese-speakers, while the programmed person who eventually took the place of the machine knew no Chinese but only the high-level language in which the machine's programming instructions were written. We should then be quite unwilling to grant that this person was thinking about the subject-matter of the questions and we should therefore have no reason to grant that the machine had been doing so either. Anyone, therefore, who wishes to investigate the potential of the computational analogy in cognitive psychology, has to meet 'J. R. Searle, op. cit., 417. 'A. M. Turing, 'Computing Machinery and Intelligence', Mind lix, 1950, 433-460. 50Op. cit., 418.</page><page sequence="26">86 II-L. JONATHAN COHEN Searle's thought-experiment head-on. It is no use trying to shelter behind appeals to the vagueness or open texture of ordinary language. One needs to draw attention to relevant possibilities that Searle's story ignores, just as Searle exploited a possibility that Turing had ignored. And there are, I suggest, two such further possibilities inherent in the kind of situation to which Turing and Searle addressed themselves. III The first possibility begins to emerge when we note that in Searle's thought-experiment the written language of the questions and answers (Chinese) and the written language of the programming instructions (English) are conceived as being totally distinct: they are assumed not to coincide or overlap in respect of any of the words or symbols employed. Yet, ifwe were to look at the actual writing of contemporary Chinese and English native-speakers, we should find an important range of shared symbolism, viz. the Arabic numberals and certain internationally accepted signs for arithmetical operations (like addition) or relations (like being greater than). Of course, if the person in Searle's locked room were instructed to respond with the shape '7' whenever the shape 'PRINT 12-5' is presented, he could do this without understanding the arithmetical symbolism. But suppose, say, that his instructions permitted several answers of varying lengths for each Chinese question and that he had been instructed to respond to these questions in such a way that for every tenth question he should select, if possible, an answer that was equal in number of characters to the difference between the number of characters used in posing that question and the total number of characters used in posing the preceding nine questions. Then he would in any case have to have some knowledge of elementary arithmetic in order to obey his instructions correctly, and it would be rather wasteful to incorporate into those instructions a whole assembly of further instructions specifying which shapes to respond with when input of the form 'PRINT A-B' was presented. Why not let him use the arithmetical knowledge that he in any case has to have, in order to answer the questions at issue here too? Such a man will certainly understand the questions and their answers. In other words if we may move on to a thought-experiment that involves</page><page sequence="27">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 87 a somewhat more complex set-up than the Turing or Schank- and-Abelson ones we shall find that the situation need no longer be one in which the person who simulates a computer has to admit that he doesn't understand any of the questions or answers. Rather, if he is adequately programmed and can understand the program, then there must also be some questions and answers that he can understand, and if he can operate the program successfully then he can be assumed capable of exercising his intelligence in answering those questions without having any need to consult a look-up table. And, just as he manifests his understanding in what he does, so too a computer that behaves like him could reasonably be said at least to be behaving as if it understands. Moreover this is not just a matter of a thought-experiment in which we have to imagine a person or machine that has been programmed appropriately. There are familiar high-level programming languages, and familiar computers to implement them, that artificially generate understanding of the kind in question. Thus in Basic the very same symbol-type '7' that is used, or rather mentioned, as a mere shape in such formulas as 'PRINT '7' '" is also used as a numeral proper in such formulas as 'IF X &gt; 7 THEN GO LINE 30', which instructs execution of the instruction on line 30 if the value of X is greater than 7, or in a formula that sets up, say, an array of seven places. Compare how Russell argued that we cannot give a purely formal characteris- ation of arithmetical procedure because 'we want our numbers to be such as can be used for counting common objects'.' If a symbol is used by a computer to count certain features of the computer's own operation, then the computer operates thereby on some property of the symbol that is not a purely formal one.7 6 B. Russell, Introduction to Mathematical Philosophy, London: Allen and Unwin, 1919, p. 10. 7 These points and related ones were made in section IV of my lecture'Semantics and the Computational Metaphor' at the 1983 Congress for Logic, Methodology and Philosophy of Science in Salzburg. The text of the lecture was subsequently circulated widely as a preprint and is due to appear shortly in the proceedings of the Congress under the title R. Barcan Marcus, G. Dorn, and P. Weingartner (eds.), Logic, Methodology and Philosophy of Science VII (Amsterdam: North Holland) to which Sloman refers. No such point was made, however, in the paper by W. A. Woods, 'Procedural semantics as a theory of meaning', to which Sloman also refers in this connection, nor was it made anywhere else, so far as I know, prior to the Salzburg presentation.</page><page sequence="28">88 II-L. JONATHAN COHEN It is far too strong to say, as Searle does, that the computer has no idea that '4' means 4. We must therefore reject the familiar pair of theses, first, that a computer operates only on the formal properties of its symbols, and, second, that correspondingly, so far as the human mind works like a computer, it too must operate only on the formal properties of its representations. An interesting philosophical question then arises here about what it is that the computer, or the person acting like a computer, must be supposed to understand-or to be behaving as if it must understand-when operating on some non-formal property of a symbol (or sequence of symbols) in this way. Are we to say that the computer or computer-mimic understands the meaning of the symbol? Obviously this cannot be right if we adopt a theory of meaning that ties meaning somehow to a framework of social convention or shared practice. Those who are unwilling to grant that a symbol may be attributed semantic properties outside such a social framework will have to claim either that being usable for counting is not a semantic property at all or that the makers, programmers or users of the computer, rather than the computer itself, are the authors of whatever counting takes place. But all that need concern us here is that being usable for counting, say, or in subtraction, is certainly not a formal property, and this suffices to allow the construction of a thought-experiment that has contrary results to Searle's. The person in the locked room must now admit that he understands some of the input requests and some of the output responses as well as he understands the instructions in accordance with which he is to operate. Perhaps it is also worth noting, however, that if we want to we can conveniently operate here with semantic concepts that are sufficiently general to embrace both the familiar kind of meaningfulness that is embedded in ordinary human language through practice or convention and also the kind of meaning- fulness that may emerge in the input or output of an appropriately programmed computer when some of its symbols may be used to count certain features of its own operation. For example, Newell8 has suggested a definition of the term 8 A. Newell, 'Physical Symbol Systems, in D. A. Norman (ed.), Perspectives on Cognitive Science, Norwood, NJ: Ablex, 1981, 37-85.</page><page sequence="29">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 89 'designate' that might be applicable in any context whatever: viz. a symbol S designates an entity E for a computer or person x just so far as, when x takes S as input, x's behaviour depends on what E is. Obviously there is a great deal more that might be said on this particular issue, and Newell's suggestion touches only the tip of a rather large iceberg. But we should be wary of supposing that old distinctions, like those between 'syntax' and 'semantics' or between 'form' and 'content', must be capable of fitting-in wholly unaltered senses-such a radically new domain of discussion as that which computational technology presents. And in this connection it has to be emphasised that computers can operate on many other non-formal properties of their input than those directly concerned with counting or elementary arithmetical relations. For example, sets of numerals can be used to designate positions in an n-dimensional matrix, and by programming the computer to change these positions in real time it is possible to represent movement in respect of certain of its properties. And there are many other ways in which appropriately programmed computers can come to treat logical, mathematical or grammatical elements in their input as designating features instantiated in their own operation. IV Nevertheless, it may be said, even if Searle was wrong not to make an exception of logical, mathematical or grammatical understanding, he was surely right so far as concerns the understanding of any message about features of the computer's environment that are external to its own operations. In imitating the human ability to understand stories about people, for example, the Schank-Abelson program operates like a versatile parrot and does not even reproduce unmistakable behavioural manifestations of understanding, let alone under- standing itself. Consequently, it will be argued, the strong AI thesis has to be rejected and computational patterns of explanation have a very limited scope in cognitive psychology. But the answer to this is again to be sought in terms of an alternative thought-experiment in which the person in the closed room is instructed to notice the ways in which at some times the Chinese symbols supplied to him correlate with changing features of his immediate environment, and at other</page><page sequence="30">90 II-L. JONATHAN COHEN times these features come to change in ways that correlate with the Chinese symbols that he has just put out. Suppose too that one of the symbols that he receives or puts out from time to time always correlates with the appearance of a small item of food. Then it hardly needs much imagination to grant that whatever the meaning of this symbol to Chinese native-speakers it will-in Newell's sense of 'designate'-come to designate food for the person in the locked room. In other words we should not expect a computationally programmed person in Searle's thought-experiment to show any understanding at all about which Chinese symbols designate which features of his environment. After all, he has not been given any opportunity to learn how to do this. But it would nevertheless be reasonable to suppose that he would respond to appropriate lessons. Analogously, if a robot is controlled by a computer in accordance with predetermined goals, and if the computer is programmed to acquire from its input--by trial and error methods-the information about its environment that it needs in order to satisfy these goals, it will exhibit, when it exploits that acquisition, an appropriate analogue of under- standing. It might come to be able to go and recharge any empty batteries, for example, when supplied with data about the position of a terminal that can provide the electric power for this. Searle rejoins to this kind of objection that the addition of such 'perceptual' and 'motor' capacities adds nothing by way of understanding, in particular, or intentionality, in general, to Schank and Abelson's original program. He claims in fact that his original thought-experiment applies also to a case in which the computer can move about and has a television camera attached to it. Suppose, he says, that unknown to the person in the locked room some of the Chinese symbols that come to him come from a television camera attached to the robot and other Chinese symbols that he is giving out serve to make the motors inside the robot move the robot's legs or arms. Then the person in the locked room, Searle insists, is still just manipulating formal symbols. He may be receiving what is called information from what is called the robot's perceptual apparatus, and giving out what are called instructions to its motor apparatus. Nevertheless, says, Searle, he doesn't know what's going on: he</page><page sequence="31">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 91 still doesn't understand anything except the rules for manipu- lating symbols in accordance with their formal properties.' But Searle has stacked the deck here. By supposing that the television camera supplies the computer-mimic with Chinese symbols he has telescoped the whole situation. He has not inserted any analogue of language-learning into the thought- experiment, whereby, initially, symbols could come to be correlated with observed or televised features of the environment and subsequently behaviour appropriate to a particular feature could result from receipt of the corresponding symbol. So the thought-experiment that he has designed is one in which robot- mimics could not possibly have an opportunity to exhibit any analogue of linguistic understanding, whereas he could have designed a thought-experiment in which there was such an opportunity. V Thus we can conceive many different degrees or kinds of approximation to the human exhibition of full linguistic understanding. There are those that Sloman distinguishes, for example; there is the matching of output-form to input-form that is achieved in the execution of programs like Schank and Abelson's; or there is the behaviour of the robot in the imaginary circumstances that I have just described. I am not arguing, however, that every feature of mental activity is computerisable, even in relation to linguistic understanding. Indeed one might expect that every computer simulation will either obviously lack, or not obviously possess, some familiar feature of human understanding. At the least it will not obviously possess that conscious feeling of comprehension that is, if we are to believe one another, so pervasive an accompaniment of successful communication. And almost certainly it will lack many of the functional connections that characterise human mental processes. But, so long as our objectives are scientific ones, this should not matter. Science progresses by abstracting certain features from a certain type of entity and investigating their causes, effects or correlations. Studying the entity as a whole, via its supposed essence, has long since turned out to be a blind alley. 9 Op. cit. 420.</page><page sequence="32">92 II-L. JONATHAN COHEN For example, the development of classical mechanics in the seventeenth century required an implicit or explicit differen- tiation between the primary and the secondary qualities of physical particles, with primary qualities being abstracted as the subject-matter of the new theory and secondaries being treated as irrelevant to physical considerations. Similarly the normal objective of cognitive psychology is to describe or explain certain properties of human beliefs, judgements, understanding etc., not to find out what beliefs, judgements, understanding etc. 'really' are. The questions studied are, for example, how understanding grows with age, or howjudgements of probability or deducibility are reached, or how beliefs are affected by the order in which data are presented, or whether memories of word-lists tend to be filed by form or by meaning, or whether mental recall follows a FIFO or a LIFO principle, and so on. Consequently the point of the computational metaphor in cognitive psychology is (or ought to be) to provide a coherent conceptual framework within which rigorously testable answers may be formulated for questions such as these. It does not (or ought not to) function as an ontological assertion of identity. A computationalist hypothesis about probabilistic reasoning, for example, may be able to abstract altogether from problems about language-learning and speech-comprehension. That is, it may be possible to deploy some of the structure of the information-processing that is specific to such reasoning, even though it may not yet be possible to deploy the structure of the information-processing that lies at the root of linguistic under- standing. Of course, psychologically unsophisticated enthusiasts in the AI world have often claimed that appropriately programmed computers do have what can be described literally as cognitive states, and therefore that awareness of how such computers work provides an explanation of how these cognitive states occur. Computationalist psychology is then taken to be treating its hypothesis that the mind is a computer as an explanatory theory: the greater the number, or the wider the variety, of cognitive states that computers are capable of having, the more support there seems then to be for the hypothesis that the mind is actually a computer. What would refute the hypothesis would be the discovery of some cognitive state that computers could</page><page sequence="33">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 93 not have. And it is around this issue that the battle-lines between Dreyfusards and artificial intelligence enthusiasts are drawn. On the one side, self-consciousness, artistic creativity, etc are advanced as non-computerisable states of mind: on the other, programs, computer architectures, etc. are sketched that are claimed to constitute the very states of mind at issue. But the mind-computer identity hypothesis is not, or ought not to be regarded as, an explanatory scientific theory of the standard kind. It predicts no psychological data, and its only scientific value is as an epigrammatic formulation for the methodology of contemporary cognitive psychology. So dis- cussion of what computers can now do, or may one day be able to do, profits cognitive psychology not because it may provide further support for the mind-computer identity hypothesis, but because it may provide useful suggestions for suitably detailed hypotheses about how particular cognitive processes operate. It then needs appropriate experiment to determine which of these hypotheses provides the best explanation of the process in question. For example, hypotheses about underlying compu- tational mechanisms may predict a reaction time that varies with the complexity of the hypothesised mechanism: if appro- priate reaction-times are found, the corresponding hypothesis is confirmed. At any rate that provides a rough general indication of the role of the computational metaphor in cognitive psychology. In practice it may well be useful to introduce refinements. Thus Marr'o distinguished between various components in the AI side of such research: the isolation of a particular information- processing problem, the formulation of the problem in compu- tational terms, the construction of an algorithm that specifies how this computation may be carried out and a practical demonstration that the algorithm is successful. And all that, it must be emphasised, still produces only one possible explanation of how the information-processing operation in question takes place. Many alternative explanations-differently constructed programs or different computational architecture-might also be available, and would need to be eliminated before any one '0 D. Marr, 'Artificial Intelligence: A Personal View', in J. Haugeland (ed.), Mind Design. Philosophy, Psychology, Artificial Intelligence, Cambridge, Massachusetts: M.I.T. Press, 1981, 130.</page><page sequence="34">94 II-L. JONATHAN COHEN account can be regarded as the best available explanation of how the operation actually takes place. Nor is it clear that psychologists' experimental techniques are as yet fine-grained enough to carry out much of this necessary elimination. For example, it is as yet an experimentally unresolved issue whether tacit deductive reasoning follows a pattern of syntactic derivation, like that which Rips" has described (in computational terms), or exploits a system of semantic modelling, like that which Johnson-Laird'" has suggested (also in computational terms), or both, or neither. But we can at least be sure that some such experimental discrimination has an indispensable part to play in computational psychology. If there are indeed any AI workers who subscribe to the second thesis of what Searle calls 'strong AI', they are mistaken. Programs like Schank and Abelson's could not suffice to explain any human ability, let alone the particular ability that they were simulating, because no mere program could suffice for this. For contemporary cognitive scientists computer simulation and experimental tests are 'complementary ways to test the validity of psychological models'.13 In sum what I am suggesting is that computational psychology is a feasible research strategy that is not open to a priori disproof by philosophical arguments. Nor can it be proved correct by refutations of such arguments or by other philosphical reasoning. Instead particular computational hypotheses, about the mech- anisms that underlie particular mental processes, are-if appropriately formulated-open to simulated validation and experimental test. Moreover, even if some features of mental " L. J. Rips, 'Reasoning as a Central Intellective Ability' in R. J. Sternberg (ed.), Advances in the Study of Human Intelligence (vol. 2), Hillsdale, N.J. Erlbaum, 1984, 105-147. 2 P. N. Johnson-Laird, 'Mental Models in Cognitive Science', in D. A. Norman (ed.), Perspectives on Cognitive Science, Norwood: Ablex, 1981, 147-191, and 'Propositional Representation, Procedural Semantics and Mental Models', in J. Mehler, E. C. T. Walker, M. Garrett (eds.), Perspective on Mental Representation: Experimental and Theoretical Studies of Cognitive Processes and Capacities, Hillsdale: Erlbaum, 1982, 111-131. 3 G. Cohen, The Psychology of Cognition, London: Academic Press, 2nd ed. 1983, 221. It may be that this is what Searle means by 'weak AI'. But all he says about 'weak AI' is that according to it 'the principal value of the computer in the study of the mind is that it gives us a very powerful tool'. And that seems to be a substantially weaker claim than is implicit in the methodology of contemporary cognitive psychology, which takes computational models to be radically indispensable at the level of theoretical explanation.</page><page sequence="35">WHAT SORTS OF MACHINES UNDERSTAND SYMBOLS 95 activity turn out-at least for the moment-to resist explanation by any such hypotheses, that would be no reason in itself to reject computational explanations where these can already be seen to be applicable. The history of science has plenty of precedents for such gradual, feature by feature, progress in theoretical explanation. It would be different if an alternative and more successful research strategy were currently available (as it one day might be). But at present no other explanatory models, with comparable scope, are on the market. Behaviourism, for example, may tell us how reinforcements operate but not why they operate as they do. And science has always wisely refused to jettison a theory that does at least some work, until a better theory appears.</page></plain_text>