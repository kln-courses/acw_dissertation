<plain_text><page sequence="1">Published online 17 February 2003 rlB THE ROYAL 1: SOCIETY Computational approaches to motor learning by imitation Stefan Schaall,2*, Auke Ijspeert" 3 and Aude Billard' 4 'Computer Science &amp; Neuroscience, University of Southern California, 3641 Watt Way, Los Angeles, CA 90089-2520, USA 2ATR Human Information Sciences, 2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0218, Japan 3School of Computer and Communication Sciences, and 4School of Engineering, Swiss Federal Institute of Technology, Lausanne, CH 1015 Lausanne, Switzerland Movement imitation requires a complex set of mechanisms that map an observed movement of a teacher onto one's own movement apparatus. Relevant problems include movement recognition, pose estimation, pose tracking, body correspondence, coordinate transformation from external to egocentric space, match- ing of observed against previously learned movement, resolution of redundant degrees-of-freedom that are unconstrained by the observation, suitable movement representations for imitation, modularization of motor control, etc. All of these topics by themselves are active research problems in computational and neurobiological sciences, such that their combination into a complete imitation system remains a daunting undertaking-indeed, one could argue that we need to understand the complete perception-action loop. As a strategy to untangle the complexity of imitation, this paper will examine imitation purely from a computational point of view, i.e. we will review statistical and mathematical approaches that have been suggested for tackling parts of the imitation problem, and discuss their merits, disadvantages and underly- ing principles. Given the focus on action recognition of other contributions in this special issue, this paper will primarily emphasize the motor side of imitation, assuming that a perceptual system has already ident- ified important features of a demonstrated movement and created their corresponding spatial information. Based on the formalization of motor control in terms of control policies and their associated performance criteria, useful taxonomies of imitation learning can be generated that clarify different approaches and future research directions. Keywords: imitation; motor control; duality of movement generation and movement recognition; motor primitives 1. INTRODUCTION Movement imitation is familiar to everybody from daily experience: a teacher demonstrates' a movement, and immediately the student is capable of approximately repeating it. In addition to a variety of social, cultural and cognitive implications that the ability to imitate entails (cf. reviews in Piaget 1951; Tomasello et al. 1993; Meltzoff &amp; Moore 1994; Byrne &amp; Russon 1998; Rizzolatti &amp; Arbib 1998; Dautenhahn &amp; Nehaniv 2002), from the viewpoint of learning, a teacher's demonstration as the starting point of one's own learning can significantly speed up the learn- ing process, as imitation usually drastically reduces the amount of trial-and-error that is needed to accomplish the movement goal by providing a good example of a success- ful movement (Schaal 1999). Thus, from a computational point of view, it is important to understand the detailed principles, algorithms and metrics that subserve imitation, starting from the visual perception of the teacher up to issuing motor commands that move the limbs of the stud- ent. *Author for correspondence (sschaal@usc.edu). One contribution of 15 to a Theme Issue 'Decoding, imitating and influencing the actions of others: the mechanisms of social interaction'. Phil. Trans. R. Soc. Lond. B (2003) 358, 537-547 DOI 10.1098/rstb.2002.1258 Figure 1 sketches the major ingredients of a conceptual imitation learning system (Schaal 1999). Visual sensory information needs to be parsed into information about objects and their spatial location in an internal or external coordinate system; the depicted organization is largely inspired by the dorsal (what) and ventral (where) stream as discovered in neuroscientific research (Van Essen &amp; Maunsell 1983). As a result, some form of postural infor- mation of the movement of the teacher and/or 3D object information about the manipulated object (if an object is involved) should become available. Subsequently, one of the major questions revolves around how such infor- mation can be converted into action. For this purpose, figure 1 alludes to the concept of movement primitives, also called 'movement schemas', 'basis behaviours', 'units of action', 'macro actions', etc. (e.g. Arbib 1981; Sternad &amp; Schaal 1999; Sutton et al. 1999; Dauten- hahn &amp; Nehaniv 2002). Movement primitives are sequences of action that accomplish a complete goal- directed behaviour. They could be as simple as an elementary action of an actuator, e.g. 'go forward', 'go backward', etc., but, as discussed in Schaal (1999), such low-level representations do not scale well to learning in systems with many degrees of freedom. Thus, it is useful for a movement primitive to code complete temporal behaviours, like 'grasping a cup', 'walking', 'a tennis ser- ? 2003 The Royal Society 537</page><page sequence="2">538 S. Schaal and others Computational approaches to motor learning by imitation performance evaluation I I I I I I I I , \ I I I motor learning , system visual input perceptual Figure 1. Conceptual sketch of an imitation learning system. The right-hand side contains primarily perceptual elements and indicates how visual information is transformed into spatial and object information. The left-hand side focuses on motor elements, illustrating how a set of movement primitives competes for a demonstrated behaviour. Motor commands are generated from input of the most appropriate primitive. Learning can adjust both movement primitives and the motor- command generator. ve', etc. Figure 1 assumes that the perceived action of the teacher is mapped onto a set of existing primitives in an assimilation phase, also suggested in Demiris &amp; Hayes (2002) and Wolpert et al. (2003). This mapping process also needs to resolve the correspondence problem con- cerning a mismatch between the teacher's body and the student's body (Dautenhahn &amp; Nehaniv 2002). Sub- sequently, the most appropriate primitive is adjusted by learning to improve the performance in an accommo- dation phase. Figure 1 indicates such a process by high- lighting the better-matching primitives with increasing linewidths. If no existing primitive is a good match for the observed behaviour, a new primitive must be gener- ated. After an initial imitation phase, self-improvement, e.g. with the help of a reinforcement-based performance evaluation criterion (Sutton &amp; Barto 1998), can refine both movement primitives and an assumed stage of motor-command generation (see ? 2b) until a desired level of motor performance is achieved. In ?? 2 and 3, we will attempt to formalize the concep- tual picture of figure 1 in the context of previous work on computational approaches to imitation. Given that Rittscher &amp; Blake (2003) already concentrate on the per- ceptual part of imitation in this issue, our review will focus on the motor side in figure 1. 2. COMPUTATIONAL IMITATION LEARNING Initially, at the beginning of the 1980s, computational imitation learning found the strongest research interest in the field of manipulator robotics, as it seemed to be a promising route to automate the tedious manual program- ming of these machines. Inspired by the ideas of artificial intelligence, symbolic reasoning was the common choice to approach imitation, mostly by parsing a demonstrated movement into some form of 'if-then' rules that, when chained together, created a finite state machine controller (e.g. Lozano-Perez 1982; Dufay &amp; Latombe 1984; Levas &amp; Selfridge 1984; Segre &amp; DeJong 1985; Segre 1988). Given the reduced computational power available at this time, a demonstration normally consisted of manu- ally 'pushing' the robot through a movement sequence and using the proprioceptive information that the robot sensed during this guided movement as basis to extract the if-then rules. In essence, many recent robotics approaches to imitation learning have remained closely related to this strategy. New elements include the use of visual input from the teacher and movement segmentation derived from computer vision algorithms (Kuniyoshi et al. 1989, 1994; Ikeuchi et al. 1993). Other projects used data gloves or marker-based observation systems as input for imitation learning (Tung &amp; Kak 1995). Phil. Trans. R. Soc. Lond. B (2003) #1&lt;</page><page sequence="3">Computational approaches to motor learning by imitation S. Schaal and others 539 More recently, research on imitation learning has been influenced increasingly by non-symbolic learning tools, for instance artificial neural networks, fuzzy logic, statistical learning, etc. (Pook &amp; Ballard 1993; Dillmann et al. 1995; Hovland et al. 1996). An even more recent trend takes inspiration of the known behavioural and neuroscientific processes of animal imitation to develop algorithms for robot programming by demonstration (e.g. Arbib et al. 2000; Billard 2000; Oztop &amp; Arbib 2002) with the goal of developing a more general and less task-specific theory of imitation learning. It is these neural computation tech- niques that we will focus on in this review, as they offer the most to both biologically inspired modelling of imi- tation and technological realizations of imitation in arti- ficial intelligence systems. (a) A computational formalization of imitation learning Successful motor control requires issuing motor com- mands for all the actuators of a movement system at the right time and of correct magnitude in response to internal and external sensations and a given behavioural goal. Thus, the problem of motor control can generally be for- malized as finding a task-specific control policy v u(t) = r(z(t),t,a), (2.1) where u denotes the vector of motor commands, z the vector of all relevant internal states of the movement sys- tem and external states of the environment, t represents the time parameter, and a stands for the vector of open parameters that need to be adjusted during learning, e.g. the weights of a neural network (Dyer &amp; McReynolds 1970). We will denote a policy that explicitly uses a dependence on time as a nonautonomous policy, whereas a policy without explicit time dependence, i.e. u(t)= 7r(z(t),a), will be called autonomous. The formu- lation in equation (2.1) is very general and can be applied to any level of analysis, like a detailed neuronal level or a more abstract joint angular level. If the function Tr were known, the task goal could be achieved from every state z of the movement system. This theoretical view allows us to reformulate imitation learning in terms of the more for- mal question of how control policies, which we also call movement primitives, can be learned (or bootstrapped) by watching a demonstration. Crucial to the issue of imitation is a second formal element, an evaluation criterion that creates a metric of the level of success of imitation J= g(z(t),u(t),t). (2.2) Without any loss of generality, we will assume that the cost J should be minimized; particular instantiations of J will be discussed in the following paragraphs. In general, J can be any kind of cost function, defined as an accumu- lative cost over a longer time horizon as is needed for mini- mizing energy, or only over one instant of time, e.g. as needed when trying to reach a particular goal state. More- over, J can be defined on variables based in any coordinate system, e.g. external, internal or a mixed set of coordi- nates. The different ways of creating control policies and metrics will prove to be a useful taxonomy of previous approaches to imitation learning and the problem of imi- tation in general. Defining the cost J for an imitation task is a complex problem. In an ideal scenario, J should capture the task goal and the quality of imitation in achieving the task goal. For instance, the task goal could be to reach for a cup, which could be formalized as a cost that penalizes the squared distance between the hand and the cup. The tea- cher's demonstration, however, may have chosen a parti- cular form of reaching for the cup, e.g. in a strangely curved hand trajectory. Thus, faithful imitation may require adding an additional term to the cost J that penal- izes deviations from the trajectory the teacher demon- strated, depending on whether the objective of imitation is solely focused on the task or also on how to move to perform the task. Hence, the cost J quickly becomes a complex, hybrid criterion defined over various objectives. In biological research, it is often difficult to discover what kind of metric the student applied when imitating (Mataric &amp; Pomplun 1998; Nehaniv &amp; Dautenhahn 1999). (b) Imitation by direct policy learning The demonstrated behaviour can be used to learn the appropriate control policy directly by supervised learning of the parameters a of the policy (cf. equation (2.1)), i.e. a nonlinear map z -u u, employing an autonomous policy and using as evaluation criterion (cf. equation (2.2)) sim- ply the squared error of reproducing u in a given state z. For this purpose, the state z and the action u of the teacher need to be observable and identifiable, and they must be meaningful for the student, i.e. match the stud- ent's kinematic and dynamic structure (cf. Dautenhahn &amp; Nehaniv 2002). This prerequisite of observability, shared by all forms of imitation learning, imposes a serious con- straint since, normally, motor commands, i.e. kinetic vari- ables, and internal variables of the teacher are hidden from the observer. Although statistical learning has methods to uncover hidden states, e.g. by Hidden Markov Models, Kalman filters or more advanced methods (Arulampalam et al. 2002), we are not aware that such techniques have been applied to imitation yet. Thus, to instantiate a movement primitive from a dem- onstration, the primitive needs to be defined in variables that can be perceived, leaving only kinematic variables as potential candidates, e.g. positions, velocities and acceler- ations. Given that the output of a movement primitive has to be interpreted as some form of a command to the motor system, usually implying a desired change of state, move- ment primitives that output a desired velocity or acceler- ation can be useful, i.e. a 'desired time-derivative' of the state information2 that is used to represent the teacher's movement. Our generic formulation of a policy in equ- ation (2.1) can, therefore, be written more suitably as (2.3) From a control theoretical point of view, this line of reasoning requires that motor control be modular, i.e. has at least separate processes for movement planning (i.e. generating the right kinematics) and execution (i.e. gener- ating the right dynamics) (Wolpert 1997; Wolpert &amp; Kawato 1998). Figure 2 illustrates two classical examples (e.g. Craig 1986) of modular control in the context of imitation learn- ing and motor primitives. In figure 2a, the demonstrated Phil. Trans. R. Soc. Lond. B (2003) z(t) = 7r(z(t),t,a).</page><page sequence="4">540 S. Schaal and others Computational approaches to motor learning by imitation (a) (b) demonstrated behaviour Figure 2. Modular motor control with movement primitives, using (a) a movement primitive defined in internal coordinates, and (b) a movement primitive defined in external coordinates. behaviour is mapped onto a movement primitive that is defined in internal coordinates of the student: joint angu- lar coordinates 0 are a good candidate as they can be extracted from visual information, a problem addressed under the name of pose estimation in computer vision (Deutscher et al. 2000; Rittscher &amp; Blake 2003). Such internal coordinates can directly serve as desired input to a motor-command execution stage (cf. figure 1), here assumed to be composed of a feedback and a feed-forward control block (Kawato 1999). Alternatively, figure 2b illustrates the subtle but important change when movement primitives are rep- resented in external coordinates, i.e. a task-level represen- tation (Saltzman &amp; Kelso 1987; Aboaf et al. 1989). For instance, the acceleration of the fingertip in the task of pole balancing would be interpreted as a task-level com- mand issued by the movement primitive in external coor- dinates, by contrast to joint angular accelerations of the entire arm and body that would be issued by a movement primitive in internal coordinates. Most often, task-level representations are easier to extract from a demonstration, and have a more compact representation. Task-level rep- resentations can also cope with a mismatch in dynamic and/or kinematic structure between the teacher and the student-only the task state is represented, not the state of motor system that performs the task. Task-level imitation requires prior knowledge of how a task-level command can be converted into a command in internal coordinates, a classic problem in control theory treated under the name of inverse kinematics (Baillieul &amp; Martin 1990), but which has found several elegant solutions in neural computation in the recent years (Bullock et al. 1993; Guenther &amp; Bar- reca 1997; D'Souza et al. 2001). In summary, movement primitives for imitation learn- ing seem to be the most useful if expressed in kinematic coordinates, either in internal (e.g. joint, muscle) space (t) = r(z(t),t,a) (2.4) or in external (task) space X(t) = r(z(t),t,a). (2.5) Note that the formulations in equations (2.4) and (2.5) intentionally use z, the variable that represents all possible state information about the movement system and the environment as input, but only output a variable that is the desired change of state of the student in the selected coordinate system, i.e., x in external space, and 0 in internal space. By dropping the explicit time dependence on the right-hand sides of equations (2.4) and (2.5), both policy formulations can be made to be autonomous. Direct policy learning from imitation can now be reviewed more precisely in the context of the discussions of the previous paragraphs and figure 2. Direct policy learning in task space was conducted for the task of pole balancing with a computer-simulated pole (Widrow &amp; Smith 1964; Nechyba &amp; Xu 1995). For this purpose, a supervised neural network was trained on task-level data recorded from a human demonstration. Similarly, several mobile robotics groups adopted imitation by direct policy learning using a 'robot teacher' (Lin 1991; Hayes &amp; Demiris 1994; Dautenhahn 1995; Grudic &amp; Lawrence 1996). For example, the 'robot student' followed the 'robot teacher's' movements in a specific environment, mimicked its kinematic, task-oriented actions, and learned to associate which action to choose in which state. After- wards, the robot student had the same competence as the teacher in this environment. An impressive application of direct policy learning in a rather complex control system, a flight simulator, was demonstrated by Sammut et al. (1992). Kinematic control actions from several human subjects were recorded and an inductive machine learning algorithm was trained to represent the control with decision trees. Subsequently, the system was able to autonomously perform various flight manoeuvres. In all these direct policy-learning approaches, there is no need for the student to know the task goal of the teacher, i.e. equation (2.2) has only imitation-specific cri- teria, but no task-specific criteria. Imitation learning is greatly simplified in this manner. However, the student will not be able to undergo self-improvement unless an explicit reward signal, usually generated from a task- Phil. Trans. R. Soc. Lond. B (2003)</page><page sequence="5">Computational approaches to motor learning by imitation S. Schaal and others 541 specific optimization criterion, is provided to the student, as in approaches discussed in the following section. Another problem with direct policy learning is that there is no guarantee that the imitated behaviour is stable, i.e. can reach the (implicit) behavioural goal from all start configurations. Lastly, imitation by direct policy learning usually generates policies that cannot be re-used for a slightly modified behavioural goal. For instance, if reach- ing for a specific target was learned by direct policy learn- ing, and the target location changes, the commands issued by the learned policy are wrong for the new target location. Such a form of imitation of is often called 'indis- criminate imitation' or 'mimicking' as it just repeats an observed action pattern without knowledge about how to modify it for a new behavioural context. (c) Imitation by learning policies from demonstrated trajectories A teacher's demonstration usually provides a rather lim- ited amount of data, best described as 'sample trajector- ies'. Various projects investigated how a stable policy can be instantiated from such small amount of information. As a crucial difference with respect to direct policy learning, it is now assumed that the task goal is known (see the follow- ing examples), and the demonstrated movement is only used as a seed for an initial policy, to be optimized by a self-improvement process. This self-learning adjusts the imitated movement to kinematic and dynamic discrep- ancies between the student and the teacher, and addition- ally ensures behavioural stability. The idea of learning from trajectories was explored with an anthropomorphic robot arm for dynamic manipulation tasks, for instance learning a tennis forehand and the game of kendama ('ball-in-the-cup') (Miyamoto et al. 1996; Miyamoto &amp; Kawato 1998). At the outset, a human dem- onstrated the task, and his/her movement trajectory was recorded with marker-based optical recording equipment (OptoTrack). This process resulted in spatio-temporal data about the movement of the manipulated object in Cartesian coordinates, as well as the movement of the actuator (arm) in terms of joint angle coordinates. For imitation learning, a hybrid internal/external evaluation criterion was chosen. Initially, the robot aimed at indis- criminate imitation of the demonstrated trajectory in task space based on position data of the endeffector, while try- ing to use an arm posture as similar as possible to the demonstrated posture of the teacher (cf. D'Souza et al. 2001). This approximation process corrected for kinem- atic differences between the teacher and the robot and resulted in a desired trajectory for the robot's motion-a desired trajectory can also be conceived of as a nonauton- omous policy (Schaal et al. 2000). Afterwards, using manually provided knowledge of the task goal in form of an optimization criterion, the robot's performance improved by trial and error learning until the task was accomplished. For this purpose, the desired endeffector trajectory of the robot was approximated by splines, and the spline nodes, called via-points, were adjusted in space and time by optimization techniques (e.g. Dyer &amp; McRey- nolds 1970) until the task was fulfilled. Using this method, the robot learned to manipulate a stochastic, dynamic environment within a few trials. A spline-based encoding of a control policy is nonauton- omous, because the via-points defining the splines are parameterized explicitly in time. There are two drawbacks in using such nonautonomous movement primitives. First, modifying the policy for a different behavioural context, e.g. a change of target in reaching or a change of timing and amplitude in a locomotion pattern, requires more complex computations in terms of scaling laws of the via- points (Kawamura &amp; Fukao 1994). Second, and more severely, nonautonomous policies are not very robust in coping with unforeseen perturbations of the movement. For instance, when abruptly holding the arm of a tennis player during a forehand swing, a nonautonomous policy would continue creating desired values for the movement system, and, owing to the explicit time dependency, these desired values would increasingly more open a large gap between the current position and the desired position. This gap can potentially cause huge motor commands that fight the advert perturbation, and, if the arm were released, it would 'jump' to catch up with the target tra- jectory; a behaviour that is undesirable in any motor sys- tem as it leads to potential damage. By contrast, autonomous movement primitives can avoid this behav- iour as the output of the policy is solely state and not time dependent, and perturbations can create inhibitive terms in the policy that ensure that the planned movement of the policy will never deviate too much from the actual pos- ition. In this vein, Ijspeert et al. (2002a,b) suggested the use of autonomous dynamical systems as an alternative to spline-based imitation learning, realizing that equations (2.4) and (2.5) are nothing but nonlinear differential equations. In their approach, a demonstrated trajectory is encoded by learning the transformation from a simple canonical attractor system to a new nonlinear attractor landscape that has the demonstrated trajectory as its unique attractor. Both limit cycle or point attractors could be realized, corresponding to rhythmic or discrete move- ment primitives. The evaluation criterion for imitation was the deviation of the reproduced trajectory from the dem- onstrated one, either in internal or external space-reach- ing the target of the movement, i.e. either a point or a limit cycle, is automatically guaranteed by shaping the attractor landscape appropriately. The dynamic systems policies were designed to provide a spatial and temporal invariant, i.e. a qualitatively similar movement will always lead to a similarly parameterized movement primitive, irrespective of the timing of the movement and the target to which the movement was executed. Coupling terms to the differen- tial equations allowed natural robustness towards external perturbations (see also Hatsopoulos (1996)). The effec- tiveness of imitation learning with these dynamic systems primitives was successfully demonstrated on a humanoid robot that learned a series of movements such as tennis forehand, tennis backhand and drumming sequences from a human teacher (figure 3), and that was subsequently able to re-use the learned movement in modified behav- ioural contexts. Another, more biologically inspired, dynamic systems approach to imitation was pursued by Billard and col- leagues (Billard 2000; Billard &amp; Mataric 2001; Billard &amp; Schaal 2001). Joint angular trajectories, recorded from human demonstrations, were segmented using zero velo- city points. The policy approximated the segment for each joint movement by a second-order differential equation Phil. Trans. R. Soc. Lond. B (2003)</page><page sequence="6">542 S. Schaal and others Computational approaches to motor learning by imitation (a) (b) Figure 3. Four frames of a tennis swing over time, progressing from the top downwards. (a) Teacher demonstration of a tennis swing; (b) imitated movement by the humanoid robot. that activated a pair of antagonistic muscles, modelled as spring-damper systems (Lacquaniti &amp; Soechting 1986). Owing to the dynamic properties of muscles, this policy generates joint angle trajectories with a bell-shaped velo- city profile similarly to human motion; the initial flexion or extension force determines entirely the trajectory and is computed using the initial acceleration of the demon- strated trajectory segment. After acquiring this movement, . . . . . . . . . . . pRlmltlVe lmltatlOn learnlNg 1S used to comblne olnt tra- jectory segments to produce whole body motion. For this purpose, a time-delay recurrent neural network is trained to reproduce the sequential activation of each joint, similar to methods of associative memory (Schwenker et al. 1996). Both speed and amplitude of movement that can be modulated by adjusting appropriate parameters in the network. This imitation system can generate complex movement sequences (figure 4) and even 'improvise' movement by randomly activating nodes in the associat- ive memory. (d ) Imitation by model-based policy learning A third approach to learning a policy from imitation employs model-based learning (Atkeson &amp; Schaal 1997a; Schaal 1997). From the demonstrated behaviour, not the policy but a predictive model of the task dynamics is approximated (cf. Wolpert et al. 1998). Given knowledge of the task goal, the task-level policy of the movement primitive can be computed with reinforcement learning procedures based on the learned model. For example, Atkeson &amp; Schaal (Atkeson &amp; Schaal 1 997a,b; Schaal 1997) showed how the model-based approach allowed an anthropomorphic robot arm to learn the task of pole- Phil. Trans. R. Soc. Lond. B (2003)</page><page sequence="7">Computational approaches to motor learning by imitation S. Schaal and others 543 ~~~~~(a)~~~~~~~~~~~~~~~~ ----- - .) = - ...-.-..----.=. =. .-........ . . - - ~~~~~~~~~I~~~~~~~H '' ,,,,, ... .. ... .. .. ~~~ ~~ ~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...... .. , _ . ! 1 I ~"" ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~.-.::::~ ............. ... . . - , C' , , 'ir lu:- .... i Jr ~ .?.....' ' -- ! - I "i1" Il _- - -~:~ - ------- _. _ _ l_ .'---;- ~ =b) E ! _ '?| * W _ - - u a - ---- ---- Figure 4. Leaming of movement sequences by imitation. (a) Teacher demonstrates movement sequence; (b) imitated movement by the humanoid robot. balancing in just a single trial, and the task of a 'pendulum swing-up' in only three to four trials. These authors also demonstrated that task-level imitation based on direct pol- icy learning, augmented with subsequent self-learning, can be rather fragile and does not necessarily provide signifi- cant learning speed improvement over pure trial-and-error learning without a demonstration. (e) Matching of demonstrated behaviour against existing movement primitives The approaches discussed in the previous sections illus- trated some computational ideas for how novel behaviours can be learned by imitation. Interesting insights into these methods can be gained by analysing the process of how a perceived behaviour is mapped onto a set of existing primitives. Two major questions (Meltzoff &amp; Moore 1997) are: what is the matching criterion for recognizing a behaviour; and in which coordinate frame does matching take place? (i) Matching based on policies with kinetic outputs If only a kinetic control policy of the movement primi- tive exists (cf. equation (2.1)), finding a matching criterion becomes difficult because kinetic outputs such as forces or torques cannot be observed from demonstrations. One solution would be to execute a primitive, observe its out- come in either internal or external kinematic space, and generate in the chosen coordinate frame a performance criterion based on the similarity between the executed and the teacher's behaviour, e.g. the squared difference of state variables over time or distance to a goal at the end of the movement. This procedure needs to be repeated for every primitive in the repertoire and is thus quite inefficient. Given that kinetic policies are also not very useful for learning novel movements by imitation (cf. ? 2b), kinetic policies seem to be of little use in imitation learning. (ii) Matching based on policies with kinematic outputs If the primitive outputs observable variables, e.g. kinem- atic commands as in equations (2.4) and (2.5), matching is highly simplified because the output of the primitive can be compared directly with the teacher's performance. Such kinematic matching assumes that the motor execution stage of figure 2 creates motor commands that faithfully realize the kinematic plans of the primitive, i.e. that motor-command generation approximately inverts the dynamics of the movement system (Kawato 1999). At least two forms of matching mechanisms are possible. One matching mechanism simply treats the demon- strated movement as a candidate for a new movement primitive and fits the parameterization of this primitive. The parameters are subsequently compared with the para- meters of all previously learned primitives, and the best matching one in memory is chosen as the winner. For this method to work, the parameterization of the movement primitive should have suitable invariances towards vari- ations of a movement, e.g. temporal and spatial scale invariance. The via-point method of Miyamoto et al. (1996) can easily be adapted for such movement recog- nition, as via-points represent a parsimonious para- meterization of a movement that is easily used in classification algorithms, e.g. nearest neighbour methods (Wada &amp; Kawato 1995). Similarly, the dynamic systems approach to motor primitives of Ijspeert et al. (2002b) cre- ates a movement parameterization that affords classi- fication in parameter space-indeed, the in-built scale and time invariances of this technique adds significant robust- ness to movement recognition in comparison to methods. The second matching paradigm is based on the idea of predictive forward models (Miall &amp; Wolpert 1996; Schaal 1997; Atkeson &amp; Schaal 1997a; Wolpert et al. 1998, 2003; Demiris &amp; Hayes 2002). While observing the teacher, each movement primitive can try to predict the temporal evol- Phil. Trans. R. Soc. Lond. B (2003)</page><page sequence="8">544 S. Schaal and others Computational approaches to motor learning by imitation ution of the observed movement based on the current state z of the teacher. The primitive with the best prediction abilities will be selected as the best match. If, as mentioned above, the motor execution stage of the control circuit (figure 2) faithfully realizes the movement plan issued by a movement primitive, the primitive can act itself as a forward model, i.e. it can predict a change in state z of the teacher (cf. equations (2.4) and (2.5)). Alter- natively, it is also possible to include prediction over the entire dynamics of the movement system. For this pur- pose, the output of the movement primitive is fed to the motor-command execution stage, whose output is sub- sequently passed through a predictive forward model of the dynamics of the student's movement system (see Demiris &amp; Hayes 2002; Wolpert et al. 2003), thus pre- dicting the change of state of movement without actually performing it. This technique will work even when the motor execution stage is less accurate in realizing desired movement kinematics, but it comes at the cost of two more levels of signal processing, i.e. the simulated motor- command generation and the need for a forward model of the motor system. Demiris &amp; Hayes (2002) realized such an imitation system in a simulated humanoid. What is particularly noteworthy in these approaches to movement recognition is the suggested bi-directional interaction between perception and action: movement rec- ognition is directly accomplished with the movement- generating mechanism. This concept is compatible with the concept of mirror neurons in neurobiology (Rizzolatti et al. 1996; Rizzolatti &amp; Arbib 1998), with the simulation theory of mind reading (Gallese &amp; Goldman 1998), and it also ties into other research projects that emphasize the bi-directional interaction of generative and recognition models (Dayan et al. 1995; Kawato 1996) in unsupervised learning. Such bi-directional theories enjoy an increasing popularity in theoretical models to perception and action as they provide useful constraints for explaining the auton- omous development of such system. (iii) Matching based on other criteria Exploiting the literature on computer vision and statisti- cal classification, a large variety of alternative approaches to movement recognition can be developed, mostly with- out taking into account mutuality criteria between move- ment generation and movement recognition. Rittscher &amp; Bake (2003) provide an overview of techniques in this vein. (f) The correspondence problem An important topic of imitation learning concerns how to map the external and internal space of the teacher to the student, often called the 'correspondence problem' (Alissandrakis et al. 2002; Byrne 2003). Solving corre- spondence in external space is usually simplified, as exter- nal coordinates (or task coordinates) are mostly independent of the kinematic and dynamic structure of the teacher. For instance, if pole balancing could be dem- onstrated by a dolphin, a human student could imitate despite the mismatch in body structure if only task-level imitation is attempted-the only transformation needed is a mapping from the teacher's body-centred external space to the student's body-centred external space, which is just a linear transformation. Correspondence in internal space is a more complex problem. Even when teacher and stud- ent have the same degrees of freedom, as it is the case with human-to-human or human-to-humanoid-robot imi- tation, the bodies of student and teacher are bound to differ in many ways, including in their ranges of motion, in their exact kinematics, and their dynamics. The map- ping is even more difficult when the teacher and student have dissimilar bodies. In that case, the student can only imitate approximately, reproducing only sub-goals or sub- states of the demonstrated motion. The correspondence problem consists of defining which sub-states of the motion can and/or should be reproduced. Dautenhahn &amp; Nehaniv (2002) proposed a general mathematical frame- work to express such a mapping function in terms of trans- fer functions across different spaces. Alissandrakis et al. (2002) implement this framework to solve the correspon- dence problem in a chess game case study. The movement of two chess pieces (e.g. queen and knight) are directed by very different rules such that the two pieces cannot replicate each other's move in just one time step. For the knight to replicate the trajectory followed by the queen, it must define several sub-goals (positions on the chessboard) through which the queen has travelled and that the knight can reach using its own movement capacities. The best strategy to define the sub-goals depends on the metric applied to measure the imitation performance. The authors compare metrics that minimize either the total number of moves required for the reproduction, or the space covered during the reproduction by the motion. (g) Imitation of complex movement sequences One final issue concerns the imitation of complex motor acts that involve learning a sequence of primitives and when to switch between them. In this context, Fagg &amp; Arbib (1998) provided a model of reaching and grasping based on the known anatomy of the fronto-parietal cir- cuits, including the mirror neuron system. Essentially, their model employed a recurrent neural network that sequenced and switched between motor schemas based on sensory cues. The work of Billard and colleagues (Billard 2000; Billard &amp; Mataric 2001; Billard &amp; Schaal 2001; ? 2c) follows a similar vein, just at a higher level of biologi- cal abstraction and more suitable for the control of real, complex robotic systems. In a robotic study, Pook &amp; Ballard (1993) used hidden Markov models to learn appropriate sequencing from demonstrated behaviour for a dexterous manipulation task. There is also large body of literature in the field of time-series segmentation (Cacciatore &amp; Nowlan 1994; Weigend et al. 1995; Pawel- zik et al. 1996) that employed competitive learning and forward models for recognition and sequencing in a way that is easily adapted for imitation learning as illustrated in figure 1. 3. SUMMARY Using the formalization of motor control in terms of generating control policies under a chosen performance criterion, we discussed computational imitation learning as methodology to bootstrap a student's control policy from a teacher's demonstration. Different methods of imi- tation were classified according to which variables were assumed observable for the student, whether variables Phil. Trans. R. Soc. Lond. B (2003)</page><page sequence="9">Computational approaches to motor learning by imitation S. Schaal and others 545 were of kinetic or kinematic nature, whether internal, external coordinates, or both were used during demon- stration, and whether the task goal was explicitly known to the student or not. Additional insights could be obtained by discussing how a demonstrated movement can be mapped onto a set of existing movement primitives. Important topics in computational imitation concerned the formation of motor primitives, their representation, their sequencing, the reciprocal interaction of movement recognition and movement generation, and the correspon- dence problem. At the current stage of research, all these issues have been modelled in various ways, demonstrating an increasingly growing formal understanding of how imi- tation learning can be accomplished. Among the most cru- cial missing points to be addressed in imitation is presumably a formalization of extracting the intent of a demonstrated movement. Billard &amp; Schaal (2002) sug- gested some initial ideas towards this goal by modelling the probability distribution over manipulated objects by the teacher, which triggered appropriate imitation behav- iour in a humanoid robot. However, a more abstract rep- resentation of task goals, perhaps as a set of generic goal taxonomies, is needed to make further progress in this area. This work was made possible by awards, nos. 9710312/ 0010312 and 0082995, of the National Science Foundation, award AC no. 98-516 by NASA, an AFOSR grant on Intelli- gent Control, the ERATO Kawato Dynamic Brain Project funded by the Japanese Science and Technology Agency, and the ATR Human Information Processing Research Labora- tories. ENDNOTES 'For this paper, only visually mediated imitation will be considered, although, at least in humans, verbal communication can supply important additional information. 2Note that instead of a formulation as a differential equation, we would also choose a difference equation, i.e. where a desired 'next state' is the output of the policy, not a desired change of state. REFERENCES Aboaf, E. W., Drucker, S. M. &amp; Atkeson, C. G. 1989 Task- level robot learing: juggling a tennis ball more accurately. In Proc. IEEE Int. Conf Robotics Automation, Scottsdale, AZ, 14- 19 May, pp. 331-348. Piscataway, NJ: IEEE. Alissandrakis, A., Nehaniv, C. L. &amp; Dautenhahn, K. 2002 Imitating with ALICE: learning to imitate corresponding actions across dissimilar embodiments. IEEE Transact. Sys- tems, Man &amp; Cybernetics, Part A: Systems Hum. 32, 482-496. Arbib, M. A. 1981 Perceptual structures and distributed motor control. In Handbook of physiology, section 2: The nervous sys- tem, vol. II, Motor control, part 1 (ed. V. B. Brooks), pp. 1449-1480. Bethesda, MD: American Physiological Society. Arbib, M. A., Billard, A., Iacoboni, M. &amp; Oztop, E. 2000 Syn- thetic brain imaging: grasping, mirror neurons and imitation. Neural Networks 13, 975-997. Arulampalam, S., Maskell, S., Gordon, N. &amp; Clapp, T. 2002 A tutorial on particle filters for on-line non-linear/non-Gaus- sian Bayesian tracking. IEEE Trans. Signal Processing 50, 174-188. Atkeson, C. G. &amp; Schaal, S. 1997a Learning tasks from a single demonstration. In IEEE Int. Conf. Robotics Automation (ICRA97), Albuquerque, NM, 20-25 April 1997, vol. 2, pp. 1706-1712. Piscataway, NJ: IEEE. Atkeson, C. G. &amp; Schaal, S. 1997b Robot learning from dem- onstration. In Machine learning: Proc. 14th Int. Conf. (ICML '97), Nashville, TN, 8-12 July 1997 (ed. D. H. Fisher Jr), pp. 12-20. San Mateo, CA: Morgan Kaufmann. Baillieul, J. &amp; Martin, D. P. 1990 Resolution of kinematic redundancy. In Proc. Symp. Appl. Math., San Diego, CA, May 1990, vol. 41, pp. 49-89. Providence, RI: American Mathematical Society. Billard, A. 2000 Learning motor skills by imitation: a biologi- cally inspired robotic model. Cybern. Systems 32, 155-193. Billard, A. &amp; Mataric, M. 2001 Learning human arm move- ments by imitation: evaluation of a biologically-inspired architecture. Robotics Autonomous Systems 941, 1-16. Billard, A. &amp; Schaal, S. 2001 A connectionist model for on- line robot learning by imitation. In IEEE Int. Conf. Intell. Robots Systems (IROS 2001), Maui, Hawaii, 29 October-3 November 2001. Billard, A. &amp; Schaal, S. 2002 Computational elements of robot learning by imitation. In Am. Math. Soc. Central Section Meeting, Madison, WI, 12-13 October 2002. Providence, RI: American Mathematical Society. Bullock, D., Grossberg, S. &amp; Guenther, F. H. 1993 A self- organizing neural model of motor equivalent reaching and tool use by a multijoint arm. J. Cogn. Neurosci. 5, 408-435. Byrne, R. W. 2003 Imitation as behavior parsing. Phil. Trans. R. Soc. Lond. B358, 529-536. (DOI 10.1098/rstb.2002. 1219.) Byrne, R. W. &amp; Russon, A. E. 1998 Learning by imitation: a hierarchical approach. Behav. Brain Sci. 21, 667-721. Cacciatore, T. W. &amp; Nowlan, S. J. 1994 Mixtures of control- lers for jump linear and non-linear plants. In Advances in neural information processing systems 6 (ed. J. D. Cowen, G. Tesauro &amp; J. Alspector), pp. 719-726. San Mateo, CA: Morgan Kaufmann. Craig, J. J. 1986 Introduction to robotics. Reading, MA: Addison-Wesley. D'Souza, A., Vijayakumar, S. &amp; Schaal, S. 2001 Learning inverse kinematics. In IEEE Int. Conf. Intell. Robots Systems (IROS 2001), Hilton Head Island, SC, 13-15 June 2000. Dautenhahn, K. 1995 Getting to know each other-artificial social intelligence for autonomous robots. Robotics Auton- omous Systems 16, 333-356. Dautenhahn, K. &amp; Nehaniv, C. L. (eds) 2002 Imitation in ani- mals and artifacts. Cambridge, MA: MIT Press. Dayan, P., Hinton, G. E., Neal, R. M. &amp; Zemel, R. S. 1995 The Helmholtz machine. Neural Comput. 7, 889-904. Demiris, J. &amp; Hayes, G. 2002 Imitation as a dual-route process featuring predictive and learning components: a biologically plausible computational model. In Imitation in animals and artifacts (ed. K. Dautenhahn &amp; C. L. Nehaniv), pp. 327- 361. Cambridge, MA: MIT Press. Deutscher, J., Blake, A. &amp; Reid, I. 2000 Articulated body motion capture by annealed particle filtering. In IEEE Com- put. Vision Pattern Recognition (CVPR 2000). Piscataway, NJ: IEEE. Dillmann, R., Kaiser, M. &amp; Ude, A. 1995 Acquisition of elementary robot skills from human demonstration. In Int. Symp. Intell. Robotic Systems (SIRS'95), 10-14 July 1999, Pisa, Italy, pp. 1-38. Dufay, B. &amp; Latombe, J. C. 1984 An approach to automatic robot programming based on inductive learning. Int. J. Robot. Res. 3, 3-20. Dyer, P. &amp; McReynolds, S. R. 1970 The computation and theory of optimal control. New York: Academic Press. Fagg, A. H. &amp; Arbib, M. A. 1998 Modeling parietal-premotor interactions in primate control of grasping. Neural Networks 11, 1277-1303. Phil. Trans. R. Soc. Lond. B (2003)</page><page sequence="10">546 S. Schaal and others Computational approaches to motor learning by imitation Gallese, V. &amp; Goldman, A. 1998 Mirror neurons and the simulation theory of mind-reading. Trends Cogn. Sci. 2, 493-501. Grudic, G. Z. &amp; Lawrence, P. D. 1996, Human-to-robot skill transfer using the SPORE approximation. In Int. Conf. Robotics Automation, Minneapolis, MN, April 1996, pp. 2962- 2967. Piscataway, NJ: IEEE. Guenther, F. H. &amp; Barreca, D. M. 1997 Neural models for flexible control of redundant systems. In Self-organization, computational maps, and motor control (ed. P. Morasso &amp; V. Sanguineti), pp. 102-108. Amsterdam: Elsevier. Hatsopoulos, N. G. 1996 Coupling the neural and physical dynamics in rhythmic movements. Neural Comput. 8, 567-581. Hayes, G. &amp; Demiris, J. 1994 A robot controller using learning by imitation. In Proc. 2nd Int. Symp. Intell. Robotic Systems, Grenoble, France, July 1994 (ed. A. Borkowski &amp; J. L. Crowley), pp. 198-204. Grenoble, France: LIFTA-IMAG. Hovland, G. E., Sikka, P. &amp; McCarragher, B.J. 1996, Skill acquisition from human demonstration using a hidden Mar- kov Model. In IEEE Int. Conf. Robotics Automation, Minnea- polis, MN, April 1996, pp. 2706-2711. Piscataway, NJ: IEEE. Ijspeert, J. A., Nakanishi, J. &amp; Schaal, S. 2002a Learning rhythmic movements by demonstration using nonlinear oscillators. In IEEE Int. Conf. Intell. Robots Systems (IROS 2002), Lausanne, 30 September-4 October 2002. Piscataway, NJ: IEEE. Ijspeert, J. A., Nakanishi, J. &amp; Schaal, S. 2002b Movement imi- tation with nonlinear dynamical systems in humanoid robots. In Int. Conf. Robotics Automation (ICRA2002), Wash- ington, DC, 11-15 May 2002. Piscataway, NJ: IEEE. Ikeuchi, K., Kawade, M. &amp; Suehiro, T. 1993 Assembly task recognition with planar, curved and mechanical contacts. In Proc. IEEE Int. Conf Robotics Automation, Atlanta, GA, May 1993, vol. 2, pp. 688-693. Piscataway, NJ: IEEE. Kawamura, S. &amp; Fukao, N. 1994 Interpolation for input torque patterns obtained through learning control. In Int. Conf. Automation, Robotics Computer Vis. (ICARCV '94), Singapore, November 1994, pp. 183-191. Kawato, M. 1996 Bi-directional theory approach to inte- gration. In Attention and performance XVI (ed. J. Konczak &amp; E. Thelen), pp. 335-367. Cambridge, MA: MIT Press. Kawato, M. 1999 Internal models for motor control and tra- jectory planning. Curr. Opin. Neurobiol. 9, 718-727. Kuniyoshi, Y., Inaba, M. &amp; Inoue, H. 1989 Teaching by show- ing: generating robot programs by visual observation of human performance. In Proc. Int. Symp. Industrial Robots, Tokyo, Japan, 4-6 October 1989, pp. 119-126. Kuniyoshi, Y., Inaba, M. &amp; Inoue, H. 1994 Learning by watching: extracting reusable task knowledge from visual observation of human performance. IEEE Trans. Robotics Automation 10, 799-822. Lacquaniti, F. &amp; Soechting, J. F. 1986 Simulation studies on the control of posture and movement in a multi-jointed limb. Biol. Cybern. 54, 367-378. Levas, A. &amp; Selfridge, M. 1984 A user-friendly high-level robot teaching system. In Int. Conf. Robotics, Atlanta, GA, March 1984, pp. 413-416. Piscataway, NJ: IEEE. Lin, L.-J. 1991 Programming robots using reinforcement learning and teaching. In Proc. 9th Natl Conf. Artificial Intell., Anaheim, CA, 14-19 July 1991, vol. 2, pp. 781-786. Menlo Park, CA: AAAI. Lozano-Perez, T. 1982 Task-Planning. In Robot motion: plan- ning and control (ed. M. Brady, J. M. Hollerbach, T. L. John- son, T. Lozano-Perez &amp; M. T. Mason), pp. 473-498. Cambridge, MA: MIT Press. Mataric, M.J. &amp; Pomplun, M. 1998 Fixation behavior in observation and imitation of human movement. Cogn. Brain Res. 7, 191-202. Meltzoff, A. N. &amp; Moore, M. K. 1994 Imitation, memory, and the representation of persons. Infant Behav. Dev. 17, 83-99. Meltzoff, A. N. &amp; Moore, M. K. 1997 Explaining facial imi- tation: a theoretical model. Early Dev. Parenting 6, 179-192. Miall, R. C. &amp; Wolpert, D. M. 1996 Forward models for physiological motor control. Neural Networks 9, 1265-1285. Miyamoto, H. &amp; Kawato, M. 1998 A tennis serve and upswing learning robot based on bi-directional theory. Neural Networks 11, 1331-1344. Miyamoto, H., Schaal, S., Gandolfo, F., Koike, Y., Osu, R., Nakano, E., Wada, Y. &amp; Kawato, M. 1996 A Kendama learning robot based on bi-directional theory. Neural Networks 9, 1281-1302. Nechyba, M. C. &amp; Xu, Y. 1995 Human skill transfer: neural networks as learners and teachers. In IEEE/RSJ Int. Conf. Intell. Robots Systems, Pittsburgh, PA, 5-9 August 1995, vol. 3, pp. 314-319. Piscataway, NJ: IEEE. Nehaniv, C. L. &amp; Dautenhahn, K. 1999 Of hummingbirds and helicopters: an algebraic framework for interdisciplinary studies of imitation and its applications. In Learning robots: an interdisciplinary approach (ed. J. Demiris &amp; A. Birk). Sin- gapore: World Scientific. Oztop, E. &amp; Arbib, M. A. 2002 Schema design and implemen- tation of the grasp-related mirror neuron system. Biol. Cybern. 87, 116-140. Pawelzik, K., Kohlmorgen, J. &amp; Muller, K. R. 1996 Annealed competition of experts for a segmentation and classification of switching dynamics. Neural Comput. 8, 340-356. Piaget, J. 1951 Play, dreams, and imitation in childhood. New York: Norton. Pook, P. K. &amp; Ballard, D. H. 1993 Recognizing teleoperated manipulations. In Proc. IEEE Int. Conf Robotics Automation, Atlanta, GA, May 1993, vol. 3, pp. 913-918. Piscataway, NJ: IEEE. Rittscher, J. &amp; Blake, A. 2003 Mathematical modelling of ani- mate and intentional motion. Phil. Trans. R. Soc. Lond. B 358, 475-490. (DOI 10.1098/rstb.2002.1259.) Rizzolatti, G. &amp; Arbib, M. A. 1998 Language within our grasp. Trends Neurosci. 21, 188-194. Rizzolatti, G., Fadiga, L., Gallese, V. &amp; Fogassi, L. 1996 Pre- motor cortex and the recognition of motor actions. Cogn. Brain Res. 3, 131-141. Saltzman, E. &amp; Kelso, S.J. A. 1987 Skilled actions: a task- dynamic approach. Psychol. Rev. 94, 84-106. Sammut, C., Hurst, S., Kedzier, D. &amp; Michie, D. 1992 Learn- ing to fly. In Proc. 9th Int. Machine Learning Conf. (ML'92), Aberdeen, Scotland, 1-3 July 1992 (ed. D. Sleeman &amp; P. Edwards), pp. 385-393. San Mateo, CA: Morgan Kauf- mann. Schaal, S. 1997 Learning from demonstration. In Advances in neural information processing systems 9 (ed. M. C. Mozer, M. Jordan &amp; T. Petsche), pp. 1040-1046. Cambridge, MA: MIT Press. Schaal, S. 1999 Is imitation learning the route to humanoid robots? Trends Cogn. Sci. 3, 233-242. Schaal, S., Sternad, D., Dean, W., Kotoska, S., Osu, R. &amp; Kawato, M. 2000 Reciprocal excitation between biological and robotic research. In Sensor fusion and decentralized control in robotic systems III, Proc. of SPIE, Boston, MA, 5-8 Nov- ember 1992, vol. 4196, pp. 30-40. Schwenker, F., Sommer, F. T. &amp; Palm, G. 1996 Iterative retrieval of sparsely coded associateve memory patterns. Neural Networks 9, 445-455. Segre, A. B. &amp; DeJong, G. 1985 Explanation-based manipu- lator learning: acquisition of planning ability through obser- vation. In IEEE Conf. Robotics Automation, St Louis, MO, March 1985, pp. 555-560. Piscataway, NJ: IEEE. Phil. Trans. R. Soc. Lond. B (2003)</page><page sequence="11">Computational approaches to motor learning by imitation S. Schaal and others 547 Segre, A. M. 1988 Machine learning of robot assembly plans. Kluwer International Series in Engineering and Computer Science. Knowledge representation, learning, and expert sys- tems. Boston, MA: Kluwer. Sternad, D. &amp; Schaal, D. 1999 Segmentation of endpoint tra- jectories does not imply segmented control. Exp. Brain Res. 124, 118-136. Sutton, R. S. &amp; Barto, A. G. 1998 Reinforcement learning: an introduction. Adaptive computation and machine learning. Cambridge, MA: MIT Press. Sutton, R. S., Singh, S., Precup, D. &amp; Ravindran, B. 1999 Improved switching among temporally abstract actions. In Advances in neural information processing systems, vol. 11. Cambridge, MA: MIT Press. Tomasello, M., Savage-Rumbaugh, S. &amp; Kruger, A. C. 1993 Imitative learning of actions on objects by children, chim- panzees, and enculturated chimpanzees. Child Dev. 64, 1688-1705. Tung, C.P. &amp; Kak, A. C. 1995 Automatic learning of assembly task using a DataGlove system. In IEEEIRSJ Int. Conf. Intell. Robots Systems, Pittsburgh, PA, vol. 1, pp. 1-8. Van Essen, D. C. &amp; Maunsell, J. M. R. 1983 Hierachical organization and functional streams in the visual cortex. Trends Neurosci. 6, 370-375. Wada, Y. &amp; Kawato, M. 1995 A theory for cursive handwriting based on the minimization principle. Biol. Cybern. 73, 3-13. Weigend, A. S., Mangeas, M. &amp; Srivastava, A. N. 1995 Non- linear gated experts for time series: discovering regimes and avoiding overfitting. Int. J. Neural Systems 6, 373-399. Widrow, B. &amp; Smith, F. W. 1964 Pattern recognizing control systems. In 1963 Comput. Inform. Sci. (COINS) Symp. Proc., pp. 288-317. Spartan. Wolpert, D. M. 1997 Computational approaches to motor control. Trends Cogn. Sci. 1, 209-216. Wolpert, D. M. &amp; Kawato, M. 1998 Multiple paired forward and inverse models for motor control. Neural Networks 11, 1317-1329. Wolpert, D.M., Miall, R. C. &amp; Kawato, M. 1998 Internal models in the cerebellum. Trends Cogn. Sci. 2, 338-347. Wolpert, D. M., Doya, K. &amp; Kawato, M. 2003 A unifying computational framework for motor control and social inter- action. Phil. Trans. R. Soc. Lond. B 358, 593-602. (DOI 10.1098/rstb.2002.1238.) Phil. Trans. R. Soc. Lond. B (2003)</page></plain_text>