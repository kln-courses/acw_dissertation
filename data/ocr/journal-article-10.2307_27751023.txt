<plain_text><page sequence="1">The Role of Trust and Deception in Virtual Societies Cristiano Castelfranchi and Yao-Hua Tan ABSTRACT: In hybrid situations where artificial agents and human agents interact, the artificial agents must be able to reason about the trustworthiness and deceptive actions of their human counterpart. Thus a theory of trust and deception is needed that will sup port interactions between agents in virtual societies. There are several theories on trust (fewer on deception!), but none that deals specifically with virtual communities. Building on these earlier theories, the role of trust and deception in virtual communities is ana lyzed, with examples to illustrate the objectives a theory of trust should fulfill. KEY WORDS AND PHRASES: Deception, multiagent systems, trust, virtual society. The inhumanity of the computer is in the fact that once programmed and put to work, it behaves in a perfectly honest way. ?Isaac Asimov Trust and Deception in Artificial Agents and Societies Electronic commerce can only succeed if the general public trusts the virtual environment. Trust, therefore, is an important issue [20, 25, 29,44]. As is well known, lack of trust is one of the main reasons that consumers and companies do not engage in electronic commerce. This paper clarifies why trust and de ception are so important in artificial virtual societies (e.g., multiagent sys tems, simulated societies in computers, human organizations, markets, communities supported and mediated by computers) and what kind of re search is needed. In recent years the topics of trust and deception in artificial agents and artificial societies have attracted a great deal of attention in connection with a new social paradigm in the field of artificial intelligence (AI) constituted by so-called intelligent or adaptive agents and multiagent systems (MAS) [52]. Computer science has in fact moved from the paradigm of an isolated ma chine to the paradigm of a network of systems and of distributed computing. Likewise, AI is moving from the paradigm of an isolated, nonsituated intelli gence to the paradigm of situated, social, collective intelligence [3, 6,11, 21]. At present, AI is dominated by the agent-based view [40]. Why has the notion of trust, so relevant in organizational and political sci ence, become an important research issue in the field of artificial intelligence, and, more broadly, in information and communication technology (ICT)? Why The research of Cristiano Castelfranchi was sponsored by the Alfebiite European Research Project (IST/FET). The authors thank Rino Falcone, Ronald Lee, Samuel Santosa, and Walter Thoen for their comments on an earlier version of this paper. International Journal of Electronic Commerce / Spring 2002, Vol. 6, No. 3, pp. 55-70. Copyright ? 2002 M.E. Sharpe, Inc. All rights reserved. 1086-4415/2002 $9.50 + 0.00.</page><page sequence="2">56 CRISTIANO CASTELFRANCHI AND YAO-HUA TAN is there any need for a computer implementation, let alone a formal theory, of such a soft notion? Both the social approach to computation and the autonomous agents para digm, and computer-mediated social interaction unavoidably entail the dif fusion of problems of trust and deception in virtual communities. These issues are also emerging in the domain of human-computer (H-C) interaction, in computer-supported collaborative work (CSCW) and, generally, in computer mediated interaction and organization, and in the "infosociety." The literature on trust offers several different definitions of this important concept. For Mayer et al. it is "the willingness of a party to be vulnerable to the actions of another party based on the expectation that the other party will perform a particular action important to the trustor, irrespective of the ability to monitor or control that other party" [32, p. 712]. Gambetta, in contrast, says that "trust is the subjective probability by which an individual A expects that another individual B performs a given action on which its welfare depends" [19]. Both of these definitions are subjective. They focus on different aspects and components of trust?one treating it as a decision, the other as an expec tation or evaluation [10]. The first definition refers to "the willingness of a party," the second to "subjective probability" Just as the level-of-trust thresh old is different for each individual, the level of actual trust in any given situa tion is different for each person. McKnight et al. provide an excellent comparison of the many different trust definitions [34]. Electronic Commerce and Secure Communication Most agent models assume secure and reliable communication between agents. Since this ideal situation seldom occurs in real life, many techniques (e.g., contracts, signatures, long-term personal relationships) have evolved to de tect and prevent deception and fraud in human communication, exchanges, and relations, thus ensuring trust between agents. Recent research recognizes trust as a key factor in e-commerce adoption [4, 24, 30]. Problems pertaining to trust are magnified in e-commerce because agents reach out far beyond their familiar trade environments. Moreover, existing paper-based techniques for fraud detection and prevention may not be adequate to establish trust in an electronic network environment where you usually never meet your trade partner face to face, and where messages can be read or copied a million times without leaving any trace [4,45,46]. Trust building requires more than secure communication via electronic networks, which can be obtained, say, with public key cryptography, because information about your trade partner received via secure communication may not be reliable. With the growing impact of e commerce distance, the importance of trust building has been steadily increas ing, and better models of trust and deception are needed. One trend in electronic communication channels is the introduction of extra agents, so-called trusted third parties, into an agent community in order to build trust among the other agents in the network. Individuals involved in human-computer interaction have a tendency to rely on social attitudes and rules in vesting trust in machines. The fact that many people mindlessly apply social rules and trust expectations to computers is revealed by a series of experimental</page><page sequence="3">INTERNATIONAL JOURNAL OF ELECTRONIC COMMERCE 57 studies reviewed by Nass and Moon [36]. For example, Fogg and Nass dem onstrate that users provided more helping behavior to a computer that had previously helped them than to another computer [18]. Moreover, the per ceived reliability and "personality" of the computer induces users to work longer, perform higher-quality work, and feel more trusting. This "socializa tion" and "anthropomorphism" of machines is both natural and helpful. And this form of trust will be even more important with believable agent-based interfaces. Indeed, different kinds of trust are needed and should be modeled and supported: Trust in the environment and the infrastructure (i.e., the socio technical system). Trust in your agent and in mediating agents. Trust in your potential partners. Trust in the authorities. These forms of trust are complementary in the sense that one can compen sate the lack of trust if another is too weak to be relied on or to be worked with cooperatively. When you have to rely on agent or partner X in the sense of expecting some action from it, the basic distinction is between the "internal" factors of trust (X's ability, knowledge, motivation, commitment, morality, social disposition, etc.) and the "external" factors of the environment [10]. If there are adverse environmental or situational conditions, your intervention will be for the purpose of establishing protective conditions and guarantees, preventing interference and obstacles, establishing rules and infrastructures. On the other hand, if you want to increase your trust in your partner, then you should work on the partner's motivation, beliefs, and disposition toward you, or on his competence and self-confidence, or choose another partner. Envi ronmental and situational trust are crucial in e-commerce and computer mediated interaction because in the electronic arena people from all over the world meet without ever having seen one other, and without having the nor mal richness of communication media. It is important to stress that when the environment and the specific circumstances are safe and reliable, less trust in X (the partner) is necessary for reliance (e.g., for transactions) [38]. On the other hand, if I strongly trust X with regard to his capacities, will ingness, and faithfulness toward me, then I can accept an environment that is less safe and reliable (i.e., that provides less external monitoring and author ity). This complementarity between the internal and external components of trust is very important in virtual societies, because it creates a demand not only for safe protocols and procedures (cryptography, etc.), but also for the creation of trustworthy electronic social controls [4,45,46]. As in human soci eties, trust in on-line agents that is based on some form of evaluation of their competence, reliability, or goodwill (reasoning, experience, perception, intu ition, etc.) is different from trust based on social constructs (i.e., constructs developed by societies to ensure trust among agents and to prevent deception and fraud) with corresponding disciplinary actions to punish violators (con tracts, signatures, long-term roles, institutions, etc.). The one complements</page><page sequence="4">58 CRISTIANO CASTELFRANCHI AND YAO-HUA TAN the other?formal bonds complement the more informal and merely interper sonal confidence [10,43]. Lewicki et al. give a typology of three different types of trust [27, 28]. The first, deterrence-based trust, arises from the idea that you trust somebody because a very strict normative rule or legal system is in force. The agent will be punished for any violation, and therefore behaves accord ing to the rules. The second type, knowledge-based trust, derives from your knowledge about the agent's competencies, motives, and goals. You trust the agent because, thanks to this knowledge about his inner mental state, you know what to expect from him. Finally, in identity-based trust you trust an agent because you know that he identifies himself with your goals and will try to serve your interests as well as possible. The agent may even do some thing for you before you are aware that you have a need for it (as children trust their parents). Lewicki and Bunker observe that in business relation ships, identity-based trust is far better than the other types because they have greater coordination costs, either because you have to introduce a normative or legal system by contract or convention, or because you have to go to the effort of searching for the motives and goals of your agent [27]. Some economists are skeptical about the role of trust in business relations. Williamson is the most outspoken [49, 50]. He claims that "it is redundant at best and can be misleading to use the term Trust7 to describe commercial ex change for which cost-effective safeguards have been devised in support of more efficient exchange" [50, p. 256]. Instead of a trust perspective, Williamson proposes what he calls an economic perspective on commercial exchanges, and in particular a transaction-cost perspective that refers to contractual safe guards rather than trust. The basic idea is that you use the utilities of your trading partner to negotiate a contract that makes it more expensive for your partner to deceive you than to comply with the contract, and for this reason you can always rely on your trading partner to do exactly what is beneficial for you (and what was agreed in the contract). To illustrate with an example. A seller promises to deliver goods to A at a certain price, but then B offers a much better price for the same goods. The seller might be tempted to sell the goods to B, but opportunistic behavior of this kind will be prevented if A can include a clause in the contract specifying that if the goods are not delivered, the seller has to pay a penalty that exceeds the extra profit that would accrue from selling them to anyone else. Thus the contract is an instrument that en ables the buyer to control the seller's opportunistic behavior. The economic perspective gives a good explanation of contracting in commercial exchanges, but the transaction-cost perspective on commercial exchange assumes that you always know the utilities of your trading partner. This assumption is un realistic for the many situations where you know very little about your trad ing partners when you are deciding whether or not to transact with them, and thus trust is more useful than contracting. Clearly, what is needed is not trust alone or contracting alone, but a reasonable combination of the two. Williamson takes the extreme position that all trust can be replaced by contracting, but this is unrealistic for commercial exchanges. As Ghoshal and Moran argue, applying contractual controls in business relationships is not only costly, but can be counterproductive, in the sense that the insistence on contracts only increases the distrust between trading partners [22]. McCauley, in his well</page><page sequence="5">INTERNATIONAL JOURNAL OF ELECTRONIC COMMERCE 59 known empirical study on contracting practices, of American companies, ob served that, in actual business practice, contracts are used for planning pur poses rather than for fighting legal battles [33]. Detailed contracts are intended mainly to establish contingency plans if something goes wrong in the busi ness relationship. One finding of his study is that only small companies that have already lost the battle with a large business partner use contracts to go to court, whereas large firms try to avoid legal battles, because lawsuits are costly and even more because they are detrimental to trust building in the relation ship between business partners. Moreover, Williamson's position is related to a rather restricted view that reduces actors' utilities to nothing but monetary and strictly economic rewards or incentives. It neglects other important goals, like morality, reputation, or what economists call relational or social capital. In addition to these criticisms of the idea that contracting can replace trust, there is a more fundamental objection. When informal personal trust is not sufficient and we put contracts or laws in place, we are replacing simple trust with a more complex kind of trust [8]. But trust remains crucial, since a contract will only be effective if we believe that the agent will not violate it, and this is precisely what trust means. Our trust in the contractor (the belief that the contractor will do what was promised) depends on our belief that the contractor is a moral person and will keep promises, or that the contrac tor is concerned about law and punishment. In other words, we trust that the agent has a motivation that is stronger than its interest in violating the contract. Of course, we also have to trust a third party (central authority, police, decentralized social control) and its ability to control and punish the contract violator. In sum, in contract situations, personal trust in an agent may be lacking, but in its place there is a higher level of trust?namely, trust in the authority that wijl enforce the contract, and trust that the agent acknowledges, is con cerned about, and respects the authority. Without trust in the agent the con tract would be useless. Furthermore, legal and moral norms are not simple threats of punishment, they exploit the individual's internal incentives and motives for doing the right thing, like moral consciousness, self-approval, or guilt feelings. Such feelings are what makes an agent moral and honest and above all reliable, and not merely acting in accordance with an inhibition against defecting based on calculations of self-interest and fear of punishment [14]. This normative aspect behind contracting requires that every agent in a community does business honestly and knows that all the other agents do so as well. Tan and Thoen present a formal analysis of trust in on-line business rela tions, showing that trust in a business community presupposes that all the participants know about one another and that they know the norms of the group [46]. As for external sanctions, concerns about bad reputation and ostracism from the community are usually much more effective than official sanctions and fines (a position supported by McCauley's finding that companies worry more about getting a bad reputation than about having to pay a fine [33]). If there is not enough personal trust in a given situation, a complementary control mechanism is needed. For example, you can ask a prospective buyer to provide a letter of credit if you do not trust him to pay for goods you are selling. The letter of credit is one of the oldest and most widely used control</page><page sequence="6">60 CRISTIANO CASTELFRANCHI AND YAO-HUA TAN mechanisms in international trade to complement a lack of personal trust be tween trade partners. A letter of credit is a guarantee from a bank (usually the buyer's bank) that the seller will be paid for the goods after shipping them to the buyer. However, what if I do not trust letters of credit because I know from personal experience that they can be falsified? This question shows that it is not only the control that supplements my personal trust, but also my trust in the control. In many cases one's trust in a control procedure is actually trust in the institution that issues the procedure. The trustworthiness of a letter of credit, for instance, depends on one's trust in the bank that issued it. Zucker has developed an interesting theory about so-called institution-based trust, illustrated with convincing examples from American economic history [54]. According to Zucker, certification institutions arose in the United States in the nineteenth century because local personal-trust networks were disintegrating as a result of the massive internal migration throughout the country caused by the developing industrialization in the northeastern states. As a substitute for personal trust, control procedures of various kinds were introduced. These derived their trustworthiness from institutions that were usually government owned or controlled. Similarly, so-called trust seals have been introduced for Web sites to create trust in on-line virtual shops (e.g., Trust-e, WebTrust, BBB on-line). These seals are social indicators that are supposed to induce trust in the control procedures. They are issued by organizations that certify the pro cedural controls of Web sites (e.g., that the Web site handles payments in a secure and reliable way, has certain return policies for purchased goods of bad quality, or complies with a privacy policy that says what it can and can not do with personal data). When on-line customers see these seals, they are more inclined to trust the Web site. The distinction between trust in the partner and trust in the infrastructure or environment is summarized in the generic trust model introduced by Tan and Thoen [45]. The basic idea of the model is that individuals only engage in transactions if their level of trust exceeds their personal threshold [8]. The personal threshold depends on the type of transaction. For example, the thresh old might be high if the value of the transaction is high, and low if the agent is a risk seeker. Figure 1 gives a graphical representation of the generic trust model. In the center of the figure is the trustor's transaction trust?the mental state that determines whether the trustor has sufficient trust to engage in a transaction. The determinants of the trustor's trust threshold are represented in the lower half of the figure. Several determinants can be distinguished, such as the potential profit for the individual, the risk involved, and the individual's attitude toward risk or risk propensity (i.e., whether the indi vidual is risk seeking, risk neutral, or risk averse). The upper half of Figure 1 represents the trust sources, such as party trust, the trust that the other party in a transaction induces in the trustor, and control trust, the trust that control mechanisms induce. Control mechanisms are the procedures and protocols that monitor and control the successful performance of a transac tion. An electronic trade transaction can be enabled in one of two ways: either by decreasing the personal threshold of the potential actor with re spect to the transaction, or by increasing the actor's trust level related to the transaction.</page><page sequence="7">INTERNATIONAL JOURNAL OF ELECTRONIC COMMERCE 61 Figure 1 Generic Trust Model Agents are situated in a physical as well as a social environment. This means that they have limited autonomy and depend on something else (nonsocial dependence) or someone else (social dependence) for the achievement of their goals [13]. In nonsocial dependence the successful execution of an action requires some external input, condition, or resource, since actions have necessary precondi tions both for their execution and for their success. Every action is crafted on an environmental (external) causal process and depends upon it for success. In the absence of the appropriate conditions, the action will be either impossible or a failure. By creating or eliminating the external action conditions, one can influ ence the behavior of an agent, inducing it to do something, preventing it from doing something, or making its action inefficacious (harmless) or successful. In social dependence the agent needs the supportive action of another agent in order to realize its ability or competence or to access its external resources. In fact, nonsocial dependence (lack of conditions or resources) produces a social dependence network, because other agents are able either to modify the critical conditions or to control the necessary resources. If agent X depends on agent Y to obtain its goal G, Y has some power over X relative to G. In consequence, Y can prevent X from realizing G. In this sense Y controls an incentive for X?it can provide X with frustration at not obtain ing G (punishment, negative reward) or can give X a positive reward by en abling X to obtain G, and it can use this as a threat or a promise in negotiations. In other words, Y can use the dependence of X (its power over X) as a basis for influencing X and controlling its behavior. This analysis shows why it is necessary to trust both the internal stuff of an agent (capabilities, knowledge, intentions, and motivations) as well as the external (social and nonsocial) conditions. Actually, control can be implemented</page><page sequence="8">62 CRISTIANO CASTELFRANCHI AND YAO-FillA TAN via external "material" conditions for preventing undesired behavior (e.g., by installing an electronic lock to block my telephone access to international calls), but obviously it can also be implemented via social dependence: threats, prom ises, incentives, and rewards. It is very important to note, however, that social control mechanisms like the normative ones are also dependent on internal trustworthy features of the agent. No norm is effective if the agent does not believe, understand, and acknowledge it as a norm and have a motive for obeying it [9,15]. From a sociological point of view, Lewicki and Bunker's distinction be tween several types of trust is useful [27]. However, from a cognitive point of view, all these types and levels presuppose specific internal motives of the agent. The only difference pertains to the agent's motive in which trust should be grounded. In deterrence-based trust one should trust not only the author ity and the effectiveness of surveillance and intervention, but also the agent's motive for avoiding punishment (shame, fear, calculation of loss, etc.). In knowledge-based trust one trusts other motives of the agent, such as the agent's selfish interest in obtaining a personal benefit of some kind. And in identity based trust one trusts that the agent has prosocial positive motives toward oneself or one's group. Thus in all these cases one has trust in the agent's internal properties, and not only in external pressures or controls. Agents and Autonomy The notion of trust is also important in other domains of agent theory beyond electronic commerce. It is apparently foundational for the very notion of agency. Agents are in fact defined by their acting "on behalf of" as well as by their autonomy. Agents are delegated to take care of some task or goal without direct intervention or supervision by the user or delegator [10]. Definitively, the delegating agent should trust them in order to be able to rely upon them. For example, trust is relevant in human-computer interaction, such as the trust relation between a user and a personal assistant (and, in general, with a com puter). It is also critical for modeling and supporting groups and teams, orga nizations, coordination, negotiation, with the related trade-off between local or individual utility and global or collective interest; or in modeling distrib uted knowledge and its dissemination, where different sources and media tors will create problem of source reliability and sincerity. Computer-Mediated Interaction In some situations the electronic medium seems to weaken the usual bonds of social control, thereby strengthening the habit or disposition to cheat. Con sider multiuser dungeons (mud), as well as discussion lists, electronic asso ciations, and so forth. The ease of adopting a false identity on the Internet and the protection offered by anonymity offers temptations to people who are prone to deception and to violating moral standards. Some people change their sex in virtual interactions or invent false on-line personalities for themselves. Ex periments on computer-supported cooperation have discovered that people</page><page sequence="9">INTERNATIONAL JOURNAL OF ELECTRONIC COMMERCE 63 are more inclined to defect in on-line communications than in face-to-face interactions [39], and a preliminary direct acquaintance between people re duces this tendency. Thus computer technology can even weaken trust rela tionships already holding in human organizations and relations, and aggravate problems of deception and trust [35]. In order to deal with trust problems in virtual societies and network technologies, several issues have to be addressed. First, virtual communities and their supporting ICT technologies are and should always be embedded in human interpersonal, social, and legal rela tionships [23, 26]. Without understanding and exploiting this social back ground, neither security nor trust will be effective. Engineers and computer scientists initially approached trust issues by introducing security protocols, authentication, cryptography, central control, and strict rules to increase secu rity Although some of these measures are surely useful and needed, the idea of total control and a purely technical solution to prevent deception and en courage non-self-interested cooperation is unrealistic. Sociopsychological and legal approaches are crucial to solving these problems. Second, social and psychological aspects (like trust) have to be incorpo rated in the technology The intelligent and autonomous agent paradigm is a suitable framework to model these typically human and social notions. Human cooperation is strongly based on social, moral, and legal notions, and in order to support cooperation, computers must "understand," at least partially, what happens among users. For example, computers should be able to manage and reason about permissions, obligations, power, roles, com mitments, and trust. Moreover, to cope with the open, unpredictable social interaction and collective activity that will emerge among artificial agents, these interactions should be based on something like organization, role, and norms. This is in fact happening in the domain of agents and MAS research, where these topics are the objectives of theoretical and formal modeling and implementation. Third, in order to understand trust and to implement it in artificial agents, a general theory of trust is needed, dealing with its cognitive and affective components, and its social functions. A theory and practical models have to be developed to answer such questions as when trust is rational, when trust is really overconfidence and risky, when trust is too weak, and when redundant control mechanisms are a waste of time or result in the loss of good opportu nities by not taking advantage of low but sufficient trust levels. Further more, what is the relation between trust and "dependability" on the sociotechnical system? What signs and qualities are a satisfactory basis for trust in a face-to-face communication, and how can they be emulated in electronic interactions [2]? Fourth, two conflicting approaches that exist in the social sciences are also found in the intelligent-agent approach to trust. Is trust reducible to a quanti tative measure, to be acquired and modified by learning or by some delibera tive calculation? For example, is trust no more than subjective probability, as some researchers claim? This quantitative approach is in fact the dominant tradition in economics, game theory, and certain schools of sociology [12, 20], and it is becoming more influential in AI and e-commerce research [5,42]. Or,</page><page sequence="10">64 CRISTIANO CASTELFRANCHI AND YAO-HUA TAN is it necessary to have a more cognitive view of trust as a complex mental structure of beliefs and goals, which would imply that the trustor has a "theory of the mind" of the trustee (possibly including personality, shared values, morality, goodwill, etc., as maintained by Falcone and Firozabadi and by Lewicki and Bunker [17, 27])? Finally, any increase in the security, safety, and social control of the techni cal, social, and legal infrastructure will be useless if the user does not perceive this reliability. Only perceived reliability counts, and determines confidence. The subjective feeling of confidence is sometimes not based on real safety, but on vague social attitudes. For example, air travel is perceived as less safe than driving a car, when in fact in several countries this does not seem to be true. An important way to induce trust is to show that everybody shares the same trusting view. Thus, to cite one example, a well-known advertisement of a courier for sending money induces trust by saying, "Millions of people trust us." The same principle holds in electronic commerce and in general in elec tronic societies and social institutions: Perceived reliability is more important than objective reliability. Deception in Virtual Communities The social approach to computation in the autonomous-agent paradigm and computer-mediated social interaction unavoidably entail the diffusion of de ception in virtual communities [7]. Deception is any act (and its results) aimed at inducing wrong beliefs or at depriving others of knowledge relevant for obtaining their goals. Obviously, people will continue to deceive each other in computer-mediated interactions just as they do in traditional social interac tions. The computer medium will provide new opportunities and ways to deceive, and perhaps to prevent deception or to defend against it (e.g., by very simple and rapid access to certification, or by instructions from some trustworthy authority). Moreover, as mentioned above, the electronic medium could make matters worse by weakening the usual bonds in social control. The habit or disposition to cheat will grow stronger because computer-mediated communication is such an anonymous medium. Even more remarkable, there will not only be problems of deception between humans (via machines) but also problems between humans and artificial entities and even among the artificial agents themselves. Agents are and will be designed, selected, or trained to deceive, and people will be deceived by and will deceive artificial agents. Agent-Agent Deception The evolution from distributed artificial intelligence (DAI) to multiagent sys tems is moving toward more self-interested agents. DAI was concerned with closed systems of agents designed to cooperate and to solve common prob lems through task allocation and coordination [37]. MAS, on the contrary, pro vides open environments of heterogeneous and self-interested agents that interact, negotiate, and coordinate with one other (competing or collaborat ing) to achieve their own goals or tasks (as assigned by their users). Agents in</page><page sequence="11">INTERNATIONAL JOURNAL OF ELECTRONIC COMMERCE 65 MAS are not necessarily benevolent, cooperative, or sincere toward one other. To achieve their own goals in spite of the interference and competition of the others, they may have to cheat and deceive for good strategic reasons. This can be done either through intelligent problem-solving and social reasoning by deliberative agents, or through learning or selection by reactive agents whose successful behavior is reinforced. Agent-User Deception Agents that support information search and retrieval, electronic commerce, or any type of social control will deceive the users or their delegated agents. The following examples illustrate this point. Information Systems Current information systems already misinform unauthorized users (whether human or software agents) in order to protect confidential information. This is a well-known problem in the field of databases, where the concept of multi level security requires deliberately wrong answers and cover stories [48]. Not only does the system avoid answering or say "I cannot give you this informa tion" or "It is secret," but it even gives false information on purpose (the pur pose, of course, is defined by the designer). Such situations are not merely instances of concealment or nondisclosure but are true lies, aimed at stopping the inquiry and misleading the unauthorized and malicious agent. Electronic Commerce Agents in electronic commerce bid on the user's behalf from a self-interested perspective [16,31,41,53]. Obviously, when a user's agent is bidding on some thing in an on-line auction, the user does not want it to honestly bid his re serve price for the good if it could possibly get the good for less money This essential aspect of bargaining thus entails an element of deception and non disclosure [47]. Bargaining is based on nondisclosure of the reserve price by each partner. Moreover, why shouldn't electronic sellers use all the traditional tricks, not to mention new ones, to sell things and introduce new forms of deceptive advertising? Personal Assistants Our personal assistants should probably deceive us if by doing so they can influence us to do the right thing, thereby securing our long-term interests against our short-term preferences or biases. In the same vein, a physician is often reticent about the side effects of a medicine in order not to discourage the patient [16, 51]. Similarly, a message for risk prevention does not usually stress the fallibility of the remedy (e.g., of a contraceptive method). In the near future, artificial agents will give many medical recommendations. Will they</page><page sequence="12">66 CRISTIANO CASTELFRANCHI AND YAO-HUA TAN have the same paternalistic (and deceptive) attitude? In all probability, per sonal assistants, in adapting information to the user's personality, will use a good deal of reticence and concealment [7, 16, 27]. Even lies cannot be ex cluded from this communication. Consider, for example, the case where my assistant knows that I have an important appointment and am usually late, or knows that I am quite verbose and have only seven minutes left to finish a conversation. It may well give me false information about the time ("You have only 15 minutes to get to your appointment!" "You have five minutes left"). People sometimes try to deceive themselves by setting their watches to the wrong time. Why should they dislike a collaborative intelligent agent that does much the same thing? And what should my business agent do if it dis covers that my economic decisions are irrational and biased, say, by the so called sunk-cost effect [1]? As a "personalized" assistant, should it always follow the user's "preferences," or should it propose a better business deal by concealing or modifying the available data? On the other hand, what if the user deceives the personal assistant? In electronic commerce, people tend to conceal their real reserve price from their agents [47]. They do so with the idea of giving them less discretion so as to obtain a better deal. In sum it is clear, even from these simple and initial observations, that there are three possibilities: (1) the agent deceives/br its principal?that is, the man datory deceives through its agent, (2) the agent deceives autonomously, (3) the agent deceives its principal or user. And this says nothing about viruses, and about fraudulent and malicious agents expressly designed and used to harm competitors or to steal money or information, whether in war or in business. Conclusion Trust and deception are key issues in the development of virtual communi ties, as was illustrated above with examples from multiagent systems and electronic commerce. While some might argue that the trust perspective is an anthropomorphic approach to virtual communities, anthropomorphism is not merely a (useful) subjective perception of technology by human users that arbitrarily ascribes human features to it so that they can apply their intuitive ways of interacting. Rather, it endows the trust perspective with an element that makes it a reasonable and practical approach to information technology With the advancement and growing sophistication of virtual communities, more and more hybrid situations occur where artificial agents have to interact at a high level with humans and have to be able to understand them, just as understanding among humans is indispensable for efficient collaboration be tween humans. Machines will be involved in real social relationships because they will have to mediate between humans and be trustworthy partners for the human user. Moreover, since artificial information agents will become autonomous, pro-active, adaptive, evolving, distributed, interacting, and able to negotiate, they will have to evolve or be endowed with social intelligence that is not necessarily benevolent or collaborative. As was argued above, the more the human-computer interaction becomes a kind of social interaction, and the more human-human and human-organization interactions are com</page><page sequence="13">INTERNATIONAL JOURNAL OF ELECTRONIC COMMERCE 67 puter-mediated, the more relevant trust issues will be. A few examples were mentioned: Trust in the environment and the infrastructure (the sociotechnical system). Trust in your agent and in mediating agents. Trust in your potential partners. Trust in the authorities. The need for trust is made more crucial by the diffusion of unreliable infor mation and because of human and artificial deception. Safety and trust build ing cannot be provided by technical means alone. Unreliability, diffidence, and distrust cannot be overcome solely by secure systems and software, for mal rules and controls, or cryptographic technologies. The discussion in this article suggested some possible focal points for dealing with such issues, of which the most important ones are listed below. The many different types of trust, including the softer and more informal ones, are all necessary to build trust and can complement one other. Control is not the opposite of trust but builds upon its more complex and institutional forms. The sociopsychological environment in which every technology is situ ated should not be underestimated. All shared technologies are sociotechnical systems in which crucial roles are played by shared knowledge, attitudes, and rules, by the community and morality of the participants, by social rela tionships, and by formal or informal institutions. Some part of this social knowl edge and capability must be incorporated into information technology, especially into adaptive and interacting agents and multiagent systems. The cognitive aspects of trust must be analyzed and operationalized, as opposed to restricting oneself to a reductive economic approach, such as Williamson's, that only considers utilities. Finally, with regard to deception, it is necessary to take account not only of dangerous and antisocial deceptive behavior, but also of the necessary forms of deception utilized in bargaining and in various prosocial benevolent forms of deception that will be used by artificial per sonal assistants. REFERENCES 1. Arkes, H., and Blumer, C. The psychology of sunk cost. Organizational Behavior and Human Decision Process, 35 (1985), 124-140. 2. Bacharach, M., and Gambetta, D.G. Trust as type detection. In C. Castelfranchi and Y.H. Tan (eds.), Deception, Fraud and Trust in Virtual Societies. Dordrecht: Kluwer, 2001, pp.1-26. 3. Bobrow, D. Dimensions of interaction. AI Magazine, 12, 3 (1991), 64-80. 4. Bons, R.W.H.; Lee, R.M; and Wagenaar, R.W. Obstacles for the develop ment of open electronic commerce. International Journal of Electronic Com merce, 2, 3 (1998), 61-83. 5. Brainov, S., and Sandholm, T. Contracting with uncertain level of trust.</page><page sequence="14">68 CRISTIANO CASTELFRANCHI AND YAO-HUA TAN In Proceedings of the 2nd Workshop on Deception, Fraud and Trust in Agent Societies, Autonomous Agents '99. Seattle, 1999, pp. 29-40. 6. Castelfranchi, C. Modeling social action for AI agents. Artificial Intelli gence, 6 (1998), 157-82. 7. Castelfranchi, C. Why computers will (necessarily) deceive us and each other. Ethics and Information Technology, 2 (2000), 113-119. 8. Castelfranchi, C, and Falcone. R. Principles of trust for multi-agent systems: Cognitive anatomy, social importance, and quantification. In Proceedings of the International Conferences on MAS?ICMAS'98. Menlo Park, CA: AAAI-MIT Press, 1998, pp. 72-79. 9. Castelfranchi C, and Falcone R. Trust and control: A dialectic link. Applied Artificial Intelligence Journal, 14 (8), 2000, 799-823. 10. Castelfranchi C, and Falcone R. Social trust: A cognitive approach. In C. Castelfranchi and Y.H. Tan (eds.), Deception, Fraud and Trust in Virtual Societies. Dordrecht: Kluwer, 2001, pp. 55-90. 11. Castelfranchi, C, and Tan, Y.H. (eds.), Deception, Fraud and Trust in Virtual Societies. Dordrecht: Kluwer, 2001. 12. Coleman, J.S. Foundations of Social Theory. Boston: Harvard University Press, 1990. 13. Conte, R., and Castelfranchi, C. Cognitive and Social Action. London: UCL Press, 1995. 14. Conte, R., and Castelfranchi, C. Are incentives good enough? In R. Conte and C. Dellarocas (eds.), Social Order in Multi-Agent Systems. Berlin: Springer Verlag, 2001, pp. 45-61. 15. Conte, R.; Castelfranchi, C; and Dignum, F. Autonomous norm-accep tance. Proceedings of the ATAE'98?Intelligent Agents V. Berlin: Springer Verlag, 1999, pp. 45-60. 16. de Rosis, F.; Grasso F.; and Berry, D. Refining medical explanation generation after evaluation. Artificial Intelligence in Medicine, 17,1 (1999), 1 36. 17. Falcone, R., and Firozabadi, B. The challenge of trust: The Autonomous Agents '98 Workshop on Deception, Fraud and Trust in Agent Societies. Knowledge Engineering Review, 14,1 (1999), 81-89. 18. Fogg, B.J., and Nass, C. Do users reciprocate to computers? In Proceed ings of the Computer-Human Interaction (HCl) Conference. Atlanta, 1997, pp. 25-31. 19. Gambetta, D.G. Can we trust trust? In D.G. Gambetta (ed.), Trust, Making and Breaking Cooperative Relations. New York: Basil Blackwell, 1988, pp. 213 237. 20. Gambetta, D.G. (ed.), Trust, Making and Breaking Cooperative Relations. New York: Basil Blackwell, 1988. 21. Gasser, L. Social conceptions of knowledge and action: DAI foundations and open systems semantics. Artificial Intelligence, 47 (1991), 107-138. 22. Ghoshal, S., and Moran, P. Bad for practice: A critique of the transaction cost theory. Academy of Management Review, 21,1 (1996), 13-47. 23. Hartmann, A. Comprehensive information technology security: A new approach to respond to ethical and social issues surrounding information security in the 21st century. In Proceedings of the IFIP TCI 11 International</page><page sequence="15">INTERNATIONAL JOURNAL OF ELECTRONIC COMMERCE 69 Conference on Information Security. Helsinki, 1995, pp. 27-32. 24. Keen, P.G.W, (ed.). Electronic Commerce Relationships: Trust by Design. Englewood Cliffs, NJ: Prentice-Hall, 1999. 25. Kramer, R.M., and Tyler, TR. (eds.). Trust in Organizations: Frontiers of Theory and Research. Thousand Oaks, CA: Sage, 1996. 26. Leiwo, ]., and Heikkuri, S. An Analysis of Ethics Foundations of Information Security in Distributed Systems. Technical Report. Helsinki: Nokia TeleC, 1996. 27. Lewicki, R.J., and Bunker, B.B. Developing and maintaining trust in work relationships. In R.M. Kramer and T.R. Tyler (eds.), Trust in Organiza tions: Frontiers of Theory and Research. Thousand Oaks, CA: Sage, 1996, pp. 114-139. 28. Lewicki, R.J.; McAllister, D.; and Bies, R. Trust and distrust: New rela tionships and realities. Academy of Management Review, 23,1 (1998), 438-458. 29. Luhmann, N. Trust and Power. New York: John Wiley, 1979. 30. Open Access to Electronic Commerce for European Small and Medium Size Companies. Memorandum of Understanding, European Commission, Directorate General DG-XIII-B. Brussels, 1999. 31. Maes, P.; Guttman, G.; and Moukas, A. Agents that buy and sell: Trans forming commerce as we know it. Communications of the ACM, 42, 3 (1999), 56-62. 32. Mayer, R.C.; Davis, J.H.; and Schoorman, F.D. An integrative model of organizational trust. Academy of Management Review, 20, 3 (1995), 709-734. 33. McCauley, S. Non-contractual relations in business: A preliminary study American Sociological Review, 28 (1963), 55-67. 34. McKnight, D.H., and Chervany, L.C. The meaning of trust. Working paper, Carlson School of Management, University of Minnesota, 1996. 35. Muhlfelder, M.; Klein, U.; Simon, S.; and Luczak, H. Teams without trust? Investigation in the influence of video-mediated communication on the origin of trust among cooperating persons. Behaviour and Information Technology, 18, 5 (1999), 349-360. 36. Nass, C, and Moon, Y Machines and mindlessness: Social responses to computers. Journal of Social Issues, 56,1 (2000), 81-103. 37. O'Hare, G., and Jennings, N.R. (eds.). Foundations of Distributed AI. New York: John Wiley, 1996. 38. Rea, T. Engendering trust in electronic environments: Roles for a trusted third party In C. Castelfranchi and YH. Tan (eds.), Deception, Fraud and Trust in Virtual Societies. Dordrecht: Kluwer, 2001, pp. 221-234. 39. Rocco, E., and Warglien, M. Computer Mediated Communication and the Emergence of uElectronic Opportunism/' WP 1996-01, University of Trento, 1996. 40. R?ssel, S.J., and Norvig, P. Artificial Intelligence: A Modem Approach. London: Prentice-Hall International, 1995. 41. Sandholm, T. Automated Negotiation. Communications of the ACM, 42, 3 (1999), 84-85. 42. Schillo, M.; Funk, P.; and Rovatsos, M. Who can you trust? Dealing with deception. In Proceedings of the 2nd Workshop on Deception, Fraud and Trust in Agent Societies, Autonomous Agents '99. Seattle, 1999, pp. 12-20.</page><page sequence="16">70 CRISTIANO CASTELFRANCHI AND YAO-HUA TAN 43. Shapiro, S.P. The social control of personal trust. American journal of Sociology, 93 (1987), 623-658. 44. Sitkin, S.B.; Rousseau, D.M.; Burt, R.S.; and Camerer, C. (eds.). Special issue on trust in and between organizations. Academy of Management Review, 23, 3 (1998). 45. Tan, Y.H., and Thoen, W. A generic model of trust in electronic com merce. International Journal of Electronic Commerce, 5, 2 (2000), 61-74. 46. Tan, Y.H., and Thoen, W. Formal aspects of a generic model of trust in electronic commerce. Decision Support Systems, in press. 47. Vulkan, N. Economic implications of agent technology and commerce. Economic Journal, 453 (1999), 67-90. 48. Wagner, G. Multi-level security in multiagent systems. In P. Kandzia and M. Klusch (eds.), Cooperative Information Agents. Berlin: Springer Verlag, 1997, pp. 272-285. 49. Williamson, O.E. Calculativeness, trust and economic organization. Journal ofEaw and Economics, 30 (1993), 131-145. 50. Williamson, O.E. The Mechanisms of Governance. New York: Oxford University Press, 1996. 51. Witte, K. The manipulative nature of health communication research. Ethical issues and guidelines. American Behavioral Scientist, 38, 2 (1994), 285 293. 52. Wooldridge, M.J. Agent-based software engineering. IEEE Proceedings on Software Engineering, 144 (1997), 26-37. 53. Wurman, PR.; Wellman, M.P; and Walsh, W.E. The Michigan Internet AuctionBot: A configurable auction server for human and software agents. In Proceedings of the Second International Conference on Autonomous Agents. Minneapolis, May 1998, pp. 301-308 54. Zucker, L.G. Production of trust: Institutional sources of economic structure, 1840-1920. Research in Organizational Behavior, 8 (1986), 53-111. CRISTIANO CASTELFRANCHI (castel@ip.rm.cnr.it) is professor of general psychol ogy in the department of communication sciences at the University of Siena, Italy. Previously, he was senior researcher at the Italian National Research Council (CNR). His principal background is in linguistics and cognitive science. His publications cen ter on the fields of artificial intelligence (AI), distributed AI, and multiagent systems. YAO-HUA TAN (ytan@feweb.vu.nl) is professor of electronic business in the depart ment of economics and business administration of the Free University in Amsterdam, and earlier was an associate professor at the Rotterdam School of Management of Erasmus University in Rotterdam and program director of its Global Electronic Mas ters (GEM) program for executive education on electronic commerce, which is a part of an international consortium of business schools. His research interests are virtual relationship building in business-to-business e-commerce, the strategic role of trust as facilitator for company participation in e-commerce, electronic negotiation and con tracting, and the use of artificial intelligence techniques to enable automation of busi ness procedures in international trade. He has served as chairman of a series of program committees at national and international conferences and has published eighty pa pers in journals and refereed conference proceedings.</page></plain_text>