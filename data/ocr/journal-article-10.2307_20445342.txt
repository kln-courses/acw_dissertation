<plain_text><page sequence="1">A Web, Not a Tree Science in the Looking Glass: What Do Scientists Really Know? By E. Brian Davies. Oxford University Press, 2003. 295 pp. Cloth, $34.50. On page 116 of Science in the Looking Glass, Davies says the following: "One should imagine mathematics not as a tree in which everything is fed by the roots of logic and set theory, but rather as a web in which every part strengthens every other part." This is similar to the message we heard from Thomas Kuhn in his 1962 book The Structure of Scientific Revolutions, where he pointed out that advancement in science is not a linear boxcar-like accumulation of facts. Kuhn and Davies remind us that both science and mathematics change and grow in a rather untidy process. The evolution of science and mathematics, it seems, is much like the evolution of many other social structures. However, unlike many other social structures, science and mathematics are surrounded by a romance that presents these disciplines as nearly divine gifts. Mathematics in particular, Davies reminds us, comes with its own myth of essential purity. It is one of the last refuges of Platonism, he reminds us, anchored in a Pla tonic mystique of pure form independent of the messy and untidy world of biology, psychology, and social convention. Davies rejects this Platonism. One of the goals of the book is to demystify mathematics and anchor it in the more earthly world of human perception, social context, and historical inertia. He tells us that most of the approaches to studying the foundation of mathematics take no account of how mathematics is created. This is a critically relevant observation. It is all too easy to romanticize something when all you know is its final form. Alternatively, witnessing the embryonic formation can remind us that even the most elegant structure is the product of earthly bricks, mortar, sweat, and labor. Bismarck once</page><page sequence="2">320 AMERICANJOURNAL OFPSYCHOLOGY, SUMMER 2006 said that people prefer not to know how their politics and their sausage are made. Maybe Bismarck should have added mathematics to that list. Along with demystifying mathematics Davies takes on the task of demythologiz ing science. In the opening paragraph of chapter 10 he says, "Science is a system of thought and should not claim to have a monopoly on truth" (p. 253). I think this is a very bold and courageous intellectual stance, especially because Davies is well aware of the history of postmodern attacks on science and assaults from religious fundamentalists. Science has also suffered from a post-Watergate, post 1960s syndrome of distrust of authorities and experts. This is the only book I have seen in which the author goes to such lengths to defend the scientific endeavor not by parading achievements and glories before the reader but rather by calling a spade a spade, facing the fictions as well as the facts. It is important to note that Davies is notjust airing dirty linen here. There is tremendous confusion overjust what science is. How does it differ from technology and engineering? How is it different from philosophy? How is it different from religion? Davies does an artful job of helping to make these distinctions clear without losing sight of the unique contribution science has made to human understanding. Science The book has 10 chapters covering a very broad range of topics: perceptual psy chology, the nature of arithmetic, mechanics and astronomy, probability, quantum physics, evolution, biology, chaos theory, and summary comments on reductionism and the philosophy of science. (Davies provides corrections and further comments to the book on his Web site; I recommend that readers make the corrections and look over the comments.) Davies, who is a professor of mathematics at King's College, London, and a fellow of the Royal Society, does not claim to resolve all the problems on the leading edge of these fields. Instead he intends to look at all of them in a historical context, see how problems have been resolved in the past, and explain how many long-established "facts" have turned out to be not so certain. His conclusion probably will surprise many readers, especially considering that he is a mathematician: "In spite of the fact that highly mathematical theories often provide very accurate predictions, we should not, on that account, think that such theories are true or that Nature is governed by mathematics. In fact the scientific theories most likely to be around in a thousand years' time are those which are the least mathematical" (p. v). To those raised on the expression "mathematics is the language of science," such statements are best read while seated. But Davies is serious about this. He points out that some of our most enduring theories do not rely heavily on mathematics. Darwin's Origin of Species, for example, contained no equations. Other revolutions in science that were not highly mathematical are plate tectonics and Linnaeus's classification of organisms. Davies notes that Galileo's work took the world by surprise even though it depended on little more than observations through a telescope. And Faraday, the discoverer of electromagnetism, had such an aversion to mathematics that he seldom used more than simple ratios to describe his work (p. 277). This is not to say that arithmetic is avoided in these areas. Of course measurements are made, data are taken, and equations are used. But Davies wants us to see the difference between these domains and other fields of study such as</page><page sequence="3">BOOK REVIEWS 321 cosmology and theoretical physics, where the very expression of the target domain is in terms of abstract mathematical symbolism. Scientific theories that can be expressed only in sophisticated mathematical terms, he says, have a very provisional status compared with those that can be expressed in common-sense terms. The status of the heliocentric solar system and Linnaeus's classification of organisms is very solid compared with current versions of string theory. And even quantum mechanics, despite its wide unifying power and remarkable predictability, is not on sure footing. That is, the elements of quantum theory are still highly mathematical and, at present, cannot be made to conform to our common-sense understanding of "object." Nor has quantum mechanics been reconciled with general relativity, another highly abstract mathematical theory. Davies is not disparaging mathematical sciences. He is simply pointing out that mathematization does not necessarily equate to realism, which brings us to what, in my opinion, is the major theme of Davies's book: Mathematics is invented, not discovered. Mathematics I think many readers, though a little troubled by the challenges Davies throws their way, will have little difficulty understanding that the scientific world is not so monolithic in nature or uniform in its changes. Most people have a sense of this anyway. We all know that human behavior, for example, is not likely to be described in terms of basic neurons, let alone by inorganic chemistry or atomic physics. What may be most surprising to readers, however, is Davies's treatment of numbers. It is one thing for science to be a patchwork of methods and a quilt of consensus, but mathematics is eternal, mathematics is stone. Except that it is not. It is here that Davies does an excellent job of anchoring mathematics in the real world of human behavior and history. He gives us many examples showing that great moments in the history of mathematics occurred without the assistance of formal theorizing. The trigonometric functions were worked out in the 15th century based on a very realistic consideration of Euclidean geometry. Newton's calculus was worked out hundreds of years before the rigorous definition of real numbers. Cauchy worked out the theory of functions of complex variables 50 years before there was a rigorous definition of complex numbers. Andjust in case these examples are not enough, Davies cites a punchy comment by Andre Weil, who said that formalism is rather like hygiene: "It is necessary for one to live a healthy life, but it is not what life is about" (p. 115). For all the doubt that can be cast up against the independence of numbers and mathematics there is always the question, "Why is the world so amenable to being described in mathematical terms?" Davies asks this question early on in chapter 3, "Arithmetic." Many people think that it is because the universe itself is mathematical. Sir James Jeans (1944, p. 165), for example, said, "The Great Architect of the Universe now begins to appear as a pure mathematician." Physi cist and mathematician Roger Penrose (1994) is an admitted Platonist, believing that there is realm of pure form, existing independently of human beings, with which mathematicians make contact when they have their insights. And Eugene Wigner's (1960) famous title, "The unreasonable effectiveness of mathematics in</page><page sequence="4">322 AMEPJCANJOURNAL OF PSYCHOLOGY, SUMMER 2006 the natural sciences," reveals a belief that the success of mathematics cannot be a coincidence, that it must reflect deep fundamental truths. These are not easy intuitions to dissuade given the great success mathematical endeavors have had in a broad range of disciplines. But the map is not the territory, and there is no a priori reason to assume that the tool we use to discover regularities in the world is identical with the features of that world. Davies begins his examination of numbers by dividing them into four broad cat egories: small, medium, large, and huge. Small numbers are 1 to 10,000, medium numbers are 10,000 to 1 trillion, large numbers are 10,000 raised to the power of 100, and huge numbers are those much bigger than large (p. 62). At first this may strike the reader as a bit of unproductive wordsmithing. But as we read on we see that these divisions tell us a lot about the reality of numbers. The divisions illustrate that, at successive degrees of generalization, there are losses as well as gains. Small numbers are numbers that can actually be used in counting. Large numbers and beyond are those that can never be reached by counting but serve us as labels for measurements. And then there are huge numbers, such as those that have, say, 10 to the 100th power digits, which are not only beyond counting but beyond the capacity of any conceivable computer. Davies says that large numbers are used only for measuring quantities and that "there are no situations in the real world in which large numbers refer to counted quantities" (p. 66). He gives examples of the number of people in the world, the number of trees in a forest, and the numbers of atoms in a cat. There are estimates of about 8 billion people in the world. Davies points out that even if one could prescribe the exact moment-the hour or second or even subsecond-at which the last person is counted, it is not a result that a person could arrive at by count ing. For trees in a forest there is the added problem of arriving at a consensus of what a tree is. Do you include saplings at all stages of growth, for example? As for the number of atoms in a cat, one might as well try to determine the number of angels dancing on the head of a pin. Is the meal the catjust ate part of the cat? Is the air going in and out of the cat's lungs part of it? And these examples are easy issues compared with the interpretive problems of quantum coherence with the local environment. Of course one could weigh the cat and do the division, but this is not counting; this is inference via measuring. One thing that that is clear from these examples is that counting is inseparable from the capacity to categorize. Someone must decide when a thing is a tree, for example. Is someone whose heart just stopped beating still a person? Is a fetus a person? And what are the criteria for deciding about the food and air in the cat? The distinction between our understanding of small numbers and our under standing of large ones is brought home by Davies reminding us about how we add. We can add by assembling small individual objects into groups, with the assembled group constituting the final number. Alternatively, we can learn a procedure by which we manipulate shorthand symbols, such as the integer symbols, for quanti ties. For small numbers we can always check the validity of the symbol manipula tion by comparing the result with our manual assembly of objects. But for large numbers this can never be done. As Davies says, "In the shift from small to large</page><page sequence="5">BOOK REVIEWS 323 numbers a subtle shift in meaning has occurred, so that for large numbers the only way of testing a claimed addition is to repeat the use of the rules" (p. 68). On what grounds are the inferences we make about large numbers justified? Davies reminds us that the only avenue we have for comprehending the proper ties of large numbers is induction and its formalization in Peano's Principle. But induction is not without its problems, Davies cautions. There are many examples in the book about the fallibility of induction. Davies tells the story about the pig who assumes that the farmer who comes to feed and care for him will always do that. Then one day the farmer takes the pig off to slaughter. And he reminds us that in science repeated testing and confirmation of a theory is not proof because one disconfirming experiment will undo it. Davies gives Peano's postulates: 0 is a number. For every number n there is a next number, which we call its successor. No two numbers have the same successor. 0 is not the successor of any number. If a statement is true for 0 and, whenever it is true for n it is always also true for the successor of n, then it is true for all numbers. The last axiom is the principle of mathematical induction. There is a choice here. Our belief in the reality of large numbers can be seen as intuitivelyjustified in the Kantian sense of being imposed on us as a property of the mind. Or we can resort to Peano's postulates, in which confidence is gleaned from a formal system of axioms. Regarding formal systems, Davies reminds us that the axioms "agree with what we know for small and medium numbers because we can see that those satisfy the stated postulates-except that the obviousness of the last one becomes less clear as the numbers increase and lose their obvious connection with counting" (p. 76). In this loss we often forget that we are dealing with conventional symbols and rules. This is confusing the map for the territory. The difference, Davies points out, is like the difference between exploring a country that existed long before the explorer arrived and building a city, "with its unlimited potential for muddle, error and growth" (p. 78). Davies introduces the reader to a difficult philosophical issue at this point. He does not resolve the problem of induction, but he said in the introduction that he did not intend to solve all the problems he brings up. Fair enough. And he also said up front that he was going to show the controversies, and he does that very well with respect to the principle of induction and the status of large num bers. However, he is clear about his own belief conceruing very big numbers: "I therefore conclude that huge numbers have only metaphysical status" (p. 116). We will return later to the distinction between small numbers, the properties of which we apprehend immediately, and our beliefs about large numbers, whose properties can only be inferred. Why mathematics? Is there an answer to Davies's question about why the world seems amenable to being described in mathematical terms? At the end of chapter 3 Davies states that</page><page sequence="6">324 AMERICANJOURNAL OF PSYCHOLOGY, SUMMER 2006 there cannot be a simple answer to this question. He notes that part of our con fidence in numbers comes from the extraordinary degree to which we can make predictions. But then he cautions that all of this is extraordinary by standards we ourselves set. This is a good point: Those who set the bar for themselves may not be the best to judge the merits of the jump. That issue aside, it is noteworthy that the domains in which we can accurately depict events in the world are dwarfed by the phenomena we cannot. The existence of chaotic phenomena and the fun damental indeterminacy of quantum mechanics are just a couple of high-profile examples. The short answer to the question about mathematics and the world is that mathematics is simply the tool by which we make the regularities of the world explicit to ourselves. Davies says that "most mathematics has grown from attempts to describe properties of the external world, so it is not a coincidence that the two match" (p. 31). The idea that mathematical systems are sculpted to match particular domains of inquiry is reinforced by the facts that there is no single overarching mathematics covering all of scientific inquiry. Quantum mechanics has its formalism, it is not the same as that used for Newtonian mechanics, and both of them are different from general relativity, for example. What is common to all inquiry is the search for regularities, often called symmetries, or conservation principles. The conservation of energy, momentum, and quantum numbers are examples of regularities. Numbers assist in the identification of the regularities in the world, but they are not the regularities themselves. Davies quotes Roger Newton on the issue of the relationship of mathematics to science: "It is not that Nature's own language is mathematics ... but that mathematics is our most ef ficient and incisive instrument for rational understanding of relations between things" (p. 278). Including the person Davies's goal is to include human beings in the examination of the fundamentals of science and mathematics. And in this spirit his marvelous insight was not to look at the final product but rather to examine how theories, theorems, systems, and sciences are made. It is in this light that I applaud him and along these lines that I fault him. He does not go far enough. He does not look at something that, in my opinion, is crucial to the endeavor to demythologize mathematics: the acquisition of mathematical skills in the developing human child. Davies touches lightly on developmental issues in his "Perception and Lan guage" chapter. Here he briefly mentions the prelinguistic skill of numerical estimation and the fact that language stimulation in the young can have the effect of forming unique synaptic connections in the brain. But he does not draw on two fields of research that would strengthen his case very much. The first is the developmental research on how children acquire their quantizing and calcula tion skills; the second is the study of metaphor and how metaphorical extensions give rise to an understanding of things that are beyond our perceptual reach. To exemplify this I will briefly give an overview of the skills children must learn just to count. There is a reason why my comments emphasize the acquisition of the ability to count. There are new versions of Platonic-like dualisms that are very influential</page><page sequence="7">BOOK REVIEWS 325 but not dwelt upon by Davies. The new dualisms revolve around regarding the natural world as computational and informatic. Both concepts are anchored in an anthropomorphic projection of the human skill of computation onto the natural world. Physicist Seth Lloyd has said, "To a physicist, all physical systems are computers" (Lloyd &amp; Ng, 2004, p. 53). Physicist Leonard Susskind (1997) echoes the computational view in a Scientific American article titled "Black Holes and the Information Paradox." Well-known philosopher Patricia Churchland and neuroscientist Terrence Sejnowski have written a book called The Computational Brain (1992), and the born-again version of Creationism, known as Intelligent Design, is anchored in the literal interpretation of DNA as a code. The view of the natural world as a computer is widespread and growing. Popular movies such as The Matrix take it for granted that the living self can be "downloaded" into computational form and still preserve the essence of being alive. A note on computation and information: My position here is that both are social conventions and not part of the nonhuman natural world. My Glossary of Cognitive Science defines computation as "an information-preserving systematic transforma tion of input to output" (Dunlop &amp; Fetzer, 1993, p. 29). This definition accurately reflects the modeling of cognitive systems in terms of signal transfer theory. In my view, all the terms in this definition depend on social conventions. The systematic transformation pertains to the translation of one code into another, and codes are human conventions. The information in the information-preserving part is also a social convention. Information, as Claude Shannon (1948) defined it, is a measure of the likelihood of a particular message. These messages, in turn, de pend on a previously conventionalized set of base units and rules for combining those units. As we will see, these elements are all derived from the acquired skill of assembling and disassembling named objects (i.e., counting). Computation is not possible without counting, and counting is a conventional skill. The confusion over computation and information was anticipated by Colin Cherry many years ago, at the very birth of the cognitive revolution. In his book On Human Communication (1957, p. 217), he cautioned that "an observer looking down a microscope, or reading instruments, is not to be equated with a listener on a telephone receiving spoken messages. Mother Nature does not communicate to us with signs or language. A communication channel should be distinguished from a channel of observation." A great deal of the mystification of information and computation that we see today results from this inappropriate identity that Cherry so prophetically warned against more than 50 years ago. Numbers and names With that as background I return to the issue of children acquiring the skill of counting. If counting can be shown to be a social conventional skill, then there is no justification for assigning this property to the nonhuman universe. This evidence, along with the material in Davies's book, will show that both numbers and counting are social conventions, undercutting the justification that these are properties of the universe or a Platonic realm. In chapter 1 Davies mentions the ability to estimate small quantities. The term for this is subitizing, although, oddly, Davies does not use the term. This is an ability shared by all vertebrates. It is a capacity to perceptually classify small numbers of</page><page sequence="8">326 AMERICANJOURNAL OF PSYCHOLOGY, SUMMER 2006 things, up to about seven, as being similar (Kaufman, Lord, Reese, &amp; Volkman, 1949). Subitizing is not counting. It is a gestalt principle of categorical clustering, the ability to generalize over particulars. It is what human infants have to start with, but again, it is not counting, it is not computing. Mix, Huttenlocher, and Levine (2002) pointed out a cluster of abilities, along with subitizing, that are needed for a child to begin to learn the fundamentals of counting. They are the recognition of set size, the recognition of ordinality of sets, the recognition of quantitative transformations, and the recognition of equiva lence. These are all nonconscious perceptual abilities that often are mistaken for symbolic computational skills. Mix et al. went to significant lengths to illustrate that these skills are not computational. Rather, they are part of the human infant's perceptual system that recognizes pattern and constancy in the environment. I encourage readers to consult their book on quantitative development. Based on the aforementioned set of perceptual abilities, the human child is ready to acquire the conventional skill of counting. I note here that without the ability to count, there can be no arithmetic; addition, subtraction, multiplication, and division are all founded on the acquired skill of symbolic counting. Along with this set of perceptual abilities, a child must acquire five nonoptional social conventions, or principles, in order to count: the stable-order-of-labels prin ciple, the one-to-one principle, the cardinal number principle, the abstraction principle, and the order irrelevance principle. I will briefly elaborate to illustrate the social and conventional nature of these skills and principles. First it should be noted that all of these are versions of naming. Counting rides on the coattails of the ability to symbolically name things. It is essentially a special form of naming. For a child, this is a confusing issue at first because he or she has only recently learned to affix a permanent name to things in the world. And now he or she must learn that this "counting" kind of name is not so permanent. You point to something and say "one" or "two," but that name comes back off; that object is not permanently a "one" or a "two." The first principle, the stable-order-of-labels principle, is simply memorizing a fixed order of symbols (e.g., 0 to 9), to be used as labels during counting. These symbols are arbitrary social conventions. With the next principle, the one-to-one principle, the child must learn that one name goes with one object only. Once you have said "two," for example, it cannot be used again during that count. For the abstraction principle the child must learn that this kind of naming can be applied to any and all objects. The order irrelevance principle means that one can start counting on any object in the collection, and the subsequent order of "naming" does not matter. And, finally, with the cardinal number principle the child learns that the last term uttered during the counting procedure does not applyjust to that object; it now represents the entire group of objects just "named." Why is all this important? It is important for the very reason Davies says. You really do not understand something until you see how it is made. Davies does one half of this job by showing the social, psychological, and historical contingencies that constitute the making of mathematics. To make a stronger case against those who would romanticize and Platonize mathematics you need the other half, an examination of the emergence of counting skills by children. I think this is actually</page><page sequence="9">BOOK REVIEWS 327 more important. One may come to agree that the invention of number systems is a social and cultural process but still believe that there is some magic at work when one calculates. Roger Penrose (1994) clearly believes this when he marvels that the brain is doing something very different from, and very far beyond, what a computer can do. Of course Penrose is right. But he gives up too early on the inquiry. Faced with the tension between his intuition that the calculating human is doing something radically different from a machine and having no understanding of an alternative explanation in terms of skill acquisition in children, he simply does what many others do: yield to mystification and postulate a transcendent connection to a realm of pure ideals. I think these considerations take the wind out of the sails of those who would mystify mathematics. More than the status of numbers must be considered. The growing anthropomorphization of the universe is founded on the computational metaphor. And computation is essentially counting. If the universe computes, as Seth Lloyd (2004) says, then what element of the universe keeps track of the stable order of symbols? Who is doing the categorization? In computing the additive mass of Andromeda, for example, who decides at what fringe star to stop counting? And who is doing the naming? Counting is just a special form of naming. What part of the universe, be it galactic space or intracellular cytoplasm, is doing the knowing, the naming, the starting, and the stopping? These are serious issues because the only example of computing we know of, human counting, depends critically on these social skills, and if the universe computes then it must have these skills. Metaphor I now want to return to Davies's distinction between small and larger numbers and the role of metaphor in our understanding of the reality of numbers. As we saw earlier, the ontological and epistemological status of small and large numbers is simply not the same. In some sense this is probably obvious to everyone. Every one knows that when the number of things gets to be huge we lose the ability to apprehend them in the same way we can apprehend a collection of, say, seven marbles. But then it is all too easy to assume that it is a simple matter of addition, and there really is no difference and thus nothing to be learned from pondering these extremes. But there is a lot to be gained by pondering the extremes of numbers. When numbers get beyond our ability to apprehend at a glance, or at a count, we resort to metaphor. The label we assign to a measurement of things like the number of stars in a galaxy, molecules in a container, or trees in a forest is really a way of conceptualizing those systems as if they were sensorially perceivable tabletop items, the kind of tokens that we perceived and handled when we learned to count. Davies does not describe the use of large number labels as metaphor, but I think it is appropriate. The utility of metaphor is to make a target domain seem comprehensible by recasting it in terms of a more familiar source domain. For example, the target domain might be the economy. Whatever the economy is, it is not something that is immediately and sensorially apprehensible. To comprehend the economy we might draw on the source domain of the body handling small items. Thus the economy could be recast as something that can be "inflated," such</page><page sequence="10">328 AMER[CANJOURNAL OF PSYCHOLOGY, SUMMER 2006 as a balloon we have blown up, or we may say that the economy can "fall" like toy blocks we have assembled and watched crumble or suffer a "chilling effect" similar to cold breezes we have felt. In a very real sense we cannot truly understand something until we arrive at a metaphor for it.JulianJaynes said that the feeling of familiarity we get when we find an appropriate metaphor is the feeling of understanding (Jaynes, 1976). In this light it is significant to note that when we try to come to terms with instrumental measurements that yield huge numbers, we always represent the result as a small number-not only that, but quite often as a small number raised to the power of yet another small number. I think that these small numbers are metaphors not much different from the metaphor of the economy as a balloon; they make the regularities that exist in some far, imperceptible realm seem familiar and thus incline us to feel familiar enough to say we understand. Return to Davies In summary, Davies's book is a courageous project much needed at this time. It is important that those who defend science and mathematics have a clear un derstanding of how it is embedded in the social and historical matrix, just as it is important that those who would attack science see that the defenders are not trying to hide skeletons in the closet. Science is truly a web, not a tree. As a web it touches all of us and should be accessible to everyone. I will end this review with a quote from the book on how mathematicians think: We start from vague pictures or ideas ... which we encapsulate by rules, and then we discover that those rules persuade us to modify our mental images. We engage in a dialog between our mental images and our ability to justify them via equations. As we understand what we are investigating more clearly, the pictures become sharper and the equations more elaborate. Only at the end of the process does anything like a formal set of axioms followed by logical proofs appear. (p. 80) This quote, in my opinion, is a very nice rendering of the human intellectual process of imagination, discovery, and invention. I highly recommend this book and hope that it is widely read by scientists and laypeople alike. Bill Rowe Santa Cruz Institute for Particle Physics 295 S. Franklin Chagrin Falls, OH 44022 E-mail: bill@scipp.ucsc.edu References Cherry, C. (1957). On human communication: A review, a survey and a criticism (3rd ed.). Cambridge, MA: MIT Press. Churchland, P. S., &amp; Sejnowski, T.J. (1992). The computational brain. Cambridge, MA: MIT Press. Dunlop, C. E. M., &amp; Fetzer, J. H. (1993). Glossary of cognitive science. New York: Paragon House. Jaynes, J. (1976). The origin of consciousness in the breakdown of the bicameral mind. Boston: Houghton Mifflin.</page><page sequence="11">BOOK REVIEWS 329 Jeans,J. (1944). The mysterious universe (new rev. ed.). New York: Macmillan. Kaufman, E. L., Lord, M. W., Reese, T. W., &amp; Volkmann,J. (1949). The discrimination of visual number. American Journal of Psychology, 62, 498-525. Kuhn, T. S. (1962). The structure of scientific revolutions. Chicago: University of Chicago Press. Lloyd, S., &amp; Ng, Y J. (2004, November). Black hole computers. Scientiflc American, 291(5), 52-61. Mix, K S., Huttenlocher, J., &amp; Levine, S. C. (2002). Quantitative development in infancy and early childhood. Oxford: Oxford University Press. Penrose, R. (1994). The emperor's new mind. Oxford: Oxford University Press. Shannon, C. E. (1948,July, October). A mathematical theory of communication. Bell System TechnicalJournal, 27, 379-423, 623-656. Susskind, L. (1997, April). Black holes and the information paradox. Scientific American, 276(4), 52-57. Wigner, E. (1960, February). The unreasonable effectiveness of mathematics in the natural sciences. Communications in Pure and Applied Mathematics, 13(1).</page></plain_text>